--- START Content from https://kmchandy.github.io/DISTRIBUTED_SYSTEM_MODELS/Basics.html ---
TITLE: DistributedSystemModels/Basics.html

This page describes a simple model and notation for distributed
  algorithms.
  The model is adequate for describing a collection of algorithms.
  Other models are introduced later.



Figure Link: https://kmchandy.github.io/DISTRIBUTED_SYSTEM_MODELS/Basics_Diagram.005.jpeg

Caption: Figure Agents and Channels


A model of a distributed system is an abstraction that ignores some
  features.
  The choice of a model is an engineering decision: We work with
  a model that is adequate to solve our problem.
  Different models are used to design algorithms in different
  settings;
  for example, a model of a system in which messages may be
  corrupted is different from one in which messages are incorruptible.We begin with asimple actor model and notation.
  The model and its mathematical properties are critical for
  describing and proving the correctness of algorithms;
  the programming notation is not important but is necessary to
  provide examples.
  Examples of some distributed algorithms are given in Python using a
  simulator and using software libraries such as aPython implementationof theAdvanced Message Queuing Protocol (AMQP).A Simple Model: A Network of Agents and ChannelsA distributed system consists of a set of agents and a set of
channels.
A channel is directed from exactly one agent to exactly one agent.The ordered pair(P, Q)represents the channel directed fromPtoQ.
A channel(P, Q)is
called anoutputofPand aninputofQ.
An agent can send messages on its output channels and receive
messages on its input channels.
Thesenderandreceiverof a channel(P, Q)arePandQ, respectively.The system has a channel(P, Q)for all agentsP,Qof the system. 
There is a channel from each agent to itself
So, if there are \(N\) agents then there are \(N^{2}\) channels.
An agent can send messages to any agent including itself.
In almost all algorithms described in this course very few of the
\(N^{2}\) channels are used.A distributed system is initiated with sets of agents and channels
  that remain unchanged. 
Agents and channels are not created or deleted during a computation.Message Communication: ChannelsThe state of a channel is aqueueconsisting of the sequence of
messages in the channel -- these are the messages that have been sent
on the channel and that have not been received.An agent sends a message by appending the message to the tail (rear)
of the queue.
A message from a nonempty queue is delivered to an agent by removing
the message from the head (front) of the queue, and calling a function
of the agent.Messages are not lost or modified in channels.
Every message sent is received.
Message delays are arbitrary.Channels have the following properties:Messages are delivered in the order sent.For all \(n\), the \(n\)-th message received on a channel is the
  \(n\)-th message sent on the channel.A message is received only after it is sent.The \(n\)-th message is received on a channel only after the
  \(n\)-th message is sent on the channel.There is no limit to the size of the queue of messages in a channel.
An agent can append a message to a channel independent of the size of
the channel.Messages Sent on Different Channels may be
Delivered out of OrderMessages sent on the same channel are delivered in the order sent.
Messages sent ondifferentchannels may not be delivered in the
order sent.The following scenario is possible.An agent \(A\) sends message \(m_{1}\) to an agent \(C\).Later, an agent \(B\) (different from \(A\)) sends message \(m_{2}\) to an
agent \(C\).Agent \(C\) receives \(m_{2}\) from \(B\) before \(C\) receives
  \(m_{1}\) from \(A\).AgentsAn agent is an object that sends and receives messages.
An agent is either waiting to receive a message or is processing a
message that it has received.
An agent that is waiting starts processing a message when the agent
receives a message.
An agent that is processing a message transitions to the waiting state
when the agent finishes processing the message.Each agent has a set of variables.
An agent's variables are local to the agent -- they cannot be accessed
by other agents.
The state of an agent is specified by the values of its variables.An agent is specified by (1) a program that initializes the agent's
variables and (2) a functionreceive(message, sender)called a callback function inmessage queuing libraries.If a waiting agentuhas a nonempty input channel then the
system selectsanynonempty channel(v, u), 
removes the messagemat the head of the channel from the channel, and calls 
thereceive(message, sender)function ofuwheremessageismandsenderisv.Areceivefunction must not be recursive: an agent cannot
receive a new message while it is executing a receive on a previous
message.
Every execution ofreceivemust terminate.An agent may have many nonempty input channels but the agent processes
only one message at a time. An agent is not interrupted while it
is executing areceive. Messages that arrive while an
agent is executing areceiveremain in channels.After an agent completes execution of areceivethe agent
returns to waiting state.An agent may send messages in itsreceivefunction.
An agent sends a message by executingsend(message, receiver)The first parameter ofsendis the message that is sent,
and the second paramenter is the agent to which the message is sent.
Execution of this statement places the message in the output channel
directed from the sender to the receiver.Seeexamplesand seeexamples of codeThe remainder of this page consists of an example.ExamplesExample of a SystemA system consists of 4 agents,pos,neg,total, andX, and the channels between
them. See "Figure Agents and Channels."
In this example, only 5 of the 16 channels are used.
The channels that are used are(pos, pos),(neg, neg),(pos, total),(neg, total), and(total, X).The flow of messages is shown by a directed graph in which vertices
represent agents and the directed edges represent the channels that
are used.Figure Agents and ChannelsThe system is started by specifying agents and initial channel states
and then starting the system.
In this example there is a message "wakeup" in the channel fromposto itself and fromnegto itself.
All other channels are empty.Example of an AgentThis is an example of an agent,total,
that receives messages from agentsposandneg,
and sends messages to agentX.
See anexample
of an implementation in Python using an Agent class.The example below does not use classes so that the algorithm is easier
to understand by readers who are not familiar with Python.The statements before thereceivefunction specify the
initial values of variables.
In this example, the initial state oftotalis given bysum = 0.# Initialization
sum = 0
# Callback function
def receive(message, sender):
   if sender == pos:
       sum = sum + message
   else:
       sum = sum - message
       if sum < 0: sum = 0
   send(sum, X)If the agent receives a message from agentposthen the
agent incrementssumby the contents of the message.
If the agent receives a message fromnegthe agent decrementssumby the
contents of the message, and if the resulting value ofsumis negative thensumis set to 0.The agent sends the
resulting value ofsumto the agentX.Example of an AgentThis agent sendsmy_data[n]to agenttotalfor each element inmy_data.# Initial values of agent's variables.
my_data = [3, 5]
n = 0
# Callback function
def receive(message, sender):
   send(message=my_data[n], receiver=total)
   n = n + 1
   if n < len(my_data):
      send(message="wakeup", receiver=pos)The agent has variablesmy_dataandnwhich are initially[3, 5]and0,
respectively.Whenposreceives a message it sendsmy_data[n]to the agenttotaland then
incrementsn.
Ifnis less than the number of elements inmy_datathenpossends itself a wakeup message.The system is initialized with a message in the channel fromposto itself.
Whenposreceives the message it sendsmy_data[0]to agenttotal,
incrementsnto 1
andpossends a message to itself, andWhenposreceives the next message it sendsmy_data[1]to agenttotal,
incrementsnto 2 and then waits.Sopossends 3 and then 5 tototaland thenposwaits forever.Agentnegis identical toposexcept that it
may have different values ofmy_data.AgentXhas no variables. It merely prints
messages that it receives.NextThe next page definesstates of a distributed system.ExamplesExamples: CodeFrequenty Asked QuestionsReview material for this page



Figure Link: https://kmchandy.github.io/DISTRIBUTED_SYSTEM_MODELS/Basics_Diagram.005.jpeg

Caption: Figure Agents and Channels


We begin with asimple actor model and notation.
  The model and its mathematical properties are critical for
  describing and proving the correctness of algorithms;
  the programming notation is not important but is necessary to
  provide examples.
  Examples of some distributed algorithms are given in Python using a
  simulator and using software libraries such as aPython implementationof theAdvanced Message Queuing Protocol (AMQP).A Simple Model: A Network of Agents and ChannelsA distributed system consists of a set of agents and a set of
channels.
A channel is directed from exactly one agent to exactly one agent.The ordered pair(P, Q)represents the channel directed fromPtoQ.
A channel(P, Q)is
called anoutputofPand aninputofQ.
An agent can send messages on its output channels and receive
messages on its input channels.
Thesenderandreceiverof a channel(P, Q)arePandQ, respectively.The system has a channel(P, Q)for all agentsP,Qof the system. 
There is a channel from each agent to itself
So, if there are \(N\) agents then there are \(N^{2}\) channels.
An agent can send messages to any agent including itself.
In almost all algorithms described in this course very few of the
\(N^{2}\) channels are used.A distributed system is initiated with sets of agents and channels
  that remain unchanged. 
Agents and channels are not created or deleted during a computation.Message Communication: ChannelsThe state of a channel is aqueueconsisting of the sequence of
messages in the channel -- these are the messages that have been sent
on the channel and that have not been received.An agent sends a message by appending the message to the tail (rear)
of the queue.
A message from a nonempty queue is delivered to an agent by removing
the message from the head (front) of the queue, and calling a function
of the agent.Messages are not lost or modified in channels.
Every message sent is received.
Message delays are arbitrary.Channels have the following properties:Messages are delivered in the order sent.For all \(n\), the \(n\)-th message received on a channel is the
  \(n\)-th message sent on the channel.A message is received only after it is sent.The \(n\)-th message is received on a channel only after the
  \(n\)-th message is sent on the channel.There is no limit to the size of the queue of messages in a channel.
An agent can append a message to a channel independent of the size of
the channel.Messages Sent on Different Channels may be
Delivered out of OrderMessages sent on the same channel are delivered in the order sent.
Messages sent ondifferentchannels may not be delivered in the
order sent.The following scenario is possible.An agent \(A\) sends message \(m_{1}\) to an agent \(C\).Later, an agent \(B\) (different from \(A\)) sends message \(m_{2}\) to an
agent \(C\).Agent \(C\) receives \(m_{2}\) from \(B\) before \(C\) receives
  \(m_{1}\) from \(A\).AgentsAn agent is an object that sends and receives messages.
An agent is either waiting to receive a message or is processing a
message that it has received.
An agent that is waiting starts processing a message when the agent
receives a message.
An agent that is processing a message transitions to the waiting state
when the agent finishes processing the message.Each agent has a set of variables.
An agent's variables are local to the agent -- they cannot be accessed
by other agents.
The state of an agent is specified by the values of its variables.An agent is specified by (1) a program that initializes the agent's
variables and (2) a functionreceive(message, sender)called a callback function inmessage queuing libraries.If a waiting agentuhas a nonempty input channel then the
system selectsanynonempty channel(v, u), 
removes the messagemat the head of the channel from the channel, and calls 
thereceive(message, sender)function ofuwheremessageismandsenderisv.Areceivefunction must not be recursive: an agent cannot
receive a new message while it is executing a receive on a previous
message.
Every execution ofreceivemust terminate.An agent may have many nonempty input channels but the agent processes
only one message at a time. An agent is not interrupted while it
is executing areceive. Messages that arrive while an
agent is executing areceiveremain in channels.After an agent completes execution of areceivethe agent
returns to waiting state.An agent may send messages in itsreceivefunction.
An agent sends a message by executingsend(message, receiver)The first parameter ofsendis the message that is sent,
and the second paramenter is the agent to which the message is sent.
Execution of this statement places the message in the output channel
directed from the sender to the receiver.Seeexamplesand seeexamples of codeThe remainder of this page consists of an example.ExamplesExample of a SystemA system consists of 4 agents,pos,neg,total, andX, and the channels between
them. See "Figure Agents and Channels."
In this example, only 5 of the 16 channels are used.
The channels that are used are(pos, pos),(neg, neg),(pos, total),(neg, total), and(total, X).The flow of messages is shown by a directed graph in which vertices
represent agents and the directed edges represent the channels that
are used.Figure Agents and ChannelsThe system is started by specifying agents and initial channel states
and then starting the system.
In this example there is a message "wakeup" in the channel fromposto itself and fromnegto itself.
All other channels are empty.Example of an AgentThis is an example of an agent,total,
that receives messages from agentsposandneg,
and sends messages to agentX.
See anexample
of an implementation in Python using an Agent class.The example below does not use classes so that the algorithm is easier
to understand by readers who are not familiar with Python.The statements before thereceivefunction specify the
initial values of variables.
In this example, the initial state oftotalis given bysum = 0.# Initialization
sum = 0
# Callback function
def receive(message, sender):
   if sender == pos:
       sum = sum + message
   else:
       sum = sum - message
       if sum < 0: sum = 0
   send(sum, X)If the agent receives a message from agentposthen the
agent incrementssumby the contents of the message.
If the agent receives a message fromnegthe agent decrementssumby the
contents of the message, and if the resulting value ofsumis negative thensumis set to 0.The agent sends the
resulting value ofsumto the agentX.Example of an AgentThis agent sendsmy_data[n]to agenttotalfor each element inmy_data.# Initial values of agent's variables.
my_data = [3, 5]
n = 0
# Callback function
def receive(message, sender):
   send(message=my_data[n], receiver=total)
   n = n + 1
   if n < len(my_data):
      send(message="wakeup", receiver=pos)The agent has variablesmy_dataandnwhich are initially[3, 5]and0,
respectively.Whenposreceives a message it sendsmy_data[n]to the agenttotaland then
incrementsn.
Ifnis less than the number of elements inmy_datathenpossends itself a wakeup message.The system is initialized with a message in the channel fromposto itself.
Whenposreceives the message it sendsmy_data[0]to agenttotal,
incrementsnto 1
andpossends a message to itself, andWhenposreceives the next message it sendsmy_data[1]to agenttotal,
incrementsnto 2 and then waits.Sopossends 3 and then 5 tototaland thenposwaits forever.Agentnegis identical toposexcept that it
may have different values ofmy_data.AgentXhas no variables. It merely prints
messages that it receives.NextThe next page definesstates of a distributed system.ExamplesExamples: CodeFrequenty Asked QuestionsReview material for this page



Figure Link: https://kmchandy.github.io/DISTRIBUTED_SYSTEM_MODELS/Basics_Diagram.005.jpeg

Caption: Figure Agents and Channels


A distributed system consists of a set of agents and a set of
channels.
A channel is directed from exactly one agent to exactly one agent.The ordered pair(P, Q)represents the channel directed fromPtoQ.
A channel(P, Q)is
called anoutputofPand aninputofQ.
An agent can send messages on its output channels and receive
messages on its input channels.
Thesenderandreceiverof a channel(P, Q)arePandQ, respectively.The system has a channel(P, Q)for all agentsP,Qof the system. 
There is a channel from each agent to itself
So, if there are \(N\) agents then there are \(N^{2}\) channels.
An agent can send messages to any agent including itself.
In almost all algorithms described in this course very few of the
\(N^{2}\) channels are used.A distributed system is initiated with sets of agents and channels
  that remain unchanged. 
Agents and channels are not created or deleted during a computation.Message Communication: ChannelsThe state of a channel is aqueueconsisting of the sequence of
messages in the channel -- these are the messages that have been sent
on the channel and that have not been received.An agent sends a message by appending the message to the tail (rear)
of the queue.
A message from a nonempty queue is delivered to an agent by removing
the message from the head (front) of the queue, and calling a function
of the agent.Messages are not lost or modified in channels.
Every message sent is received.
Message delays are arbitrary.Channels have the following properties:Messages are delivered in the order sent.For all \(n\), the \(n\)-th message received on a channel is the
  \(n\)-th message sent on the channel.A message is received only after it is sent.The \(n\)-th message is received on a channel only after the
  \(n\)-th message is sent on the channel.There is no limit to the size of the queue of messages in a channel.
An agent can append a message to a channel independent of the size of
the channel.Messages Sent on Different Channels may be
Delivered out of OrderMessages sent on the same channel are delivered in the order sent.
Messages sent ondifferentchannels may not be delivered in the
order sent.The following scenario is possible.An agent \(A\) sends message \(m_{1}\) to an agent \(C\).Later, an agent \(B\) (different from \(A\)) sends message \(m_{2}\) to an
agent \(C\).Agent \(C\) receives \(m_{2}\) from \(B\) before \(C\) receives
  \(m_{1}\) from \(A\).AgentsAn agent is an object that sends and receives messages.
An agent is either waiting to receive a message or is processing a
message that it has received.
An agent that is waiting starts processing a message when the agent
receives a message.
An agent that is processing a message transitions to the waiting state
when the agent finishes processing the message.Each agent has a set of variables.
An agent's variables are local to the agent -- they cannot be accessed
by other agents.
The state of an agent is specified by the values of its variables.An agent is specified by (1) a program that initializes the agent's
variables and (2) a functionreceive(message, sender)called a callback function inmessage queuing libraries.If a waiting agentuhas a nonempty input channel then the
system selectsanynonempty channel(v, u), 
removes the messagemat the head of the channel from the channel, and calls 
thereceive(message, sender)function ofuwheremessageismandsenderisv.Areceivefunction must not be recursive: an agent cannot
receive a new message while it is executing a receive on a previous
message.
Every execution ofreceivemust terminate.An agent may have many nonempty input channels but the agent processes
only one message at a time. An agent is not interrupted while it
is executing areceive. Messages that arrive while an
agent is executing areceiveremain in channels.After an agent completes execution of areceivethe agent
returns to waiting state.An agent may send messages in itsreceivefunction.
An agent sends a message by executingsend(message, receiver)The first parameter ofsendis the message that is sent,
and the second paramenter is the agent to which the message is sent.
Execution of this statement places the message in the output channel
directed from the sender to the receiver.Seeexamplesand seeexamples of codeThe remainder of this page consists of an example.ExamplesExample of a SystemA system consists of 4 agents,pos,neg,total, andX, and the channels between
them. See "Figure Agents and Channels."
In this example, only 5 of the 16 channels are used.
The channels that are used are(pos, pos),(neg, neg),(pos, total),(neg, total), and(total, X).The flow of messages is shown by a directed graph in which vertices
represent agents and the directed edges represent the channels that
are used.Figure Agents and ChannelsThe system is started by specifying agents and initial channel states
and then starting the system.
In this example there is a message "wakeup" in the channel fromposto itself and fromnegto itself.
All other channels are empty.Example of an AgentThis is an example of an agent,total,
that receives messages from agentsposandneg,
and sends messages to agentX.
See anexample
of an implementation in Python using an Agent class.The example below does not use classes so that the algorithm is easier
to understand by readers who are not familiar with Python.The statements before thereceivefunction specify the
initial values of variables.
In this example, the initial state oftotalis given bysum = 0.# Initialization
sum = 0
# Callback function
def receive(message, sender):
   if sender == pos:
       sum = sum + message
   else:
       sum = sum - message
       if sum < 0: sum = 0
   send(sum, X)If the agent receives a message from agentposthen the
agent incrementssumby the contents of the message.
If the agent receives a message fromnegthe agent decrementssumby the
contents of the message, and if the resulting value ofsumis negative thensumis set to 0.The agent sends the
resulting value ofsumto the agentX.Example of an AgentThis agent sendsmy_data[n]to agenttotalfor each element inmy_data.# Initial values of agent's variables.
my_data = [3, 5]
n = 0
# Callback function
def receive(message, sender):
   send(message=my_data[n], receiver=total)
   n = n + 1
   if n < len(my_data):
      send(message="wakeup", receiver=pos)The agent has variablesmy_dataandnwhich are initially[3, 5]and0,
respectively.Whenposreceives a message it sendsmy_data[n]to the agenttotaland then
incrementsn.
Ifnis less than the number of elements inmy_datathenpossends itself a wakeup message.The system is initialized with a message in the channel fromposto itself.
Whenposreceives the message it sendsmy_data[0]to agenttotal,
incrementsnto 1
andpossends a message to itself, andWhenposreceives the next message it sendsmy_data[1]to agenttotal,
incrementsnto 2 and then waits.Sopossends 3 and then 5 tototaland thenposwaits forever.Agentnegis identical toposexcept that it
may have different values ofmy_data.AgentXhas no variables. It merely prints
messages that it receives.NextThe next page definesstates of a distributed system.ExamplesExamples: CodeFrequenty Asked QuestionsReview material for this page



Figure Link: https://kmchandy.github.io/DISTRIBUTED_SYSTEM_MODELS/Basics_Diagram.005.jpeg

Caption: Figure Agents and Channels


The ordered pair(P, Q)represents the channel directed fromPtoQ.
A channel(P, Q)is
called anoutputofPand aninputofQ.
An agent can send messages on its output channels and receive
messages on its input channels.
Thesenderandreceiverof a channel(P, Q)arePandQ, respectively.The system has a channel(P, Q)for all agentsP,Qof the system. 
There is a channel from each agent to itself
So, if there are \(N\) agents then there are \(N^{2}\) channels.
An agent can send messages to any agent including itself.
In almost all algorithms described in this course very few of the
\(N^{2}\) channels are used.A distributed system is initiated with sets of agents and channels
  that remain unchanged. 
Agents and channels are not created or deleted during a computation.Message Communication: ChannelsThe state of a channel is aqueueconsisting of the sequence of
messages in the channel -- these are the messages that have been sent
on the channel and that have not been received.An agent sends a message by appending the message to the tail (rear)
of the queue.
A message from a nonempty queue is delivered to an agent by removing
the message from the head (front) of the queue, and calling a function
of the agent.Messages are not lost or modified in channels.
Every message sent is received.
Message delays are arbitrary.Channels have the following properties:Messages are delivered in the order sent.For all \(n\), the \(n\)-th message received on a channel is the
  \(n\)-th message sent on the channel.A message is received only after it is sent.The \(n\)-th message is received on a channel only after the
  \(n\)-th message is sent on the channel.There is no limit to the size of the queue of messages in a channel.
An agent can append a message to a channel independent of the size of
the channel.Messages Sent on Different Channels may be
Delivered out of OrderMessages sent on the same channel are delivered in the order sent.
Messages sent ondifferentchannels may not be delivered in the
order sent.The following scenario is possible.An agent \(A\) sends message \(m_{1}\) to an agent \(C\).Later, an agent \(B\) (different from \(A\)) sends message \(m_{2}\) to an
agent \(C\).Agent \(C\) receives \(m_{2}\) from \(B\) before \(C\) receives
  \(m_{1}\) from \(A\).AgentsAn agent is an object that sends and receives messages.
An agent is either waiting to receive a message or is processing a
message that it has received.
An agent that is waiting starts processing a message when the agent
receives a message.
An agent that is processing a message transitions to the waiting state
when the agent finishes processing the message.Each agent has a set of variables.
An agent's variables are local to the agent -- they cannot be accessed
by other agents.
The state of an agent is specified by the values of its variables.An agent is specified by (1) a program that initializes the agent's
variables and (2) a functionreceive(message, sender)called a callback function inmessage queuing libraries.If a waiting agentuhas a nonempty input channel then the
system selectsanynonempty channel(v, u), 
removes the messagemat the head of the channel from the channel, and calls 
thereceive(message, sender)function ofuwheremessageismandsenderisv.Areceivefunction must not be recursive: an agent cannot
receive a new message while it is executing a receive on a previous
message.
Every execution ofreceivemust terminate.An agent may have many nonempty input channels but the agent processes
only one message at a time. An agent is not interrupted while it
is executing areceive. Messages that arrive while an
agent is executing areceiveremain in channels.After an agent completes execution of areceivethe agent
returns to waiting state.An agent may send messages in itsreceivefunction.
An agent sends a message by executingsend(message, receiver)The first parameter ofsendis the message that is sent,
and the second paramenter is the agent to which the message is sent.
Execution of this statement places the message in the output channel
directed from the sender to the receiver.Seeexamplesand seeexamples of codeThe remainder of this page consists of an example.ExamplesExample of a SystemA system consists of 4 agents,pos,neg,total, andX, and the channels between
them. See "Figure Agents and Channels."
In this example, only 5 of the 16 channels are used.
The channels that are used are(pos, pos),(neg, neg),(pos, total),(neg, total), and(total, X).The flow of messages is shown by a directed graph in which vertices
represent agents and the directed edges represent the channels that
are used.Figure Agents and ChannelsThe system is started by specifying agents and initial channel states
and then starting the system.
In this example there is a message "wakeup" in the channel fromposto itself and fromnegto itself.
All other channels are empty.Example of an AgentThis is an example of an agent,total,
that receives messages from agentsposandneg,
and sends messages to agentX.
See anexample
of an implementation in Python using an Agent class.The example below does not use classes so that the algorithm is easier
to understand by readers who are not familiar with Python.The statements before thereceivefunction specify the
initial values of variables.
In this example, the initial state oftotalis given bysum = 0.# Initialization
sum = 0
# Callback function
def receive(message, sender):
   if sender == pos:
       sum = sum + message
   else:
       sum = sum - message
       if sum < 0: sum = 0
   send(sum, X)If the agent receives a message from agentposthen the
agent incrementssumby the contents of the message.
If the agent receives a message fromnegthe agent decrementssumby the
contents of the message, and if the resulting value ofsumis negative thensumis set to 0.The agent sends the
resulting value ofsumto the agentX.Example of an AgentThis agent sendsmy_data[n]to agenttotalfor each element inmy_data.# Initial values of agent's variables.
my_data = [3, 5]
n = 0
# Callback function
def receive(message, sender):
   send(message=my_data[n], receiver=total)
   n = n + 1
   if n < len(my_data):
      send(message="wakeup", receiver=pos)The agent has variablesmy_dataandnwhich are initially[3, 5]and0,
respectively.Whenposreceives a message it sendsmy_data[n]to the agenttotaland then
incrementsn.
Ifnis less than the number of elements inmy_datathenpossends itself a wakeup message.The system is initialized with a message in the channel fromposto itself.
Whenposreceives the message it sendsmy_data[0]to agenttotal,
incrementsnto 1
andpossends a message to itself, andWhenposreceives the next message it sendsmy_data[1]to agenttotal,
incrementsnto 2 and then waits.Sopossends 3 and then 5 tototaland thenposwaits forever.Agentnegis identical toposexcept that it
may have different values ofmy_data.AgentXhas no variables. It merely prints
messages that it receives.NextThe next page definesstates of a distributed system.ExamplesExamples: CodeFrequenty Asked QuestionsReview material for this page



Figure Link: https://kmchandy.github.io/DISTRIBUTED_SYSTEM_MODELS/Basics_Diagram.005.jpeg

Caption: Figure Agents and Channels


The system has a channel(P, Q)for all agentsP,Qof the system. 
There is a channel from each agent to itself
So, if there are \(N\) agents then there are \(N^{2}\) channels.
An agent can send messages to any agent including itself.
In almost all algorithms described in this course very few of the
\(N^{2}\) channels are used.A distributed system is initiated with sets of agents and channels
  that remain unchanged. 
Agents and channels are not created or deleted during a computation.Message Communication: ChannelsThe state of a channel is aqueueconsisting of the sequence of
messages in the channel -- these are the messages that have been sent
on the channel and that have not been received.An agent sends a message by appending the message to the tail (rear)
of the queue.
A message from a nonempty queue is delivered to an agent by removing
the message from the head (front) of the queue, and calling a function
of the agent.Messages are not lost or modified in channels.
Every message sent is received.
Message delays are arbitrary.Channels have the following properties:Messages are delivered in the order sent.For all \(n\), the \(n\)-th message received on a channel is the
  \(n\)-th message sent on the channel.A message is received only after it is sent.The \(n\)-th message is received on a channel only after the
  \(n\)-th message is sent on the channel.There is no limit to the size of the queue of messages in a channel.
An agent can append a message to a channel independent of the size of
the channel.Messages Sent on Different Channels may be
Delivered out of OrderMessages sent on the same channel are delivered in the order sent.
Messages sent ondifferentchannels may not be delivered in the
order sent.The following scenario is possible.An agent \(A\) sends message \(m_{1}\) to an agent \(C\).Later, an agent \(B\) (different from \(A\)) sends message \(m_{2}\) to an
agent \(C\).Agent \(C\) receives \(m_{2}\) from \(B\) before \(C\) receives
  \(m_{1}\) from \(A\).AgentsAn agent is an object that sends and receives messages.
An agent is either waiting to receive a message or is processing a
message that it has received.
An agent that is waiting starts processing a message when the agent
receives a message.
An agent that is processing a message transitions to the waiting state
when the agent finishes processing the message.Each agent has a set of variables.
An agent's variables are local to the agent -- they cannot be accessed
by other agents.
The state of an agent is specified by the values of its variables.An agent is specified by (1) a program that initializes the agent's
variables and (2) a functionreceive(message, sender)called a callback function inmessage queuing libraries.If a waiting agentuhas a nonempty input channel then the
system selectsanynonempty channel(v, u), 
removes the messagemat the head of the channel from the channel, and calls 
thereceive(message, sender)function ofuwheremessageismandsenderisv.Areceivefunction must not be recursive: an agent cannot
receive a new message while it is executing a receive on a previous
message.
Every execution ofreceivemust terminate.An agent may have many nonempty input channels but the agent processes
only one message at a time. An agent is not interrupted while it
is executing areceive. Messages that arrive while an
agent is executing areceiveremain in channels.After an agent completes execution of areceivethe agent
returns to waiting state.An agent may send messages in itsreceivefunction.
An agent sends a message by executingsend(message, receiver)The first parameter ofsendis the message that is sent,
and the second paramenter is the agent to which the message is sent.
Execution of this statement places the message in the output channel
directed from the sender to the receiver.Seeexamplesand seeexamples of codeThe remainder of this page consists of an example.ExamplesExample of a SystemA system consists of 4 agents,pos,neg,total, andX, and the channels between
them. See "Figure Agents and Channels."
In this example, only 5 of the 16 channels are used.
The channels that are used are(pos, pos),(neg, neg),(pos, total),(neg, total), and(total, X).The flow of messages is shown by a directed graph in which vertices
represent agents and the directed edges represent the channels that
are used.Figure Agents and ChannelsThe system is started by specifying agents and initial channel states
and then starting the system.
In this example there is a message "wakeup" in the channel fromposto itself and fromnegto itself.
All other channels are empty.Example of an AgentThis is an example of an agent,total,
that receives messages from agentsposandneg,
and sends messages to agentX.
See anexample
of an implementation in Python using an Agent class.The example below does not use classes so that the algorithm is easier
to understand by readers who are not familiar with Python.The statements before thereceivefunction specify the
initial values of variables.
In this example, the initial state oftotalis given bysum = 0.# Initialization
sum = 0
# Callback function
def receive(message, sender):
   if sender == pos:
       sum = sum + message
   else:
       sum = sum - message
       if sum < 0: sum = 0
   send(sum, X)If the agent receives a message from agentposthen the
agent incrementssumby the contents of the message.
If the agent receives a message fromnegthe agent decrementssumby the
contents of the message, and if the resulting value ofsumis negative thensumis set to 0.The agent sends the
resulting value ofsumto the agentX.Example of an AgentThis agent sendsmy_data[n]to agenttotalfor each element inmy_data.# Initial values of agent's variables.
my_data = [3, 5]
n = 0
# Callback function
def receive(message, sender):
   send(message=my_data[n], receiver=total)
   n = n + 1
   if n < len(my_data):
      send(message="wakeup", receiver=pos)The agent has variablesmy_dataandnwhich are initially[3, 5]and0,
respectively.Whenposreceives a message it sendsmy_data[n]to the agenttotaland then
incrementsn.
Ifnis less than the number of elements inmy_datathenpossends itself a wakeup message.The system is initialized with a message in the channel fromposto itself.
Whenposreceives the message it sendsmy_data[0]to agenttotal,
incrementsnto 1
andpossends a message to itself, andWhenposreceives the next message it sendsmy_data[1]to agenttotal,
incrementsnto 2 and then waits.Sopossends 3 and then 5 tototaland thenposwaits forever.Agentnegis identical toposexcept that it
may have different values ofmy_data.AgentXhas no variables. It merely prints
messages that it receives.NextThe next page definesstates of a distributed system.ExamplesExamples: CodeFrequenty Asked QuestionsReview material for this page



Figure Link: https://kmchandy.github.io/DISTRIBUTED_SYSTEM_MODELS/Basics_Diagram.005.jpeg

Caption: Figure Agents and Channels


A distributed system is initiated with sets of agents and channels
  that remain unchanged. 
Agents and channels are not created or deleted during a computation.Message Communication: ChannelsThe state of a channel is aqueueconsisting of the sequence of
messages in the channel -- these are the messages that have been sent
on the channel and that have not been received.An agent sends a message by appending the message to the tail (rear)
of the queue.
A message from a nonempty queue is delivered to an agent by removing
the message from the head (front) of the queue, and calling a function
of the agent.Messages are not lost or modified in channels.
Every message sent is received.
Message delays are arbitrary.Channels have the following properties:Messages are delivered in the order sent.For all \(n\), the \(n\)-th message received on a channel is the
  \(n\)-th message sent on the channel.A message is received only after it is sent.The \(n\)-th message is received on a channel only after the
  \(n\)-th message is sent on the channel.There is no limit to the size of the queue of messages in a channel.
An agent can append a message to a channel independent of the size of
the channel.Messages Sent on Different Channels may be
Delivered out of OrderMessages sent on the same channel are delivered in the order sent.
Messages sent ondifferentchannels may not be delivered in the
order sent.The following scenario is possible.An agent \(A\) sends message \(m_{1}\) to an agent \(C\).Later, an agent \(B\) (different from \(A\)) sends message \(m_{2}\) to an
agent \(C\).Agent \(C\) receives \(m_{2}\) from \(B\) before \(C\) receives
  \(m_{1}\) from \(A\).AgentsAn agent is an object that sends and receives messages.
An agent is either waiting to receive a message or is processing a
message that it has received.
An agent that is waiting starts processing a message when the agent
receives a message.
An agent that is processing a message transitions to the waiting state
when the agent finishes processing the message.Each agent has a set of variables.
An agent's variables are local to the agent -- they cannot be accessed
by other agents.
The state of an agent is specified by the values of its variables.An agent is specified by (1) a program that initializes the agent's
variables and (2) a functionreceive(message, sender)called a callback function inmessage queuing libraries.If a waiting agentuhas a nonempty input channel then the
system selectsanynonempty channel(v, u), 
removes the messagemat the head of the channel from the channel, and calls 
thereceive(message, sender)function ofuwheremessageismandsenderisv.Areceivefunction must not be recursive: an agent cannot
receive a new message while it is executing a receive on a previous
message.
Every execution ofreceivemust terminate.An agent may have many nonempty input channels but the agent processes
only one message at a time. An agent is not interrupted while it
is executing areceive. Messages that arrive while an
agent is executing areceiveremain in channels.After an agent completes execution of areceivethe agent
returns to waiting state.An agent may send messages in itsreceivefunction.
An agent sends a message by executingsend(message, receiver)The first parameter ofsendis the message that is sent,
and the second paramenter is the agent to which the message is sent.
Execution of this statement places the message in the output channel
directed from the sender to the receiver.Seeexamplesand seeexamples of codeThe remainder of this page consists of an example.ExamplesExample of a SystemA system consists of 4 agents,pos,neg,total, andX, and the channels between
them. See "Figure Agents and Channels."
In this example, only 5 of the 16 channels are used.
The channels that are used are(pos, pos),(neg, neg),(pos, total),(neg, total), and(total, X).The flow of messages is shown by a directed graph in which vertices
represent agents and the directed edges represent the channels that
are used.Figure Agents and ChannelsThe system is started by specifying agents and initial channel states
and then starting the system.
In this example there is a message "wakeup" in the channel fromposto itself and fromnegto itself.
All other channels are empty.Example of an AgentThis is an example of an agent,total,
that receives messages from agentsposandneg,
and sends messages to agentX.
See anexample
of an implementation in Python using an Agent class.The example below does not use classes so that the algorithm is easier
to understand by readers who are not familiar with Python.The statements before thereceivefunction specify the
initial values of variables.
In this example, the initial state oftotalis given bysum = 0.# Initialization
sum = 0
# Callback function
def receive(message, sender):
   if sender == pos:
       sum = sum + message
   else:
       sum = sum - message
       if sum < 0: sum = 0
   send(sum, X)If the agent receives a message from agentposthen the
agent incrementssumby the contents of the message.
If the agent receives a message fromnegthe agent decrementssumby the
contents of the message, and if the resulting value ofsumis negative thensumis set to 0.The agent sends the
resulting value ofsumto the agentX.Example of an AgentThis agent sendsmy_data[n]to agenttotalfor each element inmy_data.# Initial values of agent's variables.
my_data = [3, 5]
n = 0
# Callback function
def receive(message, sender):
   send(message=my_data[n], receiver=total)
   n = n + 1
   if n < len(my_data):
      send(message="wakeup", receiver=pos)The agent has variablesmy_dataandnwhich are initially[3, 5]and0,
respectively.Whenposreceives a message it sendsmy_data[n]to the agenttotaland then
incrementsn.
Ifnis less than the number of elements inmy_datathenpossends itself a wakeup message.The system is initialized with a message in the channel fromposto itself.
Whenposreceives the message it sendsmy_data[0]to agenttotal,
incrementsnto 1
andpossends a message to itself, andWhenposreceives the next message it sendsmy_data[1]to agenttotal,
incrementsnto 2 and then waits.Sopossends 3 and then 5 tototaland thenposwaits forever.Agentnegis identical toposexcept that it
may have different values ofmy_data.AgentXhas no variables. It merely prints
messages that it receives.NextThe next page definesstates of a distributed system.ExamplesExamples: CodeFrequenty Asked QuestionsReview material for this page



Figure Link: https://kmchandy.github.io/DISTRIBUTED_SYSTEM_MODELS/Basics_Diagram.005.jpeg

Caption: Figure Agents and Channels


An agent sends a message by appending the message to the tail (rear)
of the queue.
A message from a nonempty queue is delivered to an agent by removing
the message from the head (front) of the queue, and calling a function
of the agent.Messages are not lost or modified in channels.
Every message sent is received.
Message delays are arbitrary.Channels have the following properties:Messages are delivered in the order sent.For all \(n\), the \(n\)-th message received on a channel is the
  \(n\)-th message sent on the channel.A message is received only after it is sent.The \(n\)-th message is received on a channel only after the
  \(n\)-th message is sent on the channel.There is no limit to the size of the queue of messages in a channel.
An agent can append a message to a channel independent of the size of
the channel.Messages Sent on Different Channels may be
Delivered out of OrderMessages sent on the same channel are delivered in the order sent.
Messages sent ondifferentchannels may not be delivered in the
order sent.The following scenario is possible.An agent \(A\) sends message \(m_{1}\) to an agent \(C\).Later, an agent \(B\) (different from \(A\)) sends message \(m_{2}\) to an
agent \(C\).Agent \(C\) receives \(m_{2}\) from \(B\) before \(C\) receives
  \(m_{1}\) from \(A\).AgentsAn agent is an object that sends and receives messages.
An agent is either waiting to receive a message or is processing a
message that it has received.
An agent that is waiting starts processing a message when the agent
receives a message.
An agent that is processing a message transitions to the waiting state
when the agent finishes processing the message.Each agent has a set of variables.
An agent's variables are local to the agent -- they cannot be accessed
by other agents.
The state of an agent is specified by the values of its variables.An agent is specified by (1) a program that initializes the agent's
variables and (2) a functionreceive(message, sender)called a callback function inmessage queuing libraries.If a waiting agentuhas a nonempty input channel then the
system selectsanynonempty channel(v, u), 
removes the messagemat the head of the channel from the channel, and calls 
thereceive(message, sender)function ofuwheremessageismandsenderisv.Areceivefunction must not be recursive: an agent cannot
receive a new message while it is executing a receive on a previous
message.
Every execution ofreceivemust terminate.An agent may have many nonempty input channels but the agent processes
only one message at a time. An agent is not interrupted while it
is executing areceive. Messages that arrive while an
agent is executing areceiveremain in channels.After an agent completes execution of areceivethe agent
returns to waiting state.An agent may send messages in itsreceivefunction.
An agent sends a message by executingsend(message, receiver)The first parameter ofsendis the message that is sent,
and the second paramenter is the agent to which the message is sent.
Execution of this statement places the message in the output channel
directed from the sender to the receiver.Seeexamplesand seeexamples of codeThe remainder of this page consists of an example.ExamplesExample of a SystemA system consists of 4 agents,pos,neg,total, andX, and the channels between
them. See "Figure Agents and Channels."
In this example, only 5 of the 16 channels are used.
The channels that are used are(pos, pos),(neg, neg),(pos, total),(neg, total), and(total, X).The flow of messages is shown by a directed graph in which vertices
represent agents and the directed edges represent the channels that
are used.Figure Agents and ChannelsThe system is started by specifying agents and initial channel states
and then starting the system.
In this example there is a message "wakeup" in the channel fromposto itself and fromnegto itself.
All other channels are empty.Example of an AgentThis is an example of an agent,total,
that receives messages from agentsposandneg,
and sends messages to agentX.
See anexample
of an implementation in Python using an Agent class.The example below does not use classes so that the algorithm is easier
to understand by readers who are not familiar with Python.The statements before thereceivefunction specify the
initial values of variables.
In this example, the initial state oftotalis given bysum = 0.# Initialization
sum = 0
# Callback function
def receive(message, sender):
   if sender == pos:
       sum = sum + message
   else:
       sum = sum - message
       if sum < 0: sum = 0
   send(sum, X)If the agent receives a message from agentposthen the
agent incrementssumby the contents of the message.
If the agent receives a message fromnegthe agent decrementssumby the
contents of the message, and if the resulting value ofsumis negative thensumis set to 0.The agent sends the
resulting value ofsumto the agentX.Example of an AgentThis agent sendsmy_data[n]to agenttotalfor each element inmy_data.# Initial values of agent's variables.
my_data = [3, 5]
n = 0
# Callback function
def receive(message, sender):
   send(message=my_data[n], receiver=total)
   n = n + 1
   if n < len(my_data):
      send(message="wakeup", receiver=pos)The agent has variablesmy_dataandnwhich are initially[3, 5]and0,
respectively.Whenposreceives a message it sendsmy_data[n]to the agenttotaland then
incrementsn.
Ifnis less than the number of elements inmy_datathenpossends itself a wakeup message.The system is initialized with a message in the channel fromposto itself.
Whenposreceives the message it sendsmy_data[0]to agenttotal,
incrementsnto 1
andpossends a message to itself, andWhenposreceives the next message it sendsmy_data[1]to agenttotal,
incrementsnto 2 and then waits.Sopossends 3 and then 5 tototaland thenposwaits forever.Agentnegis identical toposexcept that it
may have different values ofmy_data.AgentXhas no variables. It merely prints
messages that it receives.NextThe next page definesstates of a distributed system.ExamplesExamples: CodeFrequenty Asked QuestionsReview material for this page



Figure Link: https://kmchandy.github.io/DISTRIBUTED_SYSTEM_MODELS/Basics_Diagram.005.jpeg

Caption: Figure Agents and Channels


Messages are not lost or modified in channels.
Every message sent is received.
Message delays are arbitrary.Channels have the following properties:Messages are delivered in the order sent.For all \(n\), the \(n\)-th message received on a channel is the
  \(n\)-th message sent on the channel.A message is received only after it is sent.The \(n\)-th message is received on a channel only after the
  \(n\)-th message is sent on the channel.There is no limit to the size of the queue of messages in a channel.
An agent can append a message to a channel independent of the size of
the channel.Messages Sent on Different Channels may be
Delivered out of OrderMessages sent on the same channel are delivered in the order sent.
Messages sent ondifferentchannels may not be delivered in the
order sent.The following scenario is possible.An agent \(A\) sends message \(m_{1}\) to an agent \(C\).Later, an agent \(B\) (different from \(A\)) sends message \(m_{2}\) to an
agent \(C\).Agent \(C\) receives \(m_{2}\) from \(B\) before \(C\) receives
  \(m_{1}\) from \(A\).AgentsAn agent is an object that sends and receives messages.
An agent is either waiting to receive a message or is processing a
message that it has received.
An agent that is waiting starts processing a message when the agent
receives a message.
An agent that is processing a message transitions to the waiting state
when the agent finishes processing the message.Each agent has a set of variables.
An agent's variables are local to the agent -- they cannot be accessed
by other agents.
The state of an agent is specified by the values of its variables.An agent is specified by (1) a program that initializes the agent's
variables and (2) a functionreceive(message, sender)called a callback function inmessage queuing libraries.If a waiting agentuhas a nonempty input channel then the
system selectsanynonempty channel(v, u), 
removes the messagemat the head of the channel from the channel, and calls 
thereceive(message, sender)function ofuwheremessageismandsenderisv.Areceivefunction must not be recursive: an agent cannot
receive a new message while it is executing a receive on a previous
message.
Every execution ofreceivemust terminate.An agent may have many nonempty input channels but the agent processes
only one message at a time. An agent is not interrupted while it
is executing areceive. Messages that arrive while an
agent is executing areceiveremain in channels.After an agent completes execution of areceivethe agent
returns to waiting state.An agent may send messages in itsreceivefunction.
An agent sends a message by executingsend(message, receiver)The first parameter ofsendis the message that is sent,
and the second paramenter is the agent to which the message is sent.
Execution of this statement places the message in the output channel
directed from the sender to the receiver.Seeexamplesand seeexamples of codeThe remainder of this page consists of an example.ExamplesExample of a SystemA system consists of 4 agents,pos,neg,total, andX, and the channels between
them. See "Figure Agents and Channels."
In this example, only 5 of the 16 channels are used.
The channels that are used are(pos, pos),(neg, neg),(pos, total),(neg, total), and(total, X).The flow of messages is shown by a directed graph in which vertices
represent agents and the directed edges represent the channels that
are used.Figure Agents and ChannelsThe system is started by specifying agents and initial channel states
and then starting the system.
In this example there is a message "wakeup" in the channel fromposto itself and fromnegto itself.
All other channels are empty.Example of an AgentThis is an example of an agent,total,
that receives messages from agentsposandneg,
and sends messages to agentX.
See anexample
of an implementation in Python using an Agent class.The example below does not use classes so that the algorithm is easier
to understand by readers who are not familiar with Python.The statements before thereceivefunction specify the
initial values of variables.
In this example, the initial state oftotalis given bysum = 0.# Initialization
sum = 0
# Callback function
def receive(message, sender):
   if sender == pos:
       sum = sum + message
   else:
       sum = sum - message
       if sum < 0: sum = 0
   send(sum, X)If the agent receives a message from agentposthen the
agent incrementssumby the contents of the message.
If the agent receives a message fromnegthe agent decrementssumby the
contents of the message, and if the resulting value ofsumis negative thensumis set to 0.The agent sends the
resulting value ofsumto the agentX.Example of an AgentThis agent sendsmy_data[n]to agenttotalfor each element inmy_data.# Initial values of agent's variables.
my_data = [3, 5]
n = 0
# Callback function
def receive(message, sender):
   send(message=my_data[n], receiver=total)
   n = n + 1
   if n < len(my_data):
      send(message="wakeup", receiver=pos)The agent has variablesmy_dataandnwhich are initially[3, 5]and0,
respectively.Whenposreceives a message it sendsmy_data[n]to the agenttotaland then
incrementsn.
Ifnis less than the number of elements inmy_datathenpossends itself a wakeup message.The system is initialized with a message in the channel fromposto itself.
Whenposreceives the message it sendsmy_data[0]to agenttotal,
incrementsnto 1
andpossends a message to itself, andWhenposreceives the next message it sendsmy_data[1]to agenttotal,
incrementsnto 2 and then waits.Sopossends 3 and then 5 tototaland thenposwaits forever.Agentnegis identical toposexcept that it
may have different values ofmy_data.AgentXhas no variables. It merely prints
messages that it receives.NextThe next page definesstates of a distributed system.ExamplesExamples: CodeFrequenty Asked QuestionsReview material for this page



Figure Link: https://kmchandy.github.io/DISTRIBUTED_SYSTEM_MODELS/Basics_Diagram.005.jpeg

Caption: Figure Agents and Channels


Channels have the following properties:Messages are delivered in the order sent.For all \(n\), the \(n\)-th message received on a channel is the
  \(n\)-th message sent on the channel.A message is received only after it is sent.The \(n\)-th message is received on a channel only after the
  \(n\)-th message is sent on the channel.There is no limit to the size of the queue of messages in a channel.
An agent can append a message to a channel independent of the size of
the channel.Messages Sent on Different Channels may be
Delivered out of OrderMessages sent on the same channel are delivered in the order sent.
Messages sent ondifferentchannels may not be delivered in the
order sent.The following scenario is possible.An agent \(A\) sends message \(m_{1}\) to an agent \(C\).Later, an agent \(B\) (different from \(A\)) sends message \(m_{2}\) to an
agent \(C\).Agent \(C\) receives \(m_{2}\) from \(B\) before \(C\) receives
  \(m_{1}\) from \(A\).AgentsAn agent is an object that sends and receives messages.
An agent is either waiting to receive a message or is processing a
message that it has received.
An agent that is waiting starts processing a message when the agent
receives a message.
An agent that is processing a message transitions to the waiting state
when the agent finishes processing the message.Each agent has a set of variables.
An agent's variables are local to the agent -- they cannot be accessed
by other agents.
The state of an agent is specified by the values of its variables.An agent is specified by (1) a program that initializes the agent's
variables and (2) a functionreceive(message, sender)called a callback function inmessage queuing libraries.If a waiting agentuhas a nonempty input channel then the
system selectsanynonempty channel(v, u), 
removes the messagemat the head of the channel from the channel, and calls 
thereceive(message, sender)function ofuwheremessageismandsenderisv.Areceivefunction must not be recursive: an agent cannot
receive a new message while it is executing a receive on a previous
message.
Every execution ofreceivemust terminate.An agent may have many nonempty input channels but the agent processes
only one message at a time. An agent is not interrupted while it
is executing areceive. Messages that arrive while an
agent is executing areceiveremain in channels.After an agent completes execution of areceivethe agent
returns to waiting state.An agent may send messages in itsreceivefunction.
An agent sends a message by executingsend(message, receiver)The first parameter ofsendis the message that is sent,
and the second paramenter is the agent to which the message is sent.
Execution of this statement places the message in the output channel
directed from the sender to the receiver.Seeexamplesand seeexamples of codeThe remainder of this page consists of an example.ExamplesExample of a SystemA system consists of 4 agents,pos,neg,total, andX, and the channels between
them. See "Figure Agents and Channels."
In this example, only 5 of the 16 channels are used.
The channels that are used are(pos, pos),(neg, neg),(pos, total),(neg, total), and(total, X).The flow of messages is shown by a directed graph in which vertices
represent agents and the directed edges represent the channels that
are used.Figure Agents and ChannelsThe system is started by specifying agents and initial channel states
and then starting the system.
In this example there is a message "wakeup" in the channel fromposto itself and fromnegto itself.
All other channels are empty.Example of an AgentThis is an example of an agent,total,
that receives messages from agentsposandneg,
and sends messages to agentX.
See anexample
of an implementation in Python using an Agent class.The example below does not use classes so that the algorithm is easier
to understand by readers who are not familiar with Python.The statements before thereceivefunction specify the
initial values of variables.
In this example, the initial state oftotalis given bysum = 0.# Initialization
sum = 0
# Callback function
def receive(message, sender):
   if sender == pos:
       sum = sum + message
   else:
       sum = sum - message
       if sum < 0: sum = 0
   send(sum, X)If the agent receives a message from agentposthen the
agent incrementssumby the contents of the message.
If the agent receives a message fromnegthe agent decrementssumby the
contents of the message, and if the resulting value ofsumis negative thensumis set to 0.The agent sends the
resulting value ofsumto the agentX.Example of an AgentThis agent sendsmy_data[n]to agenttotalfor each element inmy_data.# Initial values of agent's variables.
my_data = [3, 5]
n = 0
# Callback function
def receive(message, sender):
   send(message=my_data[n], receiver=total)
   n = n + 1
   if n < len(my_data):
      send(message="wakeup", receiver=pos)The agent has variablesmy_dataandnwhich are initially[3, 5]and0,
respectively.Whenposreceives a message it sendsmy_data[n]to the agenttotaland then
incrementsn.
Ifnis less than the number of elements inmy_datathenpossends itself a wakeup message.The system is initialized with a message in the channel fromposto itself.
Whenposreceives the message it sendsmy_data[0]to agenttotal,
incrementsnto 1
andpossends a message to itself, andWhenposreceives the next message it sendsmy_data[1]to agenttotal,
incrementsnto 2 and then waits.Sopossends 3 and then 5 tototaland thenposwaits forever.Agentnegis identical toposexcept that it
may have different values ofmy_data.AgentXhas no variables. It merely prints
messages that it receives.NextThe next page definesstates of a distributed system.ExamplesExamples: CodeFrequenty Asked QuestionsReview material for this page



Figure Link: https://kmchandy.github.io/DISTRIBUTED_SYSTEM_MODELS/Basics_Diagram.005.jpeg

Caption: Figure Agents and Channels


For all \(n\), the \(n\)-th message received on a channel is the
  \(n\)-th message sent on the channel.



Figure Link: https://kmchandy.github.io/DISTRIBUTED_SYSTEM_MODELS/Basics_Diagram.005.jpeg

Caption: Figure Agents and Channels


The \(n\)-th message is received on a channel only after the
  \(n\)-th message is sent on the channel.



Figure Link: https://kmchandy.github.io/DISTRIBUTED_SYSTEM_MODELS/Basics_Diagram.005.jpeg

Caption: Figure Agents and Channels


The following scenario is possible.An agent \(A\) sends message \(m_{1}\) to an agent \(C\).Later, an agent \(B\) (different from \(A\)) sends message \(m_{2}\) to an
agent \(C\).Agent \(C\) receives \(m_{2}\) from \(B\) before \(C\) receives
  \(m_{1}\) from \(A\).AgentsAn agent is an object that sends and receives messages.
An agent is either waiting to receive a message or is processing a
message that it has received.
An agent that is waiting starts processing a message when the agent
receives a message.
An agent that is processing a message transitions to the waiting state
when the agent finishes processing the message.Each agent has a set of variables.
An agent's variables are local to the agent -- they cannot be accessed
by other agents.
The state of an agent is specified by the values of its variables.An agent is specified by (1) a program that initializes the agent's
variables and (2) a functionreceive(message, sender)called a callback function inmessage queuing libraries.If a waiting agentuhas a nonempty input channel then the
system selectsanynonempty channel(v, u), 
removes the messagemat the head of the channel from the channel, and calls 
thereceive(message, sender)function ofuwheremessageismandsenderisv.Areceivefunction must not be recursive: an agent cannot
receive a new message while it is executing a receive on a previous
message.
Every execution ofreceivemust terminate.An agent may have many nonempty input channels but the agent processes
only one message at a time. An agent is not interrupted while it
is executing areceive. Messages that arrive while an
agent is executing areceiveremain in channels.After an agent completes execution of areceivethe agent
returns to waiting state.An agent may send messages in itsreceivefunction.
An agent sends a message by executingsend(message, receiver)The first parameter ofsendis the message that is sent,
and the second paramenter is the agent to which the message is sent.
Execution of this statement places the message in the output channel
directed from the sender to the receiver.Seeexamplesand seeexamples of codeThe remainder of this page consists of an example.ExamplesExample of a SystemA system consists of 4 agents,pos,neg,total, andX, and the channels between
them. See "Figure Agents and Channels."
In this example, only 5 of the 16 channels are used.
The channels that are used are(pos, pos),(neg, neg),(pos, total),(neg, total), and(total, X).The flow of messages is shown by a directed graph in which vertices
represent agents and the directed edges represent the channels that
are used.Figure Agents and ChannelsThe system is started by specifying agents and initial channel states
and then starting the system.
In this example there is a message "wakeup" in the channel fromposto itself and fromnegto itself.
All other channels are empty.Example of an AgentThis is an example of an agent,total,
that receives messages from agentsposandneg,
and sends messages to agentX.
See anexample
of an implementation in Python using an Agent class.The example below does not use classes so that the algorithm is easier
to understand by readers who are not familiar with Python.The statements before thereceivefunction specify the
initial values of variables.
In this example, the initial state oftotalis given bysum = 0.# Initialization
sum = 0
# Callback function
def receive(message, sender):
   if sender == pos:
       sum = sum + message
   else:
       sum = sum - message
       if sum < 0: sum = 0
   send(sum, X)If the agent receives a message from agentposthen the
agent incrementssumby the contents of the message.
If the agent receives a message fromnegthe agent decrementssumby the
contents of the message, and if the resulting value ofsumis negative thensumis set to 0.The agent sends the
resulting value ofsumto the agentX.Example of an AgentThis agent sendsmy_data[n]to agenttotalfor each element inmy_data.# Initial values of agent's variables.
my_data = [3, 5]
n = 0
# Callback function
def receive(message, sender):
   send(message=my_data[n], receiver=total)
   n = n + 1
   if n < len(my_data):
      send(message="wakeup", receiver=pos)The agent has variablesmy_dataandnwhich are initially[3, 5]and0,
respectively.Whenposreceives a message it sendsmy_data[n]to the agenttotaland then
incrementsn.
Ifnis less than the number of elements inmy_datathenpossends itself a wakeup message.The system is initialized with a message in the channel fromposto itself.
Whenposreceives the message it sendsmy_data[0]to agenttotal,
incrementsnto 1
andpossends a message to itself, andWhenposreceives the next message it sendsmy_data[1]to agenttotal,
incrementsnto 2 and then waits.Sopossends 3 and then 5 tototaland thenposwaits forever.Agentnegis identical toposexcept that it
may have different values ofmy_data.AgentXhas no variables. It merely prints
messages that it receives.NextThe next page definesstates of a distributed system.ExamplesExamples: CodeFrequenty Asked QuestionsReview material for this page



Figure Link: https://kmchandy.github.io/DISTRIBUTED_SYSTEM_MODELS/Basics_Diagram.005.jpeg

Caption: Figure Agents and Channels


Each agent has a set of variables.
An agent's variables are local to the agent -- they cannot be accessed
by other agents.
The state of an agent is specified by the values of its variables.An agent is specified by (1) a program that initializes the agent's
variables and (2) a functionreceive(message, sender)called a callback function inmessage queuing libraries.If a waiting agentuhas a nonempty input channel then the
system selectsanynonempty channel(v, u), 
removes the messagemat the head of the channel from the channel, and calls 
thereceive(message, sender)function ofuwheremessageismandsenderisv.Areceivefunction must not be recursive: an agent cannot
receive a new message while it is executing a receive on a previous
message.
Every execution ofreceivemust terminate.An agent may have many nonempty input channels but the agent processes
only one message at a time. An agent is not interrupted while it
is executing areceive. Messages that arrive while an
agent is executing areceiveremain in channels.After an agent completes execution of areceivethe agent
returns to waiting state.An agent may send messages in itsreceivefunction.
An agent sends a message by executingsend(message, receiver)The first parameter ofsendis the message that is sent,
and the second paramenter is the agent to which the message is sent.
Execution of this statement places the message in the output channel
directed from the sender to the receiver.Seeexamplesand seeexamples of codeThe remainder of this page consists of an example.ExamplesExample of a SystemA system consists of 4 agents,pos,neg,total, andX, and the channels between
them. See "Figure Agents and Channels."
In this example, only 5 of the 16 channels are used.
The channels that are used are(pos, pos),(neg, neg),(pos, total),(neg, total), and(total, X).The flow of messages is shown by a directed graph in which vertices
represent agents and the directed edges represent the channels that
are used.Figure Agents and ChannelsThe system is started by specifying agents and initial channel states
and then starting the system.
In this example there is a message "wakeup" in the channel fromposto itself and fromnegto itself.
All other channels are empty.Example of an AgentThis is an example of an agent,total,
that receives messages from agentsposandneg,
and sends messages to agentX.
See anexample
of an implementation in Python using an Agent class.The example below does not use classes so that the algorithm is easier
to understand by readers who are not familiar with Python.The statements before thereceivefunction specify the
initial values of variables.
In this example, the initial state oftotalis given bysum = 0.# Initialization
sum = 0
# Callback function
def receive(message, sender):
   if sender == pos:
       sum = sum + message
   else:
       sum = sum - message
       if sum < 0: sum = 0
   send(sum, X)If the agent receives a message from agentposthen the
agent incrementssumby the contents of the message.
If the agent receives a message fromnegthe agent decrementssumby the
contents of the message, and if the resulting value ofsumis negative thensumis set to 0.The agent sends the
resulting value ofsumto the agentX.Example of an AgentThis agent sendsmy_data[n]to agenttotalfor each element inmy_data.# Initial values of agent's variables.
my_data = [3, 5]
n = 0
# Callback function
def receive(message, sender):
   send(message=my_data[n], receiver=total)
   n = n + 1
   if n < len(my_data):
      send(message="wakeup", receiver=pos)The agent has variablesmy_dataandnwhich are initially[3, 5]and0,
respectively.Whenposreceives a message it sendsmy_data[n]to the agenttotaland then
incrementsn.
Ifnis less than the number of elements inmy_datathenpossends itself a wakeup message.The system is initialized with a message in the channel fromposto itself.
Whenposreceives the message it sendsmy_data[0]to agenttotal,
incrementsnto 1
andpossends a message to itself, andWhenposreceives the next message it sendsmy_data[1]to agenttotal,
incrementsnto 2 and then waits.Sopossends 3 and then 5 tototaland thenposwaits forever.Agentnegis identical toposexcept that it
may have different values ofmy_data.AgentXhas no variables. It merely prints
messages that it receives.NextThe next page definesstates of a distributed system.ExamplesExamples: CodeFrequenty Asked QuestionsReview material for this page



Figure Link: https://kmchandy.github.io/DISTRIBUTED_SYSTEM_MODELS/Basics_Diagram.005.jpeg

Caption: Figure Agents and Channels


An agent is specified by (1) a program that initializes the agent's
variables and (2) a functionreceive(message, sender)called a callback function inmessage queuing libraries.If a waiting agentuhas a nonempty input channel then the
system selectsanynonempty channel(v, u), 
removes the messagemat the head of the channel from the channel, and calls 
thereceive(message, sender)function ofuwheremessageismandsenderisv.Areceivefunction must not be recursive: an agent cannot
receive a new message while it is executing a receive on a previous
message.
Every execution ofreceivemust terminate.An agent may have many nonempty input channels but the agent processes
only one message at a time. An agent is not interrupted while it
is executing areceive. Messages that arrive while an
agent is executing areceiveremain in channels.After an agent completes execution of areceivethe agent
returns to waiting state.An agent may send messages in itsreceivefunction.
An agent sends a message by executingsend(message, receiver)The first parameter ofsendis the message that is sent,
and the second paramenter is the agent to which the message is sent.
Execution of this statement places the message in the output channel
directed from the sender to the receiver.Seeexamplesand seeexamples of codeThe remainder of this page consists of an example.ExamplesExample of a SystemA system consists of 4 agents,pos,neg,total, andX, and the channels between
them. See "Figure Agents and Channels."
In this example, only 5 of the 16 channels are used.
The channels that are used are(pos, pos),(neg, neg),(pos, total),(neg, total), and(total, X).The flow of messages is shown by a directed graph in which vertices
represent agents and the directed edges represent the channels that
are used.Figure Agents and ChannelsThe system is started by specifying agents and initial channel states
and then starting the system.
In this example there is a message "wakeup" in the channel fromposto itself and fromnegto itself.
All other channels are empty.Example of an AgentThis is an example of an agent,total,
that receives messages from agentsposandneg,
and sends messages to agentX.
See anexample
of an implementation in Python using an Agent class.The example below does not use classes so that the algorithm is easier
to understand by readers who are not familiar with Python.The statements before thereceivefunction specify the
initial values of variables.
In this example, the initial state oftotalis given bysum = 0.# Initialization
sum = 0
# Callback function
def receive(message, sender):
   if sender == pos:
       sum = sum + message
   else:
       sum = sum - message
       if sum < 0: sum = 0
   send(sum, X)If the agent receives a message from agentposthen the
agent incrementssumby the contents of the message.
If the agent receives a message fromnegthe agent decrementssumby the
contents of the message, and if the resulting value ofsumis negative thensumis set to 0.The agent sends the
resulting value ofsumto the agentX.Example of an AgentThis agent sendsmy_data[n]to agenttotalfor each element inmy_data.# Initial values of agent's variables.
my_data = [3, 5]
n = 0
# Callback function
def receive(message, sender):
   send(message=my_data[n], receiver=total)
   n = n + 1
   if n < len(my_data):
      send(message="wakeup", receiver=pos)The agent has variablesmy_dataandnwhich are initially[3, 5]and0,
respectively.Whenposreceives a message it sendsmy_data[n]to the agenttotaland then
incrementsn.
Ifnis less than the number of elements inmy_datathenpossends itself a wakeup message.The system is initialized with a message in the channel fromposto itself.
Whenposreceives the message it sendsmy_data[0]to agenttotal,
incrementsnto 1
andpossends a message to itself, andWhenposreceives the next message it sendsmy_data[1]to agenttotal,
incrementsnto 2 and then waits.Sopossends 3 and then 5 tototaland thenposwaits forever.Agentnegis identical toposexcept that it
may have different values ofmy_data.AgentXhas no variables. It merely prints
messages that it receives.NextThe next page definesstates of a distributed system.ExamplesExamples: CodeFrequenty Asked QuestionsReview material for this page



Figure Link: https://kmchandy.github.io/DISTRIBUTED_SYSTEM_MODELS/Basics_Diagram.005.jpeg

Caption: Figure Agents and Channels


If a waiting agentuhas a nonempty input channel then the
system selectsanynonempty channel(v, u), 
removes the messagemat the head of the channel from the channel, and calls 
thereceive(message, sender)function ofuwheremessageismandsenderisv.Areceivefunction must not be recursive: an agent cannot
receive a new message while it is executing a receive on a previous
message.
Every execution ofreceivemust terminate.An agent may have many nonempty input channels but the agent processes
only one message at a time. An agent is not interrupted while it
is executing areceive. Messages that arrive while an
agent is executing areceiveremain in channels.After an agent completes execution of areceivethe agent
returns to waiting state.An agent may send messages in itsreceivefunction.
An agent sends a message by executingsend(message, receiver)The first parameter ofsendis the message that is sent,
and the second paramenter is the agent to which the message is sent.
Execution of this statement places the message in the output channel
directed from the sender to the receiver.Seeexamplesand seeexamples of codeThe remainder of this page consists of an example.ExamplesExample of a SystemA system consists of 4 agents,pos,neg,total, andX, and the channels between
them. See "Figure Agents and Channels."
In this example, only 5 of the 16 channels are used.
The channels that are used are(pos, pos),(neg, neg),(pos, total),(neg, total), and(total, X).The flow of messages is shown by a directed graph in which vertices
represent agents and the directed edges represent the channels that
are used.Figure Agents and ChannelsThe system is started by specifying agents and initial channel states
and then starting the system.
In this example there is a message "wakeup" in the channel fromposto itself and fromnegto itself.
All other channels are empty.Example of an AgentThis is an example of an agent,total,
that receives messages from agentsposandneg,
and sends messages to agentX.
See anexample
of an implementation in Python using an Agent class.The example below does not use classes so that the algorithm is easier
to understand by readers who are not familiar with Python.The statements before thereceivefunction specify the
initial values of variables.
In this example, the initial state oftotalis given bysum = 0.# Initialization
sum = 0
# Callback function
def receive(message, sender):
   if sender == pos:
       sum = sum + message
   else:
       sum = sum - message
       if sum < 0: sum = 0
   send(sum, X)If the agent receives a message from agentposthen the
agent incrementssumby the contents of the message.
If the agent receives a message fromnegthe agent decrementssumby the
contents of the message, and if the resulting value ofsumis negative thensumis set to 0.The agent sends the
resulting value ofsumto the agentX.Example of an AgentThis agent sendsmy_data[n]to agenttotalfor each element inmy_data.# Initial values of agent's variables.
my_data = [3, 5]
n = 0
# Callback function
def receive(message, sender):
   send(message=my_data[n], receiver=total)
   n = n + 1
   if n < len(my_data):
      send(message="wakeup", receiver=pos)The agent has variablesmy_dataandnwhich are initially[3, 5]and0,
respectively.Whenposreceives a message it sendsmy_data[n]to the agenttotaland then
incrementsn.
Ifnis less than the number of elements inmy_datathenpossends itself a wakeup message.The system is initialized with a message in the channel fromposto itself.
Whenposreceives the message it sendsmy_data[0]to agenttotal,
incrementsnto 1
andpossends a message to itself, andWhenposreceives the next message it sendsmy_data[1]to agenttotal,
incrementsnto 2 and then waits.Sopossends 3 and then 5 tototaland thenposwaits forever.Agentnegis identical toposexcept that it
may have different values ofmy_data.AgentXhas no variables. It merely prints
messages that it receives.NextThe next page definesstates of a distributed system.ExamplesExamples: CodeFrequenty Asked QuestionsReview material for this page



Figure Link: https://kmchandy.github.io/DISTRIBUTED_SYSTEM_MODELS/Basics_Diagram.005.jpeg

Caption: Figure Agents and Channels


Areceivefunction must not be recursive: an agent cannot
receive a new message while it is executing a receive on a previous
message.
Every execution ofreceivemust terminate.An agent may have many nonempty input channels but the agent processes
only one message at a time. An agent is not interrupted while it
is executing areceive. Messages that arrive while an
agent is executing areceiveremain in channels.After an agent completes execution of areceivethe agent
returns to waiting state.An agent may send messages in itsreceivefunction.
An agent sends a message by executingsend(message, receiver)The first parameter ofsendis the message that is sent,
and the second paramenter is the agent to which the message is sent.
Execution of this statement places the message in the output channel
directed from the sender to the receiver.Seeexamplesand seeexamples of codeThe remainder of this page consists of an example.ExamplesExample of a SystemA system consists of 4 agents,pos,neg,total, andX, and the channels between
them. See "Figure Agents and Channels."
In this example, only 5 of the 16 channels are used.
The channels that are used are(pos, pos),(neg, neg),(pos, total),(neg, total), and(total, X).The flow of messages is shown by a directed graph in which vertices
represent agents and the directed edges represent the channels that
are used.Figure Agents and ChannelsThe system is started by specifying agents and initial channel states
and then starting the system.
In this example there is a message "wakeup" in the channel fromposto itself and fromnegto itself.
All other channels are empty.Example of an AgentThis is an example of an agent,total,
that receives messages from agentsposandneg,
and sends messages to agentX.
See anexample
of an implementation in Python using an Agent class.The example below does not use classes so that the algorithm is easier
to understand by readers who are not familiar with Python.The statements before thereceivefunction specify the
initial values of variables.
In this example, the initial state oftotalis given bysum = 0.# Initialization
sum = 0
# Callback function
def receive(message, sender):
   if sender == pos:
       sum = sum + message
   else:
       sum = sum - message
       if sum < 0: sum = 0
   send(sum, X)If the agent receives a message from agentposthen the
agent incrementssumby the contents of the message.
If the agent receives a message fromnegthe agent decrementssumby the
contents of the message, and if the resulting value ofsumis negative thensumis set to 0.The agent sends the
resulting value ofsumto the agentX.Example of an AgentThis agent sendsmy_data[n]to agenttotalfor each element inmy_data.# Initial values of agent's variables.
my_data = [3, 5]
n = 0
# Callback function
def receive(message, sender):
   send(message=my_data[n], receiver=total)
   n = n + 1
   if n < len(my_data):
      send(message="wakeup", receiver=pos)The agent has variablesmy_dataandnwhich are initially[3, 5]and0,
respectively.Whenposreceives a message it sendsmy_data[n]to the agenttotaland then
incrementsn.
Ifnis less than the number of elements inmy_datathenpossends itself a wakeup message.The system is initialized with a message in the channel fromposto itself.
Whenposreceives the message it sendsmy_data[0]to agenttotal,
incrementsnto 1
andpossends a message to itself, andWhenposreceives the next message it sendsmy_data[1]to agenttotal,
incrementsnto 2 and then waits.Sopossends 3 and then 5 tototaland thenposwaits forever.Agentnegis identical toposexcept that it
may have different values ofmy_data.AgentXhas no variables. It merely prints
messages that it receives.NextThe next page definesstates of a distributed system.ExamplesExamples: CodeFrequenty Asked QuestionsReview material for this page



Figure Link: https://kmchandy.github.io/DISTRIBUTED_SYSTEM_MODELS/Basics_Diagram.005.jpeg

Caption: Figure Agents and Channels


An agent may have many nonempty input channels but the agent processes
only one message at a time. An agent is not interrupted while it
is executing areceive. Messages that arrive while an
agent is executing areceiveremain in channels.After an agent completes execution of areceivethe agent
returns to waiting state.An agent may send messages in itsreceivefunction.
An agent sends a message by executingsend(message, receiver)The first parameter ofsendis the message that is sent,
and the second paramenter is the agent to which the message is sent.
Execution of this statement places the message in the output channel
directed from the sender to the receiver.Seeexamplesand seeexamples of codeThe remainder of this page consists of an example.ExamplesExample of a SystemA system consists of 4 agents,pos,neg,total, andX, and the channels between
them. See "Figure Agents and Channels."
In this example, only 5 of the 16 channels are used.
The channels that are used are(pos, pos),(neg, neg),(pos, total),(neg, total), and(total, X).The flow of messages is shown by a directed graph in which vertices
represent agents and the directed edges represent the channels that
are used.Figure Agents and ChannelsThe system is started by specifying agents and initial channel states
and then starting the system.
In this example there is a message "wakeup" in the channel fromposto itself and fromnegto itself.
All other channels are empty.Example of an AgentThis is an example of an agent,total,
that receives messages from agentsposandneg,
and sends messages to agentX.
See anexample
of an implementation in Python using an Agent class.The example below does not use classes so that the algorithm is easier
to understand by readers who are not familiar with Python.The statements before thereceivefunction specify the
initial values of variables.
In this example, the initial state oftotalis given bysum = 0.# Initialization
sum = 0
# Callback function
def receive(message, sender):
   if sender == pos:
       sum = sum + message
   else:
       sum = sum - message
       if sum < 0: sum = 0
   send(sum, X)If the agent receives a message from agentposthen the
agent incrementssumby the contents of the message.
If the agent receives a message fromnegthe agent decrementssumby the
contents of the message, and if the resulting value ofsumis negative thensumis set to 0.The agent sends the
resulting value ofsumto the agentX.Example of an AgentThis agent sendsmy_data[n]to agenttotalfor each element inmy_data.# Initial values of agent's variables.
my_data = [3, 5]
n = 0
# Callback function
def receive(message, sender):
   send(message=my_data[n], receiver=total)
   n = n + 1
   if n < len(my_data):
      send(message="wakeup", receiver=pos)The agent has variablesmy_dataandnwhich are initially[3, 5]and0,
respectively.Whenposreceives a message it sendsmy_data[n]to the agenttotaland then
incrementsn.
Ifnis less than the number of elements inmy_datathenpossends itself a wakeup message.The system is initialized with a message in the channel fromposto itself.
Whenposreceives the message it sendsmy_data[0]to agenttotal,
incrementsnto 1
andpossends a message to itself, andWhenposreceives the next message it sendsmy_data[1]to agenttotal,
incrementsnto 2 and then waits.Sopossends 3 and then 5 tototaland thenposwaits forever.Agentnegis identical toposexcept that it
may have different values ofmy_data.AgentXhas no variables. It merely prints
messages that it receives.NextThe next page definesstates of a distributed system.ExamplesExamples: CodeFrequenty Asked QuestionsReview material for this page



Figure Link: https://kmchandy.github.io/DISTRIBUTED_SYSTEM_MODELS/Basics_Diagram.005.jpeg

Caption: Figure Agents and Channels


After an agent completes execution of areceivethe agent
returns to waiting state.An agent may send messages in itsreceivefunction.
An agent sends a message by executingsend(message, receiver)The first parameter ofsendis the message that is sent,
and the second paramenter is the agent to which the message is sent.
Execution of this statement places the message in the output channel
directed from the sender to the receiver.Seeexamplesand seeexamples of codeThe remainder of this page consists of an example.ExamplesExample of a SystemA system consists of 4 agents,pos,neg,total, andX, and the channels between
them. See "Figure Agents and Channels."
In this example, only 5 of the 16 channels are used.
The channels that are used are(pos, pos),(neg, neg),(pos, total),(neg, total), and(total, X).The flow of messages is shown by a directed graph in which vertices
represent agents and the directed edges represent the channels that
are used.Figure Agents and ChannelsThe system is started by specifying agents and initial channel states
and then starting the system.
In this example there is a message "wakeup" in the channel fromposto itself and fromnegto itself.
All other channels are empty.Example of an AgentThis is an example of an agent,total,
that receives messages from agentsposandneg,
and sends messages to agentX.
See anexample
of an implementation in Python using an Agent class.The example below does not use classes so that the algorithm is easier
to understand by readers who are not familiar with Python.The statements before thereceivefunction specify the
initial values of variables.
In this example, the initial state oftotalis given bysum = 0.# Initialization
sum = 0
# Callback function
def receive(message, sender):
   if sender == pos:
       sum = sum + message
   else:
       sum = sum - message
       if sum < 0: sum = 0
   send(sum, X)If the agent receives a message from agentposthen the
agent incrementssumby the contents of the message.
If the agent receives a message fromnegthe agent decrementssumby the
contents of the message, and if the resulting value ofsumis negative thensumis set to 0.The agent sends the
resulting value ofsumto the agentX.Example of an AgentThis agent sendsmy_data[n]to agenttotalfor each element inmy_data.# Initial values of agent's variables.
my_data = [3, 5]
n = 0
# Callback function
def receive(message, sender):
   send(message=my_data[n], receiver=total)
   n = n + 1
   if n < len(my_data):
      send(message="wakeup", receiver=pos)The agent has variablesmy_dataandnwhich are initially[3, 5]and0,
respectively.Whenposreceives a message it sendsmy_data[n]to the agenttotaland then
incrementsn.
Ifnis less than the number of elements inmy_datathenpossends itself a wakeup message.The system is initialized with a message in the channel fromposto itself.
Whenposreceives the message it sendsmy_data[0]to agenttotal,
incrementsnto 1
andpossends a message to itself, andWhenposreceives the next message it sendsmy_data[1]to agenttotal,
incrementsnto 2 and then waits.Sopossends 3 and then 5 tototaland thenposwaits forever.Agentnegis identical toposexcept that it
may have different values ofmy_data.AgentXhas no variables. It merely prints
messages that it receives.NextThe next page definesstates of a distributed system.ExamplesExamples: CodeFrequenty Asked QuestionsReview material for this page



Figure Link: https://kmchandy.github.io/DISTRIBUTED_SYSTEM_MODELS/Basics_Diagram.005.jpeg

Caption: Figure Agents and Channels


An agent may send messages in itsreceivefunction.
An agent sends a message by executingsend(message, receiver)The first parameter ofsendis the message that is sent,
and the second paramenter is the agent to which the message is sent.
Execution of this statement places the message in the output channel
directed from the sender to the receiver.Seeexamplesand seeexamples of codeThe remainder of this page consists of an example.ExamplesExample of a SystemA system consists of 4 agents,pos,neg,total, andX, and the channels between
them. See "Figure Agents and Channels."
In this example, only 5 of the 16 channels are used.
The channels that are used are(pos, pos),(neg, neg),(pos, total),(neg, total), and(total, X).The flow of messages is shown by a directed graph in which vertices
represent agents and the directed edges represent the channels that
are used.Figure Agents and ChannelsThe system is started by specifying agents and initial channel states
and then starting the system.
In this example there is a message "wakeup" in the channel fromposto itself and fromnegto itself.
All other channels are empty.Example of an AgentThis is an example of an agent,total,
that receives messages from agentsposandneg,
and sends messages to agentX.
See anexample
of an implementation in Python using an Agent class.The example below does not use classes so that the algorithm is easier
to understand by readers who are not familiar with Python.The statements before thereceivefunction specify the
initial values of variables.
In this example, the initial state oftotalis given bysum = 0.# Initialization
sum = 0
# Callback function
def receive(message, sender):
   if sender == pos:
       sum = sum + message
   else:
       sum = sum - message
       if sum < 0: sum = 0
   send(sum, X)If the agent receives a message from agentposthen the
agent incrementssumby the contents of the message.
If the agent receives a message fromnegthe agent decrementssumby the
contents of the message, and if the resulting value ofsumis negative thensumis set to 0.The agent sends the
resulting value ofsumto the agentX.Example of an AgentThis agent sendsmy_data[n]to agenttotalfor each element inmy_data.# Initial values of agent's variables.
my_data = [3, 5]
n = 0
# Callback function
def receive(message, sender):
   send(message=my_data[n], receiver=total)
   n = n + 1
   if n < len(my_data):
      send(message="wakeup", receiver=pos)The agent has variablesmy_dataandnwhich are initially[3, 5]and0,
respectively.Whenposreceives a message it sendsmy_data[n]to the agenttotaland then
incrementsn.
Ifnis less than the number of elements inmy_datathenpossends itself a wakeup message.The system is initialized with a message in the channel fromposto itself.
Whenposreceives the message it sendsmy_data[0]to agenttotal,
incrementsnto 1
andpossends a message to itself, andWhenposreceives the next message it sendsmy_data[1]to agenttotal,
incrementsnto 2 and then waits.Sopossends 3 and then 5 tototaland thenposwaits forever.Agentnegis identical toposexcept that it
may have different values ofmy_data.AgentXhas no variables. It merely prints
messages that it receives.NextThe next page definesstates of a distributed system.ExamplesExamples: CodeFrequenty Asked QuestionsReview material for this page



Figure Link: https://kmchandy.github.io/DISTRIBUTED_SYSTEM_MODELS/Basics_Diagram.005.jpeg

Caption: Figure Agents and Channels


Seeexamplesand seeexamples of codeThe remainder of this page consists of an example.ExamplesExample of a SystemA system consists of 4 agents,pos,neg,total, andX, and the channels between
them. See "Figure Agents and Channels."
In this example, only 5 of the 16 channels are used.
The channels that are used are(pos, pos),(neg, neg),(pos, total),(neg, total), and(total, X).The flow of messages is shown by a directed graph in which vertices
represent agents and the directed edges represent the channels that
are used.Figure Agents and ChannelsThe system is started by specifying agents and initial channel states
and then starting the system.
In this example there is a message "wakeup" in the channel fromposto itself and fromnegto itself.
All other channels are empty.Example of an AgentThis is an example of an agent,total,
that receives messages from agentsposandneg,
and sends messages to agentX.
See anexample
of an implementation in Python using an Agent class.The example below does not use classes so that the algorithm is easier
to understand by readers who are not familiar with Python.The statements before thereceivefunction specify the
initial values of variables.
In this example, the initial state oftotalis given bysum = 0.# Initialization
sum = 0
# Callback function
def receive(message, sender):
   if sender == pos:
       sum = sum + message
   else:
       sum = sum - message
       if sum < 0: sum = 0
   send(sum, X)If the agent receives a message from agentposthen the
agent incrementssumby the contents of the message.
If the agent receives a message fromnegthe agent decrementssumby the
contents of the message, and if the resulting value ofsumis negative thensumis set to 0.The agent sends the
resulting value ofsumto the agentX.Example of an AgentThis agent sendsmy_data[n]to agenttotalfor each element inmy_data.# Initial values of agent's variables.
my_data = [3, 5]
n = 0
# Callback function
def receive(message, sender):
   send(message=my_data[n], receiver=total)
   n = n + 1
   if n < len(my_data):
      send(message="wakeup", receiver=pos)The agent has variablesmy_dataandnwhich are initially[3, 5]and0,
respectively.Whenposreceives a message it sendsmy_data[n]to the agenttotaland then
incrementsn.
Ifnis less than the number of elements inmy_datathenpossends itself a wakeup message.The system is initialized with a message in the channel fromposto itself.
Whenposreceives the message it sendsmy_data[0]to agenttotal,
incrementsnto 1
andpossends a message to itself, andWhenposreceives the next message it sendsmy_data[1]to agenttotal,
incrementsnto 2 and then waits.Sopossends 3 and then 5 tototaland thenposwaits forever.Agentnegis identical toposexcept that it
may have different values ofmy_data.AgentXhas no variables. It merely prints
messages that it receives.NextThe next page definesstates of a distributed system.ExamplesExamples: CodeFrequenty Asked QuestionsReview material for this page



Figure Link: https://kmchandy.github.io/DISTRIBUTED_SYSTEM_MODELS/Basics_Diagram.005.jpeg

Caption: Figure Agents and Channels


The flow of messages is shown by a directed graph in which vertices
represent agents and the directed edges represent the channels that
are used.Figure Agents and ChannelsThe system is started by specifying agents and initial channel states
and then starting the system.
In this example there is a message "wakeup" in the channel fromposto itself and fromnegto itself.
All other channels are empty.Example of an AgentThis is an example of an agent,total,
that receives messages from agentsposandneg,
and sends messages to agentX.
See anexample
of an implementation in Python using an Agent class.The example below does not use classes so that the algorithm is easier
to understand by readers who are not familiar with Python.The statements before thereceivefunction specify the
initial values of variables.
In this example, the initial state oftotalis given bysum = 0.# Initialization
sum = 0
# Callback function
def receive(message, sender):
   if sender == pos:
       sum = sum + message
   else:
       sum = sum - message
       if sum < 0: sum = 0
   send(sum, X)If the agent receives a message from agentposthen the
agent incrementssumby the contents of the message.
If the agent receives a message fromnegthe agent decrementssumby the
contents of the message, and if the resulting value ofsumis negative thensumis set to 0.The agent sends the
resulting value ofsumto the agentX.Example of an AgentThis agent sendsmy_data[n]to agenttotalfor each element inmy_data.# Initial values of agent's variables.
my_data = [3, 5]
n = 0
# Callback function
def receive(message, sender):
   send(message=my_data[n], receiver=total)
   n = n + 1
   if n < len(my_data):
      send(message="wakeup", receiver=pos)The agent has variablesmy_dataandnwhich are initially[3, 5]and0,
respectively.Whenposreceives a message it sendsmy_data[n]to the agenttotaland then
incrementsn.
Ifnis less than the number of elements inmy_datathenpossends itself a wakeup message.The system is initialized with a message in the channel fromposto itself.
Whenposreceives the message it sendsmy_data[0]to agenttotal,
incrementsnto 1
andpossends a message to itself, andWhenposreceives the next message it sendsmy_data[1]to agenttotal,
incrementsnto 2 and then waits.Sopossends 3 and then 5 tototaland thenposwaits forever.Agentnegis identical toposexcept that it
may have different values ofmy_data.AgentXhas no variables. It merely prints
messages that it receives.NextThe next page definesstates of a distributed system.ExamplesExamples: CodeFrequenty Asked QuestionsReview material for this page



Figure Link: https://kmchandy.github.io/DISTRIBUTED_SYSTEM_MODELS/Basics_Diagram.005.jpeg

Caption: Figure Agents and Channels


The system is started by specifying agents and initial channel states
and then starting the system.
In this example there is a message "wakeup" in the channel fromposto itself and fromnegto itself.
All other channels are empty.Example of an AgentThis is an example of an agent,total,
that receives messages from agentsposandneg,
and sends messages to agentX.
See anexample
of an implementation in Python using an Agent class.The example below does not use classes so that the algorithm is easier
to understand by readers who are not familiar with Python.The statements before thereceivefunction specify the
initial values of variables.
In this example, the initial state oftotalis given bysum = 0.# Initialization
sum = 0
# Callback function
def receive(message, sender):
   if sender == pos:
       sum = sum + message
   else:
       sum = sum - message
       if sum < 0: sum = 0
   send(sum, X)If the agent receives a message from agentposthen the
agent incrementssumby the contents of the message.
If the agent receives a message fromnegthe agent decrementssumby the
contents of the message, and if the resulting value ofsumis negative thensumis set to 0.The agent sends the
resulting value ofsumto the agentX.Example of an AgentThis agent sendsmy_data[n]to agenttotalfor each element inmy_data.# Initial values of agent's variables.
my_data = [3, 5]
n = 0
# Callback function
def receive(message, sender):
   send(message=my_data[n], receiver=total)
   n = n + 1
   if n < len(my_data):
      send(message="wakeup", receiver=pos)The agent has variablesmy_dataandnwhich are initially[3, 5]and0,
respectively.Whenposreceives a message it sendsmy_data[n]to the agenttotaland then
incrementsn.
Ifnis less than the number of elements inmy_datathenpossends itself a wakeup message.The system is initialized with a message in the channel fromposto itself.
Whenposreceives the message it sendsmy_data[0]to agenttotal,
incrementsnto 1
andpossends a message to itself, andWhenposreceives the next message it sendsmy_data[1]to agenttotal,
incrementsnto 2 and then waits.Sopossends 3 and then 5 tototaland thenposwaits forever.Agentnegis identical toposexcept that it
may have different values ofmy_data.AgentXhas no variables. It merely prints
messages that it receives.NextThe next page definesstates of a distributed system.ExamplesExamples: CodeFrequenty Asked QuestionsReview material for this page

The statements before thereceivefunction specify the
initial values of variables.
In this example, the initial state oftotalis given bysum = 0.# Initialization
sum = 0
# Callback function
def receive(message, sender):
   if sender == pos:
       sum = sum + message
   else:
       sum = sum - message
       if sum < 0: sum = 0
   send(sum, X)If the agent receives a message from agentposthen the
agent incrementssumby the contents of the message.
If the agent receives a message fromnegthe agent decrementssumby the
contents of the message, and if the resulting value ofsumis negative thensumis set to 0.The agent sends the
resulting value ofsumto the agentX.Example of an AgentThis agent sendsmy_data[n]to agenttotalfor each element inmy_data.# Initial values of agent's variables.
my_data = [3, 5]
n = 0
# Callback function
def receive(message, sender):
   send(message=my_data[n], receiver=total)
   n = n + 1
   if n < len(my_data):
      send(message="wakeup", receiver=pos)The agent has variablesmy_dataandnwhich are initially[3, 5]and0,
respectively.Whenposreceives a message it sendsmy_data[n]to the agenttotaland then
incrementsn.
Ifnis less than the number of elements inmy_datathenpossends itself a wakeup message.The system is initialized with a message in the channel fromposto itself.
Whenposreceives the message it sendsmy_data[0]to agenttotal,
incrementsnto 1
andpossends a message to itself, andWhenposreceives the next message it sendsmy_data[1]to agenttotal,
incrementsnto 2 and then waits.Sopossends 3 and then 5 tototaland thenposwaits forever.Agentnegis identical toposexcept that it
may have different values ofmy_data.AgentXhas no variables. It merely prints
messages that it receives.NextThe next page definesstates of a distributed system.ExamplesExamples: CodeFrequenty Asked QuestionsReview material for this page

The agent sends the
resulting value ofsumto the agentX.Example of an AgentThis agent sendsmy_data[n]to agenttotalfor each element inmy_data.# Initial values of agent's variables.
my_data = [3, 5]
n = 0
# Callback function
def receive(message, sender):
   send(message=my_data[n], receiver=total)
   n = n + 1
   if n < len(my_data):
      send(message="wakeup", receiver=pos)The agent has variablesmy_dataandnwhich are initially[3, 5]and0,
respectively.Whenposreceives a message it sendsmy_data[n]to the agenttotaland then
incrementsn.
Ifnis less than the number of elements inmy_datathenpossends itself a wakeup message.The system is initialized with a message in the channel fromposto itself.
Whenposreceives the message it sendsmy_data[0]to agenttotal,
incrementsnto 1
andpossends a message to itself, andWhenposreceives the next message it sendsmy_data[1]to agenttotal,
incrementsnto 2 and then waits.Sopossends 3 and then 5 tototaland thenposwaits forever.Agentnegis identical toposexcept that it
may have different values ofmy_data.AgentXhas no variables. It merely prints
messages that it receives.NextThe next page definesstates of a distributed system.ExamplesExamples: CodeFrequenty Asked QuestionsReview material for this page

Whenposreceives a message it sendsmy_data[n]to the agenttotaland then
incrementsn.
Ifnis less than the number of elements inmy_datathenpossends itself a wakeup message.The system is initialized with a message in the channel fromposto itself.
Whenposreceives the message it sendsmy_data[0]to agenttotal,
incrementsnto 1
andpossends a message to itself, andWhenposreceives the next message it sendsmy_data[1]to agenttotal,
incrementsnto 2 and then waits.Sopossends 3 and then 5 tototaland thenposwaits forever.Agentnegis identical toposexcept that it
may have different values ofmy_data.AgentXhas no variables. It merely prints
messages that it receives.NextThe next page definesstates of a distributed system.ExamplesExamples: CodeFrequenty Asked QuestionsReview material for this page

The system is initialized with a message in the channel fromposto itself.
Whenposreceives the message it sendsmy_data[0]to agenttotal,
incrementsnto 1
andpossends a message to itself, andWhenposreceives the next message it sendsmy_data[1]to agenttotal,
incrementsnto 2 and then waits.Sopossends 3 and then 5 tototaland thenposwaits forever.Agentnegis identical toposexcept that it
may have different values ofmy_data.AgentXhas no variables. It merely prints
messages that it receives.NextThe next page definesstates of a distributed system.ExamplesExamples: CodeFrequenty Asked QuestionsReview material for this page

Whenposreceives the next message it sendsmy_data[1]to agenttotal,
incrementsnto 2 and then waits.Sopossends 3 and then 5 tototaland thenposwaits forever.Agentnegis identical toposexcept that it
may have different values ofmy_data.AgentXhas no variables. It merely prints
messages that it receives.NextThe next page definesstates of a distributed system.ExamplesExamples: CodeFrequenty Asked QuestionsReview material for this page

Sopossends 3 and then 5 tototaland thenposwaits forever.Agentnegis identical toposexcept that it
may have different values ofmy_data.AgentXhas no variables. It merely prints
messages that it receives.NextThe next page definesstates of a distributed system.ExamplesExamples: CodeFrequenty Asked QuestionsReview material for this page

Agentnegis identical toposexcept that it
may have different values ofmy_data.AgentXhas no variables. It merely prints
messages that it receives.NextThe next page definesstates of a distributed system.ExamplesExamples: CodeFrequenty Asked QuestionsReview material for this page

AgentXhas no variables. It merely prints
messages that it receives.NextThe next page definesstates of a distributed system.ExamplesExamples: CodeFrequenty Asked QuestionsReview material for this page

ExamplesExamples: CodeFrequenty Asked QuestionsReview material for this page

Examples: CodeFrequenty Asked QuestionsReview material for this page

Frequenty Asked QuestionsReview material for this page

Review material for this page

K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology
--- END Content from https://kmchandy.github.io/DISTRIBUTED_SYSTEM_MODELS/Basics.html ---


--- START Content from https://kmchandy.github.io/DISTRIBUTED_SYSTEM_MODELS/BasicsExamples.html ---
TITLE: DistributedSystemModels/BasicsEExamples.html

Ignore the states of agents for the purposes of this question.QuestionIs the following scenario possible?Agent \(A\) receives a wakeup message and sends a message \(m_{1}\) to \(C\) and a
  wakeup to itself.Agent \(A\) receives a wakeup message and sends a message \(m_{2}\) to \(B\).Agent \(B\) receives \(m_{1}\) from \(A\) and sends a message \(m_{3}\) to \(C\).Agent \(C\) receives \(m_{3}\) from \(B\).Agent \(C\) receives \(m_{2}\) from \(A\).AnswerYes, the scenario is possible.
  Messages sent on a channel are delivered in the order sent on that
  channel.
  Messages sent ondifferentchannels may not be received in
  the order sent on those channels.Channels 2A system has agents \(A\) and \(B\).
  Initially the channel from \(A\) to \(A\) contains a "wakeup" message.
  All other channels are empty.Ignore the states of agents for the purposes of this question.QuestionIs the following scenario possible?Agent \(A\) receives a wakeup message and sends a message \(m_{1}\) to \(B\) and a
  wakeup to itself.Agent \(A\) receives a wakeup message and sends a message \(m_{2}\) to \(B\) and a
  wakeup to itself.Agent \(B\) receives  \(m_{2}\) from \(A\)Agent \(B\) receives  \(m_{1}\) from \(A\)AnswerNo, the scenario is not possible.
  Messages sent on a channel are delivered in the order sent on that
 channel.
 So \(A\) receives \(m_{1}\) from \(B\) before it receives \(m_{2}\)
 from \(B\).Example of a SystemConsider the example given inthe previous
 page.Assume that agentnegis the same as agentposexcept thatneg.my_data = [2, 4]At some point message 3 is in channel(pos, total)and
 message 2 is in channel(neg, total), and these are the only
 messages in these channels.
 There is a wakeup message in channel(pos, pos)and in
 channel(neg, neg).See Figure Agents and Channels Example 1.1Figure Agents and Channels Example 1.1QuestionWhat changes occur if agenttotalreceives the message
 fromposand this is the first message thattotalreceives?AnswerThe message 3 on channel(pos, total)is removed from
 the channel.
 Agenttotal's variablesumbecomes 3 and
 channel(total, X)contains message 3.See Figure Agents and Channels Example 1.2Figure Agents and Channels Example 1.2QuestionSame initial state as in the previous question.
 What changes occur if agenttotalreceives the message
 fromnegand this is the first message thattotalreceives?AnswerThe message 2 on channel(neg, total)is removed from
 the channel.
 Agenttotal's variablesumbecomes 0 and
 channel(total, X)contains message 0.See Figure Agents and Channels Example 1.3Figure Agents and Channels Example 1.3QuestionSame starting state as in the previous two questions.
 
 What changes occur if agenttotalreceives the message
 fromposand then fromneg?AnswerAfter agenttotalreceives the message
 fromposand then fromnegthe channels
 fromposandnegtototalare
 empty, and the channel fromtotaltoXcontains message 3
 followed by message 1, and agenttotal's 
 variablesumis 1.See Figure Agents and Channels Example 1.4Figure Agents and Channels Example 1.4QuestionWhat changes occur if agenttotalreceives its first message
 fromnegand its second frompos?AnswerAfter agenttotalreceives the message
 fromnegand then fromposthe channels
 fromposandnegtototalare
 empty, and the channel fromtotaltoXcontains message 0
 followed by message 3, and agenttotal's 
 variable,sum, is 3.See Figure Agents and Channels Example 1.5Figure Agents and Channels Example 1.5QuestionAgentXprints the messages that it receives. What doesXprint?AnswerXmay print3, 8, 6, 2.Iftotalreceives 3, 5 fromposand then receives 2, 4 fromneg.3, 1, 6, 2.Iftotalreceives 3 frompos, then 2 fromneg, then 5 frompos, then 4 fromneg3, 1, 0, 5.Iftotalreceives 3 frompos, then 2 fromneg,
  then 4 fromneg,
  then 5 frompos.0, 3, 8, 4.Iftotalreceives
  2 fromneg,
  then 3 frompos,
  then 5 frompos,
  then 4 fromneg.0, 3, 0, 5.Iftotalreceives
  2 fromneg,
  then 3 frompos,
  then 4 fromneg,
  then 5 frompos.0, 0, 3, 8.Iftotalreceives
  2 fromneg,
  then 4 fromneg,
  then 3 frompos,
  then 5 frompos.NextThe next page definesstates of a distributed system.ExamplesFrequenty Asked QuestionsReview material for this page



Figure Link: https://kmchandy.github.io/DISTRIBUTED_SYSTEM_MODELS/Basics_Diagram.001.jpeg

Caption: Figure Agents and Channels Example 1.1


Ignore the states of agents for the purposes of this question.QuestionIs the following scenario possible?Agent \(A\) receives a wakeup message and sends a message \(m_{1}\) to \(B\) and a
  wakeup to itself.Agent \(A\) receives a wakeup message and sends a message \(m_{2}\) to \(B\) and a
  wakeup to itself.Agent \(B\) receives  \(m_{2}\) from \(A\)Agent \(B\) receives  \(m_{1}\) from \(A\)AnswerNo, the scenario is not possible.
  Messages sent on a channel are delivered in the order sent on that
 channel.
 So \(A\) receives \(m_{1}\) from \(B\) before it receives \(m_{2}\)
 from \(B\).Example of a SystemConsider the example given inthe previous
 page.Assume that agentnegis the same as agentposexcept thatneg.my_data = [2, 4]At some point message 3 is in channel(pos, total)and
 message 2 is in channel(neg, total), and these are the only
 messages in these channels.
 There is a wakeup message in channel(pos, pos)and in
 channel(neg, neg).See Figure Agents and Channels Example 1.1Figure Agents and Channels Example 1.1QuestionWhat changes occur if agenttotalreceives the message
 fromposand this is the first message thattotalreceives?AnswerThe message 3 on channel(pos, total)is removed from
 the channel.
 Agenttotal's variablesumbecomes 3 and
 channel(total, X)contains message 3.See Figure Agents and Channels Example 1.2Figure Agents and Channels Example 1.2QuestionSame initial state as in the previous question.
 What changes occur if agenttotalreceives the message
 fromnegand this is the first message thattotalreceives?AnswerThe message 2 on channel(neg, total)is removed from
 the channel.
 Agenttotal's variablesumbecomes 0 and
 channel(total, X)contains message 0.See Figure Agents and Channels Example 1.3Figure Agents and Channels Example 1.3QuestionSame starting state as in the previous two questions.
 
 What changes occur if agenttotalreceives the message
 fromposand then fromneg?AnswerAfter agenttotalreceives the message
 fromposand then fromnegthe channels
 fromposandnegtototalare
 empty, and the channel fromtotaltoXcontains message 3
 followed by message 1, and agenttotal's 
 variablesumis 1.See Figure Agents and Channels Example 1.4Figure Agents and Channels Example 1.4QuestionWhat changes occur if agenttotalreceives its first message
 fromnegand its second frompos?AnswerAfter agenttotalreceives the message
 fromnegand then fromposthe channels
 fromposandnegtototalare
 empty, and the channel fromtotaltoXcontains message 0
 followed by message 3, and agenttotal's 
 variable,sum, is 3.See Figure Agents and Channels Example 1.5Figure Agents and Channels Example 1.5QuestionAgentXprints the messages that it receives. What doesXprint?AnswerXmay print3, 8, 6, 2.Iftotalreceives 3, 5 fromposand then receives 2, 4 fromneg.3, 1, 6, 2.Iftotalreceives 3 frompos, then 2 fromneg, then 5 frompos, then 4 fromneg3, 1, 0, 5.Iftotalreceives 3 frompos, then 2 fromneg,
  then 4 fromneg,
  then 5 frompos.0, 3, 8, 4.Iftotalreceives
  2 fromneg,
  then 3 frompos,
  then 5 frompos,
  then 4 fromneg.0, 3, 0, 5.Iftotalreceives
  2 fromneg,
  then 3 frompos,
  then 4 fromneg,
  then 5 frompos.0, 0, 3, 8.Iftotalreceives
  2 fromneg,
  then 4 fromneg,
  then 3 frompos,
  then 5 frompos.NextThe next page definesstates of a distributed system.ExamplesFrequenty Asked QuestionsReview material for this page



Figure Link: https://kmchandy.github.io/DISTRIBUTED_SYSTEM_MODELS/Basics_Diagram.001.jpeg

Caption: Figure Agents and Channels Example 1.1


See Figure Agents and Channels Example 1.1Figure Agents and Channels Example 1.1QuestionWhat changes occur if agenttotalreceives the message
 fromposand this is the first message thattotalreceives?AnswerThe message 3 on channel(pos, total)is removed from
 the channel.
 Agenttotal's variablesumbecomes 3 and
 channel(total, X)contains message 3.See Figure Agents and Channels Example 1.2Figure Agents and Channels Example 1.2QuestionSame initial state as in the previous question.
 What changes occur if agenttotalreceives the message
 fromnegand this is the first message thattotalreceives?AnswerThe message 2 on channel(neg, total)is removed from
 the channel.
 Agenttotal's variablesumbecomes 0 and
 channel(total, X)contains message 0.See Figure Agents and Channels Example 1.3Figure Agents and Channels Example 1.3QuestionSame starting state as in the previous two questions.
 
 What changes occur if agenttotalreceives the message
 fromposand then fromneg?AnswerAfter agenttotalreceives the message
 fromposand then fromnegthe channels
 fromposandnegtototalare
 empty, and the channel fromtotaltoXcontains message 3
 followed by message 1, and agenttotal's 
 variablesumis 1.See Figure Agents and Channels Example 1.4Figure Agents and Channels Example 1.4QuestionWhat changes occur if agenttotalreceives its first message
 fromnegand its second frompos?AnswerAfter agenttotalreceives the message
 fromnegand then fromposthe channels
 fromposandnegtototalare
 empty, and the channel fromtotaltoXcontains message 0
 followed by message 3, and agenttotal's 
 variable,sum, is 3.See Figure Agents and Channels Example 1.5Figure Agents and Channels Example 1.5QuestionAgentXprints the messages that it receives. What doesXprint?AnswerXmay print3, 8, 6, 2.Iftotalreceives 3, 5 fromposand then receives 2, 4 fromneg.3, 1, 6, 2.Iftotalreceives 3 frompos, then 2 fromneg, then 5 frompos, then 4 fromneg3, 1, 0, 5.Iftotalreceives 3 frompos, then 2 fromneg,
  then 4 fromneg,
  then 5 frompos.0, 3, 8, 4.Iftotalreceives
  2 fromneg,
  then 3 frompos,
  then 5 frompos,
  then 4 fromneg.0, 3, 0, 5.Iftotalreceives
  2 fromneg,
  then 3 frompos,
  then 4 fromneg,
  then 5 frompos.0, 0, 3, 8.Iftotalreceives
  2 fromneg,
  then 4 fromneg,
  then 3 frompos,
  then 5 frompos.NextThe next page definesstates of a distributed system.ExamplesFrequenty Asked QuestionsReview material for this page



Figure Link: https://kmchandy.github.io/DISTRIBUTED_SYSTEM_MODELS/Basics_Diagram.001.jpeg

Caption: Figure Agents and Channels Example 1.1


See Figure Agents and Channels Example 1.2Figure Agents and Channels Example 1.2QuestionSame initial state as in the previous question.
 What changes occur if agenttotalreceives the message
 fromnegand this is the first message thattotalreceives?AnswerThe message 2 on channel(neg, total)is removed from
 the channel.
 Agenttotal's variablesumbecomes 0 and
 channel(total, X)contains message 0.See Figure Agents and Channels Example 1.3Figure Agents and Channels Example 1.3QuestionSame starting state as in the previous two questions.
 
 What changes occur if agenttotalreceives the message
 fromposand then fromneg?AnswerAfter agenttotalreceives the message
 fromposand then fromnegthe channels
 fromposandnegtototalare
 empty, and the channel fromtotaltoXcontains message 3
 followed by message 1, and agenttotal's 
 variablesumis 1.See Figure Agents and Channels Example 1.4Figure Agents and Channels Example 1.4QuestionWhat changes occur if agenttotalreceives its first message
 fromnegand its second frompos?AnswerAfter agenttotalreceives the message
 fromnegand then fromposthe channels
 fromposandnegtototalare
 empty, and the channel fromtotaltoXcontains message 0
 followed by message 3, and agenttotal's 
 variable,sum, is 3.See Figure Agents and Channels Example 1.5Figure Agents and Channels Example 1.5QuestionAgentXprints the messages that it receives. What doesXprint?AnswerXmay print3, 8, 6, 2.Iftotalreceives 3, 5 fromposand then receives 2, 4 fromneg.3, 1, 6, 2.Iftotalreceives 3 frompos, then 2 fromneg, then 5 frompos, then 4 fromneg3, 1, 0, 5.Iftotalreceives 3 frompos, then 2 fromneg,
  then 4 fromneg,
  then 5 frompos.0, 3, 8, 4.Iftotalreceives
  2 fromneg,
  then 3 frompos,
  then 5 frompos,
  then 4 fromneg.0, 3, 0, 5.Iftotalreceives
  2 fromneg,
  then 3 frompos,
  then 4 fromneg,
  then 5 frompos.0, 0, 3, 8.Iftotalreceives
  2 fromneg,
  then 4 fromneg,
  then 3 frompos,
  then 5 frompos.NextThe next page definesstates of a distributed system.ExamplesFrequenty Asked QuestionsReview material for this page



Figure Link: https://kmchandy.github.io/DISTRIBUTED_SYSTEM_MODELS/Basics_Diagram.002.jpeg

Caption: Figure Agents and Channels Example 1.2


See Figure Agents and Channels Example 1.3Figure Agents and Channels Example 1.3QuestionSame starting state as in the previous two questions.
 
 What changes occur if agenttotalreceives the message
 fromposand then fromneg?AnswerAfter agenttotalreceives the message
 fromposand then fromnegthe channels
 fromposandnegtototalare
 empty, and the channel fromtotaltoXcontains message 3
 followed by message 1, and agenttotal's 
 variablesumis 1.See Figure Agents and Channels Example 1.4Figure Agents and Channels Example 1.4QuestionWhat changes occur if agenttotalreceives its first message
 fromnegand its second frompos?AnswerAfter agenttotalreceives the message
 fromnegand then fromposthe channels
 fromposandnegtototalare
 empty, and the channel fromtotaltoXcontains message 0
 followed by message 3, and agenttotal's 
 variable,sum, is 3.See Figure Agents and Channels Example 1.5Figure Agents and Channels Example 1.5QuestionAgentXprints the messages that it receives. What doesXprint?AnswerXmay print3, 8, 6, 2.Iftotalreceives 3, 5 fromposand then receives 2, 4 fromneg.3, 1, 6, 2.Iftotalreceives 3 frompos, then 2 fromneg, then 5 frompos, then 4 fromneg3, 1, 0, 5.Iftotalreceives 3 frompos, then 2 fromneg,
  then 4 fromneg,
  then 5 frompos.0, 3, 8, 4.Iftotalreceives
  2 fromneg,
  then 3 frompos,
  then 5 frompos,
  then 4 fromneg.0, 3, 0, 5.Iftotalreceives
  2 fromneg,
  then 3 frompos,
  then 4 fromneg,
  then 5 frompos.0, 0, 3, 8.Iftotalreceives
  2 fromneg,
  then 4 fromneg,
  then 3 frompos,
  then 5 frompos.NextThe next page definesstates of a distributed system.ExamplesFrequenty Asked QuestionsReview material for this page



Figure Link: https://kmchandy.github.io/DISTRIBUTED_SYSTEM_MODELS/Basics_Diagram.003.jpeg

Caption: Figure Agents and Channels Example 1.3


See Figure Agents and Channels Example 1.4Figure Agents and Channels Example 1.4QuestionWhat changes occur if agenttotalreceives its first message
 fromnegand its second frompos?AnswerAfter agenttotalreceives the message
 fromnegand then fromposthe channels
 fromposandnegtototalare
 empty, and the channel fromtotaltoXcontains message 0
 followed by message 3, and agenttotal's 
 variable,sum, is 3.See Figure Agents and Channels Example 1.5Figure Agents and Channels Example 1.5QuestionAgentXprints the messages that it receives. What doesXprint?AnswerXmay print3, 8, 6, 2.Iftotalreceives 3, 5 fromposand then receives 2, 4 fromneg.3, 1, 6, 2.Iftotalreceives 3 frompos, then 2 fromneg, then 5 frompos, then 4 fromneg3, 1, 0, 5.Iftotalreceives 3 frompos, then 2 fromneg,
  then 4 fromneg,
  then 5 frompos.0, 3, 8, 4.Iftotalreceives
  2 fromneg,
  then 3 frompos,
  then 5 frompos,
  then 4 fromneg.0, 3, 0, 5.Iftotalreceives
  2 fromneg,
  then 3 frompos,
  then 4 fromneg,
  then 5 frompos.0, 0, 3, 8.Iftotalreceives
  2 fromneg,
  then 4 fromneg,
  then 3 frompos,
  then 5 frompos.NextThe next page definesstates of a distributed system.ExamplesFrequenty Asked QuestionsReview material for this page



Figure Link: https://kmchandy.github.io/DISTRIBUTED_SYSTEM_MODELS/Basics_Diagram.004.jpeg

Caption: Figure Agents and Channels Example 1.4


See Figure Agents and Channels Example 1.5Figure Agents and Channels Example 1.5QuestionAgentXprints the messages that it receives. What doesXprint?AnswerXmay print3, 8, 6, 2.Iftotalreceives 3, 5 fromposand then receives 2, 4 fromneg.3, 1, 6, 2.Iftotalreceives 3 frompos, then 2 fromneg, then 5 frompos, then 4 fromneg3, 1, 0, 5.Iftotalreceives 3 frompos, then 2 fromneg,
  then 4 fromneg,
  then 5 frompos.0, 3, 8, 4.Iftotalreceives
  2 fromneg,
  then 3 frompos,
  then 5 frompos,
  then 4 fromneg.0, 3, 0, 5.Iftotalreceives
  2 fromneg,
  then 3 frompos,
  then 4 fromneg,
  then 5 frompos.0, 0, 3, 8.Iftotalreceives
  2 fromneg,
  then 4 fromneg,
  then 3 frompos,
  then 5 frompos.NextThe next page definesstates of a distributed system.ExamplesFrequenty Asked QuestionsReview material for this page



Figure Link: https://kmchandy.github.io/DISTRIBUTED_SYSTEM_MODELS/Basics_Diagram.008.jpeg

Caption: Figure Agents and Channels Example 1.5


Iftotalreceives 3, 5 fromposand then receives 2, 4 fromneg.

Iftotalreceives 3 frompos, then 2 fromneg, then 5 frompos, then 4 fromneg

Iftotalreceives 3 frompos, then 2 fromneg,
  then 4 fromneg,
  then 5 frompos.

Iftotalreceives
  2 fromneg,
  then 3 frompos,
  then 5 frompos,
  then 4 fromneg.

Iftotalreceives
  2 fromneg,
  then 3 frompos,
  then 4 fromneg,
  then 5 frompos.

Iftotalreceives
  2 fromneg,
  then 4 fromneg,
  then 3 frompos,
  then 5 frompos.

ExamplesFrequenty Asked QuestionsReview material for this page

Frequenty Asked QuestionsReview material for this page

Review material for this page

K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology
--- END Content from https://kmchandy.github.io/DISTRIBUTED_SYSTEM_MODELS/BasicsExamples.html ---


--- START Content from https://kmchandy.github.io/DISTRIBUTED_SYSTEM_MODELS/BasicsFAQ.html ---
TITLE: DistributedSystemModels/BasicsFAQ.html

Consider an example of two agents sending
  tokens to each other.
  Each agent sends a token that it holds to the other agent.
  So there is a computation in which tokens go back and forth between
  the agents forever. 
  Let's call this distributed systemD.Now, consider a system consisting of two identical, and totally
  independent, copies ofD.
  There is no connection between the two copies.
  Surely, the behavior ofDshouldn't change because of
  the presence of a completely independent network of agents.
  But, with our model, it does.A computation progresses by delivering a message fromanynonempty channel.
  So, there is an infinite computation in which messages are delivered
  in channels in one copy ofD, and no messages are
  delivered from nonempty channels in the other copyThe problem is that the selection of the nonempty channel in each
  iteration of the while loop may be unfair -- the same set of
  channels could be selected forever while other nonempty channels are
  never selected.
  The model has no provision for ensuring that a message in a channel
  will be delivered eventually.
  We will introduce fair selection,eventuality, andtemporal logiclater in the course.Model Limitations: No Construct for TimeThe only representation of time in our model is that some events occur
after others.
An event in which a message is received occurs after an event in which
that message is sent.
Time plays a critical role in the performance of algorithms even
though we never use time in proving the correctness of algorithms.There are many algorithms in which an agent sends itself aTimeout(T)message where ideally the message would be received approximatelyTseconds after it is sent.
Though we never useTin proving the correctness of algorithms we will use
timeout messages in analyzing their performance.Agents can maintain accurate clocks by using atomic clocks, Precision Time
Protocols (PTP) in local area networks, and Network Time Protocol
(NTP) servers.
Accurate clocks have the property that the time at which an event is
sent, as determined by the sender's clock, is (almost always) earlier
than the time of the event in which the message is received.
We do not, however, rule out the possibility that clocks drift apart
so that the sender's clock is far ahead of the receiver's.Some algorithms for systems with perfect clocks are simpler than those
with imperfect clocks, as we shall see.Model Limitations: Start Up, Shutdown, FailureThe model assumes that all agents and channels are initialized and
then agents start receiving messages.
The model has abarrierbetween the point at which initialization takes place and the point at
which messages are delivered.
The barrier isn't necessary in most algorithms, though it assuming its
existence helps us to focus on more important parts of the algorithm.The model does not specify how termination is detected if the
computation does terminate.
Nor does the notation have primitives for shutting down agents and
channels gracefully so that they don't continue to hold resources
after computation has terminated.
Protocols such as AMQP do have primitives for starting up and shutting
down distributed systems, but we won't discuss them here.We will describe algorithms that execute on faulty systems in which
messages may be lost, duplicated, or delivered out of order, and where
agents may stop forever or halt and restart.
We also describe algorithms with Byzantine agents.
These algorithms are based on models that are different from those
given so far.Model Limitations: Discrete State SpaceThe state space is discrete in most of the algorithms described here.
The state space of some distributed systems has both discrete and
continuous components.
The state space of a fleet of drones has continuous components.Later, we study algorithms in which the state space is
continuous.
Systems with continuous state spaces may have discrete or continuous
state transitions.Model Limitations: StaticThe network of agents and messages in the model is static.
  In contrast, distributed systems evolve; agents and channels may be
  added and deleted; agents may change; channel protocols may be
modified.Model Limitations: Simple ChannelsThe model only considers channels in which messages are delivered in
  the order sent.
  Some distributed systems have channels in which messages may be
duplicated and delivered out of order.Many models deal with channels in which senders are blocked when a
channel gets full.
Our model has no concept of a channel being full.Model Limitations: SummaryModels and notations in this course are not comprehensive -- they do not capture
most aspects of distributed systems.
We use a model that is appropriate for describing the algorithm at
hand.
The choice of a model is an engineering decision.Why use such a simplistic model?The model is indeed simplistic; however, it is adequate for describing
  and reasoning about many of the algorithms described in the first
  part of this course.
  We introduce other models later.
  We use the simplest model adequate for the problem at hand.How many channels are there in a system?In the model there is a channel from every agent to every agent.
  So, if there are \(N\) agents there are \(N^{2}\) channels.
With a thousand agents there are a million channels.
That's a lot of channels!The model allows channels between every pair of agents.
In practice very few of these channels are used.
When we implement an algorithm using AMQP, for instance, we will
declare channels.Can you describe channels in more detail?Ourbasic description of channels is
  found here.Channels in our model are asynchronous (non-blocking) and
  unidirectional.
  A sender can send messages on a channel regardless of how many
  messages have been sent in the past and how many messages have been
  received on that channel.
  The model assumes that the queue of messages in the channel has
  infinite capacity.A channel is unidirectional.
It is represented by a directed edge in a directed graph.
Messages can be sent by an agentPto an agentQalong a channel(P, Q).
AgentQcannot send a message to agentPalong channel(P, Q).A system may have a channel(P, Q)and may or may not have a channel(Q, P).
Some systems have bidirectional channels but our model does not allow
for bidirectional channels.In our model, 
a channel is directed from exactly one agent to exactly one agent.
In some systems, multiple agents can send messages on the same
channel, and multiple agents can receive messages on the same channel.
Our model does not allow for such channels.Can an agent refuse to receive a message?In this model an agent cannot refuse to receive a message.
  If a channel is not empty then a message from the channel can be
  delivered to the agentindependent of the state of the agent.Suppose you want to design an algorithm in which an agentXrefuses to receive messages from an agentZuntil it first receives a message from an agentY.
  How would you use this model?In our model, agentXhas to receive the message fromZin every state ofX.AgentXcan copy the message fromZinto a
  local queue -- a local variable ofX-- and process the
  message only after receiving the message fromY.Can a full channel block a sender from
  sending more messages?In the model a sender is never blocked from sending a message.Our model assumes that channels have unlimited capacity.
We can represent a situation in which channels have limited capacity
in the following way.
An agent has a local output queue of unlimited size.
The agent puts messages into this queue when it cannot send messages
on a channel because the channel is full.
Messages from this queue are sent on the channel when the
channel stops being full.Can you represent systems in which agents
  are created and deleted in the model?The model doesn't have features that allow agents to be created and
  deleted.A work around for the case in which agents are created or deleted a
finite number of times is as follows.
The network of agents in the model is made large enough to include
agents that have not been created as yet and also include agents that
have been deleted.An agent that has not been created is represented by an agent that is
idle, i.e., one is waiting for a message. It never receives a message
until the agent is created. The message is created by the operating
system sending a "create" message to the agent and informing other
agents that this agent has been created.An agent this is deleted is represented in the same way.
The operating system deletes an agent by sending a "delete" message to
the agent.
A deleted agent discards messages that it receives without
taking action.Can an agent process background jobs that
  are interrupted when messages arrive?No, there are no interruptions in the model.
When an agent is executing a receive it is not interrupted.There are systems in which messages have different priorities, and the
arrival of high-priority messages interrupts the execution of
low-priority messages.
Our model does not allow for interruptions.How do you represent channels that are not
  first in first out?We use queues to represent states of first in first out channels, and
  we use other data structures -- such as multisets -- to represent
  states of other types of channels.Later we describe algorithms with different types of channels.
  For example, the state of a channel in which messages may overtake
  each other is a multiset or bag.
  A message is sent on a channel by adding the message to the
  multiset.
  A nonempty multiset may deliver any message in the mutliset to the
  receiving agent.Lossy channels, and channels in which messages have priorities, are
  also modeled by defining channel states appropriately.Can the model represent sequential
  composition of distributed computations?Yes.There are problems in which we would like to start a distributed
computationP; wait forPto finish, and
then start another distributed computationQ.
We want a barrier betweenPandQ.A barrier can be implemented in several ways.
An agent, say acoordinatoragent, sends messages to all agents to startP.
The coordinator then detects termination ofP; we discuss
termination detection algorithms later.
After detecting termination ofPthe coordinator sends
messages to all agents to startQ.Can the model represent agents that make state transitions
  without receiving messages?No, the model does not represent agents that make state transitions
without receiving messages.
The effect of a state transition without receiving a message can be
simulated by an agent sending itself a message and carrying out the
transition when it receives a message from itself.For example, an agent that is computing using a set of files may
  transition to a state in which it no longer needs those files; this
transition occurs without the agent receiving a message.
We model this event by having the agent send itself a message when it
starts using the set of files, and it transits to a state in which it
no longer needs the files when it receives the message from itself.Representing internal state changes in this way is an artifice;
  however, the artifice allows us to use a very simple model in which a
  state change occurs when, and only when, a message is delivered.What is an agent?In our model,an agent is an object that sends and
receives messages.Some papers refer to an agent as anactoror as aprocess.What is a simple example of the notation?Seean example of the notation.Local clocks can be synchronized using NTP
  and other protocols. Why do you assume that clocks aren't synchronized?Local clocks can, indeed, be synchronized very accurately; however, we
  do not assume that they are perfect.We use agents' local clocks for evaluating algorithm performance but
not for proving correctness because even a small drift can cause race
conditions.Consider an algorithm in which one agent carries out a computation
starting at 1pm and another agent carries out a computation starting at 2pm.
When we prove the correctness of our algorithms, we allow for the
unlikely possibility that that the agent starting its 
computation at 2pm does so before the agent that starts a 1pm.
For evaluating performance, however, we assume that the agent that starts a 1pm
usually does so before the agent that starts at 2pm.What are other examples of models?The model we use is anactor model.For more general asynchronous models of concurrent systems seeUNITYandTLA.Perhaps the most widely used model isCSP -- Communicating Sequential Processes.An overview ofmodels used in parallel programmingdescribes models used in
shared-memory and distributed computing.NextStates of a Distributed System.Review material for this pageK. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

Now, consider a system consisting of two identical, and totally
  independent, copies ofD.
  There is no connection between the two copies.
  Surely, the behavior ofDshouldn't change because of
  the presence of a completely independent network of agents.
  But, with our model, it does.A computation progresses by delivering a message fromanynonempty channel.
  So, there is an infinite computation in which messages are delivered
  in channels in one copy ofD, and no messages are
  delivered from nonempty channels in the other copyThe problem is that the selection of the nonempty channel in each
  iteration of the while loop may be unfair -- the same set of
  channels could be selected forever while other nonempty channels are
  never selected.
  The model has no provision for ensuring that a message in a channel
  will be delivered eventually.
  We will introduce fair selection,eventuality, andtemporal logiclater in the course.Model Limitations: No Construct for TimeThe only representation of time in our model is that some events occur
after others.
An event in which a message is received occurs after an event in which
that message is sent.
Time plays a critical role in the performance of algorithms even
though we never use time in proving the correctness of algorithms.There are many algorithms in which an agent sends itself aTimeout(T)message where ideally the message would be received approximatelyTseconds after it is sent.
Though we never useTin proving the correctness of algorithms we will use
timeout messages in analyzing their performance.Agents can maintain accurate clocks by using atomic clocks, Precision Time
Protocols (PTP) in local area networks, and Network Time Protocol
(NTP) servers.
Accurate clocks have the property that the time at which an event is
sent, as determined by the sender's clock, is (almost always) earlier
than the time of the event in which the message is received.
We do not, however, rule out the possibility that clocks drift apart
so that the sender's clock is far ahead of the receiver's.Some algorithms for systems with perfect clocks are simpler than those
with imperfect clocks, as we shall see.Model Limitations: Start Up, Shutdown, FailureThe model assumes that all agents and channels are initialized and
then agents start receiving messages.
The model has abarrierbetween the point at which initialization takes place and the point at
which messages are delivered.
The barrier isn't necessary in most algorithms, though it assuming its
existence helps us to focus on more important parts of the algorithm.The model does not specify how termination is detected if the
computation does terminate.
Nor does the notation have primitives for shutting down agents and
channels gracefully so that they don't continue to hold resources
after computation has terminated.
Protocols such as AMQP do have primitives for starting up and shutting
down distributed systems, but we won't discuss them here.We will describe algorithms that execute on faulty systems in which
messages may be lost, duplicated, or delivered out of order, and where
agents may stop forever or halt and restart.
We also describe algorithms with Byzantine agents.
These algorithms are based on models that are different from those
given so far.Model Limitations: Discrete State SpaceThe state space is discrete in most of the algorithms described here.
The state space of some distributed systems has both discrete and
continuous components.
The state space of a fleet of drones has continuous components.Later, we study algorithms in which the state space is
continuous.
Systems with continuous state spaces may have discrete or continuous
state transitions.Model Limitations: StaticThe network of agents and messages in the model is static.
  In contrast, distributed systems evolve; agents and channels may be
  added and deleted; agents may change; channel protocols may be
modified.Model Limitations: Simple ChannelsThe model only considers channels in which messages are delivered in
  the order sent.
  Some distributed systems have channels in which messages may be
duplicated and delivered out of order.Many models deal with channels in which senders are blocked when a
channel gets full.
Our model has no concept of a channel being full.Model Limitations: SummaryModels and notations in this course are not comprehensive -- they do not capture
most aspects of distributed systems.
We use a model that is appropriate for describing the algorithm at
hand.
The choice of a model is an engineering decision.Why use such a simplistic model?The model is indeed simplistic; however, it is adequate for describing
  and reasoning about many of the algorithms described in the first
  part of this course.
  We introduce other models later.
  We use the simplest model adequate for the problem at hand.How many channels are there in a system?In the model there is a channel from every agent to every agent.
  So, if there are \(N\) agents there are \(N^{2}\) channels.
With a thousand agents there are a million channels.
That's a lot of channels!The model allows channels between every pair of agents.
In practice very few of these channels are used.
When we implement an algorithm using AMQP, for instance, we will
declare channels.Can you describe channels in more detail?Ourbasic description of channels is
  found here.Channels in our model are asynchronous (non-blocking) and
  unidirectional.
  A sender can send messages on a channel regardless of how many
  messages have been sent in the past and how many messages have been
  received on that channel.
  The model assumes that the queue of messages in the channel has
  infinite capacity.A channel is unidirectional.
It is represented by a directed edge in a directed graph.
Messages can be sent by an agentPto an agentQalong a channel(P, Q).
AgentQcannot send a message to agentPalong channel(P, Q).A system may have a channel(P, Q)and may or may not have a channel(Q, P).
Some systems have bidirectional channels but our model does not allow
for bidirectional channels.In our model, 
a channel is directed from exactly one agent to exactly one agent.
In some systems, multiple agents can send messages on the same
channel, and multiple agents can receive messages on the same channel.
Our model does not allow for such channels.Can an agent refuse to receive a message?In this model an agent cannot refuse to receive a message.
  If a channel is not empty then a message from the channel can be
  delivered to the agentindependent of the state of the agent.Suppose you want to design an algorithm in which an agentXrefuses to receive messages from an agentZuntil it first receives a message from an agentY.
  How would you use this model?In our model, agentXhas to receive the message fromZin every state ofX.AgentXcan copy the message fromZinto a
  local queue -- a local variable ofX-- and process the
  message only after receiving the message fromY.Can a full channel block a sender from
  sending more messages?In the model a sender is never blocked from sending a message.Our model assumes that channels have unlimited capacity.
We can represent a situation in which channels have limited capacity
in the following way.
An agent has a local output queue of unlimited size.
The agent puts messages into this queue when it cannot send messages
on a channel because the channel is full.
Messages from this queue are sent on the channel when the
channel stops being full.Can you represent systems in which agents
  are created and deleted in the model?The model doesn't have features that allow agents to be created and
  deleted.A work around for the case in which agents are created or deleted a
finite number of times is as follows.
The network of agents in the model is made large enough to include
agents that have not been created as yet and also include agents that
have been deleted.An agent that has not been created is represented by an agent that is
idle, i.e., one is waiting for a message. It never receives a message
until the agent is created. The message is created by the operating
system sending a "create" message to the agent and informing other
agents that this agent has been created.An agent this is deleted is represented in the same way.
The operating system deletes an agent by sending a "delete" message to
the agent.
A deleted agent discards messages that it receives without
taking action.Can an agent process background jobs that
  are interrupted when messages arrive?No, there are no interruptions in the model.
When an agent is executing a receive it is not interrupted.There are systems in which messages have different priorities, and the
arrival of high-priority messages interrupts the execution of
low-priority messages.
Our model does not allow for interruptions.How do you represent channels that are not
  first in first out?We use queues to represent states of first in first out channels, and
  we use other data structures -- such as multisets -- to represent
  states of other types of channels.Later we describe algorithms with different types of channels.
  For example, the state of a channel in which messages may overtake
  each other is a multiset or bag.
  A message is sent on a channel by adding the message to the
  multiset.
  A nonempty multiset may deliver any message in the mutliset to the
  receiving agent.Lossy channels, and channels in which messages have priorities, are
  also modeled by defining channel states appropriately.Can the model represent sequential
  composition of distributed computations?Yes.There are problems in which we would like to start a distributed
computationP; wait forPto finish, and
then start another distributed computationQ.
We want a barrier betweenPandQ.A barrier can be implemented in several ways.
An agent, say acoordinatoragent, sends messages to all agents to startP.
The coordinator then detects termination ofP; we discuss
termination detection algorithms later.
After detecting termination ofPthe coordinator sends
messages to all agents to startQ.Can the model represent agents that make state transitions
  without receiving messages?No, the model does not represent agents that make state transitions
without receiving messages.
The effect of a state transition without receiving a message can be
simulated by an agent sending itself a message and carrying out the
transition when it receives a message from itself.For example, an agent that is computing using a set of files may
  transition to a state in which it no longer needs those files; this
transition occurs without the agent receiving a message.
We model this event by having the agent send itself a message when it
starts using the set of files, and it transits to a state in which it
no longer needs the files when it receives the message from itself.Representing internal state changes in this way is an artifice;
  however, the artifice allows us to use a very simple model in which a
  state change occurs when, and only when, a message is delivered.What is an agent?In our model,an agent is an object that sends and
receives messages.Some papers refer to an agent as anactoror as aprocess.What is a simple example of the notation?Seean example of the notation.Local clocks can be synchronized using NTP
  and other protocols. Why do you assume that clocks aren't synchronized?Local clocks can, indeed, be synchronized very accurately; however, we
  do not assume that they are perfect.We use agents' local clocks for evaluating algorithm performance but
not for proving correctness because even a small drift can cause race
conditions.Consider an algorithm in which one agent carries out a computation
starting at 1pm and another agent carries out a computation starting at 2pm.
When we prove the correctness of our algorithms, we allow for the
unlikely possibility that that the agent starting its 
computation at 2pm does so before the agent that starts a 1pm.
For evaluating performance, however, we assume that the agent that starts a 1pm
usually does so before the agent that starts at 2pm.What are other examples of models?The model we use is anactor model.For more general asynchronous models of concurrent systems seeUNITYandTLA.Perhaps the most widely used model isCSP -- Communicating Sequential Processes.An overview ofmodels used in parallel programmingdescribes models used in
shared-memory and distributed computing.NextStates of a Distributed System.Review material for this pageK. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

A computation progresses by delivering a message fromanynonempty channel.
  So, there is an infinite computation in which messages are delivered
  in channels in one copy ofD, and no messages are
  delivered from nonempty channels in the other copyThe problem is that the selection of the nonempty channel in each
  iteration of the while loop may be unfair -- the same set of
  channels could be selected forever while other nonempty channels are
  never selected.
  The model has no provision for ensuring that a message in a channel
  will be delivered eventually.
  We will introduce fair selection,eventuality, andtemporal logiclater in the course.Model Limitations: No Construct for TimeThe only representation of time in our model is that some events occur
after others.
An event in which a message is received occurs after an event in which
that message is sent.
Time plays a critical role in the performance of algorithms even
though we never use time in proving the correctness of algorithms.There are many algorithms in which an agent sends itself aTimeout(T)message where ideally the message would be received approximatelyTseconds after it is sent.
Though we never useTin proving the correctness of algorithms we will use
timeout messages in analyzing their performance.Agents can maintain accurate clocks by using atomic clocks, Precision Time
Protocols (PTP) in local area networks, and Network Time Protocol
(NTP) servers.
Accurate clocks have the property that the time at which an event is
sent, as determined by the sender's clock, is (almost always) earlier
than the time of the event in which the message is received.
We do not, however, rule out the possibility that clocks drift apart
so that the sender's clock is far ahead of the receiver's.Some algorithms for systems with perfect clocks are simpler than those
with imperfect clocks, as we shall see.Model Limitations: Start Up, Shutdown, FailureThe model assumes that all agents and channels are initialized and
then agents start receiving messages.
The model has abarrierbetween the point at which initialization takes place and the point at
which messages are delivered.
The barrier isn't necessary in most algorithms, though it assuming its
existence helps us to focus on more important parts of the algorithm.The model does not specify how termination is detected if the
computation does terminate.
Nor does the notation have primitives for shutting down agents and
channels gracefully so that they don't continue to hold resources
after computation has terminated.
Protocols such as AMQP do have primitives for starting up and shutting
down distributed systems, but we won't discuss them here.We will describe algorithms that execute on faulty systems in which
messages may be lost, duplicated, or delivered out of order, and where
agents may stop forever or halt and restart.
We also describe algorithms with Byzantine agents.
These algorithms are based on models that are different from those
given so far.Model Limitations: Discrete State SpaceThe state space is discrete in most of the algorithms described here.
The state space of some distributed systems has both discrete and
continuous components.
The state space of a fleet of drones has continuous components.Later, we study algorithms in which the state space is
continuous.
Systems with continuous state spaces may have discrete or continuous
state transitions.Model Limitations: StaticThe network of agents and messages in the model is static.
  In contrast, distributed systems evolve; agents and channels may be
  added and deleted; agents may change; channel protocols may be
modified.Model Limitations: Simple ChannelsThe model only considers channels in which messages are delivered in
  the order sent.
  Some distributed systems have channels in which messages may be
duplicated and delivered out of order.Many models deal with channels in which senders are blocked when a
channel gets full.
Our model has no concept of a channel being full.Model Limitations: SummaryModels and notations in this course are not comprehensive -- they do not capture
most aspects of distributed systems.
We use a model that is appropriate for describing the algorithm at
hand.
The choice of a model is an engineering decision.Why use such a simplistic model?The model is indeed simplistic; however, it is adequate for describing
  and reasoning about many of the algorithms described in the first
  part of this course.
  We introduce other models later.
  We use the simplest model adequate for the problem at hand.How many channels are there in a system?In the model there is a channel from every agent to every agent.
  So, if there are \(N\) agents there are \(N^{2}\) channels.
With a thousand agents there are a million channels.
That's a lot of channels!The model allows channels between every pair of agents.
In practice very few of these channels are used.
When we implement an algorithm using AMQP, for instance, we will
declare channels.Can you describe channels in more detail?Ourbasic description of channels is
  found here.Channels in our model are asynchronous (non-blocking) and
  unidirectional.
  A sender can send messages on a channel regardless of how many
  messages have been sent in the past and how many messages have been
  received on that channel.
  The model assumes that the queue of messages in the channel has
  infinite capacity.A channel is unidirectional.
It is represented by a directed edge in a directed graph.
Messages can be sent by an agentPto an agentQalong a channel(P, Q).
AgentQcannot send a message to agentPalong channel(P, Q).A system may have a channel(P, Q)and may or may not have a channel(Q, P).
Some systems have bidirectional channels but our model does not allow
for bidirectional channels.In our model, 
a channel is directed from exactly one agent to exactly one agent.
In some systems, multiple agents can send messages on the same
channel, and multiple agents can receive messages on the same channel.
Our model does not allow for such channels.Can an agent refuse to receive a message?In this model an agent cannot refuse to receive a message.
  If a channel is not empty then a message from the channel can be
  delivered to the agentindependent of the state of the agent.Suppose you want to design an algorithm in which an agentXrefuses to receive messages from an agentZuntil it first receives a message from an agentY.
  How would you use this model?In our model, agentXhas to receive the message fromZin every state ofX.AgentXcan copy the message fromZinto a
  local queue -- a local variable ofX-- and process the
  message only after receiving the message fromY.Can a full channel block a sender from
  sending more messages?In the model a sender is never blocked from sending a message.Our model assumes that channels have unlimited capacity.
We can represent a situation in which channels have limited capacity
in the following way.
An agent has a local output queue of unlimited size.
The agent puts messages into this queue when it cannot send messages
on a channel because the channel is full.
Messages from this queue are sent on the channel when the
channel stops being full.Can you represent systems in which agents
  are created and deleted in the model?The model doesn't have features that allow agents to be created and
  deleted.A work around for the case in which agents are created or deleted a
finite number of times is as follows.
The network of agents in the model is made large enough to include
agents that have not been created as yet and also include agents that
have been deleted.An agent that has not been created is represented by an agent that is
idle, i.e., one is waiting for a message. It never receives a message
until the agent is created. The message is created by the operating
system sending a "create" message to the agent and informing other
agents that this agent has been created.An agent this is deleted is represented in the same way.
The operating system deletes an agent by sending a "delete" message to
the agent.
A deleted agent discards messages that it receives without
taking action.Can an agent process background jobs that
  are interrupted when messages arrive?No, there are no interruptions in the model.
When an agent is executing a receive it is not interrupted.There are systems in which messages have different priorities, and the
arrival of high-priority messages interrupts the execution of
low-priority messages.
Our model does not allow for interruptions.How do you represent channels that are not
  first in first out?We use queues to represent states of first in first out channels, and
  we use other data structures -- such as multisets -- to represent
  states of other types of channels.Later we describe algorithms with different types of channels.
  For example, the state of a channel in which messages may overtake
  each other is a multiset or bag.
  A message is sent on a channel by adding the message to the
  multiset.
  A nonempty multiset may deliver any message in the mutliset to the
  receiving agent.Lossy channels, and channels in which messages have priorities, are
  also modeled by defining channel states appropriately.Can the model represent sequential
  composition of distributed computations?Yes.There are problems in which we would like to start a distributed
computationP; wait forPto finish, and
then start another distributed computationQ.
We want a barrier betweenPandQ.A barrier can be implemented in several ways.
An agent, say acoordinatoragent, sends messages to all agents to startP.
The coordinator then detects termination ofP; we discuss
termination detection algorithms later.
After detecting termination ofPthe coordinator sends
messages to all agents to startQ.Can the model represent agents that make state transitions
  without receiving messages?No, the model does not represent agents that make state transitions
without receiving messages.
The effect of a state transition without receiving a message can be
simulated by an agent sending itself a message and carrying out the
transition when it receives a message from itself.For example, an agent that is computing using a set of files may
  transition to a state in which it no longer needs those files; this
transition occurs without the agent receiving a message.
We model this event by having the agent send itself a message when it
starts using the set of files, and it transits to a state in which it
no longer needs the files when it receives the message from itself.Representing internal state changes in this way is an artifice;
  however, the artifice allows us to use a very simple model in which a
  state change occurs when, and only when, a message is delivered.What is an agent?In our model,an agent is an object that sends and
receives messages.Some papers refer to an agent as anactoror as aprocess.What is a simple example of the notation?Seean example of the notation.Local clocks can be synchronized using NTP
  and other protocols. Why do you assume that clocks aren't synchronized?Local clocks can, indeed, be synchronized very accurately; however, we
  do not assume that they are perfect.We use agents' local clocks for evaluating algorithm performance but
not for proving correctness because even a small drift can cause race
conditions.Consider an algorithm in which one agent carries out a computation
starting at 1pm and another agent carries out a computation starting at 2pm.
When we prove the correctness of our algorithms, we allow for the
unlikely possibility that that the agent starting its 
computation at 2pm does so before the agent that starts a 1pm.
For evaluating performance, however, we assume that the agent that starts a 1pm
usually does so before the agent that starts at 2pm.What are other examples of models?The model we use is anactor model.For more general asynchronous models of concurrent systems seeUNITYandTLA.Perhaps the most widely used model isCSP -- Communicating Sequential Processes.An overview ofmodels used in parallel programmingdescribes models used in
shared-memory and distributed computing.NextStates of a Distributed System.Review material for this pageK. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

The problem is that the selection of the nonempty channel in each
  iteration of the while loop may be unfair -- the same set of
  channels could be selected forever while other nonempty channels are
  never selected.
  The model has no provision for ensuring that a message in a channel
  will be delivered eventually.
  We will introduce fair selection,eventuality, andtemporal logiclater in the course.Model Limitations: No Construct for TimeThe only representation of time in our model is that some events occur
after others.
An event in which a message is received occurs after an event in which
that message is sent.
Time plays a critical role in the performance of algorithms even
though we never use time in proving the correctness of algorithms.There are many algorithms in which an agent sends itself aTimeout(T)message where ideally the message would be received approximatelyTseconds after it is sent.
Though we never useTin proving the correctness of algorithms we will use
timeout messages in analyzing their performance.Agents can maintain accurate clocks by using atomic clocks, Precision Time
Protocols (PTP) in local area networks, and Network Time Protocol
(NTP) servers.
Accurate clocks have the property that the time at which an event is
sent, as determined by the sender's clock, is (almost always) earlier
than the time of the event in which the message is received.
We do not, however, rule out the possibility that clocks drift apart
so that the sender's clock is far ahead of the receiver's.Some algorithms for systems with perfect clocks are simpler than those
with imperfect clocks, as we shall see.Model Limitations: Start Up, Shutdown, FailureThe model assumes that all agents and channels are initialized and
then agents start receiving messages.
The model has abarrierbetween the point at which initialization takes place and the point at
which messages are delivered.
The barrier isn't necessary in most algorithms, though it assuming its
existence helps us to focus on more important parts of the algorithm.The model does not specify how termination is detected if the
computation does terminate.
Nor does the notation have primitives for shutting down agents and
channels gracefully so that they don't continue to hold resources
after computation has terminated.
Protocols such as AMQP do have primitives for starting up and shutting
down distributed systems, but we won't discuss them here.We will describe algorithms that execute on faulty systems in which
messages may be lost, duplicated, or delivered out of order, and where
agents may stop forever or halt and restart.
We also describe algorithms with Byzantine agents.
These algorithms are based on models that are different from those
given so far.Model Limitations: Discrete State SpaceThe state space is discrete in most of the algorithms described here.
The state space of some distributed systems has both discrete and
continuous components.
The state space of a fleet of drones has continuous components.Later, we study algorithms in which the state space is
continuous.
Systems with continuous state spaces may have discrete or continuous
state transitions.Model Limitations: StaticThe network of agents and messages in the model is static.
  In contrast, distributed systems evolve; agents and channels may be
  added and deleted; agents may change; channel protocols may be
modified.Model Limitations: Simple ChannelsThe model only considers channels in which messages are delivered in
  the order sent.
  Some distributed systems have channels in which messages may be
duplicated and delivered out of order.Many models deal with channels in which senders are blocked when a
channel gets full.
Our model has no concept of a channel being full.Model Limitations: SummaryModels and notations in this course are not comprehensive -- they do not capture
most aspects of distributed systems.
We use a model that is appropriate for describing the algorithm at
hand.
The choice of a model is an engineering decision.Why use such a simplistic model?The model is indeed simplistic; however, it is adequate for describing
  and reasoning about many of the algorithms described in the first
  part of this course.
  We introduce other models later.
  We use the simplest model adequate for the problem at hand.How many channels are there in a system?In the model there is a channel from every agent to every agent.
  So, if there are \(N\) agents there are \(N^{2}\) channels.
With a thousand agents there are a million channels.
That's a lot of channels!The model allows channels between every pair of agents.
In practice very few of these channels are used.
When we implement an algorithm using AMQP, for instance, we will
declare channels.Can you describe channels in more detail?Ourbasic description of channels is
  found here.Channels in our model are asynchronous (non-blocking) and
  unidirectional.
  A sender can send messages on a channel regardless of how many
  messages have been sent in the past and how many messages have been
  received on that channel.
  The model assumes that the queue of messages in the channel has
  infinite capacity.A channel is unidirectional.
It is represented by a directed edge in a directed graph.
Messages can be sent by an agentPto an agentQalong a channel(P, Q).
AgentQcannot send a message to agentPalong channel(P, Q).A system may have a channel(P, Q)and may or may not have a channel(Q, P).
Some systems have bidirectional channels but our model does not allow
for bidirectional channels.In our model, 
a channel is directed from exactly one agent to exactly one agent.
In some systems, multiple agents can send messages on the same
channel, and multiple agents can receive messages on the same channel.
Our model does not allow for such channels.Can an agent refuse to receive a message?In this model an agent cannot refuse to receive a message.
  If a channel is not empty then a message from the channel can be
  delivered to the agentindependent of the state of the agent.Suppose you want to design an algorithm in which an agentXrefuses to receive messages from an agentZuntil it first receives a message from an agentY.
  How would you use this model?In our model, agentXhas to receive the message fromZin every state ofX.AgentXcan copy the message fromZinto a
  local queue -- a local variable ofX-- and process the
  message only after receiving the message fromY.Can a full channel block a sender from
  sending more messages?In the model a sender is never blocked from sending a message.Our model assumes that channels have unlimited capacity.
We can represent a situation in which channels have limited capacity
in the following way.
An agent has a local output queue of unlimited size.
The agent puts messages into this queue when it cannot send messages
on a channel because the channel is full.
Messages from this queue are sent on the channel when the
channel stops being full.Can you represent systems in which agents
  are created and deleted in the model?The model doesn't have features that allow agents to be created and
  deleted.A work around for the case in which agents are created or deleted a
finite number of times is as follows.
The network of agents in the model is made large enough to include
agents that have not been created as yet and also include agents that
have been deleted.An agent that has not been created is represented by an agent that is
idle, i.e., one is waiting for a message. It never receives a message
until the agent is created. The message is created by the operating
system sending a "create" message to the agent and informing other
agents that this agent has been created.An agent this is deleted is represented in the same way.
The operating system deletes an agent by sending a "delete" message to
the agent.
A deleted agent discards messages that it receives without
taking action.Can an agent process background jobs that
  are interrupted when messages arrive?No, there are no interruptions in the model.
When an agent is executing a receive it is not interrupted.There are systems in which messages have different priorities, and the
arrival of high-priority messages interrupts the execution of
low-priority messages.
Our model does not allow for interruptions.How do you represent channels that are not
  first in first out?We use queues to represent states of first in first out channels, and
  we use other data structures -- such as multisets -- to represent
  states of other types of channels.Later we describe algorithms with different types of channels.
  For example, the state of a channel in which messages may overtake
  each other is a multiset or bag.
  A message is sent on a channel by adding the message to the
  multiset.
  A nonempty multiset may deliver any message in the mutliset to the
  receiving agent.Lossy channels, and channels in which messages have priorities, are
  also modeled by defining channel states appropriately.Can the model represent sequential
  composition of distributed computations?Yes.There are problems in which we would like to start a distributed
computationP; wait forPto finish, and
then start another distributed computationQ.
We want a barrier betweenPandQ.A barrier can be implemented in several ways.
An agent, say acoordinatoragent, sends messages to all agents to startP.
The coordinator then detects termination ofP; we discuss
termination detection algorithms later.
After detecting termination ofPthe coordinator sends
messages to all agents to startQ.Can the model represent agents that make state transitions
  without receiving messages?No, the model does not represent agents that make state transitions
without receiving messages.
The effect of a state transition without receiving a message can be
simulated by an agent sending itself a message and carrying out the
transition when it receives a message from itself.For example, an agent that is computing using a set of files may
  transition to a state in which it no longer needs those files; this
transition occurs without the agent receiving a message.
We model this event by having the agent send itself a message when it
starts using the set of files, and it transits to a state in which it
no longer needs the files when it receives the message from itself.Representing internal state changes in this way is an artifice;
  however, the artifice allows us to use a very simple model in which a
  state change occurs when, and only when, a message is delivered.What is an agent?In our model,an agent is an object that sends and
receives messages.Some papers refer to an agent as anactoror as aprocess.What is a simple example of the notation?Seean example of the notation.Local clocks can be synchronized using NTP
  and other protocols. Why do you assume that clocks aren't synchronized?Local clocks can, indeed, be synchronized very accurately; however, we
  do not assume that they are perfect.We use agents' local clocks for evaluating algorithm performance but
not for proving correctness because even a small drift can cause race
conditions.Consider an algorithm in which one agent carries out a computation
starting at 1pm and another agent carries out a computation starting at 2pm.
When we prove the correctness of our algorithms, we allow for the
unlikely possibility that that the agent starting its 
computation at 2pm does so before the agent that starts a 1pm.
For evaluating performance, however, we assume that the agent that starts a 1pm
usually does so before the agent that starts at 2pm.What are other examples of models?The model we use is anactor model.For more general asynchronous models of concurrent systems seeUNITYandTLA.Perhaps the most widely used model isCSP -- Communicating Sequential Processes.An overview ofmodels used in parallel programmingdescribes models used in
shared-memory and distributed computing.NextStates of a Distributed System.Review material for this pageK. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

There are many algorithms in which an agent sends itself aTimeout(T)message where ideally the message would be received approximatelyTseconds after it is sent.
Though we never useTin proving the correctness of algorithms we will use
timeout messages in analyzing their performance.Agents can maintain accurate clocks by using atomic clocks, Precision Time
Protocols (PTP) in local area networks, and Network Time Protocol
(NTP) servers.
Accurate clocks have the property that the time at which an event is
sent, as determined by the sender's clock, is (almost always) earlier
than the time of the event in which the message is received.
We do not, however, rule out the possibility that clocks drift apart
so that the sender's clock is far ahead of the receiver's.Some algorithms for systems with perfect clocks are simpler than those
with imperfect clocks, as we shall see.Model Limitations: Start Up, Shutdown, FailureThe model assumes that all agents and channels are initialized and
then agents start receiving messages.
The model has abarrierbetween the point at which initialization takes place and the point at
which messages are delivered.
The barrier isn't necessary in most algorithms, though it assuming its
existence helps us to focus on more important parts of the algorithm.The model does not specify how termination is detected if the
computation does terminate.
Nor does the notation have primitives for shutting down agents and
channels gracefully so that they don't continue to hold resources
after computation has terminated.
Protocols such as AMQP do have primitives for starting up and shutting
down distributed systems, but we won't discuss them here.We will describe algorithms that execute on faulty systems in which
messages may be lost, duplicated, or delivered out of order, and where
agents may stop forever or halt and restart.
We also describe algorithms with Byzantine agents.
These algorithms are based on models that are different from those
given so far.Model Limitations: Discrete State SpaceThe state space is discrete in most of the algorithms described here.
The state space of some distributed systems has both discrete and
continuous components.
The state space of a fleet of drones has continuous components.Later, we study algorithms in which the state space is
continuous.
Systems with continuous state spaces may have discrete or continuous
state transitions.Model Limitations: StaticThe network of agents and messages in the model is static.
  In contrast, distributed systems evolve; agents and channels may be
  added and deleted; agents may change; channel protocols may be
modified.Model Limitations: Simple ChannelsThe model only considers channels in which messages are delivered in
  the order sent.
  Some distributed systems have channels in which messages may be
duplicated and delivered out of order.Many models deal with channels in which senders are blocked when a
channel gets full.
Our model has no concept of a channel being full.Model Limitations: SummaryModels and notations in this course are not comprehensive -- they do not capture
most aspects of distributed systems.
We use a model that is appropriate for describing the algorithm at
hand.
The choice of a model is an engineering decision.Why use such a simplistic model?The model is indeed simplistic; however, it is adequate for describing
  and reasoning about many of the algorithms described in the first
  part of this course.
  We introduce other models later.
  We use the simplest model adequate for the problem at hand.How many channels are there in a system?In the model there is a channel from every agent to every agent.
  So, if there are \(N\) agents there are \(N^{2}\) channels.
With a thousand agents there are a million channels.
That's a lot of channels!The model allows channels between every pair of agents.
In practice very few of these channels are used.
When we implement an algorithm using AMQP, for instance, we will
declare channels.Can you describe channels in more detail?Ourbasic description of channels is
  found here.Channels in our model are asynchronous (non-blocking) and
  unidirectional.
  A sender can send messages on a channel regardless of how many
  messages have been sent in the past and how many messages have been
  received on that channel.
  The model assumes that the queue of messages in the channel has
  infinite capacity.A channel is unidirectional.
It is represented by a directed edge in a directed graph.
Messages can be sent by an agentPto an agentQalong a channel(P, Q).
AgentQcannot send a message to agentPalong channel(P, Q).A system may have a channel(P, Q)and may or may not have a channel(Q, P).
Some systems have bidirectional channels but our model does not allow
for bidirectional channels.In our model, 
a channel is directed from exactly one agent to exactly one agent.
In some systems, multiple agents can send messages on the same
channel, and multiple agents can receive messages on the same channel.
Our model does not allow for such channels.Can an agent refuse to receive a message?In this model an agent cannot refuse to receive a message.
  If a channel is not empty then a message from the channel can be
  delivered to the agentindependent of the state of the agent.Suppose you want to design an algorithm in which an agentXrefuses to receive messages from an agentZuntil it first receives a message from an agentY.
  How would you use this model?In our model, agentXhas to receive the message fromZin every state ofX.AgentXcan copy the message fromZinto a
  local queue -- a local variable ofX-- and process the
  message only after receiving the message fromY.Can a full channel block a sender from
  sending more messages?In the model a sender is never blocked from sending a message.Our model assumes that channels have unlimited capacity.
We can represent a situation in which channels have limited capacity
in the following way.
An agent has a local output queue of unlimited size.
The agent puts messages into this queue when it cannot send messages
on a channel because the channel is full.
Messages from this queue are sent on the channel when the
channel stops being full.Can you represent systems in which agents
  are created and deleted in the model?The model doesn't have features that allow agents to be created and
  deleted.A work around for the case in which agents are created or deleted a
finite number of times is as follows.
The network of agents in the model is made large enough to include
agents that have not been created as yet and also include agents that
have been deleted.An agent that has not been created is represented by an agent that is
idle, i.e., one is waiting for a message. It never receives a message
until the agent is created. The message is created by the operating
system sending a "create" message to the agent and informing other
agents that this agent has been created.An agent this is deleted is represented in the same way.
The operating system deletes an agent by sending a "delete" message to
the agent.
A deleted agent discards messages that it receives without
taking action.Can an agent process background jobs that
  are interrupted when messages arrive?No, there are no interruptions in the model.
When an agent is executing a receive it is not interrupted.There are systems in which messages have different priorities, and the
arrival of high-priority messages interrupts the execution of
low-priority messages.
Our model does not allow for interruptions.How do you represent channels that are not
  first in first out?We use queues to represent states of first in first out channels, and
  we use other data structures -- such as multisets -- to represent
  states of other types of channels.Later we describe algorithms with different types of channels.
  For example, the state of a channel in which messages may overtake
  each other is a multiset or bag.
  A message is sent on a channel by adding the message to the
  multiset.
  A nonempty multiset may deliver any message in the mutliset to the
  receiving agent.Lossy channels, and channels in which messages have priorities, are
  also modeled by defining channel states appropriately.Can the model represent sequential
  composition of distributed computations?Yes.There are problems in which we would like to start a distributed
computationP; wait forPto finish, and
then start another distributed computationQ.
We want a barrier betweenPandQ.A barrier can be implemented in several ways.
An agent, say acoordinatoragent, sends messages to all agents to startP.
The coordinator then detects termination ofP; we discuss
termination detection algorithms later.
After detecting termination ofPthe coordinator sends
messages to all agents to startQ.Can the model represent agents that make state transitions
  without receiving messages?No, the model does not represent agents that make state transitions
without receiving messages.
The effect of a state transition without receiving a message can be
simulated by an agent sending itself a message and carrying out the
transition when it receives a message from itself.For example, an agent that is computing using a set of files may
  transition to a state in which it no longer needs those files; this
transition occurs without the agent receiving a message.
We model this event by having the agent send itself a message when it
starts using the set of files, and it transits to a state in which it
no longer needs the files when it receives the message from itself.Representing internal state changes in this way is an artifice;
  however, the artifice allows us to use a very simple model in which a
  state change occurs when, and only when, a message is delivered.What is an agent?In our model,an agent is an object that sends and
receives messages.Some papers refer to an agent as anactoror as aprocess.What is a simple example of the notation?Seean example of the notation.Local clocks can be synchronized using NTP
  and other protocols. Why do you assume that clocks aren't synchronized?Local clocks can, indeed, be synchronized very accurately; however, we
  do not assume that they are perfect.We use agents' local clocks for evaluating algorithm performance but
not for proving correctness because even a small drift can cause race
conditions.Consider an algorithm in which one agent carries out a computation
starting at 1pm and another agent carries out a computation starting at 2pm.
When we prove the correctness of our algorithms, we allow for the
unlikely possibility that that the agent starting its 
computation at 2pm does so before the agent that starts a 1pm.
For evaluating performance, however, we assume that the agent that starts a 1pm
usually does so before the agent that starts at 2pm.What are other examples of models?The model we use is anactor model.For more general asynchronous models of concurrent systems seeUNITYandTLA.Perhaps the most widely used model isCSP -- Communicating Sequential Processes.An overview ofmodels used in parallel programmingdescribes models used in
shared-memory and distributed computing.NextStates of a Distributed System.Review material for this pageK. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

Agents can maintain accurate clocks by using atomic clocks, Precision Time
Protocols (PTP) in local area networks, and Network Time Protocol
(NTP) servers.
Accurate clocks have the property that the time at which an event is
sent, as determined by the sender's clock, is (almost always) earlier
than the time of the event in which the message is received.
We do not, however, rule out the possibility that clocks drift apart
so that the sender's clock is far ahead of the receiver's.Some algorithms for systems with perfect clocks are simpler than those
with imperfect clocks, as we shall see.Model Limitations: Start Up, Shutdown, FailureThe model assumes that all agents and channels are initialized and
then agents start receiving messages.
The model has abarrierbetween the point at which initialization takes place and the point at
which messages are delivered.
The barrier isn't necessary in most algorithms, though it assuming its
existence helps us to focus on more important parts of the algorithm.The model does not specify how termination is detected if the
computation does terminate.
Nor does the notation have primitives for shutting down agents and
channels gracefully so that they don't continue to hold resources
after computation has terminated.
Protocols such as AMQP do have primitives for starting up and shutting
down distributed systems, but we won't discuss them here.We will describe algorithms that execute on faulty systems in which
messages may be lost, duplicated, or delivered out of order, and where
agents may stop forever or halt and restart.
We also describe algorithms with Byzantine agents.
These algorithms are based on models that are different from those
given so far.Model Limitations: Discrete State SpaceThe state space is discrete in most of the algorithms described here.
The state space of some distributed systems has both discrete and
continuous components.
The state space of a fleet of drones has continuous components.Later, we study algorithms in which the state space is
continuous.
Systems with continuous state spaces may have discrete or continuous
state transitions.Model Limitations: StaticThe network of agents and messages in the model is static.
  In contrast, distributed systems evolve; agents and channels may be
  added and deleted; agents may change; channel protocols may be
modified.Model Limitations: Simple ChannelsThe model only considers channels in which messages are delivered in
  the order sent.
  Some distributed systems have channels in which messages may be
duplicated and delivered out of order.Many models deal with channels in which senders are blocked when a
channel gets full.
Our model has no concept of a channel being full.Model Limitations: SummaryModels and notations in this course are not comprehensive -- they do not capture
most aspects of distributed systems.
We use a model that is appropriate for describing the algorithm at
hand.
The choice of a model is an engineering decision.Why use such a simplistic model?The model is indeed simplistic; however, it is adequate for describing
  and reasoning about many of the algorithms described in the first
  part of this course.
  We introduce other models later.
  We use the simplest model adequate for the problem at hand.How many channels are there in a system?In the model there is a channel from every agent to every agent.
  So, if there are \(N\) agents there are \(N^{2}\) channels.
With a thousand agents there are a million channels.
That's a lot of channels!The model allows channels between every pair of agents.
In practice very few of these channels are used.
When we implement an algorithm using AMQP, for instance, we will
declare channels.Can you describe channels in more detail?Ourbasic description of channels is
  found here.Channels in our model are asynchronous (non-blocking) and
  unidirectional.
  A sender can send messages on a channel regardless of how many
  messages have been sent in the past and how many messages have been
  received on that channel.
  The model assumes that the queue of messages in the channel has
  infinite capacity.A channel is unidirectional.
It is represented by a directed edge in a directed graph.
Messages can be sent by an agentPto an agentQalong a channel(P, Q).
AgentQcannot send a message to agentPalong channel(P, Q).A system may have a channel(P, Q)and may or may not have a channel(Q, P).
Some systems have bidirectional channels but our model does not allow
for bidirectional channels.In our model, 
a channel is directed from exactly one agent to exactly one agent.
In some systems, multiple agents can send messages on the same
channel, and multiple agents can receive messages on the same channel.
Our model does not allow for such channels.Can an agent refuse to receive a message?In this model an agent cannot refuse to receive a message.
  If a channel is not empty then a message from the channel can be
  delivered to the agentindependent of the state of the agent.Suppose you want to design an algorithm in which an agentXrefuses to receive messages from an agentZuntil it first receives a message from an agentY.
  How would you use this model?In our model, agentXhas to receive the message fromZin every state ofX.AgentXcan copy the message fromZinto a
  local queue -- a local variable ofX-- and process the
  message only after receiving the message fromY.Can a full channel block a sender from
  sending more messages?In the model a sender is never blocked from sending a message.Our model assumes that channels have unlimited capacity.
We can represent a situation in which channels have limited capacity
in the following way.
An agent has a local output queue of unlimited size.
The agent puts messages into this queue when it cannot send messages
on a channel because the channel is full.
Messages from this queue are sent on the channel when the
channel stops being full.Can you represent systems in which agents
  are created and deleted in the model?The model doesn't have features that allow agents to be created and
  deleted.A work around for the case in which agents are created or deleted a
finite number of times is as follows.
The network of agents in the model is made large enough to include
agents that have not been created as yet and also include agents that
have been deleted.An agent that has not been created is represented by an agent that is
idle, i.e., one is waiting for a message. It never receives a message
until the agent is created. The message is created by the operating
system sending a "create" message to the agent and informing other
agents that this agent has been created.An agent this is deleted is represented in the same way.
The operating system deletes an agent by sending a "delete" message to
the agent.
A deleted agent discards messages that it receives without
taking action.Can an agent process background jobs that
  are interrupted when messages arrive?No, there are no interruptions in the model.
When an agent is executing a receive it is not interrupted.There are systems in which messages have different priorities, and the
arrival of high-priority messages interrupts the execution of
low-priority messages.
Our model does not allow for interruptions.How do you represent channels that are not
  first in first out?We use queues to represent states of first in first out channels, and
  we use other data structures -- such as multisets -- to represent
  states of other types of channels.Later we describe algorithms with different types of channels.
  For example, the state of a channel in which messages may overtake
  each other is a multiset or bag.
  A message is sent on a channel by adding the message to the
  multiset.
  A nonempty multiset may deliver any message in the mutliset to the
  receiving agent.Lossy channels, and channels in which messages have priorities, are
  also modeled by defining channel states appropriately.Can the model represent sequential
  composition of distributed computations?Yes.There are problems in which we would like to start a distributed
computationP; wait forPto finish, and
then start another distributed computationQ.
We want a barrier betweenPandQ.A barrier can be implemented in several ways.
An agent, say acoordinatoragent, sends messages to all agents to startP.
The coordinator then detects termination ofP; we discuss
termination detection algorithms later.
After detecting termination ofPthe coordinator sends
messages to all agents to startQ.Can the model represent agents that make state transitions
  without receiving messages?No, the model does not represent agents that make state transitions
without receiving messages.
The effect of a state transition without receiving a message can be
simulated by an agent sending itself a message and carrying out the
transition when it receives a message from itself.For example, an agent that is computing using a set of files may
  transition to a state in which it no longer needs those files; this
transition occurs without the agent receiving a message.
We model this event by having the agent send itself a message when it
starts using the set of files, and it transits to a state in which it
no longer needs the files when it receives the message from itself.Representing internal state changes in this way is an artifice;
  however, the artifice allows us to use a very simple model in which a
  state change occurs when, and only when, a message is delivered.What is an agent?In our model,an agent is an object that sends and
receives messages.Some papers refer to an agent as anactoror as aprocess.What is a simple example of the notation?Seean example of the notation.Local clocks can be synchronized using NTP
  and other protocols. Why do you assume that clocks aren't synchronized?Local clocks can, indeed, be synchronized very accurately; however, we
  do not assume that they are perfect.We use agents' local clocks for evaluating algorithm performance but
not for proving correctness because even a small drift can cause race
conditions.Consider an algorithm in which one agent carries out a computation
starting at 1pm and another agent carries out a computation starting at 2pm.
When we prove the correctness of our algorithms, we allow for the
unlikely possibility that that the agent starting its 
computation at 2pm does so before the agent that starts a 1pm.
For evaluating performance, however, we assume that the agent that starts a 1pm
usually does so before the agent that starts at 2pm.What are other examples of models?The model we use is anactor model.For more general asynchronous models of concurrent systems seeUNITYandTLA.Perhaps the most widely used model isCSP -- Communicating Sequential Processes.An overview ofmodels used in parallel programmingdescribes models used in
shared-memory and distributed computing.NextStates of a Distributed System.Review material for this pageK. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

Some algorithms for systems with perfect clocks are simpler than those
with imperfect clocks, as we shall see.Model Limitations: Start Up, Shutdown, FailureThe model assumes that all agents and channels are initialized and
then agents start receiving messages.
The model has abarrierbetween the point at which initialization takes place and the point at
which messages are delivered.
The barrier isn't necessary in most algorithms, though it assuming its
existence helps us to focus on more important parts of the algorithm.The model does not specify how termination is detected if the
computation does terminate.
Nor does the notation have primitives for shutting down agents and
channels gracefully so that they don't continue to hold resources
after computation has terminated.
Protocols such as AMQP do have primitives for starting up and shutting
down distributed systems, but we won't discuss them here.We will describe algorithms that execute on faulty systems in which
messages may be lost, duplicated, or delivered out of order, and where
agents may stop forever or halt and restart.
We also describe algorithms with Byzantine agents.
These algorithms are based on models that are different from those
given so far.Model Limitations: Discrete State SpaceThe state space is discrete in most of the algorithms described here.
The state space of some distributed systems has both discrete and
continuous components.
The state space of a fleet of drones has continuous components.Later, we study algorithms in which the state space is
continuous.
Systems with continuous state spaces may have discrete or continuous
state transitions.Model Limitations: StaticThe network of agents and messages in the model is static.
  In contrast, distributed systems evolve; agents and channels may be
  added and deleted; agents may change; channel protocols may be
modified.Model Limitations: Simple ChannelsThe model only considers channels in which messages are delivered in
  the order sent.
  Some distributed systems have channels in which messages may be
duplicated and delivered out of order.Many models deal with channels in which senders are blocked when a
channel gets full.
Our model has no concept of a channel being full.Model Limitations: SummaryModels and notations in this course are not comprehensive -- they do not capture
most aspects of distributed systems.
We use a model that is appropriate for describing the algorithm at
hand.
The choice of a model is an engineering decision.Why use such a simplistic model?The model is indeed simplistic; however, it is adequate for describing
  and reasoning about many of the algorithms described in the first
  part of this course.
  We introduce other models later.
  We use the simplest model adequate for the problem at hand.How many channels are there in a system?In the model there is a channel from every agent to every agent.
  So, if there are \(N\) agents there are \(N^{2}\) channels.
With a thousand agents there are a million channels.
That's a lot of channels!The model allows channels between every pair of agents.
In practice very few of these channels are used.
When we implement an algorithm using AMQP, for instance, we will
declare channels.Can you describe channels in more detail?Ourbasic description of channels is
  found here.Channels in our model are asynchronous (non-blocking) and
  unidirectional.
  A sender can send messages on a channel regardless of how many
  messages have been sent in the past and how many messages have been
  received on that channel.
  The model assumes that the queue of messages in the channel has
  infinite capacity.A channel is unidirectional.
It is represented by a directed edge in a directed graph.
Messages can be sent by an agentPto an agentQalong a channel(P, Q).
AgentQcannot send a message to agentPalong channel(P, Q).A system may have a channel(P, Q)and may or may not have a channel(Q, P).
Some systems have bidirectional channels but our model does not allow
for bidirectional channels.In our model, 
a channel is directed from exactly one agent to exactly one agent.
In some systems, multiple agents can send messages on the same
channel, and multiple agents can receive messages on the same channel.
Our model does not allow for such channels.Can an agent refuse to receive a message?In this model an agent cannot refuse to receive a message.
  If a channel is not empty then a message from the channel can be
  delivered to the agentindependent of the state of the agent.Suppose you want to design an algorithm in which an agentXrefuses to receive messages from an agentZuntil it first receives a message from an agentY.
  How would you use this model?In our model, agentXhas to receive the message fromZin every state ofX.AgentXcan copy the message fromZinto a
  local queue -- a local variable ofX-- and process the
  message only after receiving the message fromY.Can a full channel block a sender from
  sending more messages?In the model a sender is never blocked from sending a message.Our model assumes that channels have unlimited capacity.
We can represent a situation in which channels have limited capacity
in the following way.
An agent has a local output queue of unlimited size.
The agent puts messages into this queue when it cannot send messages
on a channel because the channel is full.
Messages from this queue are sent on the channel when the
channel stops being full.Can you represent systems in which agents
  are created and deleted in the model?The model doesn't have features that allow agents to be created and
  deleted.A work around for the case in which agents are created or deleted a
finite number of times is as follows.
The network of agents in the model is made large enough to include
agents that have not been created as yet and also include agents that
have been deleted.An agent that has not been created is represented by an agent that is
idle, i.e., one is waiting for a message. It never receives a message
until the agent is created. The message is created by the operating
system sending a "create" message to the agent and informing other
agents that this agent has been created.An agent this is deleted is represented in the same way.
The operating system deletes an agent by sending a "delete" message to
the agent.
A deleted agent discards messages that it receives without
taking action.Can an agent process background jobs that
  are interrupted when messages arrive?No, there are no interruptions in the model.
When an agent is executing a receive it is not interrupted.There are systems in which messages have different priorities, and the
arrival of high-priority messages interrupts the execution of
low-priority messages.
Our model does not allow for interruptions.How do you represent channels that are not
  first in first out?We use queues to represent states of first in first out channels, and
  we use other data structures -- such as multisets -- to represent
  states of other types of channels.Later we describe algorithms with different types of channels.
  For example, the state of a channel in which messages may overtake
  each other is a multiset or bag.
  A message is sent on a channel by adding the message to the
  multiset.
  A nonempty multiset may deliver any message in the mutliset to the
  receiving agent.Lossy channels, and channels in which messages have priorities, are
  also modeled by defining channel states appropriately.Can the model represent sequential
  composition of distributed computations?Yes.There are problems in which we would like to start a distributed
computationP; wait forPto finish, and
then start another distributed computationQ.
We want a barrier betweenPandQ.A barrier can be implemented in several ways.
An agent, say acoordinatoragent, sends messages to all agents to startP.
The coordinator then detects termination ofP; we discuss
termination detection algorithms later.
After detecting termination ofPthe coordinator sends
messages to all agents to startQ.Can the model represent agents that make state transitions
  without receiving messages?No, the model does not represent agents that make state transitions
without receiving messages.
The effect of a state transition without receiving a message can be
simulated by an agent sending itself a message and carrying out the
transition when it receives a message from itself.For example, an agent that is computing using a set of files may
  transition to a state in which it no longer needs those files; this
transition occurs without the agent receiving a message.
We model this event by having the agent send itself a message when it
starts using the set of files, and it transits to a state in which it
no longer needs the files when it receives the message from itself.Representing internal state changes in this way is an artifice;
  however, the artifice allows us to use a very simple model in which a
  state change occurs when, and only when, a message is delivered.What is an agent?In our model,an agent is an object that sends and
receives messages.Some papers refer to an agent as anactoror as aprocess.What is a simple example of the notation?Seean example of the notation.Local clocks can be synchronized using NTP
  and other protocols. Why do you assume that clocks aren't synchronized?Local clocks can, indeed, be synchronized very accurately; however, we
  do not assume that they are perfect.We use agents' local clocks for evaluating algorithm performance but
not for proving correctness because even a small drift can cause race
conditions.Consider an algorithm in which one agent carries out a computation
starting at 1pm and another agent carries out a computation starting at 2pm.
When we prove the correctness of our algorithms, we allow for the
unlikely possibility that that the agent starting its 
computation at 2pm does so before the agent that starts a 1pm.
For evaluating performance, however, we assume that the agent that starts a 1pm
usually does so before the agent that starts at 2pm.What are other examples of models?The model we use is anactor model.For more general asynchronous models of concurrent systems seeUNITYandTLA.Perhaps the most widely used model isCSP -- Communicating Sequential Processes.An overview ofmodels used in parallel programmingdescribes models used in
shared-memory and distributed computing.NextStates of a Distributed System.Review material for this pageK. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

The model does not specify how termination is detected if the
computation does terminate.
Nor does the notation have primitives for shutting down agents and
channels gracefully so that they don't continue to hold resources
after computation has terminated.
Protocols such as AMQP do have primitives for starting up and shutting
down distributed systems, but we won't discuss them here.We will describe algorithms that execute on faulty systems in which
messages may be lost, duplicated, or delivered out of order, and where
agents may stop forever or halt and restart.
We also describe algorithms with Byzantine agents.
These algorithms are based on models that are different from those
given so far.Model Limitations: Discrete State SpaceThe state space is discrete in most of the algorithms described here.
The state space of some distributed systems has both discrete and
continuous components.
The state space of a fleet of drones has continuous components.Later, we study algorithms in which the state space is
continuous.
Systems with continuous state spaces may have discrete or continuous
state transitions.Model Limitations: StaticThe network of agents and messages in the model is static.
  In contrast, distributed systems evolve; agents and channels may be
  added and deleted; agents may change; channel protocols may be
modified.Model Limitations: Simple ChannelsThe model only considers channels in which messages are delivered in
  the order sent.
  Some distributed systems have channels in which messages may be
duplicated and delivered out of order.Many models deal with channels in which senders are blocked when a
channel gets full.
Our model has no concept of a channel being full.Model Limitations: SummaryModels and notations in this course are not comprehensive -- they do not capture
most aspects of distributed systems.
We use a model that is appropriate for describing the algorithm at
hand.
The choice of a model is an engineering decision.Why use such a simplistic model?The model is indeed simplistic; however, it is adequate for describing
  and reasoning about many of the algorithms described in the first
  part of this course.
  We introduce other models later.
  We use the simplest model adequate for the problem at hand.How many channels are there in a system?In the model there is a channel from every agent to every agent.
  So, if there are \(N\) agents there are \(N^{2}\) channels.
With a thousand agents there are a million channels.
That's a lot of channels!The model allows channels between every pair of agents.
In practice very few of these channels are used.
When we implement an algorithm using AMQP, for instance, we will
declare channels.Can you describe channels in more detail?Ourbasic description of channels is
  found here.Channels in our model are asynchronous (non-blocking) and
  unidirectional.
  A sender can send messages on a channel regardless of how many
  messages have been sent in the past and how many messages have been
  received on that channel.
  The model assumes that the queue of messages in the channel has
  infinite capacity.A channel is unidirectional.
It is represented by a directed edge in a directed graph.
Messages can be sent by an agentPto an agentQalong a channel(P, Q).
AgentQcannot send a message to agentPalong channel(P, Q).A system may have a channel(P, Q)and may or may not have a channel(Q, P).
Some systems have bidirectional channels but our model does not allow
for bidirectional channels.In our model, 
a channel is directed from exactly one agent to exactly one agent.
In some systems, multiple agents can send messages on the same
channel, and multiple agents can receive messages on the same channel.
Our model does not allow for such channels.Can an agent refuse to receive a message?In this model an agent cannot refuse to receive a message.
  If a channel is not empty then a message from the channel can be
  delivered to the agentindependent of the state of the agent.Suppose you want to design an algorithm in which an agentXrefuses to receive messages from an agentZuntil it first receives a message from an agentY.
  How would you use this model?In our model, agentXhas to receive the message fromZin every state ofX.AgentXcan copy the message fromZinto a
  local queue -- a local variable ofX-- and process the
  message only after receiving the message fromY.Can a full channel block a sender from
  sending more messages?In the model a sender is never blocked from sending a message.Our model assumes that channels have unlimited capacity.
We can represent a situation in which channels have limited capacity
in the following way.
An agent has a local output queue of unlimited size.
The agent puts messages into this queue when it cannot send messages
on a channel because the channel is full.
Messages from this queue are sent on the channel when the
channel stops being full.Can you represent systems in which agents
  are created and deleted in the model?The model doesn't have features that allow agents to be created and
  deleted.A work around for the case in which agents are created or deleted a
finite number of times is as follows.
The network of agents in the model is made large enough to include
agents that have not been created as yet and also include agents that
have been deleted.An agent that has not been created is represented by an agent that is
idle, i.e., one is waiting for a message. It never receives a message
until the agent is created. The message is created by the operating
system sending a "create" message to the agent and informing other
agents that this agent has been created.An agent this is deleted is represented in the same way.
The operating system deletes an agent by sending a "delete" message to
the agent.
A deleted agent discards messages that it receives without
taking action.Can an agent process background jobs that
  are interrupted when messages arrive?No, there are no interruptions in the model.
When an agent is executing a receive it is not interrupted.There are systems in which messages have different priorities, and the
arrival of high-priority messages interrupts the execution of
low-priority messages.
Our model does not allow for interruptions.How do you represent channels that are not
  first in first out?We use queues to represent states of first in first out channels, and
  we use other data structures -- such as multisets -- to represent
  states of other types of channels.Later we describe algorithms with different types of channels.
  For example, the state of a channel in which messages may overtake
  each other is a multiset or bag.
  A message is sent on a channel by adding the message to the
  multiset.
  A nonempty multiset may deliver any message in the mutliset to the
  receiving agent.Lossy channels, and channels in which messages have priorities, are
  also modeled by defining channel states appropriately.Can the model represent sequential
  composition of distributed computations?Yes.There are problems in which we would like to start a distributed
computationP; wait forPto finish, and
then start another distributed computationQ.
We want a barrier betweenPandQ.A barrier can be implemented in several ways.
An agent, say acoordinatoragent, sends messages to all agents to startP.
The coordinator then detects termination ofP; we discuss
termination detection algorithms later.
After detecting termination ofPthe coordinator sends
messages to all agents to startQ.Can the model represent agents that make state transitions
  without receiving messages?No, the model does not represent agents that make state transitions
without receiving messages.
The effect of a state transition without receiving a message can be
simulated by an agent sending itself a message and carrying out the
transition when it receives a message from itself.For example, an agent that is computing using a set of files may
  transition to a state in which it no longer needs those files; this
transition occurs without the agent receiving a message.
We model this event by having the agent send itself a message when it
starts using the set of files, and it transits to a state in which it
no longer needs the files when it receives the message from itself.Representing internal state changes in this way is an artifice;
  however, the artifice allows us to use a very simple model in which a
  state change occurs when, and only when, a message is delivered.What is an agent?In our model,an agent is an object that sends and
receives messages.Some papers refer to an agent as anactoror as aprocess.What is a simple example of the notation?Seean example of the notation.Local clocks can be synchronized using NTP
  and other protocols. Why do you assume that clocks aren't synchronized?Local clocks can, indeed, be synchronized very accurately; however, we
  do not assume that they are perfect.We use agents' local clocks for evaluating algorithm performance but
not for proving correctness because even a small drift can cause race
conditions.Consider an algorithm in which one agent carries out a computation
starting at 1pm and another agent carries out a computation starting at 2pm.
When we prove the correctness of our algorithms, we allow for the
unlikely possibility that that the agent starting its 
computation at 2pm does so before the agent that starts a 1pm.
For evaluating performance, however, we assume that the agent that starts a 1pm
usually does so before the agent that starts at 2pm.What are other examples of models?The model we use is anactor model.For more general asynchronous models of concurrent systems seeUNITYandTLA.Perhaps the most widely used model isCSP -- Communicating Sequential Processes.An overview ofmodels used in parallel programmingdescribes models used in
shared-memory and distributed computing.NextStates of a Distributed System.Review material for this pageK. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

We will describe algorithms that execute on faulty systems in which
messages may be lost, duplicated, or delivered out of order, and where
agents may stop forever or halt and restart.
We also describe algorithms with Byzantine agents.
These algorithms are based on models that are different from those
given so far.Model Limitations: Discrete State SpaceThe state space is discrete in most of the algorithms described here.
The state space of some distributed systems has both discrete and
continuous components.
The state space of a fleet of drones has continuous components.Later, we study algorithms in which the state space is
continuous.
Systems with continuous state spaces may have discrete or continuous
state transitions.Model Limitations: StaticThe network of agents and messages in the model is static.
  In contrast, distributed systems evolve; agents and channels may be
  added and deleted; agents may change; channel protocols may be
modified.Model Limitations: Simple ChannelsThe model only considers channels in which messages are delivered in
  the order sent.
  Some distributed systems have channels in which messages may be
duplicated and delivered out of order.Many models deal with channels in which senders are blocked when a
channel gets full.
Our model has no concept of a channel being full.Model Limitations: SummaryModels and notations in this course are not comprehensive -- they do not capture
most aspects of distributed systems.
We use a model that is appropriate for describing the algorithm at
hand.
The choice of a model is an engineering decision.Why use such a simplistic model?The model is indeed simplistic; however, it is adequate for describing
  and reasoning about many of the algorithms described in the first
  part of this course.
  We introduce other models later.
  We use the simplest model adequate for the problem at hand.How many channels are there in a system?In the model there is a channel from every agent to every agent.
  So, if there are \(N\) agents there are \(N^{2}\) channels.
With a thousand agents there are a million channels.
That's a lot of channels!The model allows channels between every pair of agents.
In practice very few of these channels are used.
When we implement an algorithm using AMQP, for instance, we will
declare channels.Can you describe channels in more detail?Ourbasic description of channels is
  found here.Channels in our model are asynchronous (non-blocking) and
  unidirectional.
  A sender can send messages on a channel regardless of how many
  messages have been sent in the past and how many messages have been
  received on that channel.
  The model assumes that the queue of messages in the channel has
  infinite capacity.A channel is unidirectional.
It is represented by a directed edge in a directed graph.
Messages can be sent by an agentPto an agentQalong a channel(P, Q).
AgentQcannot send a message to agentPalong channel(P, Q).A system may have a channel(P, Q)and may or may not have a channel(Q, P).
Some systems have bidirectional channels but our model does not allow
for bidirectional channels.In our model, 
a channel is directed from exactly one agent to exactly one agent.
In some systems, multiple agents can send messages on the same
channel, and multiple agents can receive messages on the same channel.
Our model does not allow for such channels.Can an agent refuse to receive a message?In this model an agent cannot refuse to receive a message.
  If a channel is not empty then a message from the channel can be
  delivered to the agentindependent of the state of the agent.Suppose you want to design an algorithm in which an agentXrefuses to receive messages from an agentZuntil it first receives a message from an agentY.
  How would you use this model?In our model, agentXhas to receive the message fromZin every state ofX.AgentXcan copy the message fromZinto a
  local queue -- a local variable ofX-- and process the
  message only after receiving the message fromY.Can a full channel block a sender from
  sending more messages?In the model a sender is never blocked from sending a message.Our model assumes that channels have unlimited capacity.
We can represent a situation in which channels have limited capacity
in the following way.
An agent has a local output queue of unlimited size.
The agent puts messages into this queue when it cannot send messages
on a channel because the channel is full.
Messages from this queue are sent on the channel when the
channel stops being full.Can you represent systems in which agents
  are created and deleted in the model?The model doesn't have features that allow agents to be created and
  deleted.A work around for the case in which agents are created or deleted a
finite number of times is as follows.
The network of agents in the model is made large enough to include
agents that have not been created as yet and also include agents that
have been deleted.An agent that has not been created is represented by an agent that is
idle, i.e., one is waiting for a message. It never receives a message
until the agent is created. The message is created by the operating
system sending a "create" message to the agent and informing other
agents that this agent has been created.An agent this is deleted is represented in the same way.
The operating system deletes an agent by sending a "delete" message to
the agent.
A deleted agent discards messages that it receives without
taking action.Can an agent process background jobs that
  are interrupted when messages arrive?No, there are no interruptions in the model.
When an agent is executing a receive it is not interrupted.There are systems in which messages have different priorities, and the
arrival of high-priority messages interrupts the execution of
low-priority messages.
Our model does not allow for interruptions.How do you represent channels that are not
  first in first out?We use queues to represent states of first in first out channels, and
  we use other data structures -- such as multisets -- to represent
  states of other types of channels.Later we describe algorithms with different types of channels.
  For example, the state of a channel in which messages may overtake
  each other is a multiset or bag.
  A message is sent on a channel by adding the message to the
  multiset.
  A nonempty multiset may deliver any message in the mutliset to the
  receiving agent.Lossy channels, and channels in which messages have priorities, are
  also modeled by defining channel states appropriately.Can the model represent sequential
  composition of distributed computations?Yes.There are problems in which we would like to start a distributed
computationP; wait forPto finish, and
then start another distributed computationQ.
We want a barrier betweenPandQ.A barrier can be implemented in several ways.
An agent, say acoordinatoragent, sends messages to all agents to startP.
The coordinator then detects termination ofP; we discuss
termination detection algorithms later.
After detecting termination ofPthe coordinator sends
messages to all agents to startQ.Can the model represent agents that make state transitions
  without receiving messages?No, the model does not represent agents that make state transitions
without receiving messages.
The effect of a state transition without receiving a message can be
simulated by an agent sending itself a message and carrying out the
transition when it receives a message from itself.For example, an agent that is computing using a set of files may
  transition to a state in which it no longer needs those files; this
transition occurs without the agent receiving a message.
We model this event by having the agent send itself a message when it
starts using the set of files, and it transits to a state in which it
no longer needs the files when it receives the message from itself.Representing internal state changes in this way is an artifice;
  however, the artifice allows us to use a very simple model in which a
  state change occurs when, and only when, a message is delivered.What is an agent?In our model,an agent is an object that sends and
receives messages.Some papers refer to an agent as anactoror as aprocess.What is a simple example of the notation?Seean example of the notation.Local clocks can be synchronized using NTP
  and other protocols. Why do you assume that clocks aren't synchronized?Local clocks can, indeed, be synchronized very accurately; however, we
  do not assume that they are perfect.We use agents' local clocks for evaluating algorithm performance but
not for proving correctness because even a small drift can cause race
conditions.Consider an algorithm in which one agent carries out a computation
starting at 1pm and another agent carries out a computation starting at 2pm.
When we prove the correctness of our algorithms, we allow for the
unlikely possibility that that the agent starting its 
computation at 2pm does so before the agent that starts a 1pm.
For evaluating performance, however, we assume that the agent that starts a 1pm
usually does so before the agent that starts at 2pm.What are other examples of models?The model we use is anactor model.For more general asynchronous models of concurrent systems seeUNITYandTLA.Perhaps the most widely used model isCSP -- Communicating Sequential Processes.An overview ofmodels used in parallel programmingdescribes models used in
shared-memory and distributed computing.NextStates of a Distributed System.Review material for this pageK. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

Later, we study algorithms in which the state space is
continuous.
Systems with continuous state spaces may have discrete or continuous
state transitions.Model Limitations: StaticThe network of agents and messages in the model is static.
  In contrast, distributed systems evolve; agents and channels may be
  added and deleted; agents may change; channel protocols may be
modified.Model Limitations: Simple ChannelsThe model only considers channels in which messages are delivered in
  the order sent.
  Some distributed systems have channels in which messages may be
duplicated and delivered out of order.Many models deal with channels in which senders are blocked when a
channel gets full.
Our model has no concept of a channel being full.Model Limitations: SummaryModels and notations in this course are not comprehensive -- they do not capture
most aspects of distributed systems.
We use a model that is appropriate for describing the algorithm at
hand.
The choice of a model is an engineering decision.Why use such a simplistic model?The model is indeed simplistic; however, it is adequate for describing
  and reasoning about many of the algorithms described in the first
  part of this course.
  We introduce other models later.
  We use the simplest model adequate for the problem at hand.How many channels are there in a system?In the model there is a channel from every agent to every agent.
  So, if there are \(N\) agents there are \(N^{2}\) channels.
With a thousand agents there are a million channels.
That's a lot of channels!The model allows channels between every pair of agents.
In practice very few of these channels are used.
When we implement an algorithm using AMQP, for instance, we will
declare channels.Can you describe channels in more detail?Ourbasic description of channels is
  found here.Channels in our model are asynchronous (non-blocking) and
  unidirectional.
  A sender can send messages on a channel regardless of how many
  messages have been sent in the past and how many messages have been
  received on that channel.
  The model assumes that the queue of messages in the channel has
  infinite capacity.A channel is unidirectional.
It is represented by a directed edge in a directed graph.
Messages can be sent by an agentPto an agentQalong a channel(P, Q).
AgentQcannot send a message to agentPalong channel(P, Q).A system may have a channel(P, Q)and may or may not have a channel(Q, P).
Some systems have bidirectional channels but our model does not allow
for bidirectional channels.In our model, 
a channel is directed from exactly one agent to exactly one agent.
In some systems, multiple agents can send messages on the same
channel, and multiple agents can receive messages on the same channel.
Our model does not allow for such channels.Can an agent refuse to receive a message?In this model an agent cannot refuse to receive a message.
  If a channel is not empty then a message from the channel can be
  delivered to the agentindependent of the state of the agent.Suppose you want to design an algorithm in which an agentXrefuses to receive messages from an agentZuntil it first receives a message from an agentY.
  How would you use this model?In our model, agentXhas to receive the message fromZin every state ofX.AgentXcan copy the message fromZinto a
  local queue -- a local variable ofX-- and process the
  message only after receiving the message fromY.Can a full channel block a sender from
  sending more messages?In the model a sender is never blocked from sending a message.Our model assumes that channels have unlimited capacity.
We can represent a situation in which channels have limited capacity
in the following way.
An agent has a local output queue of unlimited size.
The agent puts messages into this queue when it cannot send messages
on a channel because the channel is full.
Messages from this queue are sent on the channel when the
channel stops being full.Can you represent systems in which agents
  are created and deleted in the model?The model doesn't have features that allow agents to be created and
  deleted.A work around for the case in which agents are created or deleted a
finite number of times is as follows.
The network of agents in the model is made large enough to include
agents that have not been created as yet and also include agents that
have been deleted.An agent that has not been created is represented by an agent that is
idle, i.e., one is waiting for a message. It never receives a message
until the agent is created. The message is created by the operating
system sending a "create" message to the agent and informing other
agents that this agent has been created.An agent this is deleted is represented in the same way.
The operating system deletes an agent by sending a "delete" message to
the agent.
A deleted agent discards messages that it receives without
taking action.Can an agent process background jobs that
  are interrupted when messages arrive?No, there are no interruptions in the model.
When an agent is executing a receive it is not interrupted.There are systems in which messages have different priorities, and the
arrival of high-priority messages interrupts the execution of
low-priority messages.
Our model does not allow for interruptions.How do you represent channels that are not
  first in first out?We use queues to represent states of first in first out channels, and
  we use other data structures -- such as multisets -- to represent
  states of other types of channels.Later we describe algorithms with different types of channels.
  For example, the state of a channel in which messages may overtake
  each other is a multiset or bag.
  A message is sent on a channel by adding the message to the
  multiset.
  A nonempty multiset may deliver any message in the mutliset to the
  receiving agent.Lossy channels, and channels in which messages have priorities, are
  also modeled by defining channel states appropriately.Can the model represent sequential
  composition of distributed computations?Yes.There are problems in which we would like to start a distributed
computationP; wait forPto finish, and
then start another distributed computationQ.
We want a barrier betweenPandQ.A barrier can be implemented in several ways.
An agent, say acoordinatoragent, sends messages to all agents to startP.
The coordinator then detects termination ofP; we discuss
termination detection algorithms later.
After detecting termination ofPthe coordinator sends
messages to all agents to startQ.Can the model represent agents that make state transitions
  without receiving messages?No, the model does not represent agents that make state transitions
without receiving messages.
The effect of a state transition without receiving a message can be
simulated by an agent sending itself a message and carrying out the
transition when it receives a message from itself.For example, an agent that is computing using a set of files may
  transition to a state in which it no longer needs those files; this
transition occurs without the agent receiving a message.
We model this event by having the agent send itself a message when it
starts using the set of files, and it transits to a state in which it
no longer needs the files when it receives the message from itself.Representing internal state changes in this way is an artifice;
  however, the artifice allows us to use a very simple model in which a
  state change occurs when, and only when, a message is delivered.What is an agent?In our model,an agent is an object that sends and
receives messages.Some papers refer to an agent as anactoror as aprocess.What is a simple example of the notation?Seean example of the notation.Local clocks can be synchronized using NTP
  and other protocols. Why do you assume that clocks aren't synchronized?Local clocks can, indeed, be synchronized very accurately; however, we
  do not assume that they are perfect.We use agents' local clocks for evaluating algorithm performance but
not for proving correctness because even a small drift can cause race
conditions.Consider an algorithm in which one agent carries out a computation
starting at 1pm and another agent carries out a computation starting at 2pm.
When we prove the correctness of our algorithms, we allow for the
unlikely possibility that that the agent starting its 
computation at 2pm does so before the agent that starts a 1pm.
For evaluating performance, however, we assume that the agent that starts a 1pm
usually does so before the agent that starts at 2pm.What are other examples of models?The model we use is anactor model.For more general asynchronous models of concurrent systems seeUNITYandTLA.Perhaps the most widely used model isCSP -- Communicating Sequential Processes.An overview ofmodels used in parallel programmingdescribes models used in
shared-memory and distributed computing.NextStates of a Distributed System.Review material for this pageK. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

Many models deal with channels in which senders are blocked when a
channel gets full.
Our model has no concept of a channel being full.Model Limitations: SummaryModels and notations in this course are not comprehensive -- they do not capture
most aspects of distributed systems.
We use a model that is appropriate for describing the algorithm at
hand.
The choice of a model is an engineering decision.Why use such a simplistic model?The model is indeed simplistic; however, it is adequate for describing
  and reasoning about many of the algorithms described in the first
  part of this course.
  We introduce other models later.
  We use the simplest model adequate for the problem at hand.How many channels are there in a system?In the model there is a channel from every agent to every agent.
  So, if there are \(N\) agents there are \(N^{2}\) channels.
With a thousand agents there are a million channels.
That's a lot of channels!The model allows channels between every pair of agents.
In practice very few of these channels are used.
When we implement an algorithm using AMQP, for instance, we will
declare channels.Can you describe channels in more detail?Ourbasic description of channels is
  found here.Channels in our model are asynchronous (non-blocking) and
  unidirectional.
  A sender can send messages on a channel regardless of how many
  messages have been sent in the past and how many messages have been
  received on that channel.
  The model assumes that the queue of messages in the channel has
  infinite capacity.A channel is unidirectional.
It is represented by a directed edge in a directed graph.
Messages can be sent by an agentPto an agentQalong a channel(P, Q).
AgentQcannot send a message to agentPalong channel(P, Q).A system may have a channel(P, Q)and may or may not have a channel(Q, P).
Some systems have bidirectional channels but our model does not allow
for bidirectional channels.In our model, 
a channel is directed from exactly one agent to exactly one agent.
In some systems, multiple agents can send messages on the same
channel, and multiple agents can receive messages on the same channel.
Our model does not allow for such channels.Can an agent refuse to receive a message?In this model an agent cannot refuse to receive a message.
  If a channel is not empty then a message from the channel can be
  delivered to the agentindependent of the state of the agent.Suppose you want to design an algorithm in which an agentXrefuses to receive messages from an agentZuntil it first receives a message from an agentY.
  How would you use this model?In our model, agentXhas to receive the message fromZin every state ofX.AgentXcan copy the message fromZinto a
  local queue -- a local variable ofX-- and process the
  message only after receiving the message fromY.Can a full channel block a sender from
  sending more messages?In the model a sender is never blocked from sending a message.Our model assumes that channels have unlimited capacity.
We can represent a situation in which channels have limited capacity
in the following way.
An agent has a local output queue of unlimited size.
The agent puts messages into this queue when it cannot send messages
on a channel because the channel is full.
Messages from this queue are sent on the channel when the
channel stops being full.Can you represent systems in which agents
  are created and deleted in the model?The model doesn't have features that allow agents to be created and
  deleted.A work around for the case in which agents are created or deleted a
finite number of times is as follows.
The network of agents in the model is made large enough to include
agents that have not been created as yet and also include agents that
have been deleted.An agent that has not been created is represented by an agent that is
idle, i.e., one is waiting for a message. It never receives a message
until the agent is created. The message is created by the operating
system sending a "create" message to the agent and informing other
agents that this agent has been created.An agent this is deleted is represented in the same way.
The operating system deletes an agent by sending a "delete" message to
the agent.
A deleted agent discards messages that it receives without
taking action.Can an agent process background jobs that
  are interrupted when messages arrive?No, there are no interruptions in the model.
When an agent is executing a receive it is not interrupted.There are systems in which messages have different priorities, and the
arrival of high-priority messages interrupts the execution of
low-priority messages.
Our model does not allow for interruptions.How do you represent channels that are not
  first in first out?We use queues to represent states of first in first out channels, and
  we use other data structures -- such as multisets -- to represent
  states of other types of channels.Later we describe algorithms with different types of channels.
  For example, the state of a channel in which messages may overtake
  each other is a multiset or bag.
  A message is sent on a channel by adding the message to the
  multiset.
  A nonempty multiset may deliver any message in the mutliset to the
  receiving agent.Lossy channels, and channels in which messages have priorities, are
  also modeled by defining channel states appropriately.Can the model represent sequential
  composition of distributed computations?Yes.There are problems in which we would like to start a distributed
computationP; wait forPto finish, and
then start another distributed computationQ.
We want a barrier betweenPandQ.A barrier can be implemented in several ways.
An agent, say acoordinatoragent, sends messages to all agents to startP.
The coordinator then detects termination ofP; we discuss
termination detection algorithms later.
After detecting termination ofPthe coordinator sends
messages to all agents to startQ.Can the model represent agents that make state transitions
  without receiving messages?No, the model does not represent agents that make state transitions
without receiving messages.
The effect of a state transition without receiving a message can be
simulated by an agent sending itself a message and carrying out the
transition when it receives a message from itself.For example, an agent that is computing using a set of files may
  transition to a state in which it no longer needs those files; this
transition occurs without the agent receiving a message.
We model this event by having the agent send itself a message when it
starts using the set of files, and it transits to a state in which it
no longer needs the files when it receives the message from itself.Representing internal state changes in this way is an artifice;
  however, the artifice allows us to use a very simple model in which a
  state change occurs when, and only when, a message is delivered.What is an agent?In our model,an agent is an object that sends and
receives messages.Some papers refer to an agent as anactoror as aprocess.What is a simple example of the notation?Seean example of the notation.Local clocks can be synchronized using NTP
  and other protocols. Why do you assume that clocks aren't synchronized?Local clocks can, indeed, be synchronized very accurately; however, we
  do not assume that they are perfect.We use agents' local clocks for evaluating algorithm performance but
not for proving correctness because even a small drift can cause race
conditions.Consider an algorithm in which one agent carries out a computation
starting at 1pm and another agent carries out a computation starting at 2pm.
When we prove the correctness of our algorithms, we allow for the
unlikely possibility that that the agent starting its 
computation at 2pm does so before the agent that starts a 1pm.
For evaluating performance, however, we assume that the agent that starts a 1pm
usually does so before the agent that starts at 2pm.What are other examples of models?The model we use is anactor model.For more general asynchronous models of concurrent systems seeUNITYandTLA.Perhaps the most widely used model isCSP -- Communicating Sequential Processes.An overview ofmodels used in parallel programmingdescribes models used in
shared-memory and distributed computing.NextStates of a Distributed System.Review material for this pageK. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

The model allows channels between every pair of agents.
In practice very few of these channels are used.
When we implement an algorithm using AMQP, for instance, we will
declare channels.Can you describe channels in more detail?Ourbasic description of channels is
  found here.Channels in our model are asynchronous (non-blocking) and
  unidirectional.
  A sender can send messages on a channel regardless of how many
  messages have been sent in the past and how many messages have been
  received on that channel.
  The model assumes that the queue of messages in the channel has
  infinite capacity.A channel is unidirectional.
It is represented by a directed edge in a directed graph.
Messages can be sent by an agentPto an agentQalong a channel(P, Q).
AgentQcannot send a message to agentPalong channel(P, Q).A system may have a channel(P, Q)and may or may not have a channel(Q, P).
Some systems have bidirectional channels but our model does not allow
for bidirectional channels.In our model, 
a channel is directed from exactly one agent to exactly one agent.
In some systems, multiple agents can send messages on the same
channel, and multiple agents can receive messages on the same channel.
Our model does not allow for such channels.Can an agent refuse to receive a message?In this model an agent cannot refuse to receive a message.
  If a channel is not empty then a message from the channel can be
  delivered to the agentindependent of the state of the agent.Suppose you want to design an algorithm in which an agentXrefuses to receive messages from an agentZuntil it first receives a message from an agentY.
  How would you use this model?In our model, agentXhas to receive the message fromZin every state ofX.AgentXcan copy the message fromZinto a
  local queue -- a local variable ofX-- and process the
  message only after receiving the message fromY.Can a full channel block a sender from
  sending more messages?In the model a sender is never blocked from sending a message.Our model assumes that channels have unlimited capacity.
We can represent a situation in which channels have limited capacity
in the following way.
An agent has a local output queue of unlimited size.
The agent puts messages into this queue when it cannot send messages
on a channel because the channel is full.
Messages from this queue are sent on the channel when the
channel stops being full.Can you represent systems in which agents
  are created and deleted in the model?The model doesn't have features that allow agents to be created and
  deleted.A work around for the case in which agents are created or deleted a
finite number of times is as follows.
The network of agents in the model is made large enough to include
agents that have not been created as yet and also include agents that
have been deleted.An agent that has not been created is represented by an agent that is
idle, i.e., one is waiting for a message. It never receives a message
until the agent is created. The message is created by the operating
system sending a "create" message to the agent and informing other
agents that this agent has been created.An agent this is deleted is represented in the same way.
The operating system deletes an agent by sending a "delete" message to
the agent.
A deleted agent discards messages that it receives without
taking action.Can an agent process background jobs that
  are interrupted when messages arrive?No, there are no interruptions in the model.
When an agent is executing a receive it is not interrupted.There are systems in which messages have different priorities, and the
arrival of high-priority messages interrupts the execution of
low-priority messages.
Our model does not allow for interruptions.How do you represent channels that are not
  first in first out?We use queues to represent states of first in first out channels, and
  we use other data structures -- such as multisets -- to represent
  states of other types of channels.Later we describe algorithms with different types of channels.
  For example, the state of a channel in which messages may overtake
  each other is a multiset or bag.
  A message is sent on a channel by adding the message to the
  multiset.
  A nonempty multiset may deliver any message in the mutliset to the
  receiving agent.Lossy channels, and channels in which messages have priorities, are
  also modeled by defining channel states appropriately.Can the model represent sequential
  composition of distributed computations?Yes.There are problems in which we would like to start a distributed
computationP; wait forPto finish, and
then start another distributed computationQ.
We want a barrier betweenPandQ.A barrier can be implemented in several ways.
An agent, say acoordinatoragent, sends messages to all agents to startP.
The coordinator then detects termination ofP; we discuss
termination detection algorithms later.
After detecting termination ofPthe coordinator sends
messages to all agents to startQ.Can the model represent agents that make state transitions
  without receiving messages?No, the model does not represent agents that make state transitions
without receiving messages.
The effect of a state transition without receiving a message can be
simulated by an agent sending itself a message and carrying out the
transition when it receives a message from itself.For example, an agent that is computing using a set of files may
  transition to a state in which it no longer needs those files; this
transition occurs without the agent receiving a message.
We model this event by having the agent send itself a message when it
starts using the set of files, and it transits to a state in which it
no longer needs the files when it receives the message from itself.Representing internal state changes in this way is an artifice;
  however, the artifice allows us to use a very simple model in which a
  state change occurs when, and only when, a message is delivered.What is an agent?In our model,an agent is an object that sends and
receives messages.Some papers refer to an agent as anactoror as aprocess.What is a simple example of the notation?Seean example of the notation.Local clocks can be synchronized using NTP
  and other protocols. Why do you assume that clocks aren't synchronized?Local clocks can, indeed, be synchronized very accurately; however, we
  do not assume that they are perfect.We use agents' local clocks for evaluating algorithm performance but
not for proving correctness because even a small drift can cause race
conditions.Consider an algorithm in which one agent carries out a computation
starting at 1pm and another agent carries out a computation starting at 2pm.
When we prove the correctness of our algorithms, we allow for the
unlikely possibility that that the agent starting its 
computation at 2pm does so before the agent that starts a 1pm.
For evaluating performance, however, we assume that the agent that starts a 1pm
usually does so before the agent that starts at 2pm.What are other examples of models?The model we use is anactor model.For more general asynchronous models of concurrent systems seeUNITYandTLA.Perhaps the most widely used model isCSP -- Communicating Sequential Processes.An overview ofmodels used in parallel programmingdescribes models used in
shared-memory and distributed computing.NextStates of a Distributed System.Review material for this pageK. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

A channel is unidirectional.
It is represented by a directed edge in a directed graph.
Messages can be sent by an agentPto an agentQalong a channel(P, Q).
AgentQcannot send a message to agentPalong channel(P, Q).A system may have a channel(P, Q)and may or may not have a channel(Q, P).
Some systems have bidirectional channels but our model does not allow
for bidirectional channels.In our model, 
a channel is directed from exactly one agent to exactly one agent.
In some systems, multiple agents can send messages on the same
channel, and multiple agents can receive messages on the same channel.
Our model does not allow for such channels.Can an agent refuse to receive a message?In this model an agent cannot refuse to receive a message.
  If a channel is not empty then a message from the channel can be
  delivered to the agentindependent of the state of the agent.Suppose you want to design an algorithm in which an agentXrefuses to receive messages from an agentZuntil it first receives a message from an agentY.
  How would you use this model?In our model, agentXhas to receive the message fromZin every state ofX.AgentXcan copy the message fromZinto a
  local queue -- a local variable ofX-- and process the
  message only after receiving the message fromY.Can a full channel block a sender from
  sending more messages?In the model a sender is never blocked from sending a message.Our model assumes that channels have unlimited capacity.
We can represent a situation in which channels have limited capacity
in the following way.
An agent has a local output queue of unlimited size.
The agent puts messages into this queue when it cannot send messages
on a channel because the channel is full.
Messages from this queue are sent on the channel when the
channel stops being full.Can you represent systems in which agents
  are created and deleted in the model?The model doesn't have features that allow agents to be created and
  deleted.A work around for the case in which agents are created or deleted a
finite number of times is as follows.
The network of agents in the model is made large enough to include
agents that have not been created as yet and also include agents that
have been deleted.An agent that has not been created is represented by an agent that is
idle, i.e., one is waiting for a message. It never receives a message
until the agent is created. The message is created by the operating
system sending a "create" message to the agent and informing other
agents that this agent has been created.An agent this is deleted is represented in the same way.
The operating system deletes an agent by sending a "delete" message to
the agent.
A deleted agent discards messages that it receives without
taking action.Can an agent process background jobs that
  are interrupted when messages arrive?No, there are no interruptions in the model.
When an agent is executing a receive it is not interrupted.There are systems in which messages have different priorities, and the
arrival of high-priority messages interrupts the execution of
low-priority messages.
Our model does not allow for interruptions.How do you represent channels that are not
  first in first out?We use queues to represent states of first in first out channels, and
  we use other data structures -- such as multisets -- to represent
  states of other types of channels.Later we describe algorithms with different types of channels.
  For example, the state of a channel in which messages may overtake
  each other is a multiset or bag.
  A message is sent on a channel by adding the message to the
  multiset.
  A nonempty multiset may deliver any message in the mutliset to the
  receiving agent.Lossy channels, and channels in which messages have priorities, are
  also modeled by defining channel states appropriately.Can the model represent sequential
  composition of distributed computations?Yes.There are problems in which we would like to start a distributed
computationP; wait forPto finish, and
then start another distributed computationQ.
We want a barrier betweenPandQ.A barrier can be implemented in several ways.
An agent, say acoordinatoragent, sends messages to all agents to startP.
The coordinator then detects termination ofP; we discuss
termination detection algorithms later.
After detecting termination ofPthe coordinator sends
messages to all agents to startQ.Can the model represent agents that make state transitions
  without receiving messages?No, the model does not represent agents that make state transitions
without receiving messages.
The effect of a state transition without receiving a message can be
simulated by an agent sending itself a message and carrying out the
transition when it receives a message from itself.For example, an agent that is computing using a set of files may
  transition to a state in which it no longer needs those files; this
transition occurs without the agent receiving a message.
We model this event by having the agent send itself a message when it
starts using the set of files, and it transits to a state in which it
no longer needs the files when it receives the message from itself.Representing internal state changes in this way is an artifice;
  however, the artifice allows us to use a very simple model in which a
  state change occurs when, and only when, a message is delivered.What is an agent?In our model,an agent is an object that sends and
receives messages.Some papers refer to an agent as anactoror as aprocess.What is a simple example of the notation?Seean example of the notation.Local clocks can be synchronized using NTP
  and other protocols. Why do you assume that clocks aren't synchronized?Local clocks can, indeed, be synchronized very accurately; however, we
  do not assume that they are perfect.We use agents' local clocks for evaluating algorithm performance but
not for proving correctness because even a small drift can cause race
conditions.Consider an algorithm in which one agent carries out a computation
starting at 1pm and another agent carries out a computation starting at 2pm.
When we prove the correctness of our algorithms, we allow for the
unlikely possibility that that the agent starting its 
computation at 2pm does so before the agent that starts a 1pm.
For evaluating performance, however, we assume that the agent that starts a 1pm
usually does so before the agent that starts at 2pm.What are other examples of models?The model we use is anactor model.For more general asynchronous models of concurrent systems seeUNITYandTLA.Perhaps the most widely used model isCSP -- Communicating Sequential Processes.An overview ofmodels used in parallel programmingdescribes models used in
shared-memory and distributed computing.NextStates of a Distributed System.Review material for this pageK. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

A system may have a channel(P, Q)and may or may not have a channel(Q, P).
Some systems have bidirectional channels but our model does not allow
for bidirectional channels.In our model, 
a channel is directed from exactly one agent to exactly one agent.
In some systems, multiple agents can send messages on the same
channel, and multiple agents can receive messages on the same channel.
Our model does not allow for such channels.Can an agent refuse to receive a message?In this model an agent cannot refuse to receive a message.
  If a channel is not empty then a message from the channel can be
  delivered to the agentindependent of the state of the agent.Suppose you want to design an algorithm in which an agentXrefuses to receive messages from an agentZuntil it first receives a message from an agentY.
  How would you use this model?In our model, agentXhas to receive the message fromZin every state ofX.AgentXcan copy the message fromZinto a
  local queue -- a local variable ofX-- and process the
  message only after receiving the message fromY.Can a full channel block a sender from
  sending more messages?In the model a sender is never blocked from sending a message.Our model assumes that channels have unlimited capacity.
We can represent a situation in which channels have limited capacity
in the following way.
An agent has a local output queue of unlimited size.
The agent puts messages into this queue when it cannot send messages
on a channel because the channel is full.
Messages from this queue are sent on the channel when the
channel stops being full.Can you represent systems in which agents
  are created and deleted in the model?The model doesn't have features that allow agents to be created and
  deleted.A work around for the case in which agents are created or deleted a
finite number of times is as follows.
The network of agents in the model is made large enough to include
agents that have not been created as yet and also include agents that
have been deleted.An agent that has not been created is represented by an agent that is
idle, i.e., one is waiting for a message. It never receives a message
until the agent is created. The message is created by the operating
system sending a "create" message to the agent and informing other
agents that this agent has been created.An agent this is deleted is represented in the same way.
The operating system deletes an agent by sending a "delete" message to
the agent.
A deleted agent discards messages that it receives without
taking action.Can an agent process background jobs that
  are interrupted when messages arrive?No, there are no interruptions in the model.
When an agent is executing a receive it is not interrupted.There are systems in which messages have different priorities, and the
arrival of high-priority messages interrupts the execution of
low-priority messages.
Our model does not allow for interruptions.How do you represent channels that are not
  first in first out?We use queues to represent states of first in first out channels, and
  we use other data structures -- such as multisets -- to represent
  states of other types of channels.Later we describe algorithms with different types of channels.
  For example, the state of a channel in which messages may overtake
  each other is a multiset or bag.
  A message is sent on a channel by adding the message to the
  multiset.
  A nonempty multiset may deliver any message in the mutliset to the
  receiving agent.Lossy channels, and channels in which messages have priorities, are
  also modeled by defining channel states appropriately.Can the model represent sequential
  composition of distributed computations?Yes.There are problems in which we would like to start a distributed
computationP; wait forPto finish, and
then start another distributed computationQ.
We want a barrier betweenPandQ.A barrier can be implemented in several ways.
An agent, say acoordinatoragent, sends messages to all agents to startP.
The coordinator then detects termination ofP; we discuss
termination detection algorithms later.
After detecting termination ofPthe coordinator sends
messages to all agents to startQ.Can the model represent agents that make state transitions
  without receiving messages?No, the model does not represent agents that make state transitions
without receiving messages.
The effect of a state transition without receiving a message can be
simulated by an agent sending itself a message and carrying out the
transition when it receives a message from itself.For example, an agent that is computing using a set of files may
  transition to a state in which it no longer needs those files; this
transition occurs without the agent receiving a message.
We model this event by having the agent send itself a message when it
starts using the set of files, and it transits to a state in which it
no longer needs the files when it receives the message from itself.Representing internal state changes in this way is an artifice;
  however, the artifice allows us to use a very simple model in which a
  state change occurs when, and only when, a message is delivered.What is an agent?In our model,an agent is an object that sends and
receives messages.Some papers refer to an agent as anactoror as aprocess.What is a simple example of the notation?Seean example of the notation.Local clocks can be synchronized using NTP
  and other protocols. Why do you assume that clocks aren't synchronized?Local clocks can, indeed, be synchronized very accurately; however, we
  do not assume that they are perfect.We use agents' local clocks for evaluating algorithm performance but
not for proving correctness because even a small drift can cause race
conditions.Consider an algorithm in which one agent carries out a computation
starting at 1pm and another agent carries out a computation starting at 2pm.
When we prove the correctness of our algorithms, we allow for the
unlikely possibility that that the agent starting its 
computation at 2pm does so before the agent that starts a 1pm.
For evaluating performance, however, we assume that the agent that starts a 1pm
usually does so before the agent that starts at 2pm.What are other examples of models?The model we use is anactor model.For more general asynchronous models of concurrent systems seeUNITYandTLA.Perhaps the most widely used model isCSP -- Communicating Sequential Processes.An overview ofmodels used in parallel programmingdescribes models used in
shared-memory and distributed computing.NextStates of a Distributed System.Review material for this pageK. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

In our model, 
a channel is directed from exactly one agent to exactly one agent.
In some systems, multiple agents can send messages on the same
channel, and multiple agents can receive messages on the same channel.
Our model does not allow for such channels.Can an agent refuse to receive a message?In this model an agent cannot refuse to receive a message.
  If a channel is not empty then a message from the channel can be
  delivered to the agentindependent of the state of the agent.Suppose you want to design an algorithm in which an agentXrefuses to receive messages from an agentZuntil it first receives a message from an agentY.
  How would you use this model?In our model, agentXhas to receive the message fromZin every state ofX.AgentXcan copy the message fromZinto a
  local queue -- a local variable ofX-- and process the
  message only after receiving the message fromY.Can a full channel block a sender from
  sending more messages?In the model a sender is never blocked from sending a message.Our model assumes that channels have unlimited capacity.
We can represent a situation in which channels have limited capacity
in the following way.
An agent has a local output queue of unlimited size.
The agent puts messages into this queue when it cannot send messages
on a channel because the channel is full.
Messages from this queue are sent on the channel when the
channel stops being full.Can you represent systems in which agents
  are created and deleted in the model?The model doesn't have features that allow agents to be created and
  deleted.A work around for the case in which agents are created or deleted a
finite number of times is as follows.
The network of agents in the model is made large enough to include
agents that have not been created as yet and also include agents that
have been deleted.An agent that has not been created is represented by an agent that is
idle, i.e., one is waiting for a message. It never receives a message
until the agent is created. The message is created by the operating
system sending a "create" message to the agent and informing other
agents that this agent has been created.An agent this is deleted is represented in the same way.
The operating system deletes an agent by sending a "delete" message to
the agent.
A deleted agent discards messages that it receives without
taking action.Can an agent process background jobs that
  are interrupted when messages arrive?No, there are no interruptions in the model.
When an agent is executing a receive it is not interrupted.There are systems in which messages have different priorities, and the
arrival of high-priority messages interrupts the execution of
low-priority messages.
Our model does not allow for interruptions.How do you represent channels that are not
  first in first out?We use queues to represent states of first in first out channels, and
  we use other data structures -- such as multisets -- to represent
  states of other types of channels.Later we describe algorithms with different types of channels.
  For example, the state of a channel in which messages may overtake
  each other is a multiset or bag.
  A message is sent on a channel by adding the message to the
  multiset.
  A nonempty multiset may deliver any message in the mutliset to the
  receiving agent.Lossy channels, and channels in which messages have priorities, are
  also modeled by defining channel states appropriately.Can the model represent sequential
  composition of distributed computations?Yes.There are problems in which we would like to start a distributed
computationP; wait forPto finish, and
then start another distributed computationQ.
We want a barrier betweenPandQ.A barrier can be implemented in several ways.
An agent, say acoordinatoragent, sends messages to all agents to startP.
The coordinator then detects termination ofP; we discuss
termination detection algorithms later.
After detecting termination ofPthe coordinator sends
messages to all agents to startQ.Can the model represent agents that make state transitions
  without receiving messages?No, the model does not represent agents that make state transitions
without receiving messages.
The effect of a state transition without receiving a message can be
simulated by an agent sending itself a message and carrying out the
transition when it receives a message from itself.For example, an agent that is computing using a set of files may
  transition to a state in which it no longer needs those files; this
transition occurs without the agent receiving a message.
We model this event by having the agent send itself a message when it
starts using the set of files, and it transits to a state in which it
no longer needs the files when it receives the message from itself.Representing internal state changes in this way is an artifice;
  however, the artifice allows us to use a very simple model in which a
  state change occurs when, and only when, a message is delivered.What is an agent?In our model,an agent is an object that sends and
receives messages.Some papers refer to an agent as anactoror as aprocess.What is a simple example of the notation?Seean example of the notation.Local clocks can be synchronized using NTP
  and other protocols. Why do you assume that clocks aren't synchronized?Local clocks can, indeed, be synchronized very accurately; however, we
  do not assume that they are perfect.We use agents' local clocks for evaluating algorithm performance but
not for proving correctness because even a small drift can cause race
conditions.Consider an algorithm in which one agent carries out a computation
starting at 1pm and another agent carries out a computation starting at 2pm.
When we prove the correctness of our algorithms, we allow for the
unlikely possibility that that the agent starting its 
computation at 2pm does so before the agent that starts a 1pm.
For evaluating performance, however, we assume that the agent that starts a 1pm
usually does so before the agent that starts at 2pm.What are other examples of models?The model we use is anactor model.For more general asynchronous models of concurrent systems seeUNITYandTLA.Perhaps the most widely used model isCSP -- Communicating Sequential Processes.An overview ofmodels used in parallel programmingdescribes models used in
shared-memory and distributed computing.NextStates of a Distributed System.Review material for this pageK. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

Suppose you want to design an algorithm in which an agentXrefuses to receive messages from an agentZuntil it first receives a message from an agentY.
  How would you use this model?In our model, agentXhas to receive the message fromZin every state ofX.AgentXcan copy the message fromZinto a
  local queue -- a local variable ofX-- and process the
  message only after receiving the message fromY.Can a full channel block a sender from
  sending more messages?In the model a sender is never blocked from sending a message.Our model assumes that channels have unlimited capacity.
We can represent a situation in which channels have limited capacity
in the following way.
An agent has a local output queue of unlimited size.
The agent puts messages into this queue when it cannot send messages
on a channel because the channel is full.
Messages from this queue are sent on the channel when the
channel stops being full.Can you represent systems in which agents
  are created and deleted in the model?The model doesn't have features that allow agents to be created and
  deleted.A work around for the case in which agents are created or deleted a
finite number of times is as follows.
The network of agents in the model is made large enough to include
agents that have not been created as yet and also include agents that
have been deleted.An agent that has not been created is represented by an agent that is
idle, i.e., one is waiting for a message. It never receives a message
until the agent is created. The message is created by the operating
system sending a "create" message to the agent and informing other
agents that this agent has been created.An agent this is deleted is represented in the same way.
The operating system deletes an agent by sending a "delete" message to
the agent.
A deleted agent discards messages that it receives without
taking action.Can an agent process background jobs that
  are interrupted when messages arrive?No, there are no interruptions in the model.
When an agent is executing a receive it is not interrupted.There are systems in which messages have different priorities, and the
arrival of high-priority messages interrupts the execution of
low-priority messages.
Our model does not allow for interruptions.How do you represent channels that are not
  first in first out?We use queues to represent states of first in first out channels, and
  we use other data structures -- such as multisets -- to represent
  states of other types of channels.Later we describe algorithms with different types of channels.
  For example, the state of a channel in which messages may overtake
  each other is a multiset or bag.
  A message is sent on a channel by adding the message to the
  multiset.
  A nonempty multiset may deliver any message in the mutliset to the
  receiving agent.Lossy channels, and channels in which messages have priorities, are
  also modeled by defining channel states appropriately.Can the model represent sequential
  composition of distributed computations?Yes.There are problems in which we would like to start a distributed
computationP; wait forPto finish, and
then start another distributed computationQ.
We want a barrier betweenPandQ.A barrier can be implemented in several ways.
An agent, say acoordinatoragent, sends messages to all agents to startP.
The coordinator then detects termination ofP; we discuss
termination detection algorithms later.
After detecting termination ofPthe coordinator sends
messages to all agents to startQ.Can the model represent agents that make state transitions
  without receiving messages?No, the model does not represent agents that make state transitions
without receiving messages.
The effect of a state transition without receiving a message can be
simulated by an agent sending itself a message and carrying out the
transition when it receives a message from itself.For example, an agent that is computing using a set of files may
  transition to a state in which it no longer needs those files; this
transition occurs without the agent receiving a message.
We model this event by having the agent send itself a message when it
starts using the set of files, and it transits to a state in which it
no longer needs the files when it receives the message from itself.Representing internal state changes in this way is an artifice;
  however, the artifice allows us to use a very simple model in which a
  state change occurs when, and only when, a message is delivered.What is an agent?In our model,an agent is an object that sends and
receives messages.Some papers refer to an agent as anactoror as aprocess.What is a simple example of the notation?Seean example of the notation.Local clocks can be synchronized using NTP
  and other protocols. Why do you assume that clocks aren't synchronized?Local clocks can, indeed, be synchronized very accurately; however, we
  do not assume that they are perfect.We use agents' local clocks for evaluating algorithm performance but
not for proving correctness because even a small drift can cause race
conditions.Consider an algorithm in which one agent carries out a computation
starting at 1pm and another agent carries out a computation starting at 2pm.
When we prove the correctness of our algorithms, we allow for the
unlikely possibility that that the agent starting its 
computation at 2pm does so before the agent that starts a 1pm.
For evaluating performance, however, we assume that the agent that starts a 1pm
usually does so before the agent that starts at 2pm.What are other examples of models?The model we use is anactor model.For more general asynchronous models of concurrent systems seeUNITYandTLA.Perhaps the most widely used model isCSP -- Communicating Sequential Processes.An overview ofmodels used in parallel programmingdescribes models used in
shared-memory and distributed computing.NextStates of a Distributed System.Review material for this pageK. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

In our model, agentXhas to receive the message fromZin every state ofX.AgentXcan copy the message fromZinto a
  local queue -- a local variable ofX-- and process the
  message only after receiving the message fromY.Can a full channel block a sender from
  sending more messages?In the model a sender is never blocked from sending a message.Our model assumes that channels have unlimited capacity.
We can represent a situation in which channels have limited capacity
in the following way.
An agent has a local output queue of unlimited size.
The agent puts messages into this queue when it cannot send messages
on a channel because the channel is full.
Messages from this queue are sent on the channel when the
channel stops being full.Can you represent systems in which agents
  are created and deleted in the model?The model doesn't have features that allow agents to be created and
  deleted.A work around for the case in which agents are created or deleted a
finite number of times is as follows.
The network of agents in the model is made large enough to include
agents that have not been created as yet and also include agents that
have been deleted.An agent that has not been created is represented by an agent that is
idle, i.e., one is waiting for a message. It never receives a message
until the agent is created. The message is created by the operating
system sending a "create" message to the agent and informing other
agents that this agent has been created.An agent this is deleted is represented in the same way.
The operating system deletes an agent by sending a "delete" message to
the agent.
A deleted agent discards messages that it receives without
taking action.Can an agent process background jobs that
  are interrupted when messages arrive?No, there are no interruptions in the model.
When an agent is executing a receive it is not interrupted.There are systems in which messages have different priorities, and the
arrival of high-priority messages interrupts the execution of
low-priority messages.
Our model does not allow for interruptions.How do you represent channels that are not
  first in first out?We use queues to represent states of first in first out channels, and
  we use other data structures -- such as multisets -- to represent
  states of other types of channels.Later we describe algorithms with different types of channels.
  For example, the state of a channel in which messages may overtake
  each other is a multiset or bag.
  A message is sent on a channel by adding the message to the
  multiset.
  A nonempty multiset may deliver any message in the mutliset to the
  receiving agent.Lossy channels, and channels in which messages have priorities, are
  also modeled by defining channel states appropriately.Can the model represent sequential
  composition of distributed computations?Yes.There are problems in which we would like to start a distributed
computationP; wait forPto finish, and
then start another distributed computationQ.
We want a barrier betweenPandQ.A barrier can be implemented in several ways.
An agent, say acoordinatoragent, sends messages to all agents to startP.
The coordinator then detects termination ofP; we discuss
termination detection algorithms later.
After detecting termination ofPthe coordinator sends
messages to all agents to startQ.Can the model represent agents that make state transitions
  without receiving messages?No, the model does not represent agents that make state transitions
without receiving messages.
The effect of a state transition without receiving a message can be
simulated by an agent sending itself a message and carrying out the
transition when it receives a message from itself.For example, an agent that is computing using a set of files may
  transition to a state in which it no longer needs those files; this
transition occurs without the agent receiving a message.
We model this event by having the agent send itself a message when it
starts using the set of files, and it transits to a state in which it
no longer needs the files when it receives the message from itself.Representing internal state changes in this way is an artifice;
  however, the artifice allows us to use a very simple model in which a
  state change occurs when, and only when, a message is delivered.What is an agent?In our model,an agent is an object that sends and
receives messages.Some papers refer to an agent as anactoror as aprocess.What is a simple example of the notation?Seean example of the notation.Local clocks can be synchronized using NTP
  and other protocols. Why do you assume that clocks aren't synchronized?Local clocks can, indeed, be synchronized very accurately; however, we
  do not assume that they are perfect.We use agents' local clocks for evaluating algorithm performance but
not for proving correctness because even a small drift can cause race
conditions.Consider an algorithm in which one agent carries out a computation
starting at 1pm and another agent carries out a computation starting at 2pm.
When we prove the correctness of our algorithms, we allow for the
unlikely possibility that that the agent starting its 
computation at 2pm does so before the agent that starts a 1pm.
For evaluating performance, however, we assume that the agent that starts a 1pm
usually does so before the agent that starts at 2pm.What are other examples of models?The model we use is anactor model.For more general asynchronous models of concurrent systems seeUNITYandTLA.Perhaps the most widely used model isCSP -- Communicating Sequential Processes.An overview ofmodels used in parallel programmingdescribes models used in
shared-memory and distributed computing.NextStates of a Distributed System.Review material for this pageK. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

AgentXcan copy the message fromZinto a
  local queue -- a local variable ofX-- and process the
  message only after receiving the message fromY.Can a full channel block a sender from
  sending more messages?In the model a sender is never blocked from sending a message.Our model assumes that channels have unlimited capacity.
We can represent a situation in which channels have limited capacity
in the following way.
An agent has a local output queue of unlimited size.
The agent puts messages into this queue when it cannot send messages
on a channel because the channel is full.
Messages from this queue are sent on the channel when the
channel stops being full.Can you represent systems in which agents
  are created and deleted in the model?The model doesn't have features that allow agents to be created and
  deleted.A work around for the case in which agents are created or deleted a
finite number of times is as follows.
The network of agents in the model is made large enough to include
agents that have not been created as yet and also include agents that
have been deleted.An agent that has not been created is represented by an agent that is
idle, i.e., one is waiting for a message. It never receives a message
until the agent is created. The message is created by the operating
system sending a "create" message to the agent and informing other
agents that this agent has been created.An agent this is deleted is represented in the same way.
The operating system deletes an agent by sending a "delete" message to
the agent.
A deleted agent discards messages that it receives without
taking action.Can an agent process background jobs that
  are interrupted when messages arrive?No, there are no interruptions in the model.
When an agent is executing a receive it is not interrupted.There are systems in which messages have different priorities, and the
arrival of high-priority messages interrupts the execution of
low-priority messages.
Our model does not allow for interruptions.How do you represent channels that are not
  first in first out?We use queues to represent states of first in first out channels, and
  we use other data structures -- such as multisets -- to represent
  states of other types of channels.Later we describe algorithms with different types of channels.
  For example, the state of a channel in which messages may overtake
  each other is a multiset or bag.
  A message is sent on a channel by adding the message to the
  multiset.
  A nonempty multiset may deliver any message in the mutliset to the
  receiving agent.Lossy channels, and channels in which messages have priorities, are
  also modeled by defining channel states appropriately.Can the model represent sequential
  composition of distributed computations?Yes.There are problems in which we would like to start a distributed
computationP; wait forPto finish, and
then start another distributed computationQ.
We want a barrier betweenPandQ.A barrier can be implemented in several ways.
An agent, say acoordinatoragent, sends messages to all agents to startP.
The coordinator then detects termination ofP; we discuss
termination detection algorithms later.
After detecting termination ofPthe coordinator sends
messages to all agents to startQ.Can the model represent agents that make state transitions
  without receiving messages?No, the model does not represent agents that make state transitions
without receiving messages.
The effect of a state transition without receiving a message can be
simulated by an agent sending itself a message and carrying out the
transition when it receives a message from itself.For example, an agent that is computing using a set of files may
  transition to a state in which it no longer needs those files; this
transition occurs without the agent receiving a message.
We model this event by having the agent send itself a message when it
starts using the set of files, and it transits to a state in which it
no longer needs the files when it receives the message from itself.Representing internal state changes in this way is an artifice;
  however, the artifice allows us to use a very simple model in which a
  state change occurs when, and only when, a message is delivered.What is an agent?In our model,an agent is an object that sends and
receives messages.Some papers refer to an agent as anactoror as aprocess.What is a simple example of the notation?Seean example of the notation.Local clocks can be synchronized using NTP
  and other protocols. Why do you assume that clocks aren't synchronized?Local clocks can, indeed, be synchronized very accurately; however, we
  do not assume that they are perfect.We use agents' local clocks for evaluating algorithm performance but
not for proving correctness because even a small drift can cause race
conditions.Consider an algorithm in which one agent carries out a computation
starting at 1pm and another agent carries out a computation starting at 2pm.
When we prove the correctness of our algorithms, we allow for the
unlikely possibility that that the agent starting its 
computation at 2pm does so before the agent that starts a 1pm.
For evaluating performance, however, we assume that the agent that starts a 1pm
usually does so before the agent that starts at 2pm.What are other examples of models?The model we use is anactor model.For more general asynchronous models of concurrent systems seeUNITYandTLA.Perhaps the most widely used model isCSP -- Communicating Sequential Processes.An overview ofmodels used in parallel programmingdescribes models used in
shared-memory and distributed computing.NextStates of a Distributed System.Review material for this pageK. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

Our model assumes that channels have unlimited capacity.
We can represent a situation in which channels have limited capacity
in the following way.
An agent has a local output queue of unlimited size.
The agent puts messages into this queue when it cannot send messages
on a channel because the channel is full.
Messages from this queue are sent on the channel when the
channel stops being full.Can you represent systems in which agents
  are created and deleted in the model?The model doesn't have features that allow agents to be created and
  deleted.A work around for the case in which agents are created or deleted a
finite number of times is as follows.
The network of agents in the model is made large enough to include
agents that have not been created as yet and also include agents that
have been deleted.An agent that has not been created is represented by an agent that is
idle, i.e., one is waiting for a message. It never receives a message
until the agent is created. The message is created by the operating
system sending a "create" message to the agent and informing other
agents that this agent has been created.An agent this is deleted is represented in the same way.
The operating system deletes an agent by sending a "delete" message to
the agent.
A deleted agent discards messages that it receives without
taking action.Can an agent process background jobs that
  are interrupted when messages arrive?No, there are no interruptions in the model.
When an agent is executing a receive it is not interrupted.There are systems in which messages have different priorities, and the
arrival of high-priority messages interrupts the execution of
low-priority messages.
Our model does not allow for interruptions.How do you represent channels that are not
  first in first out?We use queues to represent states of first in first out channels, and
  we use other data structures -- such as multisets -- to represent
  states of other types of channels.Later we describe algorithms with different types of channels.
  For example, the state of a channel in which messages may overtake
  each other is a multiset or bag.
  A message is sent on a channel by adding the message to the
  multiset.
  A nonempty multiset may deliver any message in the mutliset to the
  receiving agent.Lossy channels, and channels in which messages have priorities, are
  also modeled by defining channel states appropriately.Can the model represent sequential
  composition of distributed computations?Yes.There are problems in which we would like to start a distributed
computationP; wait forPto finish, and
then start another distributed computationQ.
We want a barrier betweenPandQ.A barrier can be implemented in several ways.
An agent, say acoordinatoragent, sends messages to all agents to startP.
The coordinator then detects termination ofP; we discuss
termination detection algorithms later.
After detecting termination ofPthe coordinator sends
messages to all agents to startQ.Can the model represent agents that make state transitions
  without receiving messages?No, the model does not represent agents that make state transitions
without receiving messages.
The effect of a state transition without receiving a message can be
simulated by an agent sending itself a message and carrying out the
transition when it receives a message from itself.For example, an agent that is computing using a set of files may
  transition to a state in which it no longer needs those files; this
transition occurs without the agent receiving a message.
We model this event by having the agent send itself a message when it
starts using the set of files, and it transits to a state in which it
no longer needs the files when it receives the message from itself.Representing internal state changes in this way is an artifice;
  however, the artifice allows us to use a very simple model in which a
  state change occurs when, and only when, a message is delivered.What is an agent?In our model,an agent is an object that sends and
receives messages.Some papers refer to an agent as anactoror as aprocess.What is a simple example of the notation?Seean example of the notation.Local clocks can be synchronized using NTP
  and other protocols. Why do you assume that clocks aren't synchronized?Local clocks can, indeed, be synchronized very accurately; however, we
  do not assume that they are perfect.We use agents' local clocks for evaluating algorithm performance but
not for proving correctness because even a small drift can cause race
conditions.Consider an algorithm in which one agent carries out a computation
starting at 1pm and another agent carries out a computation starting at 2pm.
When we prove the correctness of our algorithms, we allow for the
unlikely possibility that that the agent starting its 
computation at 2pm does so before the agent that starts a 1pm.
For evaluating performance, however, we assume that the agent that starts a 1pm
usually does so before the agent that starts at 2pm.What are other examples of models?The model we use is anactor model.For more general asynchronous models of concurrent systems seeUNITYandTLA.Perhaps the most widely used model isCSP -- Communicating Sequential Processes.An overview ofmodels used in parallel programmingdescribes models used in
shared-memory and distributed computing.NextStates of a Distributed System.Review material for this pageK. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

A work around for the case in which agents are created or deleted a
finite number of times is as follows.
The network of agents in the model is made large enough to include
agents that have not been created as yet and also include agents that
have been deleted.An agent that has not been created is represented by an agent that is
idle, i.e., one is waiting for a message. It never receives a message
until the agent is created. The message is created by the operating
system sending a "create" message to the agent and informing other
agents that this agent has been created.An agent this is deleted is represented in the same way.
The operating system deletes an agent by sending a "delete" message to
the agent.
A deleted agent discards messages that it receives without
taking action.Can an agent process background jobs that
  are interrupted when messages arrive?No, there are no interruptions in the model.
When an agent is executing a receive it is not interrupted.There are systems in which messages have different priorities, and the
arrival of high-priority messages interrupts the execution of
low-priority messages.
Our model does not allow for interruptions.How do you represent channels that are not
  first in first out?We use queues to represent states of first in first out channels, and
  we use other data structures -- such as multisets -- to represent
  states of other types of channels.Later we describe algorithms with different types of channels.
  For example, the state of a channel in which messages may overtake
  each other is a multiset or bag.
  A message is sent on a channel by adding the message to the
  multiset.
  A nonempty multiset may deliver any message in the mutliset to the
  receiving agent.Lossy channels, and channels in which messages have priorities, are
  also modeled by defining channel states appropriately.Can the model represent sequential
  composition of distributed computations?Yes.There are problems in which we would like to start a distributed
computationP; wait forPto finish, and
then start another distributed computationQ.
We want a barrier betweenPandQ.A barrier can be implemented in several ways.
An agent, say acoordinatoragent, sends messages to all agents to startP.
The coordinator then detects termination ofP; we discuss
termination detection algorithms later.
After detecting termination ofPthe coordinator sends
messages to all agents to startQ.Can the model represent agents that make state transitions
  without receiving messages?No, the model does not represent agents that make state transitions
without receiving messages.
The effect of a state transition without receiving a message can be
simulated by an agent sending itself a message and carrying out the
transition when it receives a message from itself.For example, an agent that is computing using a set of files may
  transition to a state in which it no longer needs those files; this
transition occurs without the agent receiving a message.
We model this event by having the agent send itself a message when it
starts using the set of files, and it transits to a state in which it
no longer needs the files when it receives the message from itself.Representing internal state changes in this way is an artifice;
  however, the artifice allows us to use a very simple model in which a
  state change occurs when, and only when, a message is delivered.What is an agent?In our model,an agent is an object that sends and
receives messages.Some papers refer to an agent as anactoror as aprocess.What is a simple example of the notation?Seean example of the notation.Local clocks can be synchronized using NTP
  and other protocols. Why do you assume that clocks aren't synchronized?Local clocks can, indeed, be synchronized very accurately; however, we
  do not assume that they are perfect.We use agents' local clocks for evaluating algorithm performance but
not for proving correctness because even a small drift can cause race
conditions.Consider an algorithm in which one agent carries out a computation
starting at 1pm and another agent carries out a computation starting at 2pm.
When we prove the correctness of our algorithms, we allow for the
unlikely possibility that that the agent starting its 
computation at 2pm does so before the agent that starts a 1pm.
For evaluating performance, however, we assume that the agent that starts a 1pm
usually does so before the agent that starts at 2pm.What are other examples of models?The model we use is anactor model.For more general asynchronous models of concurrent systems seeUNITYandTLA.Perhaps the most widely used model isCSP -- Communicating Sequential Processes.An overview ofmodels used in parallel programmingdescribes models used in
shared-memory and distributed computing.NextStates of a Distributed System.Review material for this pageK. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

An agent that has not been created is represented by an agent that is
idle, i.e., one is waiting for a message. It never receives a message
until the agent is created. The message is created by the operating
system sending a "create" message to the agent and informing other
agents that this agent has been created.An agent this is deleted is represented in the same way.
The operating system deletes an agent by sending a "delete" message to
the agent.
A deleted agent discards messages that it receives without
taking action.Can an agent process background jobs that
  are interrupted when messages arrive?No, there are no interruptions in the model.
When an agent is executing a receive it is not interrupted.There are systems in which messages have different priorities, and the
arrival of high-priority messages interrupts the execution of
low-priority messages.
Our model does not allow for interruptions.How do you represent channels that are not
  first in first out?We use queues to represent states of first in first out channels, and
  we use other data structures -- such as multisets -- to represent
  states of other types of channels.Later we describe algorithms with different types of channels.
  For example, the state of a channel in which messages may overtake
  each other is a multiset or bag.
  A message is sent on a channel by adding the message to the
  multiset.
  A nonempty multiset may deliver any message in the mutliset to the
  receiving agent.Lossy channels, and channels in which messages have priorities, are
  also modeled by defining channel states appropriately.Can the model represent sequential
  composition of distributed computations?Yes.There are problems in which we would like to start a distributed
computationP; wait forPto finish, and
then start another distributed computationQ.
We want a barrier betweenPandQ.A barrier can be implemented in several ways.
An agent, say acoordinatoragent, sends messages to all agents to startP.
The coordinator then detects termination ofP; we discuss
termination detection algorithms later.
After detecting termination ofPthe coordinator sends
messages to all agents to startQ.Can the model represent agents that make state transitions
  without receiving messages?No, the model does not represent agents that make state transitions
without receiving messages.
The effect of a state transition without receiving a message can be
simulated by an agent sending itself a message and carrying out the
transition when it receives a message from itself.For example, an agent that is computing using a set of files may
  transition to a state in which it no longer needs those files; this
transition occurs without the agent receiving a message.
We model this event by having the agent send itself a message when it
starts using the set of files, and it transits to a state in which it
no longer needs the files when it receives the message from itself.Representing internal state changes in this way is an artifice;
  however, the artifice allows us to use a very simple model in which a
  state change occurs when, and only when, a message is delivered.What is an agent?In our model,an agent is an object that sends and
receives messages.Some papers refer to an agent as anactoror as aprocess.What is a simple example of the notation?Seean example of the notation.Local clocks can be synchronized using NTP
  and other protocols. Why do you assume that clocks aren't synchronized?Local clocks can, indeed, be synchronized very accurately; however, we
  do not assume that they are perfect.We use agents' local clocks for evaluating algorithm performance but
not for proving correctness because even a small drift can cause race
conditions.Consider an algorithm in which one agent carries out a computation
starting at 1pm and another agent carries out a computation starting at 2pm.
When we prove the correctness of our algorithms, we allow for the
unlikely possibility that that the agent starting its 
computation at 2pm does so before the agent that starts a 1pm.
For evaluating performance, however, we assume that the agent that starts a 1pm
usually does so before the agent that starts at 2pm.What are other examples of models?The model we use is anactor model.For more general asynchronous models of concurrent systems seeUNITYandTLA.Perhaps the most widely used model isCSP -- Communicating Sequential Processes.An overview ofmodels used in parallel programmingdescribes models used in
shared-memory and distributed computing.NextStates of a Distributed System.Review material for this pageK. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

An agent this is deleted is represented in the same way.
The operating system deletes an agent by sending a "delete" message to
the agent.
A deleted agent discards messages that it receives without
taking action.Can an agent process background jobs that
  are interrupted when messages arrive?No, there are no interruptions in the model.
When an agent is executing a receive it is not interrupted.There are systems in which messages have different priorities, and the
arrival of high-priority messages interrupts the execution of
low-priority messages.
Our model does not allow for interruptions.How do you represent channels that are not
  first in first out?We use queues to represent states of first in first out channels, and
  we use other data structures -- such as multisets -- to represent
  states of other types of channels.Later we describe algorithms with different types of channels.
  For example, the state of a channel in which messages may overtake
  each other is a multiset or bag.
  A message is sent on a channel by adding the message to the
  multiset.
  A nonempty multiset may deliver any message in the mutliset to the
  receiving agent.Lossy channels, and channels in which messages have priorities, are
  also modeled by defining channel states appropriately.Can the model represent sequential
  composition of distributed computations?Yes.There are problems in which we would like to start a distributed
computationP; wait forPto finish, and
then start another distributed computationQ.
We want a barrier betweenPandQ.A barrier can be implemented in several ways.
An agent, say acoordinatoragent, sends messages to all agents to startP.
The coordinator then detects termination ofP; we discuss
termination detection algorithms later.
After detecting termination ofPthe coordinator sends
messages to all agents to startQ.Can the model represent agents that make state transitions
  without receiving messages?No, the model does not represent agents that make state transitions
without receiving messages.
The effect of a state transition without receiving a message can be
simulated by an agent sending itself a message and carrying out the
transition when it receives a message from itself.For example, an agent that is computing using a set of files may
  transition to a state in which it no longer needs those files; this
transition occurs without the agent receiving a message.
We model this event by having the agent send itself a message when it
starts using the set of files, and it transits to a state in which it
no longer needs the files when it receives the message from itself.Representing internal state changes in this way is an artifice;
  however, the artifice allows us to use a very simple model in which a
  state change occurs when, and only when, a message is delivered.What is an agent?In our model,an agent is an object that sends and
receives messages.Some papers refer to an agent as anactoror as aprocess.What is a simple example of the notation?Seean example of the notation.Local clocks can be synchronized using NTP
  and other protocols. Why do you assume that clocks aren't synchronized?Local clocks can, indeed, be synchronized very accurately; however, we
  do not assume that they are perfect.We use agents' local clocks for evaluating algorithm performance but
not for proving correctness because even a small drift can cause race
conditions.Consider an algorithm in which one agent carries out a computation
starting at 1pm and another agent carries out a computation starting at 2pm.
When we prove the correctness of our algorithms, we allow for the
unlikely possibility that that the agent starting its 
computation at 2pm does so before the agent that starts a 1pm.
For evaluating performance, however, we assume that the agent that starts a 1pm
usually does so before the agent that starts at 2pm.What are other examples of models?The model we use is anactor model.For more general asynchronous models of concurrent systems seeUNITYandTLA.Perhaps the most widely used model isCSP -- Communicating Sequential Processes.An overview ofmodels used in parallel programmingdescribes models used in
shared-memory and distributed computing.NextStates of a Distributed System.Review material for this pageK. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

There are systems in which messages have different priorities, and the
arrival of high-priority messages interrupts the execution of
low-priority messages.
Our model does not allow for interruptions.How do you represent channels that are not
  first in first out?We use queues to represent states of first in first out channels, and
  we use other data structures -- such as multisets -- to represent
  states of other types of channels.Later we describe algorithms with different types of channels.
  For example, the state of a channel in which messages may overtake
  each other is a multiset or bag.
  A message is sent on a channel by adding the message to the
  multiset.
  A nonempty multiset may deliver any message in the mutliset to the
  receiving agent.Lossy channels, and channels in which messages have priorities, are
  also modeled by defining channel states appropriately.Can the model represent sequential
  composition of distributed computations?Yes.There are problems in which we would like to start a distributed
computationP; wait forPto finish, and
then start another distributed computationQ.
We want a barrier betweenPandQ.A barrier can be implemented in several ways.
An agent, say acoordinatoragent, sends messages to all agents to startP.
The coordinator then detects termination ofP; we discuss
termination detection algorithms later.
After detecting termination ofPthe coordinator sends
messages to all agents to startQ.Can the model represent agents that make state transitions
  without receiving messages?No, the model does not represent agents that make state transitions
without receiving messages.
The effect of a state transition without receiving a message can be
simulated by an agent sending itself a message and carrying out the
transition when it receives a message from itself.For example, an agent that is computing using a set of files may
  transition to a state in which it no longer needs those files; this
transition occurs without the agent receiving a message.
We model this event by having the agent send itself a message when it
starts using the set of files, and it transits to a state in which it
no longer needs the files when it receives the message from itself.Representing internal state changes in this way is an artifice;
  however, the artifice allows us to use a very simple model in which a
  state change occurs when, and only when, a message is delivered.What is an agent?In our model,an agent is an object that sends and
receives messages.Some papers refer to an agent as anactoror as aprocess.What is a simple example of the notation?Seean example of the notation.Local clocks can be synchronized using NTP
  and other protocols. Why do you assume that clocks aren't synchronized?Local clocks can, indeed, be synchronized very accurately; however, we
  do not assume that they are perfect.We use agents' local clocks for evaluating algorithm performance but
not for proving correctness because even a small drift can cause race
conditions.Consider an algorithm in which one agent carries out a computation
starting at 1pm and another agent carries out a computation starting at 2pm.
When we prove the correctness of our algorithms, we allow for the
unlikely possibility that that the agent starting its 
computation at 2pm does so before the agent that starts a 1pm.
For evaluating performance, however, we assume that the agent that starts a 1pm
usually does so before the agent that starts at 2pm.What are other examples of models?The model we use is anactor model.For more general asynchronous models of concurrent systems seeUNITYandTLA.Perhaps the most widely used model isCSP -- Communicating Sequential Processes.An overview ofmodels used in parallel programmingdescribes models used in
shared-memory and distributed computing.NextStates of a Distributed System.Review material for this pageK. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

Later we describe algorithms with different types of channels.
  For example, the state of a channel in which messages may overtake
  each other is a multiset or bag.
  A message is sent on a channel by adding the message to the
  multiset.
  A nonempty multiset may deliver any message in the mutliset to the
  receiving agent.Lossy channels, and channels in which messages have priorities, are
  also modeled by defining channel states appropriately.Can the model represent sequential
  composition of distributed computations?Yes.There are problems in which we would like to start a distributed
computationP; wait forPto finish, and
then start another distributed computationQ.
We want a barrier betweenPandQ.A barrier can be implemented in several ways.
An agent, say acoordinatoragent, sends messages to all agents to startP.
The coordinator then detects termination ofP; we discuss
termination detection algorithms later.
After detecting termination ofPthe coordinator sends
messages to all agents to startQ.Can the model represent agents that make state transitions
  without receiving messages?No, the model does not represent agents that make state transitions
without receiving messages.
The effect of a state transition without receiving a message can be
simulated by an agent sending itself a message and carrying out the
transition when it receives a message from itself.For example, an agent that is computing using a set of files may
  transition to a state in which it no longer needs those files; this
transition occurs without the agent receiving a message.
We model this event by having the agent send itself a message when it
starts using the set of files, and it transits to a state in which it
no longer needs the files when it receives the message from itself.Representing internal state changes in this way is an artifice;
  however, the artifice allows us to use a very simple model in which a
  state change occurs when, and only when, a message is delivered.What is an agent?In our model,an agent is an object that sends and
receives messages.Some papers refer to an agent as anactoror as aprocess.What is a simple example of the notation?Seean example of the notation.Local clocks can be synchronized using NTP
  and other protocols. Why do you assume that clocks aren't synchronized?Local clocks can, indeed, be synchronized very accurately; however, we
  do not assume that they are perfect.We use agents' local clocks for evaluating algorithm performance but
not for proving correctness because even a small drift can cause race
conditions.Consider an algorithm in which one agent carries out a computation
starting at 1pm and another agent carries out a computation starting at 2pm.
When we prove the correctness of our algorithms, we allow for the
unlikely possibility that that the agent starting its 
computation at 2pm does so before the agent that starts a 1pm.
For evaluating performance, however, we assume that the agent that starts a 1pm
usually does so before the agent that starts at 2pm.What are other examples of models?The model we use is anactor model.For more general asynchronous models of concurrent systems seeUNITYandTLA.Perhaps the most widely used model isCSP -- Communicating Sequential Processes.An overview ofmodels used in parallel programmingdescribes models used in
shared-memory and distributed computing.NextStates of a Distributed System.Review material for this pageK. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

Lossy channels, and channels in which messages have priorities, are
  also modeled by defining channel states appropriately.Can the model represent sequential
  composition of distributed computations?Yes.There are problems in which we would like to start a distributed
computationP; wait forPto finish, and
then start another distributed computationQ.
We want a barrier betweenPandQ.A barrier can be implemented in several ways.
An agent, say acoordinatoragent, sends messages to all agents to startP.
The coordinator then detects termination ofP; we discuss
termination detection algorithms later.
After detecting termination ofPthe coordinator sends
messages to all agents to startQ.Can the model represent agents that make state transitions
  without receiving messages?No, the model does not represent agents that make state transitions
without receiving messages.
The effect of a state transition without receiving a message can be
simulated by an agent sending itself a message and carrying out the
transition when it receives a message from itself.For example, an agent that is computing using a set of files may
  transition to a state in which it no longer needs those files; this
transition occurs without the agent receiving a message.
We model this event by having the agent send itself a message when it
starts using the set of files, and it transits to a state in which it
no longer needs the files when it receives the message from itself.Representing internal state changes in this way is an artifice;
  however, the artifice allows us to use a very simple model in which a
  state change occurs when, and only when, a message is delivered.What is an agent?In our model,an agent is an object that sends and
receives messages.Some papers refer to an agent as anactoror as aprocess.What is a simple example of the notation?Seean example of the notation.Local clocks can be synchronized using NTP
  and other protocols. Why do you assume that clocks aren't synchronized?Local clocks can, indeed, be synchronized very accurately; however, we
  do not assume that they are perfect.We use agents' local clocks for evaluating algorithm performance but
not for proving correctness because even a small drift can cause race
conditions.Consider an algorithm in which one agent carries out a computation
starting at 1pm and another agent carries out a computation starting at 2pm.
When we prove the correctness of our algorithms, we allow for the
unlikely possibility that that the agent starting its 
computation at 2pm does so before the agent that starts a 1pm.
For evaluating performance, however, we assume that the agent that starts a 1pm
usually does so before the agent that starts at 2pm.What are other examples of models?The model we use is anactor model.For more general asynchronous models of concurrent systems seeUNITYandTLA.Perhaps the most widely used model isCSP -- Communicating Sequential Processes.An overview ofmodels used in parallel programmingdescribes models used in
shared-memory and distributed computing.NextStates of a Distributed System.Review material for this pageK. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

There are problems in which we would like to start a distributed
computationP; wait forPto finish, and
then start another distributed computationQ.
We want a barrier betweenPandQ.A barrier can be implemented in several ways.
An agent, say acoordinatoragent, sends messages to all agents to startP.
The coordinator then detects termination ofP; we discuss
termination detection algorithms later.
After detecting termination ofPthe coordinator sends
messages to all agents to startQ.Can the model represent agents that make state transitions
  without receiving messages?No, the model does not represent agents that make state transitions
without receiving messages.
The effect of a state transition without receiving a message can be
simulated by an agent sending itself a message and carrying out the
transition when it receives a message from itself.For example, an agent that is computing using a set of files may
  transition to a state in which it no longer needs those files; this
transition occurs without the agent receiving a message.
We model this event by having the agent send itself a message when it
starts using the set of files, and it transits to a state in which it
no longer needs the files when it receives the message from itself.Representing internal state changes in this way is an artifice;
  however, the artifice allows us to use a very simple model in which a
  state change occurs when, and only when, a message is delivered.What is an agent?In our model,an agent is an object that sends and
receives messages.Some papers refer to an agent as anactoror as aprocess.What is a simple example of the notation?Seean example of the notation.Local clocks can be synchronized using NTP
  and other protocols. Why do you assume that clocks aren't synchronized?Local clocks can, indeed, be synchronized very accurately; however, we
  do not assume that they are perfect.We use agents' local clocks for evaluating algorithm performance but
not for proving correctness because even a small drift can cause race
conditions.Consider an algorithm in which one agent carries out a computation
starting at 1pm and another agent carries out a computation starting at 2pm.
When we prove the correctness of our algorithms, we allow for the
unlikely possibility that that the agent starting its 
computation at 2pm does so before the agent that starts a 1pm.
For evaluating performance, however, we assume that the agent that starts a 1pm
usually does so before the agent that starts at 2pm.What are other examples of models?The model we use is anactor model.For more general asynchronous models of concurrent systems seeUNITYandTLA.Perhaps the most widely used model isCSP -- Communicating Sequential Processes.An overview ofmodels used in parallel programmingdescribes models used in
shared-memory and distributed computing.NextStates of a Distributed System.Review material for this pageK. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

A barrier can be implemented in several ways.
An agent, say acoordinatoragent, sends messages to all agents to startP.
The coordinator then detects termination ofP; we discuss
termination detection algorithms later.
After detecting termination ofPthe coordinator sends
messages to all agents to startQ.Can the model represent agents that make state transitions
  without receiving messages?No, the model does not represent agents that make state transitions
without receiving messages.
The effect of a state transition without receiving a message can be
simulated by an agent sending itself a message and carrying out the
transition when it receives a message from itself.For example, an agent that is computing using a set of files may
  transition to a state in which it no longer needs those files; this
transition occurs without the agent receiving a message.
We model this event by having the agent send itself a message when it
starts using the set of files, and it transits to a state in which it
no longer needs the files when it receives the message from itself.Representing internal state changes in this way is an artifice;
  however, the artifice allows us to use a very simple model in which a
  state change occurs when, and only when, a message is delivered.What is an agent?In our model,an agent is an object that sends and
receives messages.Some papers refer to an agent as anactoror as aprocess.What is a simple example of the notation?Seean example of the notation.Local clocks can be synchronized using NTP
  and other protocols. Why do you assume that clocks aren't synchronized?Local clocks can, indeed, be synchronized very accurately; however, we
  do not assume that they are perfect.We use agents' local clocks for evaluating algorithm performance but
not for proving correctness because even a small drift can cause race
conditions.Consider an algorithm in which one agent carries out a computation
starting at 1pm and another agent carries out a computation starting at 2pm.
When we prove the correctness of our algorithms, we allow for the
unlikely possibility that that the agent starting its 
computation at 2pm does so before the agent that starts a 1pm.
For evaluating performance, however, we assume that the agent that starts a 1pm
usually does so before the agent that starts at 2pm.What are other examples of models?The model we use is anactor model.For more general asynchronous models of concurrent systems seeUNITYandTLA.Perhaps the most widely used model isCSP -- Communicating Sequential Processes.An overview ofmodels used in parallel programmingdescribes models used in
shared-memory and distributed computing.NextStates of a Distributed System.Review material for this pageK. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

For example, an agent that is computing using a set of files may
  transition to a state in which it no longer needs those files; this
transition occurs without the agent receiving a message.
We model this event by having the agent send itself a message when it
starts using the set of files, and it transits to a state in which it
no longer needs the files when it receives the message from itself.Representing internal state changes in this way is an artifice;
  however, the artifice allows us to use a very simple model in which a
  state change occurs when, and only when, a message is delivered.What is an agent?In our model,an agent is an object that sends and
receives messages.Some papers refer to an agent as anactoror as aprocess.What is a simple example of the notation?Seean example of the notation.Local clocks can be synchronized using NTP
  and other protocols. Why do you assume that clocks aren't synchronized?Local clocks can, indeed, be synchronized very accurately; however, we
  do not assume that they are perfect.We use agents' local clocks for evaluating algorithm performance but
not for proving correctness because even a small drift can cause race
conditions.Consider an algorithm in which one agent carries out a computation
starting at 1pm and another agent carries out a computation starting at 2pm.
When we prove the correctness of our algorithms, we allow for the
unlikely possibility that that the agent starting its 
computation at 2pm does so before the agent that starts a 1pm.
For evaluating performance, however, we assume that the agent that starts a 1pm
usually does so before the agent that starts at 2pm.What are other examples of models?The model we use is anactor model.For more general asynchronous models of concurrent systems seeUNITYandTLA.Perhaps the most widely used model isCSP -- Communicating Sequential Processes.An overview ofmodels used in parallel programmingdescribes models used in
shared-memory and distributed computing.NextStates of a Distributed System.Review material for this pageK. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

Representing internal state changes in this way is an artifice;
  however, the artifice allows us to use a very simple model in which a
  state change occurs when, and only when, a message is delivered.What is an agent?In our model,an agent is an object that sends and
receives messages.Some papers refer to an agent as anactoror as aprocess.What is a simple example of the notation?Seean example of the notation.Local clocks can be synchronized using NTP
  and other protocols. Why do you assume that clocks aren't synchronized?Local clocks can, indeed, be synchronized very accurately; however, we
  do not assume that they are perfect.We use agents' local clocks for evaluating algorithm performance but
not for proving correctness because even a small drift can cause race
conditions.Consider an algorithm in which one agent carries out a computation
starting at 1pm and another agent carries out a computation starting at 2pm.
When we prove the correctness of our algorithms, we allow for the
unlikely possibility that that the agent starting its 
computation at 2pm does so before the agent that starts a 1pm.
For evaluating performance, however, we assume that the agent that starts a 1pm
usually does so before the agent that starts at 2pm.What are other examples of models?The model we use is anactor model.For more general asynchronous models of concurrent systems seeUNITYandTLA.Perhaps the most widely used model isCSP -- Communicating Sequential Processes.An overview ofmodels used in parallel programmingdescribes models used in
shared-memory and distributed computing.NextStates of a Distributed System.Review material for this pageK. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

We use agents' local clocks for evaluating algorithm performance but
not for proving correctness because even a small drift can cause race
conditions.Consider an algorithm in which one agent carries out a computation
starting at 1pm and another agent carries out a computation starting at 2pm.
When we prove the correctness of our algorithms, we allow for the
unlikely possibility that that the agent starting its 
computation at 2pm does so before the agent that starts a 1pm.
For evaluating performance, however, we assume that the agent that starts a 1pm
usually does so before the agent that starts at 2pm.What are other examples of models?The model we use is anactor model.For more general asynchronous models of concurrent systems seeUNITYandTLA.Perhaps the most widely used model isCSP -- Communicating Sequential Processes.An overview ofmodels used in parallel programmingdescribes models used in
shared-memory and distributed computing.NextStates of a Distributed System.Review material for this pageK. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

Consider an algorithm in which one agent carries out a computation
starting at 1pm and another agent carries out a computation starting at 2pm.
When we prove the correctness of our algorithms, we allow for the
unlikely possibility that that the agent starting its 
computation at 2pm does so before the agent that starts a 1pm.
For evaluating performance, however, we assume that the agent that starts a 1pm
usually does so before the agent that starts at 2pm.What are other examples of models?The model we use is anactor model.For more general asynchronous models of concurrent systems seeUNITYandTLA.Perhaps the most widely used model isCSP -- Communicating Sequential Processes.An overview ofmodels used in parallel programmingdescribes models used in
shared-memory and distributed computing.NextStates of a Distributed System.Review material for this pageK. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

Perhaps the most widely used model isCSP -- Communicating Sequential Processes.An overview ofmodels used in parallel programmingdescribes models used in
shared-memory and distributed computing.NextStates of a Distributed System.Review material for this pageK. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

An overview ofmodels used in parallel programmingdescribes models used in
shared-memory and distributed computing.NextStates of a Distributed System.Review material for this pageK. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

Review material for this pageK. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology
--- END Content from https://kmchandy.github.io/DISTRIBUTED_SYSTEM_MODELS/BasicsFAQ.html ---


--- START Content from https://kmchandy.github.io/DISTRIBUTED_SYSTEM_MODELS/BasicsReview.html ---
TITLE: DistributedSystemModels/BasicsReview.html

A distributed system consists of a set of agents and a set of
  channels.What is an agent? Give an example.What is the state of an agent in the example?What is a channel?What is the state of a channel?When does the state of an agent change?Describe how the state of a channel changes.What are the restrictions on areceivefunction of an
    agent?The parameters of areceivefunction are the message
    that arrived and the sender of the message. Why is it helpful for
    the receiver to have information about the sender of the message?NextStates of a Distributed System.Frequenty Asked QuestionsK. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

Frequenty Asked QuestionsK. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology
--- END Content from https://kmchandy.github.io/DISTRIBUTED_SYSTEM_MODELS/BasicsReview.html ---


--- START Content from https://kmchandy.github.io/DISTRIBUTED_SYSTEM_MODELS/BasicsStates.html ---
TITLE: DistributedSystemModels/BasicsStates.html

This page describes the state of a system and changes to a system
  state.
  The model uses a simple abstraction of state and state transitions.

The state of a channel is a queue consisting of the sequence of
  messages sent on the channel that have not been delivered.
  The state of an agent is given by the values of its variables.State TransitionsThe state of a system changes when, and only when, a message is
  delivered to an agent.Execution of areceiveis AtomicWe ignore the changes in the state of an agent while it is executing
  areceive.
  We only consider an agent's state before
  it starts, and after it completes, executing areceive.
  
  We treat execution of areceiveas anatomic
  operation.A state transition occurs when one agent gets one message on one of
  its input channels and executes itsreceivefunction on
  that message.Event: Specification of the Change in State
  during a State TransitionThe change in state that occurs when an agentureceives a message,msg, from an agentvis specified by the 4-tuple:u's statesbefore it executes thereceive.The sender,v, of the message, and the message,msg, that is received,u's states'after it completes execution
  of thereceive.A list of ordered pairs(m, w), wheremis
  the message sent to agentw,
  during the execution of thereceive.The 4-tuple is called anevent.The first two elements of the tuple are called theinputs to the event, and
the last two elements are theoutputs of the event.The State Transition Caused by an EventThe event specified by the above 4-tuple
can be executed in a stateSif the inputs to the event are as specified byS,
i.e., in stateSthe state ofuiss, and the message at the head of the
channel(v, u)ismsg.The occurrence of the event causes a transition to a stateS'which is specified as follows.
The state ofuinS'iss'.
The change in the states ofu's output channels are
specified by the list in part 4 of the 4-tuple:
messagemis appended to the tail of
the sequence of messages in channel(v, w), for all(m, w)in the list.The states of other agents and channels are the same inSandS'.Each execution of an agent'sreceivefunction is
specified by an event.
We say that an agentu"executes event e",
or "event e occurs at agentu"
to mean that the agent executes areceivethat is
specified bye.If multiple agents change state concurrently then we treat the state
changes as occurring one at a time in any order.Events and State TransitionsAn event is a 4-tuple that specifies thechangein state during
a state transition.
An event provides no information about the agents and channels that do
not change state in the state transition.An event is different from a state transition because a state
transition specifies the states, before and after the transition, ofallagents and channels, including the states of agents and
channels that remain unchanged in the transition.For example, when an online customer deposits money into an account, the event
describing the action specifies the amount in the account before and
after the transition, the check that is deposited, and the
acknowledgment that is sent.
The event does not describe the states of all the other accounts in
the bank.Example of an EventThis is an example of an event at agent,pos.
In this example,
the state ofposbefore the event is given by the values
of its variablesmy_data = [3, 5]andn =
0.
In the event,posreceives a "wakeup" message from
itself.pos's state after the event ismy_data = [3,
5]andn  = 1.
During the eventpossends a "wakeup" message to itself
and 3 to agenttotal.
The event is specified by the following tuple:pos's state before it executes thereceiveis given by the values of its variablesmy_data = [3,
  5]andn = 0.The sender of the message that is received ispos, and
  the message is "wakeup".pos's state after it completes execution
  of thereceiveis given bymy_data = [3,
  5]andn = 1.The messages sent during the execution ofreceiveare
  (i) "wakeup" toposand  (ii)3tototal.NextThe next page discussescomputations of distributed systems.ExamplesFrequenty Asked QuestionsK. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

The state of a system changes when, and only when, a message is
  delivered to an agent.Execution of areceiveis AtomicWe ignore the changes in the state of an agent while it is executing
  areceive.
  We only consider an agent's state before
  it starts, and after it completes, executing areceive.
  
  We treat execution of areceiveas anatomic
  operation.A state transition occurs when one agent gets one message on one of
  its input channels and executes itsreceivefunction on
  that message.Event: Specification of the Change in State
  during a State TransitionThe change in state that occurs when an agentureceives a message,msg, from an agentvis specified by the 4-tuple:u's statesbefore it executes thereceive.The sender,v, of the message, and the message,msg, that is received,u's states'after it completes execution
  of thereceive.A list of ordered pairs(m, w), wheremis
  the message sent to agentw,
  during the execution of thereceive.The 4-tuple is called anevent.The first two elements of the tuple are called theinputs to the event, and
the last two elements are theoutputs of the event.The State Transition Caused by an EventThe event specified by the above 4-tuple
can be executed in a stateSif the inputs to the event are as specified byS,
i.e., in stateSthe state ofuiss, and the message at the head of the
channel(v, u)ismsg.The occurrence of the event causes a transition to a stateS'which is specified as follows.
The state ofuinS'iss'.
The change in the states ofu's output channels are
specified by the list in part 4 of the 4-tuple:
messagemis appended to the tail of
the sequence of messages in channel(v, w), for all(m, w)in the list.The states of other agents and channels are the same inSandS'.Each execution of an agent'sreceivefunction is
specified by an event.
We say that an agentu"executes event e",
or "event e occurs at agentu"
to mean that the agent executes areceivethat is
specified bye.If multiple agents change state concurrently then we treat the state
changes as occurring one at a time in any order.Events and State TransitionsAn event is a 4-tuple that specifies thechangein state during
a state transition.
An event provides no information about the agents and channels that do
not change state in the state transition.An event is different from a state transition because a state
transition specifies the states, before and after the transition, ofallagents and channels, including the states of agents and
channels that remain unchanged in the transition.For example, when an online customer deposits money into an account, the event
describing the action specifies the amount in the account before and
after the transition, the check that is deposited, and the
acknowledgment that is sent.
The event does not describe the states of all the other accounts in
the bank.Example of an EventThis is an example of an event at agent,pos.
In this example,
the state ofposbefore the event is given by the values
of its variablesmy_data = [3, 5]andn =
0.
In the event,posreceives a "wakeup" message from
itself.pos's state after the event ismy_data = [3,
5]andn  = 1.
During the eventpossends a "wakeup" message to itself
and 3 to agenttotal.
The event is specified by the following tuple:pos's state before it executes thereceiveis given by the values of its variablesmy_data = [3,
  5]andn = 0.The sender of the message that is received ispos, and
  the message is "wakeup".pos's state after it completes execution
  of thereceiveis given bymy_data = [3,
  5]andn = 1.The messages sent during the execution ofreceiveare
  (i) "wakeup" toposand  (ii)3tototal.NextThe next page discussescomputations of distributed systems.ExamplesFrequenty Asked QuestionsK. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

The occurrence of the event causes a transition to a stateS'which is specified as follows.
The state ofuinS'iss'.
The change in the states ofu's output channels are
specified by the list in part 4 of the 4-tuple:
messagemis appended to the tail of
the sequence of messages in channel(v, w), for all(m, w)in the list.The states of other agents and channels are the same inSandS'.Each execution of an agent'sreceivefunction is
specified by an event.
We say that an agentu"executes event e",
or "event e occurs at agentu"
to mean that the agent executes areceivethat is
specified bye.If multiple agents change state concurrently then we treat the state
changes as occurring one at a time in any order.Events and State TransitionsAn event is a 4-tuple that specifies thechangein state during
a state transition.
An event provides no information about the agents and channels that do
not change state in the state transition.An event is different from a state transition because a state
transition specifies the states, before and after the transition, ofallagents and channels, including the states of agents and
channels that remain unchanged in the transition.For example, when an online customer deposits money into an account, the event
describing the action specifies the amount in the account before and
after the transition, the check that is deposited, and the
acknowledgment that is sent.
The event does not describe the states of all the other accounts in
the bank.Example of an EventThis is an example of an event at agent,pos.
In this example,
the state ofposbefore the event is given by the values
of its variablesmy_data = [3, 5]andn =
0.
In the event,posreceives a "wakeup" message from
itself.pos's state after the event ismy_data = [3,
5]andn  = 1.
During the eventpossends a "wakeup" message to itself
and 3 to agenttotal.
The event is specified by the following tuple:pos's state before it executes thereceiveis given by the values of its variablesmy_data = [3,
  5]andn = 0.The sender of the message that is received ispos, and
  the message is "wakeup".pos's state after it completes execution
  of thereceiveis given bymy_data = [3,
  5]andn = 1.The messages sent during the execution ofreceiveare
  (i) "wakeup" toposand  (ii)3tototal.NextThe next page discussescomputations of distributed systems.ExamplesFrequenty Asked QuestionsK. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

The states of other agents and channels are the same inSandS'.Each execution of an agent'sreceivefunction is
specified by an event.
We say that an agentu"executes event e",
or "event e occurs at agentu"
to mean that the agent executes areceivethat is
specified bye.If multiple agents change state concurrently then we treat the state
changes as occurring one at a time in any order.Events and State TransitionsAn event is a 4-tuple that specifies thechangein state during
a state transition.
An event provides no information about the agents and channels that do
not change state in the state transition.An event is different from a state transition because a state
transition specifies the states, before and after the transition, ofallagents and channels, including the states of agents and
channels that remain unchanged in the transition.For example, when an online customer deposits money into an account, the event
describing the action specifies the amount in the account before and
after the transition, the check that is deposited, and the
acknowledgment that is sent.
The event does not describe the states of all the other accounts in
the bank.Example of an EventThis is an example of an event at agent,pos.
In this example,
the state ofposbefore the event is given by the values
of its variablesmy_data = [3, 5]andn =
0.
In the event,posreceives a "wakeup" message from
itself.pos's state after the event ismy_data = [3,
5]andn  = 1.
During the eventpossends a "wakeup" message to itself
and 3 to agenttotal.
The event is specified by the following tuple:pos's state before it executes thereceiveis given by the values of its variablesmy_data = [3,
  5]andn = 0.The sender of the message that is received ispos, and
  the message is "wakeup".pos's state after it completes execution
  of thereceiveis given bymy_data = [3,
  5]andn = 1.The messages sent during the execution ofreceiveare
  (i) "wakeup" toposand  (ii)3tototal.NextThe next page discussescomputations of distributed systems.ExamplesFrequenty Asked QuestionsK. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

Each execution of an agent'sreceivefunction is
specified by an event.
We say that an agentu"executes event e",
or "event e occurs at agentu"
to mean that the agent executes areceivethat is
specified bye.If multiple agents change state concurrently then we treat the state
changes as occurring one at a time in any order.Events and State TransitionsAn event is a 4-tuple that specifies thechangein state during
a state transition.
An event provides no information about the agents and channels that do
not change state in the state transition.An event is different from a state transition because a state
transition specifies the states, before and after the transition, ofallagents and channels, including the states of agents and
channels that remain unchanged in the transition.For example, when an online customer deposits money into an account, the event
describing the action specifies the amount in the account before and
after the transition, the check that is deposited, and the
acknowledgment that is sent.
The event does not describe the states of all the other accounts in
the bank.Example of an EventThis is an example of an event at agent,pos.
In this example,
the state ofposbefore the event is given by the values
of its variablesmy_data = [3, 5]andn =
0.
In the event,posreceives a "wakeup" message from
itself.pos's state after the event ismy_data = [3,
5]andn  = 1.
During the eventpossends a "wakeup" message to itself
and 3 to agenttotal.
The event is specified by the following tuple:pos's state before it executes thereceiveis given by the values of its variablesmy_data = [3,
  5]andn = 0.The sender of the message that is received ispos, and
  the message is "wakeup".pos's state after it completes execution
  of thereceiveis given bymy_data = [3,
  5]andn = 1.The messages sent during the execution ofreceiveare
  (i) "wakeup" toposand  (ii)3tototal.NextThe next page discussescomputations of distributed systems.ExamplesFrequenty Asked QuestionsK. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

If multiple agents change state concurrently then we treat the state
changes as occurring one at a time in any order.Events and State TransitionsAn event is a 4-tuple that specifies thechangein state during
a state transition.
An event provides no information about the agents and channels that do
not change state in the state transition.An event is different from a state transition because a state
transition specifies the states, before and after the transition, ofallagents and channels, including the states of agents and
channels that remain unchanged in the transition.For example, when an online customer deposits money into an account, the event
describing the action specifies the amount in the account before and
after the transition, the check that is deposited, and the
acknowledgment that is sent.
The event does not describe the states of all the other accounts in
the bank.Example of an EventThis is an example of an event at agent,pos.
In this example,
the state ofposbefore the event is given by the values
of its variablesmy_data = [3, 5]andn =
0.
In the event,posreceives a "wakeup" message from
itself.pos's state after the event ismy_data = [3,
5]andn  = 1.
During the eventpossends a "wakeup" message to itself
and 3 to agenttotal.
The event is specified by the following tuple:pos's state before it executes thereceiveis given by the values of its variablesmy_data = [3,
  5]andn = 0.The sender of the message that is received ispos, and
  the message is "wakeup".pos's state after it completes execution
  of thereceiveis given bymy_data = [3,
  5]andn = 1.The messages sent during the execution ofreceiveare
  (i) "wakeup" toposand  (ii)3tototal.NextThe next page discussescomputations of distributed systems.ExamplesFrequenty Asked QuestionsK. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

An event is different from a state transition because a state
transition specifies the states, before and after the transition, ofallagents and channels, including the states of agents and
channels that remain unchanged in the transition.For example, when an online customer deposits money into an account, the event
describing the action specifies the amount in the account before and
after the transition, the check that is deposited, and the
acknowledgment that is sent.
The event does not describe the states of all the other accounts in
the bank.Example of an EventThis is an example of an event at agent,pos.
In this example,
the state ofposbefore the event is given by the values
of its variablesmy_data = [3, 5]andn =
0.
In the event,posreceives a "wakeup" message from
itself.pos's state after the event ismy_data = [3,
5]andn  = 1.
During the eventpossends a "wakeup" message to itself
and 3 to agenttotal.
The event is specified by the following tuple:pos's state before it executes thereceiveis given by the values of its variablesmy_data = [3,
  5]andn = 0.The sender of the message that is received ispos, and
  the message is "wakeup".pos's state after it completes execution
  of thereceiveis given bymy_data = [3,
  5]andn = 1.The messages sent during the execution ofreceiveare
  (i) "wakeup" toposand  (ii)3tototal.NextThe next page discussescomputations of distributed systems.ExamplesFrequenty Asked QuestionsK. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

For example, when an online customer deposits money into an account, the event
describing the action specifies the amount in the account before and
after the transition, the check that is deposited, and the
acknowledgment that is sent.
The event does not describe the states of all the other accounts in
the bank.Example of an EventThis is an example of an event at agent,pos.
In this example,
the state ofposbefore the event is given by the values
of its variablesmy_data = [3, 5]andn =
0.
In the event,posreceives a "wakeup" message from
itself.pos's state after the event ismy_data = [3,
5]andn  = 1.
During the eventpossends a "wakeup" message to itself
and 3 to agenttotal.
The event is specified by the following tuple:pos's state before it executes thereceiveis given by the values of its variablesmy_data = [3,
  5]andn = 0.The sender of the message that is received ispos, and
  the message is "wakeup".pos's state after it completes execution
  of thereceiveis given bymy_data = [3,
  5]andn = 1.The messages sent during the execution ofreceiveare
  (i) "wakeup" toposand  (ii)3tototal.NextThe next page discussescomputations of distributed systems.ExamplesFrequenty Asked QuestionsK. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

pos's state after the event ismy_data = [3,
5]andn  = 1.
During the eventpossends a "wakeup" message to itself
and 3 to agenttotal.
The event is specified by the following tuple:pos's state before it executes thereceiveis given by the values of its variablesmy_data = [3,
  5]andn = 0.The sender of the message that is received ispos, and
  the message is "wakeup".pos's state after it completes execution
  of thereceiveis given bymy_data = [3,
  5]andn = 1.The messages sent during the execution ofreceiveare
  (i) "wakeup" toposand  (ii)3tototal.NextThe next page discussescomputations of distributed systems.ExamplesFrequenty Asked QuestionsK. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

ExamplesFrequenty Asked QuestionsK. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

Frequenty Asked QuestionsK. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology
--- END Content from https://kmchandy.github.io/DISTRIBUTED_SYSTEM_MODELS/BasicsStates.html ---


--- START Content from https://kmchandy.github.io/DISTRIBUTED_SYSTEM_MODELS/BasicsStatesExamples.html ---
TITLE: DistributedSystemModels/BasicsStatesExamples.html

Each child has a countdown of the number of times it tosses balloons.
  Each time a child tosses a balloon it decrements its countdown value.
After a child's countdown reaches 0 the child pops balloons that it receives
  (and doesn't toss popped balloons).
  Let nX and nY be the countdown values of X and Y, respectively.Figure State 1: Initial StateFigure State 2. Y receives balloon. nY becomes 1Figure State 3. X receives balloon. nX becomes 0Figure State 4.  Y receives balloon. nY becomes 0Figure State 5. X pops balloon. nX becomes 1Figure State 6. X pops balloon. nX becomes 0QuestionHow is the state specified for this system?AnswerThe state of a channel is the number of balloons in it.
The states of agent X and Y are the countdown values nX and nY,
respectively.For simplicity we will abuse notation and use (X, Y) to represent both
the channel and the state of the channel, i.e., the number of messages
in the channel.
So the state of the system is the tuple: [nX, nY, (X, Y), (Y, X)].Algorithm for X# Initial State
nX = 1
# Callback function
def receive(message=m, sender=Y):
   if nX > 0:
      nX = nX - 1
      send(message=m, receiver=Y)The algorithm for Y is identical except that initially nY = 2.QuestionA computation is specified as a sequence of states with a transition
from each state to the next. What is the sequence of states
corresponding to the diagrams: Figure State 1 to Figure State 6?AnswerThe sequence of states corresponding to the diagrams is as follows.Initial State: Channel (X, Y) has one balloon and channel (Y, X) has 
  one balloon. And the countdown for X is 2 and the countdown for Y is
  3. We represent this state by:nX = 1, nY = 2, (X, Y) = 1, (Y, X) = 1The sequence of steps from the initial state are as follows.Y receives a balloon and sends it back. So the state
  becomes:nX = 1, nY = 1, (X, Y) = 0, (Y, X) = 2X receives a balloon and  sends it back. So the state
  becomesnX = 0, nY = 1, (X, Y) = 1, (Y, X) = 1Y receives a balloon and sends it back. So the state
  becomesnX = 0, nY = 0, (X, Y) = 0, (Y, X) = 2X receives a balloon and pops it. So the state
  becomesnX = 0, nY = 0, (X, Y) = 0, (Y, X) = 1X receives a balloon and pops it. So the state
  becomesnX = 0, nY = 0, (X, Y) = 0, (Y, X) = 0There are no further steps in this computation.QuestionWhat is the event that causes the first state transition?
This is the event in which Y receives a balloon for the first time.AnswerEvent: Y receives a balloon for the first timeThis event is specified by the 4-tuple:Y's state before the event is nY = 2The message that is received is 1 (balloon) and the message is from
  X.Y's state after the event is nY = 1The message that is sent in the event is 1 (balloon) and the message is to
  X.QuestionSpecify the state transition caused by the event in which Y receives a
balloon for the first time.AnswerThe execution of this event at Y causes a state transition fromnX = 1, nY = 2, (X, Y) = 1, (Y, X) = 1tonX = 1, nY = 1, (X, Y) = 0, (Y, X) = 2QuestionIn what states can the event in which Y receives the balloon for the
first time be executed?AnswerThis event can be executed in any state S in which the inputs to the
event are specified by the state, i.e., in S:Y's state in S is nY = 2The channel (X, Y) has a message 1 at its head.QuestionSpecify the event in which X receives a balloon for the first time.AnswerThis event is specified by the 4-tuple:X's state before the event is nX = 1The message that is received is 1 (balloon) and the message is from
  Y.X's state after the event is nX = 0The message that is sent in the event is 1 (balloon) and the message is to
  Y.QuestionWhat is the state transition caused by the event in which X receives a
balloon for the first time?AnswerThe execution of this event at X causes a state transition fromnX = 1, nY = 1, (X, Y) = 0, (Y, X) = 2tonX = 0, nY = 1, (X, Y) = 1, (Y, X) = 1Example fromPreviousPagesQuestion about State TransitionsWhat transitions are possible from the state shown in the following
diagram from theexample given earlier?Figure State S_0There are four transitions possible corresponding to the four
non-empty channels.
The transitions from stateS_0are to statesS_1,
S_2, S_3, S_4shown below in Figure State S_1, Figure State
S_2, Figure State S_3, and Figure State S_4, respectively. 
They are shown in the following diagrams.Figure State S_1: eventtotalreceives 3
  fromposFigure State S_2: eventtotalreceives 2
  fromnegFigure State S_3: eventposreceives wakeup
  fromposFigure State S_4: eventnegreceives wakeup
  fromnegNextComputations of a Distributed System.Review material for this pageK. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology



Figure Link: https://kmchandy.github.io/DISTRIBUTED_SYSTEM_MODELS/BasicsStatesExamples.001.jpeg

Caption: Figure State 1: Initial State


For simplicity we will abuse notation and use (X, Y) to represent both
the channel and the state of the channel, i.e., the number of messages
in the channel.
So the state of the system is the tuple: [nX, nY, (X, Y), (Y, X)].Algorithm for X# Initial State
nX = 1
# Callback function
def receive(message=m, sender=Y):
   if nX > 0:
      nX = nX - 1
      send(message=m, receiver=Y)The algorithm for Y is identical except that initially nY = 2.QuestionA computation is specified as a sequence of states with a transition
from each state to the next. What is the sequence of states
corresponding to the diagrams: Figure State 1 to Figure State 6?AnswerThe sequence of states corresponding to the diagrams is as follows.Initial State: Channel (X, Y) has one balloon and channel (Y, X) has 
  one balloon. And the countdown for X is 2 and the countdown for Y is
  3. We represent this state by:nX = 1, nY = 2, (X, Y) = 1, (Y, X) = 1The sequence of steps from the initial state are as follows.Y receives a balloon and sends it back. So the state
  becomes:nX = 1, nY = 1, (X, Y) = 0, (Y, X) = 2X receives a balloon and  sends it back. So the state
  becomesnX = 0, nY = 1, (X, Y) = 1, (Y, X) = 1Y receives a balloon and sends it back. So the state
  becomesnX = 0, nY = 0, (X, Y) = 0, (Y, X) = 2X receives a balloon and pops it. So the state
  becomesnX = 0, nY = 0, (X, Y) = 0, (Y, X) = 1X receives a balloon and pops it. So the state
  becomesnX = 0, nY = 0, (X, Y) = 0, (Y, X) = 0There are no further steps in this computation.QuestionWhat is the event that causes the first state transition?
This is the event in which Y receives a balloon for the first time.AnswerEvent: Y receives a balloon for the first timeThis event is specified by the 4-tuple:Y's state before the event is nY = 2The message that is received is 1 (balloon) and the message is from
  X.Y's state after the event is nY = 1The message that is sent in the event is 1 (balloon) and the message is to
  X.QuestionSpecify the state transition caused by the event in which Y receives a
balloon for the first time.AnswerThe execution of this event at Y causes a state transition fromnX = 1, nY = 2, (X, Y) = 1, (Y, X) = 1tonX = 1, nY = 1, (X, Y) = 0, (Y, X) = 2QuestionIn what states can the event in which Y receives the balloon for the
first time be executed?AnswerThis event can be executed in any state S in which the inputs to the
event are specified by the state, i.e., in S:Y's state in S is nY = 2The channel (X, Y) has a message 1 at its head.QuestionSpecify the event in which X receives a balloon for the first time.AnswerThis event is specified by the 4-tuple:X's state before the event is nX = 1The message that is received is 1 (balloon) and the message is from
  Y.X's state after the event is nX = 0The message that is sent in the event is 1 (balloon) and the message is to
  Y.QuestionWhat is the state transition caused by the event in which X receives a
balloon for the first time?AnswerThe execution of this event at X causes a state transition fromnX = 1, nY = 1, (X, Y) = 0, (Y, X) = 2tonX = 0, nY = 1, (X, Y) = 1, (Y, X) = 1Example fromPreviousPagesQuestion about State TransitionsWhat transitions are possible from the state shown in the following
diagram from theexample given earlier?Figure State S_0There are four transitions possible corresponding to the four
non-empty channels.
The transitions from stateS_0are to statesS_1,
S_2, S_3, S_4shown below in Figure State S_1, Figure State
S_2, Figure State S_3, and Figure State S_4, respectively. 
They are shown in the following diagrams.Figure State S_1: eventtotalreceives 3
  fromposFigure State S_2: eventtotalreceives 2
  fromnegFigure State S_3: eventposreceives wakeup
  fromposFigure State S_4: eventnegreceives wakeup
  fromnegNextComputations of a Distributed System.Review material for this pageK. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology



Figure Link: https://kmchandy.github.io/DISTRIBUTED_SYSTEM_MODELS/Basics_Diagram.001.jpeg

Caption: Figure State S_0


Initial State: Channel (X, Y) has one balloon and channel (Y, X) has 
  one balloon. And the countdown for X is 2 and the countdown for Y is
  3. We represent this state by:nX = 1, nY = 2, (X, Y) = 1, (Y, X) = 1The sequence of steps from the initial state are as follows.Y receives a balloon and sends it back. So the state
  becomes:nX = 1, nY = 1, (X, Y) = 0, (Y, X) = 2X receives a balloon and  sends it back. So the state
  becomesnX = 0, nY = 1, (X, Y) = 1, (Y, X) = 1Y receives a balloon and sends it back. So the state
  becomesnX = 0, nY = 0, (X, Y) = 0, (Y, X) = 2X receives a balloon and pops it. So the state
  becomesnX = 0, nY = 0, (X, Y) = 0, (Y, X) = 1X receives a balloon and pops it. So the state
  becomesnX = 0, nY = 0, (X, Y) = 0, (Y, X) = 0There are no further steps in this computation.QuestionWhat is the event that causes the first state transition?
This is the event in which Y receives a balloon for the first time.AnswerEvent: Y receives a balloon for the first timeThis event is specified by the 4-tuple:Y's state before the event is nY = 2The message that is received is 1 (balloon) and the message is from
  X.Y's state after the event is nY = 1The message that is sent in the event is 1 (balloon) and the message is to
  X.QuestionSpecify the state transition caused by the event in which Y receives a
balloon for the first time.AnswerThe execution of this event at Y causes a state transition fromnX = 1, nY = 2, (X, Y) = 1, (Y, X) = 1tonX = 1, nY = 1, (X, Y) = 0, (Y, X) = 2QuestionIn what states can the event in which Y receives the balloon for the
first time be executed?AnswerThis event can be executed in any state S in which the inputs to the
event are specified by the state, i.e., in S:Y's state in S is nY = 2The channel (X, Y) has a message 1 at its head.QuestionSpecify the event in which X receives a balloon for the first time.AnswerThis event is specified by the 4-tuple:X's state before the event is nX = 1The message that is received is 1 (balloon) and the message is from
  Y.X's state after the event is nX = 0The message that is sent in the event is 1 (balloon) and the message is to
  Y.QuestionWhat is the state transition caused by the event in which X receives a
balloon for the first time?AnswerThe execution of this event at X causes a state transition fromnX = 1, nY = 1, (X, Y) = 0, (Y, X) = 2tonX = 0, nY = 1, (X, Y) = 1, (Y, X) = 1Example fromPreviousPagesQuestion about State TransitionsWhat transitions are possible from the state shown in the following
diagram from theexample given earlier?Figure State S_0There are four transitions possible corresponding to the four
non-empty channels.
The transitions from stateS_0are to statesS_1,
S_2, S_3, S_4shown below in Figure State S_1, Figure State
S_2, Figure State S_3, and Figure State S_4, respectively. 
They are shown in the following diagrams.Figure State S_1: eventtotalreceives 3
  fromposFigure State S_2: eventtotalreceives 2
  fromnegFigure State S_3: eventposreceives wakeup
  fromposFigure State S_4: eventnegreceives wakeup
  fromnegNextComputations of a Distributed System.Review material for this pageK. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology



Figure Link: https://kmchandy.github.io/DISTRIBUTED_SYSTEM_MODELS/Basics_Diagram.001.jpeg

Caption: Figure State S_0


The sequence of steps from the initial state are as follows.Y receives a balloon and sends it back. So the state
  becomes:nX = 1, nY = 1, (X, Y) = 0, (Y, X) = 2X receives a balloon and  sends it back. So the state
  becomesnX = 0, nY = 1, (X, Y) = 1, (Y, X) = 1Y receives a balloon and sends it back. So the state
  becomesnX = 0, nY = 0, (X, Y) = 0, (Y, X) = 2X receives a balloon and pops it. So the state
  becomesnX = 0, nY = 0, (X, Y) = 0, (Y, X) = 1X receives a balloon and pops it. So the state
  becomesnX = 0, nY = 0, (X, Y) = 0, (Y, X) = 0There are no further steps in this computation.QuestionWhat is the event that causes the first state transition?
This is the event in which Y receives a balloon for the first time.AnswerEvent: Y receives a balloon for the first timeThis event is specified by the 4-tuple:Y's state before the event is nY = 2The message that is received is 1 (balloon) and the message is from
  X.Y's state after the event is nY = 1The message that is sent in the event is 1 (balloon) and the message is to
  X.QuestionSpecify the state transition caused by the event in which Y receives a
balloon for the first time.AnswerThe execution of this event at Y causes a state transition fromnX = 1, nY = 2, (X, Y) = 1, (Y, X) = 1tonX = 1, nY = 1, (X, Y) = 0, (Y, X) = 2QuestionIn what states can the event in which Y receives the balloon for the
first time be executed?AnswerThis event can be executed in any state S in which the inputs to the
event are specified by the state, i.e., in S:Y's state in S is nY = 2The channel (X, Y) has a message 1 at its head.QuestionSpecify the event in which X receives a balloon for the first time.AnswerThis event is specified by the 4-tuple:X's state before the event is nX = 1The message that is received is 1 (balloon) and the message is from
  Y.X's state after the event is nX = 0The message that is sent in the event is 1 (balloon) and the message is to
  Y.QuestionWhat is the state transition caused by the event in which X receives a
balloon for the first time?AnswerThe execution of this event at X causes a state transition fromnX = 1, nY = 1, (X, Y) = 0, (Y, X) = 2tonX = 0, nY = 1, (X, Y) = 1, (Y, X) = 1Example fromPreviousPagesQuestion about State TransitionsWhat transitions are possible from the state shown in the following
diagram from theexample given earlier?Figure State S_0There are four transitions possible corresponding to the four
non-empty channels.
The transitions from stateS_0are to statesS_1,
S_2, S_3, S_4shown below in Figure State S_1, Figure State
S_2, Figure State S_3, and Figure State S_4, respectively. 
They are shown in the following diagrams.Figure State S_1: eventtotalreceives 3
  fromposFigure State S_2: eventtotalreceives 2
  fromnegFigure State S_3: eventposreceives wakeup
  fromposFigure State S_4: eventnegreceives wakeup
  fromnegNextComputations of a Distributed System.Review material for this pageK. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology



Figure Link: https://kmchandy.github.io/DISTRIBUTED_SYSTEM_MODELS/Basics_Diagram.001.jpeg

Caption: Figure State S_0


Event: Y receives a balloon for the first time



Figure Link: https://kmchandy.github.io/DISTRIBUTED_SYSTEM_MODELS/Basics_Diagram.001.jpeg

Caption: Figure State S_0


This event can be executed in any state S in which the inputs to the
event are specified by the state, i.e., in S:Y's state in S is nY = 2The channel (X, Y) has a message 1 at its head.QuestionSpecify the event in which X receives a balloon for the first time.AnswerThis event is specified by the 4-tuple:X's state before the event is nX = 1The message that is received is 1 (balloon) and the message is from
  Y.X's state after the event is nX = 0The message that is sent in the event is 1 (balloon) and the message is to
  Y.QuestionWhat is the state transition caused by the event in which X receives a
balloon for the first time?AnswerThe execution of this event at X causes a state transition fromnX = 1, nY = 1, (X, Y) = 0, (Y, X) = 2tonX = 0, nY = 1, (X, Y) = 1, (Y, X) = 1Example fromPreviousPagesQuestion about State TransitionsWhat transitions are possible from the state shown in the following
diagram from theexample given earlier?Figure State S_0There are four transitions possible corresponding to the four
non-empty channels.
The transitions from stateS_0are to statesS_1,
S_2, S_3, S_4shown below in Figure State S_1, Figure State
S_2, Figure State S_3, and Figure State S_4, respectively. 
They are shown in the following diagrams.Figure State S_1: eventtotalreceives 3
  fromposFigure State S_2: eventtotalreceives 2
  fromnegFigure State S_3: eventposreceives wakeup
  fromposFigure State S_4: eventnegreceives wakeup
  fromnegNextComputations of a Distributed System.Review material for this pageK. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology



Figure Link: https://kmchandy.github.io/DISTRIBUTED_SYSTEM_MODELS/Basics_Diagram.001.jpeg

Caption: Figure State S_0


Review material for this pageK. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology
--- END Content from https://kmchandy.github.io/DISTRIBUTED_SYSTEM_MODELS/BasicsStatesExamples.html ---


--- START Content from https://kmchandy.github.io/DISTRIBUTED_SYSTEM_MODELS/BasicsStatesFAQ.html ---
TITLE: DistributedSystemModels/BasicsStatesFAQ.html

Exactly what is a state?

See the definition ofstate in our model.See acomprehensive description of states in computer
  systems.When do state transitions occur?In our model, thestate of a system changeswhen, and only when, a message is
  delivered to an agent.What is an event?Don't agents change states continuously?Yes, agent states may change continuously.
  
  The complete state of an agent includes its program counter which
  points to the next statement to be executed by the agent.
  Our model ignores continuous changes in state of an agent while it
  is executing areceiveand restricts
  attention to the state of an agent when it is waiting.In our model, an agent is waiting for a message or processing a
  message that it received.
  We only deal with the state of the agent after it has finished
  processing the message and is back in the waiting state.
  The changes that occur while areceiveis being
  executed are ignored by the model.What happens if an agent goes to sleep
  during areceive?An example that helps to answer this question is as follows.def receive(message, sender):
   send ("hello", receiver_1)
   time.sleep(1000)
   send ("world", receiver_2)The agent sends a message "hello" toreceiver_1then sleeps for
  1000 seconds and then sends a message toreceiver_2.
  We may expect enough time to elapse between the messages to allowreceiver_1to send a message toreceiver_2before the "world" message gets toreceiver_2.Our model does not deal with time.
  It treats the execution of thereceiveas an atomic
  operation.
  The effect of the followingreceivewith thesleepremoved is the same as the previousreceive.def receive(message, sender):
   send ("hello", receiver_1)
   send ("world", receiver_2)Time is important in the behavior of distributed systems.
We don't use time in reasoning about the correctness of distributed
systems because it's safer not to depend on clock accuracy.Can two agents receive messages at exactly
  the same time? Won't that result in a state transition in which two
  agents change state?If two agents receive messages at exactly the same time then the
  events at which the messages are received are independent and so
they can be executed in arbitrary order.Suppose agent \(v\) receives a message \(m\) from agent \(u\) at exactly the
same time that \(u\) receives a message \(m'\) from \(v\).
Then before the steps, the channel from \(v\) to \(u\) contained
message \(m\) and the channel from \(u\) to \(v\) contained
message \(m'\).If \(u\) receives \(m'\) first it makes no difference to \(m\) being
at the head of the channel from \(v\) to \(u\).
So, the order in which the steps occur is irrelevant.Many agents are changing state and sending and receiving messages
concurrently.
We ignore these concurrent changes and restrict attention state
changes in which one agent receives one message.
We can analyze systems with concurrent changes using our simple model.Can an agent receive multiple messages in a
  single state transition? Can an agent send multiple messages to
  another agent in a single state transition?Our model only allows one message to be received in a state transition
and one message to be sent to an agent in the transition.Extending the model to allow a sequence of messages to be sent to an
agent in a single transition isn't necessary for the algorithms that
we discuss; however, extending the model to do so is straightforward
and you should try to do so.The model does not fully capture reality.
A model is an engineering choice. It is an abstraction of reality that
helps us develop algorithms.NextStates of a Distributed System.Review material for this pageK. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

When do state transitions occur?

What is an event?

Don't agents change states continuously?

In our model, an agent is waiting for a message or processing a
  message that it received.
  We only deal with the state of the agent after it has finished
  processing the message and is back in the waiting state.
  The changes that occur while areceiveis being
  executed are ignored by the model.What happens if an agent goes to sleep
  during areceive?An example that helps to answer this question is as follows.def receive(message, sender):
   send ("hello", receiver_1)
   time.sleep(1000)
   send ("world", receiver_2)The agent sends a message "hello" toreceiver_1then sleeps for
  1000 seconds and then sends a message toreceiver_2.
  We may expect enough time to elapse between the messages to allowreceiver_1to send a message toreceiver_2before the "world" message gets toreceiver_2.Our model does not deal with time.
  It treats the execution of thereceiveas an atomic
  operation.
  The effect of the followingreceivewith thesleepremoved is the same as the previousreceive.def receive(message, sender):
   send ("hello", receiver_1)
   send ("world", receiver_2)Time is important in the behavior of distributed systems.
We don't use time in reasoning about the correctness of distributed
systems because it's safer not to depend on clock accuracy.Can two agents receive messages at exactly
  the same time? Won't that result in a state transition in which two
  agents change state?If two agents receive messages at exactly the same time then the
  events at which the messages are received are independent and so
they can be executed in arbitrary order.Suppose agent \(v\) receives a message \(m\) from agent \(u\) at exactly the
same time that \(u\) receives a message \(m'\) from \(v\).
Then before the steps, the channel from \(v\) to \(u\) contained
message \(m\) and the channel from \(u\) to \(v\) contained
message \(m'\).If \(u\) receives \(m'\) first it makes no difference to \(m\) being
at the head of the channel from \(v\) to \(u\).
So, the order in which the steps occur is irrelevant.Many agents are changing state and sending and receiving messages
concurrently.
We ignore these concurrent changes and restrict attention state
changes in which one agent receives one message.
We can analyze systems with concurrent changes using our simple model.Can an agent receive multiple messages in a
  single state transition? Can an agent send multiple messages to
  another agent in a single state transition?Our model only allows one message to be received in a state transition
and one message to be sent to an agent in the transition.Extending the model to allow a sequence of messages to be sent to an
agent in a single transition isn't necessary for the algorithms that
we discuss; however, extending the model to do so is straightforward
and you should try to do so.The model does not fully capture reality.
A model is an engineering choice. It is an abstraction of reality that
helps us develop algorithms.NextStates of a Distributed System.Review material for this pageK. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

What happens if an agent goes to sleep
  during areceive?

Our model does not deal with time.
  It treats the execution of thereceiveas an atomic
  operation.
  The effect of the followingreceivewith thesleepremoved is the same as the previousreceive.def receive(message, sender):
   send ("hello", receiver_1)
   send ("world", receiver_2)Time is important in the behavior of distributed systems.
We don't use time in reasoning about the correctness of distributed
systems because it's safer not to depend on clock accuracy.Can two agents receive messages at exactly
  the same time? Won't that result in a state transition in which two
  agents change state?If two agents receive messages at exactly the same time then the
  events at which the messages are received are independent and so
they can be executed in arbitrary order.Suppose agent \(v\) receives a message \(m\) from agent \(u\) at exactly the
same time that \(u\) receives a message \(m'\) from \(v\).
Then before the steps, the channel from \(v\) to \(u\) contained
message \(m\) and the channel from \(u\) to \(v\) contained
message \(m'\).If \(u\) receives \(m'\) first it makes no difference to \(m\) being
at the head of the channel from \(v\) to \(u\).
So, the order in which the steps occur is irrelevant.Many agents are changing state and sending and receiving messages
concurrently.
We ignore these concurrent changes and restrict attention state
changes in which one agent receives one message.
We can analyze systems with concurrent changes using our simple model.Can an agent receive multiple messages in a
  single state transition? Can an agent send multiple messages to
  another agent in a single state transition?Our model only allows one message to be received in a state transition
and one message to be sent to an agent in the transition.Extending the model to allow a sequence of messages to be sent to an
agent in a single transition isn't necessary for the algorithms that
we discuss; however, extending the model to do so is straightforward
and you should try to do so.The model does not fully capture reality.
A model is an engineering choice. It is an abstraction of reality that
helps us develop algorithms.NextStates of a Distributed System.Review material for this pageK. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

Can two agents receive messages at exactly
  the same time? Won't that result in a state transition in which two
  agents change state?

Suppose agent \(v\) receives a message \(m\) from agent \(u\) at exactly the
same time that \(u\) receives a message \(m'\) from \(v\).
Then before the steps, the channel from \(v\) to \(u\) contained
message \(m\) and the channel from \(u\) to \(v\) contained
message \(m'\).If \(u\) receives \(m'\) first it makes no difference to \(m\) being
at the head of the channel from \(v\) to \(u\).
So, the order in which the steps occur is irrelevant.Many agents are changing state and sending and receiving messages
concurrently.
We ignore these concurrent changes and restrict attention state
changes in which one agent receives one message.
We can analyze systems with concurrent changes using our simple model.Can an agent receive multiple messages in a
  single state transition? Can an agent send multiple messages to
  another agent in a single state transition?Our model only allows one message to be received in a state transition
and one message to be sent to an agent in the transition.Extending the model to allow a sequence of messages to be sent to an
agent in a single transition isn't necessary for the algorithms that
we discuss; however, extending the model to do so is straightforward
and you should try to do so.The model does not fully capture reality.
A model is an engineering choice. It is an abstraction of reality that
helps us develop algorithms.NextStates of a Distributed System.Review material for this pageK. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

If \(u\) receives \(m'\) first it makes no difference to \(m\) being
at the head of the channel from \(v\) to \(u\).
So, the order in which the steps occur is irrelevant.Many agents are changing state and sending and receiving messages
concurrently.
We ignore these concurrent changes and restrict attention state
changes in which one agent receives one message.
We can analyze systems with concurrent changes using our simple model.Can an agent receive multiple messages in a
  single state transition? Can an agent send multiple messages to
  another agent in a single state transition?Our model only allows one message to be received in a state transition
and one message to be sent to an agent in the transition.Extending the model to allow a sequence of messages to be sent to an
agent in a single transition isn't necessary for the algorithms that
we discuss; however, extending the model to do so is straightforward
and you should try to do so.The model does not fully capture reality.
A model is an engineering choice. It is an abstraction of reality that
helps us develop algorithms.NextStates of a Distributed System.Review material for this pageK. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

Many agents are changing state and sending and receiving messages
concurrently.
We ignore these concurrent changes and restrict attention state
changes in which one agent receives one message.
We can analyze systems with concurrent changes using our simple model.Can an agent receive multiple messages in a
  single state transition? Can an agent send multiple messages to
  another agent in a single state transition?Our model only allows one message to be received in a state transition
and one message to be sent to an agent in the transition.Extending the model to allow a sequence of messages to be sent to an
agent in a single transition isn't necessary for the algorithms that
we discuss; however, extending the model to do so is straightforward
and you should try to do so.The model does not fully capture reality.
A model is an engineering choice. It is an abstraction of reality that
helps us develop algorithms.NextStates of a Distributed System.Review material for this pageK. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

Can an agent receive multiple messages in a
  single state transition? Can an agent send multiple messages to
  another agent in a single state transition?

Extending the model to allow a sequence of messages to be sent to an
agent in a single transition isn't necessary for the algorithms that
we discuss; however, extending the model to do so is straightforward
and you should try to do so.The model does not fully capture reality.
A model is an engineering choice. It is an abstraction of reality that
helps us develop algorithms.NextStates of a Distributed System.Review material for this pageK. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

The model does not fully capture reality.
A model is an engineering choice. It is an abstraction of reality that
helps us develop algorithms.NextStates of a Distributed System.Review material for this pageK. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

Review material for this pageK. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology
--- END Content from https://kmchandy.github.io/DISTRIBUTED_SYSTEM_MODELS/BasicsStatesFAQ.html ---


--- START Content from https://kmchandy.github.io/DISTRIBUTED_SYSTEM_MODELS/BasicsStatesReview.html ---
TITLE: DistributedSystemModels/BasicsStatesReview.html

A distributed system consists of a set of agents and a set of
  channels.What is the state of a distributed system?When does the state of a distributed system change?Consider an example in which an agent sleeps for some time when it
    processes a message. Does the amount of time that the agent sleeps
    change the state transitions of the system?When does the state of an agent change?Describe how the state of a channel changes.What are the restrictions on areceivefunction of an
    agent?The parameters of areceivefunction are the message
    that arrived and the sender of the message. Why is it helpful for
    the receiver to have information about the sender of the message?NextStates of a Distributed System.Frequenty Asked QuestionsK. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

Frequenty Asked QuestionsK. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology
--- END Content from https://kmchandy.github.io/DISTRIBUTED_SYSTEM_MODELS/BasicsStatesReview.html ---


--- START Content from https://kmchandy.github.io/DISTRIBUTED_SYSTEM_MODELS/Computations.html ---
TITLE: DistributedSystemModels/Computations.html

A computation of a system is a sequence of states of the system where there exists an
event of the system that causes a transition from each state in the
  sequence to the next.
  We use techniques for proving sequential programs to prove
  properties of computations.DefinitionA computation is a sequence of states,
 where there exists an
event that causes a transition from each
state in the sequence to the next.A computation may be finite or infinite.
  A computation may start in any state.
  We will often prove properties of computations that start in an
  initial state of the system; however, a computation is not
  restricted to starting in an initial system state.A computation \([S_{0}, S_{1}, S_{2}, \ldots]\)
  can also be specified by the initial state, \(S_{0}\), of the
  computation and a sequence 
of events \([e_{0}, e_{1}, \ldots, ]\) where execution of 
 \(e_{i}\) causes a transition from \(S_{i}\)  to \(S_{i+1}\).Astepof the computation is an execution of a single event in this
  sequence.
  The same event can occur multiple times in a computation.
  Each occurrence of an event is a separate step of the computation.A Sequential Programming Representation of ComputationsConsider a sequential program consisting of an initialization
and the followingwhileloop.n = 0
S[n] is an initial state
while there exists an event that can be executed:
   # state is S[n]
   execute any executable event
   n = n + 1The loop terminates when there are no executable events.
The selection of which
event to execute in an iteration is nondeterministic --anyexecutable
event can be selected.States in the Loop and in a ComputationThe sequence of statesS[0], S[1], S[2], ...S[k]that occur
in the loop, for anyk\(\geq\) 0, is a computation.
Then-th iteration of the loop is then-th step in the
computation, and this step causes a transition from stateS[n]to stateS[n+1], forn\(\geq\) 0.Properties of Loops and
Distributed SystemsA benefit of using the states in a while loop to represent the states 
of a computation is that we can use familiar techniques
---loop invariantandloop variants---
for reasoning about loops in sequential programming to reason about
distributed algorithms too.
SeeEffective Theories in Programming Practicefor
examples of these techniquesNextThe next webpage discussesdata flow in computations.ExamplesFrequenty Asked QuestionsK. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

A computation may be finite or infinite.
  A computation may start in any state.
  We will often prove properties of computations that start in an
  initial state of the system; however, a computation is not
  restricted to starting in an initial system state.A computation \([S_{0}, S_{1}, S_{2}, \ldots]\)
  can also be specified by the initial state, \(S_{0}\), of the
  computation and a sequence 
of events \([e_{0}, e_{1}, \ldots, ]\) where execution of 
 \(e_{i}\) causes a transition from \(S_{i}\)  to \(S_{i+1}\).Astepof the computation is an execution of a single event in this
  sequence.
  The same event can occur multiple times in a computation.
  Each occurrence of an event is a separate step of the computation.A Sequential Programming Representation of ComputationsConsider a sequential program consisting of an initialization
and the followingwhileloop.n = 0
S[n] is an initial state
while there exists an event that can be executed:
   # state is S[n]
   execute any executable event
   n = n + 1The loop terminates when there are no executable events.
The selection of which
event to execute in an iteration is nondeterministic --anyexecutable
event can be selected.States in the Loop and in a ComputationThe sequence of statesS[0], S[1], S[2], ...S[k]that occur
in the loop, for anyk\(\geq\) 0, is a computation.
Then-th iteration of the loop is then-th step in the
computation, and this step causes a transition from stateS[n]to stateS[n+1], forn\(\geq\) 0.Properties of Loops and
Distributed SystemsA benefit of using the states in a while loop to represent the states 
of a computation is that we can use familiar techniques
---loop invariantandloop variants---
for reasoning about loops in sequential programming to reason about
distributed algorithms too.
SeeEffective Theories in Programming Practicefor
examples of these techniquesNextThe next webpage discussesdata flow in computations.ExamplesFrequenty Asked QuestionsK. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

A computation \([S_{0}, S_{1}, S_{2}, \ldots]\)
  can also be specified by the initial state, \(S_{0}\), of the
  computation and a sequence 
of events \([e_{0}, e_{1}, \ldots, ]\) where execution of 
 \(e_{i}\) causes a transition from \(S_{i}\)  to \(S_{i+1}\).Astepof the computation is an execution of a single event in this
  sequence.
  The same event can occur multiple times in a computation.
  Each occurrence of an event is a separate step of the computation.A Sequential Programming Representation of ComputationsConsider a sequential program consisting of an initialization
and the followingwhileloop.n = 0
S[n] is an initial state
while there exists an event that can be executed:
   # state is S[n]
   execute any executable event
   n = n + 1The loop terminates when there are no executable events.
The selection of which
event to execute in an iteration is nondeterministic --anyexecutable
event can be selected.States in the Loop and in a ComputationThe sequence of statesS[0], S[1], S[2], ...S[k]that occur
in the loop, for anyk\(\geq\) 0, is a computation.
Then-th iteration of the loop is then-th step in the
computation, and this step causes a transition from stateS[n]to stateS[n+1], forn\(\geq\) 0.Properties of Loops and
Distributed SystemsA benefit of using the states in a while loop to represent the states 
of a computation is that we can use familiar techniques
---loop invariantandloop variants---
for reasoning about loops in sequential programming to reason about
distributed algorithms too.
SeeEffective Theories in Programming Practicefor
examples of these techniquesNextThe next webpage discussesdata flow in computations.ExamplesFrequenty Asked QuestionsK. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

Astepof the computation is an execution of a single event in this
  sequence.
  The same event can occur multiple times in a computation.
  Each occurrence of an event is a separate step of the computation.A Sequential Programming Representation of ComputationsConsider a sequential program consisting of an initialization
and the followingwhileloop.n = 0
S[n] is an initial state
while there exists an event that can be executed:
   # state is S[n]
   execute any executable event
   n = n + 1The loop terminates when there are no executable events.
The selection of which
event to execute in an iteration is nondeterministic --anyexecutable
event can be selected.States in the Loop and in a ComputationThe sequence of statesS[0], S[1], S[2], ...S[k]that occur
in the loop, for anyk\(\geq\) 0, is a computation.
Then-th iteration of the loop is then-th step in the
computation, and this step causes a transition from stateS[n]to stateS[n+1], forn\(\geq\) 0.Properties of Loops and
Distributed SystemsA benefit of using the states in a while loop to represent the states 
of a computation is that we can use familiar techniques
---loop invariantandloop variants---
for reasoning about loops in sequential programming to reason about
distributed algorithms too.
SeeEffective Theories in Programming Practicefor
examples of these techniquesNextThe next webpage discussesdata flow in computations.ExamplesFrequenty Asked QuestionsK. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

Consider a sequential program consisting of an initialization
and the followingwhileloop.n = 0
S[n] is an initial state
while there exists an event that can be executed:
   # state is S[n]
   execute any executable event
   n = n + 1The loop terminates when there are no executable events.
The selection of which
event to execute in an iteration is nondeterministic --anyexecutable
event can be selected.States in the Loop and in a ComputationThe sequence of statesS[0], S[1], S[2], ...S[k]that occur
in the loop, for anyk\(\geq\) 0, is a computation.
Then-th iteration of the loop is then-th step in the
computation, and this step causes a transition from stateS[n]to stateS[n+1], forn\(\geq\) 0.Properties of Loops and
Distributed SystemsA benefit of using the states in a while loop to represent the states 
of a computation is that we can use familiar techniques
---loop invariantandloop variants---
for reasoning about loops in sequential programming to reason about
distributed algorithms too.
SeeEffective Theories in Programming Practicefor
examples of these techniquesNextThe next webpage discussesdata flow in computations.ExamplesFrequenty Asked QuestionsK. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

The next webpage discussesdata flow in computations.ExamplesFrequenty Asked QuestionsK. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

ExamplesFrequenty Asked QuestionsK. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

Frequenty Asked QuestionsK. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology
--- END Content from https://kmchandy.github.io/DISTRIBUTED_SYSTEM_MODELS/Computations.html ---


--- START Content from https://kmchandy.github.io/DISTRIBUTED_SYSTEM_MODELS/ComputationsExamples.html ---
TITLE: DistributedSystemModels/ComputationsExamples.html

Each child has a countdown of the number of times it tosses balloons.
  Each time a child tosses a balloon it decrements its countdown value.
After a child's countdown reaches 0 the child pops balloons that it receives
(and doesn't toss popped balloons).QuestionHow is a state of this system specified?AnswerThe state of a channel is the number of balloons in it.
The state of an agent (X or Y) is the countdown value nX or nY,
  respectively.
  The state of the system is given by the states of its agents and channels.QuestionWhat is an example of a computation of the system?AnswerAn example of a computation is
  shown in the diagrams below where the computation is State 1 to
  State 2 to .. State 6.A Computation shown as a Sequence of DiagramsFigure State 1: Initial StateFigure State 2. Y receives balloon. nY becomes 1Figure State 3. X receives balloon. nX becomes 0Figure State 4.  Y receives balloon. nY becomes 0Figure State 5. X pops balloon. nX becomes 1Figure State 6. X pops balloon. nX becomes 0QuestionSpecify the computation which is shown above as a sequence of diagrams
as a sequence of states.AnswerThe sequence of states, shown below, is the computation shown in the
diagrams above. You can verify that there exists a transition from
each state in the sequence to the next. Also see that each
transition is caused by exactly one agent receiving exactly one
message on one of the agent's input channels.Initial State: Channel (X, Y) has one balloon and channel (Y, X) has 
  one balloon. And the countdown for X is 2 and the countdown for Y is
  3. We represent this state by:nX = 1, nY = 2, (X, Y) = 1, (Y, X) = 1The sequence of steps from the initial state are as follows.Y receives a balloon and sends it back. So the state
  becomes:nX = 1, nY = 1, (X, Y) = 0, (Y, X) = 2X receives a balloon and  sends it back. So the state
  becomesnX = 0, nY = 1, (X, Y) = 1, (Y, X) = 1Y receives a balloon and sends it back. So the state
  becomesnX = 0, nY = 0, (X, Y) = 0, (Y, X) = 2X receives a balloon and pops it. So the state
  becomesnX = 0, nY = 0, (X, Y) = 0, (Y, X) = 1X receives a balloon and pops it. So the state
  becomesnX = 0, nY = 0, (X, Y) = 0, (Y, X) = 0There are no further steps in this computation.QuestionWhat are other transitions from the initial state of the computation?AnswerLet's look at the event in step 2 of the above computation. The event
is as follows.Event: X receives a balloon for the first timeThis event is specified by the 4-tuple:X's state before the event is nX = 1The message that is received is 1 (balloon) and the message is from
  Y.X's state after the event is nX = 0The message that is sent in the event is 1 (balloon) and the message is to
  Y.This event can be executed in any state S which satisfies the inputs
to this event:X's state before the event is nX = 1The message at the head of channel (Y, X) is 1.So, this event can be executed in the initial state.The execution of this event causes a transition
from the initial state to a state shown in the following diagram:Figure Another Transition from the Initial StateQuestionCan you give examples of other computations starting in the same
state?AnswerInitial State: Channel (X, Y) has one balloon and channel (Y, X) has
  one balloon. And the countdown for X is 2 and the countdown for Y is
  3. We represent this state by:nX = 1, nY = 2, (X, Y) = 1, (Y, X) = 1X receives a balloon and sends it back. So the state
  becomes:nX = 0, nY = 2, (X, Y) = 2, (Y, X) = 0Y receives a balloon and  sends it back. So the state
  becomesnX = 0, nY = 1, (X, Y) = 1, (Y, X) = 1X receives a balloon and pops it. So the state
  becomesnX = 0, nY = 1, (X, Y) = 1, (Y, X) = 0Y receives a balloon sends it back. So the state
  becomesnX = 0, nY = 0, (X, Y) = 0, (Y, X) = 1X receives a balloon and pops it. So the state
  becomesnX = 0, nY = 0, (X, Y) = 0, (Y, X) = 0There are no further steps in this computation.Some states (e.g. State 2) in the first example of a computation don't occur in the
second example, and vice versa.A Computation of the System fromPrevious PagesQuestionWhat is an example of a computation, starting from the initial state,
of theexample?AnswerThe following sequence of diagrams shows a computation of the example.
Each diagram shows a state of the system. There is a
transition from state S_k to state S_(k+1)Figure State S_0: Initial StateFigure State S_1Figure State S_2Figure State S_3Figure State S_4Figure State S_5Figure State S_6Figure State S_7Figure State S_8Figure State S_9Invariants, Loop Variants and Termination in
the Simple Balloon ExampleQuestionGive an example of an invariant for the balloon example.AnswerAn invariant of the system is:Number of balloons is at most 2which is equivalent to:(X, Y) + (Y, X) <= 2(Here we are abusing notation using (X, Y) for a channel and also for
the state of the channel.)QuestionHow do you prove that this condition is invariant?AnswerTo prove that this predicate (Boolean condition) is an invariant we
prove that (1) it holds initially 
and (2) and for all state transitions, if the invariant holds in the
state before the transition then it holds in the state after the transition.Initially:(X, Y) = 1and(Y, X) = 1So, the predicate holds initially.A state transition in which a balloon is returned does not change the value
of(X, Y) + (Y, X).A state transition in which a balloon is popped decreases(X, Y)
+ (Y, X).Therefore if the predicate holds before a state
transition then it holds after the transition.QuestionIs the following an invariant of the system?((X, Y) + (Y, X) = 2) or (nX = 0) or (nY = 0)AnswerYes, this predicate is an invariant.To prove that this predicate is an invariant we
prove that (1) it holds initially 
and (2) and for all state transitions, if the invariant holds in the
state before the transition then it holds in the state after the
transition.Initially,(X, Y) = 1and(Y, X) = 1.
Therefore the predicate holds initially.Let S be the state before a transition to a state S'.
We will prove that if the predicate holds in S then the predicate
holds in S'.Consider two cases:nX = 0 or nY = 0 in S.Because nX and nY do not increase,
  and nX and nY do not become negative,
  in the execution of an event, it follows that nX = 0 or nY = 0 in S'.
  Therefore the predicate holds in S'.nX > 0 and nY > 0 in S.Because the invariant holds in S it follows that(X, Y) + (Y,
  X) = 2in S.Because nY > 0 the execution of an event in which Y receives a
  balloon leaves(X, Y) + (Y, X)unchanged.Because nX > 0 the execution of an event in which X receives a
  balloon leaves(X, Y) + (Y, X)unchanged.Therefore(X, Y) + (Y, X) = 2in S'.
  So, the predicate holds in S'.Example of a Loop VariantQuestionHow do you prove that the algorithm terminates?AnswerTo prove that the algorithm terminates we use the loop variant:nX + nY + (X, Y) + (Y, X)The loop variant is a function of the state (i.e. variables) of the system.
In this example, the loop variant maps states of the program to
integers.We must show that  (1) the loop variant is bounded below and we cancarry
out inductionon the values of the loop variant, i.e., it can
decrease only a finite number of times before it reaches a lower
bound, and (2) the executions of all steps in all computations that
start at initial states reduce the value of the loop variant.
The specific lower bound is irrelevant for the proof of termination.In this example, 0 is (obviously) a lower bound.
Next we show that the execution of any event in any state reduces the
value of the loop variant.When Y receives a balloon if nY is positive then nY
decreases which the loop variant, and if nY is 0 then
(X, Y) which decreases the loop variant.Similarly every event in which X receives a balloon also reduces the
loop variant. Thus the executions of all events reduce the loop variant.Therefore the algorithm terminates execution.QuestionWhat can you prove about the state of the system when the computation
terminates? 
We have shown that computations terminate and we given examples of
invariants.AnswerAt termination all channels are empty.
Let's prove that nX or nY is 0 at termination.From the invariants if:(X, Y) + (Y, X)\(\neq\) 2 then nX
or nY is 0.At termination,(X, Y) + (Y, X)\(=\) 0, and the result follows.More Examples of Invariants and Loop VariantsWe reason about the correctness of many distributed algorithms in
  in the same way that we reason about sequentialwhileloops, by using invariants and loop variants.Example: GCD using a While LoopAloop invariantis an assertion about the state of the program that holds before and
  after each iteration of the loop.
  You can look upmany examplesof loop invariants on the web.Example: InvariantA loop invariant in the following greatest common
  divisor (gcd) program is shown in the body of the loop as the
  assertiongcd(x, y) = gcd(X, Y).x, y = X, Y
while x != y:
  # assertion: gcd(x, y) = gcd(X, Y)
  if x > y: x = x - y
  else: y = y - xInvariant of a Distributed System for GCDAn invariant of a distributed system is a
  predicates that holds in all states reachable from initial states.
  An invariant of a distributed system is the loop invariant of the
  following while loop.
  An invariantInvis shown
  as an assertion in the body of the loop.while there exists a nonempty input channel in the system:
   # assertion: Inv
   select a nonempty channel (u, v) in the system
   let the head of channel (u, v) be msg
   v executes receive(msg, u)Example of a Distributed GCDWe are given a strongly connected network of agents -- i.e.,
  there is a path from each agent to every other agent.
  Each agent has a local variablen, which is initialized
  as a positive integer.
  
  Let GCD be the gcd (greatest commond divisor) of the
  initial values ofX.nof all agentsX.n.
  The following distributed algorithm terminates and at terminationX.n = GCDfor all agentsX.n.
In the code,successorsis the list of successor agents
in a network of agents, and the agent only sends messages to its
successors.def receive(message, sender):
   if n != message:
      n = gcd(n, message)
      for successor in successors:
         send(n, successor)An invariant of the distributed algorithm is as follows.
The gcd of:all messages in channels andX.nfor all agentsXis GCD.ProofThe proof that the assertion holds initially is trivial.
Prove that if the assertion holds before any event 
then it continues to hold after the event.Let the event be agentAreceiving a messagem.
After the event,mis no longer in the channel;n = gcd(n, m); and messages with the new value ofnare in the output channels fromA.
The proof that the assertion continues to hold in the post-event state
is straightforward.Example of an InvariantAnother invariant for the gcd example, given earlier, is as follows.
For all channels(X, Y)in the network:Channel(X, Y)is empty andX.nis a
  multiple ofY.n, orThe last message in channel(X, Y)ismwherem = X.n.ProofThe assertion holds initially because the second condition holds. 
Prove that if the assertion holds before any event
then it continues to hold after the event.When agentXexecutes a receive that changesX.nthe agent sends a messagemtoYwherem = X.n, and so the second condition
holds.When agentYreceivesmthe agent setsY.n = gcd(Y.n, m), and somis a multiple of
the new value ofY.n.
If channel(X, Y)remains nonempty then the second
condition holds.
If(X, Y)becomes empty then the first condition holds
becausem = X.nandmis a multiple ofY.n.Loop VariantQuestion about Loop Variant in the Sequential
GCDWhat is an example of a loop variant for the following loop?x, y = X, Y
while x != y:
  # assertion: gcd(x, y) = gcd(X, Y)
  if x > y: x = x - y
  else: y = y - xAnswerf(x, y) = x + yis an example of a loop variant for the
following reasons.fis a function of the state of the program.fhas integer values.fis bounded below. The bound is 2 becausexandyare bounded below because of the invariantsx\(\geq\) 1, andy\(\geq\) 1.
  The precise bounds don't matter.Each execution of the loop decreasesf.We can carry out an induction onfbecausefhas integer values and is bounded below.Question about Loop Variants in Distributed AlgorithmsLet's look at the example of the distributed algorithm given
earlier? The example is repeated below.We are given a strongly connected network of agents -- i.e.,
  there is a path from each agent to every other agent.
  Each agent has a local variablen, which is initialized
  as a positive integer.
  
  Let GCD be the gcd (greatest commond divisor) of the
  initial values ofX.nof all agentsX.n.
In the code,successorsis the list of successor agents
in a network of agents, and the agent only sends messages to its
successors.def receive(message, sender):
   if n != message):
      n = gcd(n, message)
      for successor in successors:
         send(n, successor)QuestionWhat is an example of a loop variant for the distributed algorithm?AnswerLetfbe the tuple(N, M)whereNis the sum ofX.nfor all agentsX, andMis the total number of messages in
channels.Comparisons of tuples are made lexicographically.
For example (2, 1) > (1, 10), and (2, 1) < (2, 2).fis bounded below by (0, 0).An induction can be carried out on a tuple of integers.
Next we show that the execution of each event reducesf.When an agentXreceives a messagem,ifX.n\(\neq\)mthenX.nbecomesgcd(X.n, m), and so the new value ofX.nis less than its previous value, and so execution
  of the event decreasesNwhich decreasesf.ifX.n\(=\)mthen the event removes
  messagemfrom a channel and does not add messages to
  channels. So the event decreasesMwhich decreasesf.Termination Condition of a While LoopWhat can we say about the state at termination of the while loop in
the gcd example:x, y = X, Y
while x != y:
  # assertion: gcd(x, y) = gcd(X, Y)
  if x > y: x = x - y
  else: y = y - xThe while loop terminates whenx = y.
From the invariant, at termination:gcd(x, y) = gcd(x, x) = x = gcd(X, Y)Thereforexandyaregcd(X, Y)at termination of the while loop.Termination Condition of a Distributed AlgorithmWhat is the termination condition of the distributed gcd algorithm in
which thereceiveof each agent is as follows?def receive(message, sender):
   if n != message:
      n = gcd(n, message)
      for successor in successors:
         send(n, successor)At termination all channels are empty.
From the invariant, if channel (X, Y) is empty then X.n is a multiple
of Y.n.Therefore when all channels are empty, for every channel (X, Y) in the
system X.n is a multiple of Y.n.
Because the directed graph of agents and edges specified assuccessorsof agents is strongly
connected:for all agents X, Y in the system: X.n is a multiple of Y.nTherefore X.n = Y.n for all agents X, Y.
From the invariant, X.n is GCD.NextThe next webpage discussesdata flow in computations.Frequenty Asked QuestionsK. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology



Figure Link: https://kmchandy.github.io/DISTRIBUTED_SYSTEM_MODELS/BasicsStatesExamples.001.jpeg

Caption: Figure State 1: Initial State


Initial State: Channel (X, Y) has one balloon and channel (Y, X) has 
  one balloon. And the countdown for X is 2 and the countdown for Y is
  3. We represent this state by:nX = 1, nY = 2, (X, Y) = 1, (Y, X) = 1The sequence of steps from the initial state are as follows.Y receives a balloon and sends it back. So the state
  becomes:nX = 1, nY = 1, (X, Y) = 0, (Y, X) = 2X receives a balloon and  sends it back. So the state
  becomesnX = 0, nY = 1, (X, Y) = 1, (Y, X) = 1Y receives a balloon and sends it back. So the state
  becomesnX = 0, nY = 0, (X, Y) = 0, (Y, X) = 2X receives a balloon and pops it. So the state
  becomesnX = 0, nY = 0, (X, Y) = 0, (Y, X) = 1X receives a balloon and pops it. So the state
  becomesnX = 0, nY = 0, (X, Y) = 0, (Y, X) = 0There are no further steps in this computation.QuestionWhat are other transitions from the initial state of the computation?AnswerLet's look at the event in step 2 of the above computation. The event
is as follows.Event: X receives a balloon for the first timeThis event is specified by the 4-tuple:X's state before the event is nX = 1The message that is received is 1 (balloon) and the message is from
  Y.X's state after the event is nX = 0The message that is sent in the event is 1 (balloon) and the message is to
  Y.This event can be executed in any state S which satisfies the inputs
to this event:X's state before the event is nX = 1The message at the head of channel (Y, X) is 1.So, this event can be executed in the initial state.The execution of this event causes a transition
from the initial state to a state shown in the following diagram:Figure Another Transition from the Initial StateQuestionCan you give examples of other computations starting in the same
state?AnswerInitial State: Channel (X, Y) has one balloon and channel (Y, X) has
  one balloon. And the countdown for X is 2 and the countdown for Y is
  3. We represent this state by:nX = 1, nY = 2, (X, Y) = 1, (Y, X) = 1X receives a balloon and sends it back. So the state
  becomes:nX = 0, nY = 2, (X, Y) = 2, (Y, X) = 0Y receives a balloon and  sends it back. So the state
  becomesnX = 0, nY = 1, (X, Y) = 1, (Y, X) = 1X receives a balloon and pops it. So the state
  becomesnX = 0, nY = 1, (X, Y) = 1, (Y, X) = 0Y receives a balloon sends it back. So the state
  becomesnX = 0, nY = 0, (X, Y) = 0, (Y, X) = 1X receives a balloon and pops it. So the state
  becomesnX = 0, nY = 0, (X, Y) = 0, (Y, X) = 0There are no further steps in this computation.Some states (e.g. State 2) in the first example of a computation don't occur in the
second example, and vice versa.A Computation of the System fromPrevious PagesQuestionWhat is an example of a computation, starting from the initial state,
of theexample?AnswerThe following sequence of diagrams shows a computation of the example.
Each diagram shows a state of the system. There is a
transition from state S_k to state S_(k+1)Figure State S_0: Initial StateFigure State S_1Figure State S_2Figure State S_3Figure State S_4Figure State S_5Figure State S_6Figure State S_7Figure State S_8Figure State S_9Invariants, Loop Variants and Termination in
the Simple Balloon ExampleQuestionGive an example of an invariant for the balloon example.AnswerAn invariant of the system is:Number of balloons is at most 2which is equivalent to:(X, Y) + (Y, X) <= 2(Here we are abusing notation using (X, Y) for a channel and also for
the state of the channel.)QuestionHow do you prove that this condition is invariant?AnswerTo prove that this predicate (Boolean condition) is an invariant we
prove that (1) it holds initially 
and (2) and for all state transitions, if the invariant holds in the
state before the transition then it holds in the state after the transition.Initially:(X, Y) = 1and(Y, X) = 1So, the predicate holds initially.A state transition in which a balloon is returned does not change the value
of(X, Y) + (Y, X).A state transition in which a balloon is popped decreases(X, Y)
+ (Y, X).Therefore if the predicate holds before a state
transition then it holds after the transition.QuestionIs the following an invariant of the system?((X, Y) + (Y, X) = 2) or (nX = 0) or (nY = 0)AnswerYes, this predicate is an invariant.To prove that this predicate is an invariant we
prove that (1) it holds initially 
and (2) and for all state transitions, if the invariant holds in the
state before the transition then it holds in the state after the
transition.Initially,(X, Y) = 1and(Y, X) = 1.
Therefore the predicate holds initially.Let S be the state before a transition to a state S'.
We will prove that if the predicate holds in S then the predicate
holds in S'.Consider two cases:nX = 0 or nY = 0 in S.Because nX and nY do not increase,
  and nX and nY do not become negative,
  in the execution of an event, it follows that nX = 0 or nY = 0 in S'.
  Therefore the predicate holds in S'.nX > 0 and nY > 0 in S.Because the invariant holds in S it follows that(X, Y) + (Y,
  X) = 2in S.Because nY > 0 the execution of an event in which Y receives a
  balloon leaves(X, Y) + (Y, X)unchanged.Because nX > 0 the execution of an event in which X receives a
  balloon leaves(X, Y) + (Y, X)unchanged.Therefore(X, Y) + (Y, X) = 2in S'.
  So, the predicate holds in S'.Example of a Loop VariantQuestionHow do you prove that the algorithm terminates?AnswerTo prove that the algorithm terminates we use the loop variant:nX + nY + (X, Y) + (Y, X)The loop variant is a function of the state (i.e. variables) of the system.
In this example, the loop variant maps states of the program to
integers.We must show that  (1) the loop variant is bounded below and we cancarry
out inductionon the values of the loop variant, i.e., it can
decrease only a finite number of times before it reaches a lower
bound, and (2) the executions of all steps in all computations that
start at initial states reduce the value of the loop variant.
The specific lower bound is irrelevant for the proof of termination.In this example, 0 is (obviously) a lower bound.
Next we show that the execution of any event in any state reduces the
value of the loop variant.When Y receives a balloon if nY is positive then nY
decreases which the loop variant, and if nY is 0 then
(X, Y) which decreases the loop variant.Similarly every event in which X receives a balloon also reduces the
loop variant. Thus the executions of all events reduce the loop variant.Therefore the algorithm terminates execution.QuestionWhat can you prove about the state of the system when the computation
terminates? 
We have shown that computations terminate and we given examples of
invariants.AnswerAt termination all channels are empty.
Let's prove that nX or nY is 0 at termination.From the invariants if:(X, Y) + (Y, X)\(\neq\) 2 then nX
or nY is 0.At termination,(X, Y) + (Y, X)\(=\) 0, and the result follows.More Examples of Invariants and Loop VariantsWe reason about the correctness of many distributed algorithms in
  in the same way that we reason about sequentialwhileloops, by using invariants and loop variants.Example: GCD using a While LoopAloop invariantis an assertion about the state of the program that holds before and
  after each iteration of the loop.
  You can look upmany examplesof loop invariants on the web.Example: InvariantA loop invariant in the following greatest common
  divisor (gcd) program is shown in the body of the loop as the
  assertiongcd(x, y) = gcd(X, Y).x, y = X, Y
while x != y:
  # assertion: gcd(x, y) = gcd(X, Y)
  if x > y: x = x - y
  else: y = y - xInvariant of a Distributed System for GCDAn invariant of a distributed system is a
  predicates that holds in all states reachable from initial states.
  An invariant of a distributed system is the loop invariant of the
  following while loop.
  An invariantInvis shown
  as an assertion in the body of the loop.while there exists a nonempty input channel in the system:
   # assertion: Inv
   select a nonempty channel (u, v) in the system
   let the head of channel (u, v) be msg
   v executes receive(msg, u)Example of a Distributed GCDWe are given a strongly connected network of agents -- i.e.,
  there is a path from each agent to every other agent.
  Each agent has a local variablen, which is initialized
  as a positive integer.
  
  Let GCD be the gcd (greatest commond divisor) of the
  initial values ofX.nof all agentsX.n.
  The following distributed algorithm terminates and at terminationX.n = GCDfor all agentsX.n.
In the code,successorsis the list of successor agents
in a network of agents, and the agent only sends messages to its
successors.def receive(message, sender):
   if n != message:
      n = gcd(n, message)
      for successor in successors:
         send(n, successor)An invariant of the distributed algorithm is as follows.
The gcd of:all messages in channels andX.nfor all agentsXis GCD.ProofThe proof that the assertion holds initially is trivial.
Prove that if the assertion holds before any event 
then it continues to hold after the event.Let the event be agentAreceiving a messagem.
After the event,mis no longer in the channel;n = gcd(n, m); and messages with the new value ofnare in the output channels fromA.
The proof that the assertion continues to hold in the post-event state
is straightforward.Example of an InvariantAnother invariant for the gcd example, given earlier, is as follows.
For all channels(X, Y)in the network:Channel(X, Y)is empty andX.nis a
  multiple ofY.n, orThe last message in channel(X, Y)ismwherem = X.n.ProofThe assertion holds initially because the second condition holds. 
Prove that if the assertion holds before any event
then it continues to hold after the event.When agentXexecutes a receive that changesX.nthe agent sends a messagemtoYwherem = X.n, and so the second condition
holds.When agentYreceivesmthe agent setsY.n = gcd(Y.n, m), and somis a multiple of
the new value ofY.n.
If channel(X, Y)remains nonempty then the second
condition holds.
If(X, Y)becomes empty then the first condition holds
becausem = X.nandmis a multiple ofY.n.Loop VariantQuestion about Loop Variant in the Sequential
GCDWhat is an example of a loop variant for the following loop?x, y = X, Y
while x != y:
  # assertion: gcd(x, y) = gcd(X, Y)
  if x > y: x = x - y
  else: y = y - xAnswerf(x, y) = x + yis an example of a loop variant for the
following reasons.fis a function of the state of the program.fhas integer values.fis bounded below. The bound is 2 becausexandyare bounded below because of the invariantsx\(\geq\) 1, andy\(\geq\) 1.
  The precise bounds don't matter.Each execution of the loop decreasesf.We can carry out an induction onfbecausefhas integer values and is bounded below.Question about Loop Variants in Distributed AlgorithmsLet's look at the example of the distributed algorithm given
earlier? The example is repeated below.We are given a strongly connected network of agents -- i.e.,
  there is a path from each agent to every other agent.
  Each agent has a local variablen, which is initialized
  as a positive integer.
  
  Let GCD be the gcd (greatest commond divisor) of the
  initial values ofX.nof all agentsX.n.
In the code,successorsis the list of successor agents
in a network of agents, and the agent only sends messages to its
successors.def receive(message, sender):
   if n != message):
      n = gcd(n, message)
      for successor in successors:
         send(n, successor)QuestionWhat is an example of a loop variant for the distributed algorithm?AnswerLetfbe the tuple(N, M)whereNis the sum ofX.nfor all agentsX, andMis the total number of messages in
channels.Comparisons of tuples are made lexicographically.
For example (2, 1) > (1, 10), and (2, 1) < (2, 2).fis bounded below by (0, 0).An induction can be carried out on a tuple of integers.
Next we show that the execution of each event reducesf.When an agentXreceives a messagem,ifX.n\(\neq\)mthenX.nbecomesgcd(X.n, m), and so the new value ofX.nis less than its previous value, and so execution
  of the event decreasesNwhich decreasesf.ifX.n\(=\)mthen the event removes
  messagemfrom a channel and does not add messages to
  channels. So the event decreasesMwhich decreasesf.Termination Condition of a While LoopWhat can we say about the state at termination of the while loop in
the gcd example:x, y = X, Y
while x != y:
  # assertion: gcd(x, y) = gcd(X, Y)
  if x > y: x = x - y
  else: y = y - xThe while loop terminates whenx = y.
From the invariant, at termination:gcd(x, y) = gcd(x, x) = x = gcd(X, Y)Thereforexandyaregcd(X, Y)at termination of the while loop.Termination Condition of a Distributed AlgorithmWhat is the termination condition of the distributed gcd algorithm in
which thereceiveof each agent is as follows?def receive(message, sender):
   if n != message:
      n = gcd(n, message)
      for successor in successors:
         send(n, successor)At termination all channels are empty.
From the invariant, if channel (X, Y) is empty then X.n is a multiple
of Y.n.Therefore when all channels are empty, for every channel (X, Y) in the
system X.n is a multiple of Y.n.
Because the directed graph of agents and edges specified assuccessorsof agents is strongly
connected:for all agents X, Y in the system: X.n is a multiple of Y.nTherefore X.n = Y.n for all agents X, Y.
From the invariant, X.n is GCD.NextThe next webpage discussesdata flow in computations.Frequenty Asked QuestionsK. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology



Figure Link: https://kmchandy.github.io/DISTRIBUTED_SYSTEM_MODELS/BasicsStatesExamples.007.jpeg

Caption: Figure Another Transition from the Initial State


The sequence of steps from the initial state are as follows.Y receives a balloon and sends it back. So the state
  becomes:nX = 1, nY = 1, (X, Y) = 0, (Y, X) = 2X receives a balloon and  sends it back. So the state
  becomesnX = 0, nY = 1, (X, Y) = 1, (Y, X) = 1Y receives a balloon and sends it back. So the state
  becomesnX = 0, nY = 0, (X, Y) = 0, (Y, X) = 2X receives a balloon and pops it. So the state
  becomesnX = 0, nY = 0, (X, Y) = 0, (Y, X) = 1X receives a balloon and pops it. So the state
  becomesnX = 0, nY = 0, (X, Y) = 0, (Y, X) = 0There are no further steps in this computation.QuestionWhat are other transitions from the initial state of the computation?AnswerLet's look at the event in step 2 of the above computation. The event
is as follows.Event: X receives a balloon for the first timeThis event is specified by the 4-tuple:X's state before the event is nX = 1The message that is received is 1 (balloon) and the message is from
  Y.X's state after the event is nX = 0The message that is sent in the event is 1 (balloon) and the message is to
  Y.This event can be executed in any state S which satisfies the inputs
to this event:X's state before the event is nX = 1The message at the head of channel (Y, X) is 1.So, this event can be executed in the initial state.The execution of this event causes a transition
from the initial state to a state shown in the following diagram:Figure Another Transition from the Initial StateQuestionCan you give examples of other computations starting in the same
state?AnswerInitial State: Channel (X, Y) has one balloon and channel (Y, X) has
  one balloon. And the countdown for X is 2 and the countdown for Y is
  3. We represent this state by:nX = 1, nY = 2, (X, Y) = 1, (Y, X) = 1X receives a balloon and sends it back. So the state
  becomes:nX = 0, nY = 2, (X, Y) = 2, (Y, X) = 0Y receives a balloon and  sends it back. So the state
  becomesnX = 0, nY = 1, (X, Y) = 1, (Y, X) = 1X receives a balloon and pops it. So the state
  becomesnX = 0, nY = 1, (X, Y) = 1, (Y, X) = 0Y receives a balloon sends it back. So the state
  becomesnX = 0, nY = 0, (X, Y) = 0, (Y, X) = 1X receives a balloon and pops it. So the state
  becomesnX = 0, nY = 0, (X, Y) = 0, (Y, X) = 0There are no further steps in this computation.Some states (e.g. State 2) in the first example of a computation don't occur in the
second example, and vice versa.A Computation of the System fromPrevious PagesQuestionWhat is an example of a computation, starting from the initial state,
of theexample?AnswerThe following sequence of diagrams shows a computation of the example.
Each diagram shows a state of the system. There is a
transition from state S_k to state S_(k+1)Figure State S_0: Initial StateFigure State S_1Figure State S_2Figure State S_3Figure State S_4Figure State S_5Figure State S_6Figure State S_7Figure State S_8Figure State S_9Invariants, Loop Variants and Termination in
the Simple Balloon ExampleQuestionGive an example of an invariant for the balloon example.AnswerAn invariant of the system is:Number of balloons is at most 2which is equivalent to:(X, Y) + (Y, X) <= 2(Here we are abusing notation using (X, Y) for a channel and also for
the state of the channel.)QuestionHow do you prove that this condition is invariant?AnswerTo prove that this predicate (Boolean condition) is an invariant we
prove that (1) it holds initially 
and (2) and for all state transitions, if the invariant holds in the
state before the transition then it holds in the state after the transition.Initially:(X, Y) = 1and(Y, X) = 1So, the predicate holds initially.A state transition in which a balloon is returned does not change the value
of(X, Y) + (Y, X).A state transition in which a balloon is popped decreases(X, Y)
+ (Y, X).Therefore if the predicate holds before a state
transition then it holds after the transition.QuestionIs the following an invariant of the system?((X, Y) + (Y, X) = 2) or (nX = 0) or (nY = 0)AnswerYes, this predicate is an invariant.To prove that this predicate is an invariant we
prove that (1) it holds initially 
and (2) and for all state transitions, if the invariant holds in the
state before the transition then it holds in the state after the
transition.Initially,(X, Y) = 1and(Y, X) = 1.
Therefore the predicate holds initially.Let S be the state before a transition to a state S'.
We will prove that if the predicate holds in S then the predicate
holds in S'.Consider two cases:nX = 0 or nY = 0 in S.Because nX and nY do not increase,
  and nX and nY do not become negative,
  in the execution of an event, it follows that nX = 0 or nY = 0 in S'.
  Therefore the predicate holds in S'.nX > 0 and nY > 0 in S.Because the invariant holds in S it follows that(X, Y) + (Y,
  X) = 2in S.Because nY > 0 the execution of an event in which Y receives a
  balloon leaves(X, Y) + (Y, X)unchanged.Because nX > 0 the execution of an event in which X receives a
  balloon leaves(X, Y) + (Y, X)unchanged.Therefore(X, Y) + (Y, X) = 2in S'.
  So, the predicate holds in S'.Example of a Loop VariantQuestionHow do you prove that the algorithm terminates?AnswerTo prove that the algorithm terminates we use the loop variant:nX + nY + (X, Y) + (Y, X)The loop variant is a function of the state (i.e. variables) of the system.
In this example, the loop variant maps states of the program to
integers.We must show that  (1) the loop variant is bounded below and we cancarry
out inductionon the values of the loop variant, i.e., it can
decrease only a finite number of times before it reaches a lower
bound, and (2) the executions of all steps in all computations that
start at initial states reduce the value of the loop variant.
The specific lower bound is irrelevant for the proof of termination.In this example, 0 is (obviously) a lower bound.
Next we show that the execution of any event in any state reduces the
value of the loop variant.When Y receives a balloon if nY is positive then nY
decreases which the loop variant, and if nY is 0 then
(X, Y) which decreases the loop variant.Similarly every event in which X receives a balloon also reduces the
loop variant. Thus the executions of all events reduce the loop variant.Therefore the algorithm terminates execution.QuestionWhat can you prove about the state of the system when the computation
terminates? 
We have shown that computations terminate and we given examples of
invariants.AnswerAt termination all channels are empty.
Let's prove that nX or nY is 0 at termination.From the invariants if:(X, Y) + (Y, X)\(\neq\) 2 then nX
or nY is 0.At termination,(X, Y) + (Y, X)\(=\) 0, and the result follows.More Examples of Invariants and Loop VariantsWe reason about the correctness of many distributed algorithms in
  in the same way that we reason about sequentialwhileloops, by using invariants and loop variants.Example: GCD using a While LoopAloop invariantis an assertion about the state of the program that holds before and
  after each iteration of the loop.
  You can look upmany examplesof loop invariants on the web.Example: InvariantA loop invariant in the following greatest common
  divisor (gcd) program is shown in the body of the loop as the
  assertiongcd(x, y) = gcd(X, Y).x, y = X, Y
while x != y:
  # assertion: gcd(x, y) = gcd(X, Y)
  if x > y: x = x - y
  else: y = y - xInvariant of a Distributed System for GCDAn invariant of a distributed system is a
  predicates that holds in all states reachable from initial states.
  An invariant of a distributed system is the loop invariant of the
  following while loop.
  An invariantInvis shown
  as an assertion in the body of the loop.while there exists a nonempty input channel in the system:
   # assertion: Inv
   select a nonempty channel (u, v) in the system
   let the head of channel (u, v) be msg
   v executes receive(msg, u)Example of a Distributed GCDWe are given a strongly connected network of agents -- i.e.,
  there is a path from each agent to every other agent.
  Each agent has a local variablen, which is initialized
  as a positive integer.
  
  Let GCD be the gcd (greatest commond divisor) of the
  initial values ofX.nof all agentsX.n.
  The following distributed algorithm terminates and at terminationX.n = GCDfor all agentsX.n.
In the code,successorsis the list of successor agents
in a network of agents, and the agent only sends messages to its
successors.def receive(message, sender):
   if n != message:
      n = gcd(n, message)
      for successor in successors:
         send(n, successor)An invariant of the distributed algorithm is as follows.
The gcd of:all messages in channels andX.nfor all agentsXis GCD.ProofThe proof that the assertion holds initially is trivial.
Prove that if the assertion holds before any event 
then it continues to hold after the event.Let the event be agentAreceiving a messagem.
After the event,mis no longer in the channel;n = gcd(n, m); and messages with the new value ofnare in the output channels fromA.
The proof that the assertion continues to hold in the post-event state
is straightforward.Example of an InvariantAnother invariant for the gcd example, given earlier, is as follows.
For all channels(X, Y)in the network:Channel(X, Y)is empty andX.nis a
  multiple ofY.n, orThe last message in channel(X, Y)ismwherem = X.n.ProofThe assertion holds initially because the second condition holds. 
Prove that if the assertion holds before any event
then it continues to hold after the event.When agentXexecutes a receive that changesX.nthe agent sends a messagemtoYwherem = X.n, and so the second condition
holds.When agentYreceivesmthe agent setsY.n = gcd(Y.n, m), and somis a multiple of
the new value ofY.n.
If channel(X, Y)remains nonempty then the second
condition holds.
If(X, Y)becomes empty then the first condition holds
becausem = X.nandmis a multiple ofY.n.Loop VariantQuestion about Loop Variant in the Sequential
GCDWhat is an example of a loop variant for the following loop?x, y = X, Y
while x != y:
  # assertion: gcd(x, y) = gcd(X, Y)
  if x > y: x = x - y
  else: y = y - xAnswerf(x, y) = x + yis an example of a loop variant for the
following reasons.fis a function of the state of the program.fhas integer values.fis bounded below. The bound is 2 becausexandyare bounded below because of the invariantsx\(\geq\) 1, andy\(\geq\) 1.
  The precise bounds don't matter.Each execution of the loop decreasesf.We can carry out an induction onfbecausefhas integer values and is bounded below.Question about Loop Variants in Distributed AlgorithmsLet's look at the example of the distributed algorithm given
earlier? The example is repeated below.We are given a strongly connected network of agents -- i.e.,
  there is a path from each agent to every other agent.
  Each agent has a local variablen, which is initialized
  as a positive integer.
  
  Let GCD be the gcd (greatest commond divisor) of the
  initial values ofX.nof all agentsX.n.
In the code,successorsis the list of successor agents
in a network of agents, and the agent only sends messages to its
successors.def receive(message, sender):
   if n != message):
      n = gcd(n, message)
      for successor in successors:
         send(n, successor)QuestionWhat is an example of a loop variant for the distributed algorithm?AnswerLetfbe the tuple(N, M)whereNis the sum ofX.nfor all agentsX, andMis the total number of messages in
channels.Comparisons of tuples are made lexicographically.
For example (2, 1) > (1, 10), and (2, 1) < (2, 2).fis bounded below by (0, 0).An induction can be carried out on a tuple of integers.
Next we show that the execution of each event reducesf.When an agentXreceives a messagem,ifX.n\(\neq\)mthenX.nbecomesgcd(X.n, m), and so the new value ofX.nis less than its previous value, and so execution
  of the event decreasesNwhich decreasesf.ifX.n\(=\)mthen the event removes
  messagemfrom a channel and does not add messages to
  channels. So the event decreasesMwhich decreasesf.Termination Condition of a While LoopWhat can we say about the state at termination of the while loop in
the gcd example:x, y = X, Y
while x != y:
  # assertion: gcd(x, y) = gcd(X, Y)
  if x > y: x = x - y
  else: y = y - xThe while loop terminates whenx = y.
From the invariant, at termination:gcd(x, y) = gcd(x, x) = x = gcd(X, Y)Thereforexandyaregcd(X, Y)at termination of the while loop.Termination Condition of a Distributed AlgorithmWhat is the termination condition of the distributed gcd algorithm in
which thereceiveof each agent is as follows?def receive(message, sender):
   if n != message:
      n = gcd(n, message)
      for successor in successors:
         send(n, successor)At termination all channels are empty.
From the invariant, if channel (X, Y) is empty then X.n is a multiple
of Y.n.Therefore when all channels are empty, for every channel (X, Y) in the
system X.n is a multiple of Y.n.
Because the directed graph of agents and edges specified assuccessorsof agents is strongly
connected:for all agents X, Y in the system: X.n is a multiple of Y.nTherefore X.n = Y.n for all agents X, Y.
From the invariant, X.n is GCD.NextThe next webpage discussesdata flow in computations.Frequenty Asked QuestionsK. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology



Figure Link: https://kmchandy.github.io/DISTRIBUTED_SYSTEM_MODELS/BasicsStatesExamples.007.jpeg

Caption: Figure Another Transition from the Initial State


Event: X receives a balloon for the first time



Figure Link: https://kmchandy.github.io/DISTRIBUTED_SYSTEM_MODELS/BasicsStatesExamples.007.jpeg

Caption: Figure Another Transition from the Initial State


The execution of this event causes a transition
from the initial state to a state shown in the following diagram:Figure Another Transition from the Initial StateQuestionCan you give examples of other computations starting in the same
state?AnswerInitial State: Channel (X, Y) has one balloon and channel (Y, X) has
  one balloon. And the countdown for X is 2 and the countdown for Y is
  3. We represent this state by:nX = 1, nY = 2, (X, Y) = 1, (Y, X) = 1X receives a balloon and sends it back. So the state
  becomes:nX = 0, nY = 2, (X, Y) = 2, (Y, X) = 0Y receives a balloon and  sends it back. So the state
  becomesnX = 0, nY = 1, (X, Y) = 1, (Y, X) = 1X receives a balloon and pops it. So the state
  becomesnX = 0, nY = 1, (X, Y) = 1, (Y, X) = 0Y receives a balloon sends it back. So the state
  becomesnX = 0, nY = 0, (X, Y) = 0, (Y, X) = 1X receives a balloon and pops it. So the state
  becomesnX = 0, nY = 0, (X, Y) = 0, (Y, X) = 0There are no further steps in this computation.Some states (e.g. State 2) in the first example of a computation don't occur in the
second example, and vice versa.A Computation of the System fromPrevious PagesQuestionWhat is an example of a computation, starting from the initial state,
of theexample?AnswerThe following sequence of diagrams shows a computation of the example.
Each diagram shows a state of the system. There is a
transition from state S_k to state S_(k+1)Figure State S_0: Initial StateFigure State S_1Figure State S_2Figure State S_3Figure State S_4Figure State S_5Figure State S_6Figure State S_7Figure State S_8Figure State S_9Invariants, Loop Variants and Termination in
the Simple Balloon ExampleQuestionGive an example of an invariant for the balloon example.AnswerAn invariant of the system is:Number of balloons is at most 2which is equivalent to:(X, Y) + (Y, X) <= 2(Here we are abusing notation using (X, Y) for a channel and also for
the state of the channel.)QuestionHow do you prove that this condition is invariant?AnswerTo prove that this predicate (Boolean condition) is an invariant we
prove that (1) it holds initially 
and (2) and for all state transitions, if the invariant holds in the
state before the transition then it holds in the state after the transition.Initially:(X, Y) = 1and(Y, X) = 1So, the predicate holds initially.A state transition in which a balloon is returned does not change the value
of(X, Y) + (Y, X).A state transition in which a balloon is popped decreases(X, Y)
+ (Y, X).Therefore if the predicate holds before a state
transition then it holds after the transition.QuestionIs the following an invariant of the system?((X, Y) + (Y, X) = 2) or (nX = 0) or (nY = 0)AnswerYes, this predicate is an invariant.To prove that this predicate is an invariant we
prove that (1) it holds initially 
and (2) and for all state transitions, if the invariant holds in the
state before the transition then it holds in the state after the
transition.Initially,(X, Y) = 1and(Y, X) = 1.
Therefore the predicate holds initially.Let S be the state before a transition to a state S'.
We will prove that if the predicate holds in S then the predicate
holds in S'.Consider two cases:nX = 0 or nY = 0 in S.Because nX and nY do not increase,
  and nX and nY do not become negative,
  in the execution of an event, it follows that nX = 0 or nY = 0 in S'.
  Therefore the predicate holds in S'.nX > 0 and nY > 0 in S.Because the invariant holds in S it follows that(X, Y) + (Y,
  X) = 2in S.Because nY > 0 the execution of an event in which Y receives a
  balloon leaves(X, Y) + (Y, X)unchanged.Because nX > 0 the execution of an event in which X receives a
  balloon leaves(X, Y) + (Y, X)unchanged.Therefore(X, Y) + (Y, X) = 2in S'.
  So, the predicate holds in S'.Example of a Loop VariantQuestionHow do you prove that the algorithm terminates?AnswerTo prove that the algorithm terminates we use the loop variant:nX + nY + (X, Y) + (Y, X)The loop variant is a function of the state (i.e. variables) of the system.
In this example, the loop variant maps states of the program to
integers.We must show that  (1) the loop variant is bounded below and we cancarry
out inductionon the values of the loop variant, i.e., it can
decrease only a finite number of times before it reaches a lower
bound, and (2) the executions of all steps in all computations that
start at initial states reduce the value of the loop variant.
The specific lower bound is irrelevant for the proof of termination.In this example, 0 is (obviously) a lower bound.
Next we show that the execution of any event in any state reduces the
value of the loop variant.When Y receives a balloon if nY is positive then nY
decreases which the loop variant, and if nY is 0 then
(X, Y) which decreases the loop variant.Similarly every event in which X receives a balloon also reduces the
loop variant. Thus the executions of all events reduce the loop variant.Therefore the algorithm terminates execution.QuestionWhat can you prove about the state of the system when the computation
terminates? 
We have shown that computations terminate and we given examples of
invariants.AnswerAt termination all channels are empty.
Let's prove that nX or nY is 0 at termination.From the invariants if:(X, Y) + (Y, X)\(\neq\) 2 then nX
or nY is 0.At termination,(X, Y) + (Y, X)\(=\) 0, and the result follows.More Examples of Invariants and Loop VariantsWe reason about the correctness of many distributed algorithms in
  in the same way that we reason about sequentialwhileloops, by using invariants and loop variants.Example: GCD using a While LoopAloop invariantis an assertion about the state of the program that holds before and
  after each iteration of the loop.
  You can look upmany examplesof loop invariants on the web.Example: InvariantA loop invariant in the following greatest common
  divisor (gcd) program is shown in the body of the loop as the
  assertiongcd(x, y) = gcd(X, Y).x, y = X, Y
while x != y:
  # assertion: gcd(x, y) = gcd(X, Y)
  if x > y: x = x - y
  else: y = y - xInvariant of a Distributed System for GCDAn invariant of a distributed system is a
  predicates that holds in all states reachable from initial states.
  An invariant of a distributed system is the loop invariant of the
  following while loop.
  An invariantInvis shown
  as an assertion in the body of the loop.while there exists a nonempty input channel in the system:
   # assertion: Inv
   select a nonempty channel (u, v) in the system
   let the head of channel (u, v) be msg
   v executes receive(msg, u)Example of a Distributed GCDWe are given a strongly connected network of agents -- i.e.,
  there is a path from each agent to every other agent.
  Each agent has a local variablen, which is initialized
  as a positive integer.
  
  Let GCD be the gcd (greatest commond divisor) of the
  initial values ofX.nof all agentsX.n.
  The following distributed algorithm terminates and at terminationX.n = GCDfor all agentsX.n.
In the code,successorsis the list of successor agents
in a network of agents, and the agent only sends messages to its
successors.def receive(message, sender):
   if n != message:
      n = gcd(n, message)
      for successor in successors:
         send(n, successor)An invariant of the distributed algorithm is as follows.
The gcd of:all messages in channels andX.nfor all agentsXis GCD.ProofThe proof that the assertion holds initially is trivial.
Prove that if the assertion holds before any event 
then it continues to hold after the event.Let the event be agentAreceiving a messagem.
After the event,mis no longer in the channel;n = gcd(n, m); and messages with the new value ofnare in the output channels fromA.
The proof that the assertion continues to hold in the post-event state
is straightforward.Example of an InvariantAnother invariant for the gcd example, given earlier, is as follows.
For all channels(X, Y)in the network:Channel(X, Y)is empty andX.nis a
  multiple ofY.n, orThe last message in channel(X, Y)ismwherem = X.n.ProofThe assertion holds initially because the second condition holds. 
Prove that if the assertion holds before any event
then it continues to hold after the event.When agentXexecutes a receive that changesX.nthe agent sends a messagemtoYwherem = X.n, and so the second condition
holds.When agentYreceivesmthe agent setsY.n = gcd(Y.n, m), and somis a multiple of
the new value ofY.n.
If channel(X, Y)remains nonempty then the second
condition holds.
If(X, Y)becomes empty then the first condition holds
becausem = X.nandmis a multiple ofY.n.Loop VariantQuestion about Loop Variant in the Sequential
GCDWhat is an example of a loop variant for the following loop?x, y = X, Y
while x != y:
  # assertion: gcd(x, y) = gcd(X, Y)
  if x > y: x = x - y
  else: y = y - xAnswerf(x, y) = x + yis an example of a loop variant for the
following reasons.fis a function of the state of the program.fhas integer values.fis bounded below. The bound is 2 becausexandyare bounded below because of the invariantsx\(\geq\) 1, andy\(\geq\) 1.
  The precise bounds don't matter.Each execution of the loop decreasesf.We can carry out an induction onfbecausefhas integer values and is bounded below.Question about Loop Variants in Distributed AlgorithmsLet's look at the example of the distributed algorithm given
earlier? The example is repeated below.We are given a strongly connected network of agents -- i.e.,
  there is a path from each agent to every other agent.
  Each agent has a local variablen, which is initialized
  as a positive integer.
  
  Let GCD be the gcd (greatest commond divisor) of the
  initial values ofX.nof all agentsX.n.
In the code,successorsis the list of successor agents
in a network of agents, and the agent only sends messages to its
successors.def receive(message, sender):
   if n != message):
      n = gcd(n, message)
      for successor in successors:
         send(n, successor)QuestionWhat is an example of a loop variant for the distributed algorithm?AnswerLetfbe the tuple(N, M)whereNis the sum ofX.nfor all agentsX, andMis the total number of messages in
channels.Comparisons of tuples are made lexicographically.
For example (2, 1) > (1, 10), and (2, 1) < (2, 2).fis bounded below by (0, 0).An induction can be carried out on a tuple of integers.
Next we show that the execution of each event reducesf.When an agentXreceives a messagem,ifX.n\(\neq\)mthenX.nbecomesgcd(X.n, m), and so the new value ofX.nis less than its previous value, and so execution
  of the event decreasesNwhich decreasesf.ifX.n\(=\)mthen the event removes
  messagemfrom a channel and does not add messages to
  channels. So the event decreasesMwhich decreasesf.Termination Condition of a While LoopWhat can we say about the state at termination of the while loop in
the gcd example:x, y = X, Y
while x != y:
  # assertion: gcd(x, y) = gcd(X, Y)
  if x > y: x = x - y
  else: y = y - xThe while loop terminates whenx = y.
From the invariant, at termination:gcd(x, y) = gcd(x, x) = x = gcd(X, Y)Thereforexandyaregcd(X, Y)at termination of the while loop.Termination Condition of a Distributed AlgorithmWhat is the termination condition of the distributed gcd algorithm in
which thereceiveof each agent is as follows?def receive(message, sender):
   if n != message:
      n = gcd(n, message)
      for successor in successors:
         send(n, successor)At termination all channels are empty.
From the invariant, if channel (X, Y) is empty then X.n is a multiple
of Y.n.Therefore when all channels are empty, for every channel (X, Y) in the
system X.n is a multiple of Y.n.
Because the directed graph of agents and edges specified assuccessorsof agents is strongly
connected:for all agents X, Y in the system: X.n is a multiple of Y.nTherefore X.n = Y.n for all agents X, Y.
From the invariant, X.n is GCD.NextThe next webpage discussesdata flow in computations.Frequenty Asked QuestionsK. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology



Figure Link: https://kmchandy.github.io/DISTRIBUTED_SYSTEM_MODELS/BasicsStatesExamples.007.jpeg

Caption: Figure Another Transition from the Initial State


Some states (e.g. State 2) in the first example of a computation don't occur in the
second example, and vice versa.A Computation of the System fromPrevious PagesQuestionWhat is an example of a computation, starting from the initial state,
of theexample?AnswerThe following sequence of diagrams shows a computation of the example.
Each diagram shows a state of the system. There is a
transition from state S_k to state S_(k+1)Figure State S_0: Initial StateFigure State S_1Figure State S_2Figure State S_3Figure State S_4Figure State S_5Figure State S_6Figure State S_7Figure State S_8Figure State S_9Invariants, Loop Variants and Termination in
the Simple Balloon ExampleQuestionGive an example of an invariant for the balloon example.AnswerAn invariant of the system is:Number of balloons is at most 2which is equivalent to:(X, Y) + (Y, X) <= 2(Here we are abusing notation using (X, Y) for a channel and also for
the state of the channel.)QuestionHow do you prove that this condition is invariant?AnswerTo prove that this predicate (Boolean condition) is an invariant we
prove that (1) it holds initially 
and (2) and for all state transitions, if the invariant holds in the
state before the transition then it holds in the state after the transition.Initially:(X, Y) = 1and(Y, X) = 1So, the predicate holds initially.A state transition in which a balloon is returned does not change the value
of(X, Y) + (Y, X).A state transition in which a balloon is popped decreases(X, Y)
+ (Y, X).Therefore if the predicate holds before a state
transition then it holds after the transition.QuestionIs the following an invariant of the system?((X, Y) + (Y, X) = 2) or (nX = 0) or (nY = 0)AnswerYes, this predicate is an invariant.To prove that this predicate is an invariant we
prove that (1) it holds initially 
and (2) and for all state transitions, if the invariant holds in the
state before the transition then it holds in the state after the
transition.Initially,(X, Y) = 1and(Y, X) = 1.
Therefore the predicate holds initially.Let S be the state before a transition to a state S'.
We will prove that if the predicate holds in S then the predicate
holds in S'.Consider two cases:nX = 0 or nY = 0 in S.Because nX and nY do not increase,
  and nX and nY do not become negative,
  in the execution of an event, it follows that nX = 0 or nY = 0 in S'.
  Therefore the predicate holds in S'.nX > 0 and nY > 0 in S.Because the invariant holds in S it follows that(X, Y) + (Y,
  X) = 2in S.Because nY > 0 the execution of an event in which Y receives a
  balloon leaves(X, Y) + (Y, X)unchanged.Because nX > 0 the execution of an event in which X receives a
  balloon leaves(X, Y) + (Y, X)unchanged.Therefore(X, Y) + (Y, X) = 2in S'.
  So, the predicate holds in S'.Example of a Loop VariantQuestionHow do you prove that the algorithm terminates?AnswerTo prove that the algorithm terminates we use the loop variant:nX + nY + (X, Y) + (Y, X)The loop variant is a function of the state (i.e. variables) of the system.
In this example, the loop variant maps states of the program to
integers.We must show that  (1) the loop variant is bounded below and we cancarry
out inductionon the values of the loop variant, i.e., it can
decrease only a finite number of times before it reaches a lower
bound, and (2) the executions of all steps in all computations that
start at initial states reduce the value of the loop variant.
The specific lower bound is irrelevant for the proof of termination.In this example, 0 is (obviously) a lower bound.
Next we show that the execution of any event in any state reduces the
value of the loop variant.When Y receives a balloon if nY is positive then nY
decreases which the loop variant, and if nY is 0 then
(X, Y) which decreases the loop variant.Similarly every event in which X receives a balloon also reduces the
loop variant. Thus the executions of all events reduce the loop variant.Therefore the algorithm terminates execution.QuestionWhat can you prove about the state of the system when the computation
terminates? 
We have shown that computations terminate and we given examples of
invariants.AnswerAt termination all channels are empty.
Let's prove that nX or nY is 0 at termination.From the invariants if:(X, Y) + (Y, X)\(\neq\) 2 then nX
or nY is 0.At termination,(X, Y) + (Y, X)\(=\) 0, and the result follows.More Examples of Invariants and Loop VariantsWe reason about the correctness of many distributed algorithms in
  in the same way that we reason about sequentialwhileloops, by using invariants and loop variants.Example: GCD using a While LoopAloop invariantis an assertion about the state of the program that holds before and
  after each iteration of the loop.
  You can look upmany examplesof loop invariants on the web.Example: InvariantA loop invariant in the following greatest common
  divisor (gcd) program is shown in the body of the loop as the
  assertiongcd(x, y) = gcd(X, Y).x, y = X, Y
while x != y:
  # assertion: gcd(x, y) = gcd(X, Y)
  if x > y: x = x - y
  else: y = y - xInvariant of a Distributed System for GCDAn invariant of a distributed system is a
  predicates that holds in all states reachable from initial states.
  An invariant of a distributed system is the loop invariant of the
  following while loop.
  An invariantInvis shown
  as an assertion in the body of the loop.while there exists a nonempty input channel in the system:
   # assertion: Inv
   select a nonempty channel (u, v) in the system
   let the head of channel (u, v) be msg
   v executes receive(msg, u)Example of a Distributed GCDWe are given a strongly connected network of agents -- i.e.,
  there is a path from each agent to every other agent.
  Each agent has a local variablen, which is initialized
  as a positive integer.
  
  Let GCD be the gcd (greatest commond divisor) of the
  initial values ofX.nof all agentsX.n.
  The following distributed algorithm terminates and at terminationX.n = GCDfor all agentsX.n.
In the code,successorsis the list of successor agents
in a network of agents, and the agent only sends messages to its
successors.def receive(message, sender):
   if n != message:
      n = gcd(n, message)
      for successor in successors:
         send(n, successor)An invariant of the distributed algorithm is as follows.
The gcd of:all messages in channels andX.nfor all agentsXis GCD.ProofThe proof that the assertion holds initially is trivial.
Prove that if the assertion holds before any event 
then it continues to hold after the event.Let the event be agentAreceiving a messagem.
After the event,mis no longer in the channel;n = gcd(n, m); and messages with the new value ofnare in the output channels fromA.
The proof that the assertion continues to hold in the post-event state
is straightforward.Example of an InvariantAnother invariant for the gcd example, given earlier, is as follows.
For all channels(X, Y)in the network:Channel(X, Y)is empty andX.nis a
  multiple ofY.n, orThe last message in channel(X, Y)ismwherem = X.n.ProofThe assertion holds initially because the second condition holds. 
Prove that if the assertion holds before any event
then it continues to hold after the event.When agentXexecutes a receive that changesX.nthe agent sends a messagemtoYwherem = X.n, and so the second condition
holds.When agentYreceivesmthe agent setsY.n = gcd(Y.n, m), and somis a multiple of
the new value ofY.n.
If channel(X, Y)remains nonempty then the second
condition holds.
If(X, Y)becomes empty then the first condition holds
becausem = X.nandmis a multiple ofY.n.Loop VariantQuestion about Loop Variant in the Sequential
GCDWhat is an example of a loop variant for the following loop?x, y = X, Y
while x != y:
  # assertion: gcd(x, y) = gcd(X, Y)
  if x > y: x = x - y
  else: y = y - xAnswerf(x, y) = x + yis an example of a loop variant for the
following reasons.fis a function of the state of the program.fhas integer values.fis bounded below. The bound is 2 becausexandyare bounded below because of the invariantsx\(\geq\) 1, andy\(\geq\) 1.
  The precise bounds don't matter.Each execution of the loop decreasesf.We can carry out an induction onfbecausefhas integer values and is bounded below.Question about Loop Variants in Distributed AlgorithmsLet's look at the example of the distributed algorithm given
earlier? The example is repeated below.We are given a strongly connected network of agents -- i.e.,
  there is a path from each agent to every other agent.
  Each agent has a local variablen, which is initialized
  as a positive integer.
  
  Let GCD be the gcd (greatest commond divisor) of the
  initial values ofX.nof all agentsX.n.
In the code,successorsis the list of successor agents
in a network of agents, and the agent only sends messages to its
successors.def receive(message, sender):
   if n != message):
      n = gcd(n, message)
      for successor in successors:
         send(n, successor)QuestionWhat is an example of a loop variant for the distributed algorithm?AnswerLetfbe the tuple(N, M)whereNis the sum ofX.nfor all agentsX, andMis the total number of messages in
channels.Comparisons of tuples are made lexicographically.
For example (2, 1) > (1, 10), and (2, 1) < (2, 2).fis bounded below by (0, 0).An induction can be carried out on a tuple of integers.
Next we show that the execution of each event reducesf.When an agentXreceives a messagem,ifX.n\(\neq\)mthenX.nbecomesgcd(X.n, m), and so the new value ofX.nis less than its previous value, and so execution
  of the event decreasesNwhich decreasesf.ifX.n\(=\)mthen the event removes
  messagemfrom a channel and does not add messages to
  channels. So the event decreasesMwhich decreasesf.Termination Condition of a While LoopWhat can we say about the state at termination of the while loop in
the gcd example:x, y = X, Y
while x != y:
  # assertion: gcd(x, y) = gcd(X, Y)
  if x > y: x = x - y
  else: y = y - xThe while loop terminates whenx = y.
From the invariant, at termination:gcd(x, y) = gcd(x, x) = x = gcd(X, Y)Thereforexandyaregcd(X, Y)at termination of the while loop.Termination Condition of a Distributed AlgorithmWhat is the termination condition of the distributed gcd algorithm in
which thereceiveof each agent is as follows?def receive(message, sender):
   if n != message:
      n = gcd(n, message)
      for successor in successors:
         send(n, successor)At termination all channels are empty.
From the invariant, if channel (X, Y) is empty then X.n is a multiple
of Y.n.Therefore when all channels are empty, for every channel (X, Y) in the
system X.n is a multiple of Y.n.
Because the directed graph of agents and edges specified assuccessorsof agents is strongly
connected:for all agents X, Y in the system: X.n is a multiple of Y.nTherefore X.n = Y.n for all agents X, Y.
From the invariant, X.n is GCD.NextThe next webpage discussesdata flow in computations.Frequenty Asked QuestionsK. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology



Figure Link: https://kmchandy.github.io/DISTRIBUTED_SYSTEM_MODELS/ComputationExamples.001.jpeg

Caption: Figure State S_0: Initial State


Number of balloons is at most 2which is equivalent to:(X, Y) + (Y, X) <= 2(Here we are abusing notation using (X, Y) for a channel and also for
the state of the channel.)QuestionHow do you prove that this condition is invariant?AnswerTo prove that this predicate (Boolean condition) is an invariant we
prove that (1) it holds initially 
and (2) and for all state transitions, if the invariant holds in the
state before the transition then it holds in the state after the transition.Initially:(X, Y) = 1and(Y, X) = 1So, the predicate holds initially.A state transition in which a balloon is returned does not change the value
of(X, Y) + (Y, X).A state transition in which a balloon is popped decreases(X, Y)
+ (Y, X).Therefore if the predicate holds before a state
transition then it holds after the transition.QuestionIs the following an invariant of the system?((X, Y) + (Y, X) = 2) or (nX = 0) or (nY = 0)AnswerYes, this predicate is an invariant.To prove that this predicate is an invariant we
prove that (1) it holds initially 
and (2) and for all state transitions, if the invariant holds in the
state before the transition then it holds in the state after the
transition.Initially,(X, Y) = 1and(Y, X) = 1.
Therefore the predicate holds initially.Let S be the state before a transition to a state S'.
We will prove that if the predicate holds in S then the predicate
holds in S'.Consider two cases:nX = 0 or nY = 0 in S.Because nX and nY do not increase,
  and nX and nY do not become negative,
  in the execution of an event, it follows that nX = 0 or nY = 0 in S'.
  Therefore the predicate holds in S'.nX > 0 and nY > 0 in S.Because the invariant holds in S it follows that(X, Y) + (Y,
  X) = 2in S.Because nY > 0 the execution of an event in which Y receives a
  balloon leaves(X, Y) + (Y, X)unchanged.Because nX > 0 the execution of an event in which X receives a
  balloon leaves(X, Y) + (Y, X)unchanged.Therefore(X, Y) + (Y, X) = 2in S'.
  So, the predicate holds in S'.Example of a Loop VariantQuestionHow do you prove that the algorithm terminates?AnswerTo prove that the algorithm terminates we use the loop variant:nX + nY + (X, Y) + (Y, X)The loop variant is a function of the state (i.e. variables) of the system.
In this example, the loop variant maps states of the program to
integers.We must show that  (1) the loop variant is bounded below and we cancarry
out inductionon the values of the loop variant, i.e., it can
decrease only a finite number of times before it reaches a lower
bound, and (2) the executions of all steps in all computations that
start at initial states reduce the value of the loop variant.
The specific lower bound is irrelevant for the proof of termination.In this example, 0 is (obviously) a lower bound.
Next we show that the execution of any event in any state reduces the
value of the loop variant.When Y receives a balloon if nY is positive then nY
decreases which the loop variant, and if nY is 0 then
(X, Y) which decreases the loop variant.Similarly every event in which X receives a balloon also reduces the
loop variant. Thus the executions of all events reduce the loop variant.Therefore the algorithm terminates execution.QuestionWhat can you prove about the state of the system when the computation
terminates? 
We have shown that computations terminate and we given examples of
invariants.AnswerAt termination all channels are empty.
Let's prove that nX or nY is 0 at termination.From the invariants if:(X, Y) + (Y, X)\(\neq\) 2 then nX
or nY is 0.At termination,(X, Y) + (Y, X)\(=\) 0, and the result follows.More Examples of Invariants and Loop VariantsWe reason about the correctness of many distributed algorithms in
  in the same way that we reason about sequentialwhileloops, by using invariants and loop variants.Example: GCD using a While LoopAloop invariantis an assertion about the state of the program that holds before and
  after each iteration of the loop.
  You can look upmany examplesof loop invariants on the web.Example: InvariantA loop invariant in the following greatest common
  divisor (gcd) program is shown in the body of the loop as the
  assertiongcd(x, y) = gcd(X, Y).x, y = X, Y
while x != y:
  # assertion: gcd(x, y) = gcd(X, Y)
  if x > y: x = x - y
  else: y = y - xInvariant of a Distributed System for GCDAn invariant of a distributed system is a
  predicates that holds in all states reachable from initial states.
  An invariant of a distributed system is the loop invariant of the
  following while loop.
  An invariantInvis shown
  as an assertion in the body of the loop.while there exists a nonempty input channel in the system:
   # assertion: Inv
   select a nonempty channel (u, v) in the system
   let the head of channel (u, v) be msg
   v executes receive(msg, u)Example of a Distributed GCDWe are given a strongly connected network of agents -- i.e.,
  there is a path from each agent to every other agent.
  Each agent has a local variablen, which is initialized
  as a positive integer.
  
  Let GCD be the gcd (greatest commond divisor) of the
  initial values ofX.nof all agentsX.n.
  The following distributed algorithm terminates and at terminationX.n = GCDfor all agentsX.n.
In the code,successorsis the list of successor agents
in a network of agents, and the agent only sends messages to its
successors.def receive(message, sender):
   if n != message:
      n = gcd(n, message)
      for successor in successors:
         send(n, successor)An invariant of the distributed algorithm is as follows.
The gcd of:all messages in channels andX.nfor all agentsXis GCD.ProofThe proof that the assertion holds initially is trivial.
Prove that if the assertion holds before any event 
then it continues to hold after the event.Let the event be agentAreceiving a messagem.
After the event,mis no longer in the channel;n = gcd(n, m); and messages with the new value ofnare in the output channels fromA.
The proof that the assertion continues to hold in the post-event state
is straightforward.Example of an InvariantAnother invariant for the gcd example, given earlier, is as follows.
For all channels(X, Y)in the network:Channel(X, Y)is empty andX.nis a
  multiple ofY.n, orThe last message in channel(X, Y)ismwherem = X.n.ProofThe assertion holds initially because the second condition holds. 
Prove that if the assertion holds before any event
then it continues to hold after the event.When agentXexecutes a receive that changesX.nthe agent sends a messagemtoYwherem = X.n, and so the second condition
holds.When agentYreceivesmthe agent setsY.n = gcd(Y.n, m), and somis a multiple of
the new value ofY.n.
If channel(X, Y)remains nonempty then the second
condition holds.
If(X, Y)becomes empty then the first condition holds
becausem = X.nandmis a multiple ofY.n.Loop VariantQuestion about Loop Variant in the Sequential
GCDWhat is an example of a loop variant for the following loop?x, y = X, Y
while x != y:
  # assertion: gcd(x, y) = gcd(X, Y)
  if x > y: x = x - y
  else: y = y - xAnswerf(x, y) = x + yis an example of a loop variant for the
following reasons.fis a function of the state of the program.fhas integer values.fis bounded below. The bound is 2 becausexandyare bounded below because of the invariantsx\(\geq\) 1, andy\(\geq\) 1.
  The precise bounds don't matter.Each execution of the loop decreasesf.We can carry out an induction onfbecausefhas integer values and is bounded below.Question about Loop Variants in Distributed AlgorithmsLet's look at the example of the distributed algorithm given
earlier? The example is repeated below.We are given a strongly connected network of agents -- i.e.,
  there is a path from each agent to every other agent.
  Each agent has a local variablen, which is initialized
  as a positive integer.
  
  Let GCD be the gcd (greatest commond divisor) of the
  initial values ofX.nof all agentsX.n.
In the code,successorsis the list of successor agents
in a network of agents, and the agent only sends messages to its
successors.def receive(message, sender):
   if n != message):
      n = gcd(n, message)
      for successor in successors:
         send(n, successor)QuestionWhat is an example of a loop variant for the distributed algorithm?AnswerLetfbe the tuple(N, M)whereNis the sum ofX.nfor all agentsX, andMis the total number of messages in
channels.Comparisons of tuples are made lexicographically.
For example (2, 1) > (1, 10), and (2, 1) < (2, 2).fis bounded below by (0, 0).An induction can be carried out on a tuple of integers.
Next we show that the execution of each event reducesf.When an agentXreceives a messagem,ifX.n\(\neq\)mthenX.nbecomesgcd(X.n, m), and so the new value ofX.nis less than its previous value, and so execution
  of the event decreasesNwhich decreasesf.ifX.n\(=\)mthen the event removes
  messagemfrom a channel and does not add messages to
  channels. So the event decreasesMwhich decreasesf.Termination Condition of a While LoopWhat can we say about the state at termination of the while loop in
the gcd example:x, y = X, Y
while x != y:
  # assertion: gcd(x, y) = gcd(X, Y)
  if x > y: x = x - y
  else: y = y - xThe while loop terminates whenx = y.
From the invariant, at termination:gcd(x, y) = gcd(x, x) = x = gcd(X, Y)Thereforexandyaregcd(X, Y)at termination of the while loop.Termination Condition of a Distributed AlgorithmWhat is the termination condition of the distributed gcd algorithm in
which thereceiveof each agent is as follows?def receive(message, sender):
   if n != message:
      n = gcd(n, message)
      for successor in successors:
         send(n, successor)At termination all channels are empty.
From the invariant, if channel (X, Y) is empty then X.n is a multiple
of Y.n.Therefore when all channels are empty, for every channel (X, Y) in the
system X.n is a multiple of Y.n.
Because the directed graph of agents and edges specified assuccessorsof agents is strongly
connected:for all agents X, Y in the system: X.n is a multiple of Y.nTherefore X.n = Y.n for all agents X, Y.
From the invariant, X.n is GCD.NextThe next webpage discussesdata flow in computations.Frequenty Asked QuestionsK. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

which is equivalent to:(X, Y) + (Y, X) <= 2(Here we are abusing notation using (X, Y) for a channel and also for
the state of the channel.)QuestionHow do you prove that this condition is invariant?AnswerTo prove that this predicate (Boolean condition) is an invariant we
prove that (1) it holds initially 
and (2) and for all state transitions, if the invariant holds in the
state before the transition then it holds in the state after the transition.Initially:(X, Y) = 1and(Y, X) = 1So, the predicate holds initially.A state transition in which a balloon is returned does not change the value
of(X, Y) + (Y, X).A state transition in which a balloon is popped decreases(X, Y)
+ (Y, X).Therefore if the predicate holds before a state
transition then it holds after the transition.QuestionIs the following an invariant of the system?((X, Y) + (Y, X) = 2) or (nX = 0) or (nY = 0)AnswerYes, this predicate is an invariant.To prove that this predicate is an invariant we
prove that (1) it holds initially 
and (2) and for all state transitions, if the invariant holds in the
state before the transition then it holds in the state after the
transition.Initially,(X, Y) = 1and(Y, X) = 1.
Therefore the predicate holds initially.Let S be the state before a transition to a state S'.
We will prove that if the predicate holds in S then the predicate
holds in S'.Consider two cases:nX = 0 or nY = 0 in S.Because nX and nY do not increase,
  and nX and nY do not become negative,
  in the execution of an event, it follows that nX = 0 or nY = 0 in S'.
  Therefore the predicate holds in S'.nX > 0 and nY > 0 in S.Because the invariant holds in S it follows that(X, Y) + (Y,
  X) = 2in S.Because nY > 0 the execution of an event in which Y receives a
  balloon leaves(X, Y) + (Y, X)unchanged.Because nX > 0 the execution of an event in which X receives a
  balloon leaves(X, Y) + (Y, X)unchanged.Therefore(X, Y) + (Y, X) = 2in S'.
  So, the predicate holds in S'.Example of a Loop VariantQuestionHow do you prove that the algorithm terminates?AnswerTo prove that the algorithm terminates we use the loop variant:nX + nY + (X, Y) + (Y, X)The loop variant is a function of the state (i.e. variables) of the system.
In this example, the loop variant maps states of the program to
integers.We must show that  (1) the loop variant is bounded below and we cancarry
out inductionon the values of the loop variant, i.e., it can
decrease only a finite number of times before it reaches a lower
bound, and (2) the executions of all steps in all computations that
start at initial states reduce the value of the loop variant.
The specific lower bound is irrelevant for the proof of termination.In this example, 0 is (obviously) a lower bound.
Next we show that the execution of any event in any state reduces the
value of the loop variant.When Y receives a balloon if nY is positive then nY
decreases which the loop variant, and if nY is 0 then
(X, Y) which decreases the loop variant.Similarly every event in which X receives a balloon also reduces the
loop variant. Thus the executions of all events reduce the loop variant.Therefore the algorithm terminates execution.QuestionWhat can you prove about the state of the system when the computation
terminates? 
We have shown that computations terminate and we given examples of
invariants.AnswerAt termination all channels are empty.
Let's prove that nX or nY is 0 at termination.From the invariants if:(X, Y) + (Y, X)\(\neq\) 2 then nX
or nY is 0.At termination,(X, Y) + (Y, X)\(=\) 0, and the result follows.More Examples of Invariants and Loop VariantsWe reason about the correctness of many distributed algorithms in
  in the same way that we reason about sequentialwhileloops, by using invariants and loop variants.Example: GCD using a While LoopAloop invariantis an assertion about the state of the program that holds before and
  after each iteration of the loop.
  You can look upmany examplesof loop invariants on the web.Example: InvariantA loop invariant in the following greatest common
  divisor (gcd) program is shown in the body of the loop as the
  assertiongcd(x, y) = gcd(X, Y).x, y = X, Y
while x != y:
  # assertion: gcd(x, y) = gcd(X, Y)
  if x > y: x = x - y
  else: y = y - xInvariant of a Distributed System for GCDAn invariant of a distributed system is a
  predicates that holds in all states reachable from initial states.
  An invariant of a distributed system is the loop invariant of the
  following while loop.
  An invariantInvis shown
  as an assertion in the body of the loop.while there exists a nonempty input channel in the system:
   # assertion: Inv
   select a nonempty channel (u, v) in the system
   let the head of channel (u, v) be msg
   v executes receive(msg, u)Example of a Distributed GCDWe are given a strongly connected network of agents -- i.e.,
  there is a path from each agent to every other agent.
  Each agent has a local variablen, which is initialized
  as a positive integer.
  
  Let GCD be the gcd (greatest commond divisor) of the
  initial values ofX.nof all agentsX.n.
  The following distributed algorithm terminates and at terminationX.n = GCDfor all agentsX.n.
In the code,successorsis the list of successor agents
in a network of agents, and the agent only sends messages to its
successors.def receive(message, sender):
   if n != message:
      n = gcd(n, message)
      for successor in successors:
         send(n, successor)An invariant of the distributed algorithm is as follows.
The gcd of:all messages in channels andX.nfor all agentsXis GCD.ProofThe proof that the assertion holds initially is trivial.
Prove that if the assertion holds before any event 
then it continues to hold after the event.Let the event be agentAreceiving a messagem.
After the event,mis no longer in the channel;n = gcd(n, m); and messages with the new value ofnare in the output channels fromA.
The proof that the assertion continues to hold in the post-event state
is straightforward.Example of an InvariantAnother invariant for the gcd example, given earlier, is as follows.
For all channels(X, Y)in the network:Channel(X, Y)is empty andX.nis a
  multiple ofY.n, orThe last message in channel(X, Y)ismwherem = X.n.ProofThe assertion holds initially because the second condition holds. 
Prove that if the assertion holds before any event
then it continues to hold after the event.When agentXexecutes a receive that changesX.nthe agent sends a messagemtoYwherem = X.n, and so the second condition
holds.When agentYreceivesmthe agent setsY.n = gcd(Y.n, m), and somis a multiple of
the new value ofY.n.
If channel(X, Y)remains nonempty then the second
condition holds.
If(X, Y)becomes empty then the first condition holds
becausem = X.nandmis a multiple ofY.n.Loop VariantQuestion about Loop Variant in the Sequential
GCDWhat is an example of a loop variant for the following loop?x, y = X, Y
while x != y:
  # assertion: gcd(x, y) = gcd(X, Y)
  if x > y: x = x - y
  else: y = y - xAnswerf(x, y) = x + yis an example of a loop variant for the
following reasons.fis a function of the state of the program.fhas integer values.fis bounded below. The bound is 2 becausexandyare bounded below because of the invariantsx\(\geq\) 1, andy\(\geq\) 1.
  The precise bounds don't matter.Each execution of the loop decreasesf.We can carry out an induction onfbecausefhas integer values and is bounded below.Question about Loop Variants in Distributed AlgorithmsLet's look at the example of the distributed algorithm given
earlier? The example is repeated below.We are given a strongly connected network of agents -- i.e.,
  there is a path from each agent to every other agent.
  Each agent has a local variablen, which is initialized
  as a positive integer.
  
  Let GCD be the gcd (greatest commond divisor) of the
  initial values ofX.nof all agentsX.n.
In the code,successorsis the list of successor agents
in a network of agents, and the agent only sends messages to its
successors.def receive(message, sender):
   if n != message):
      n = gcd(n, message)
      for successor in successors:
         send(n, successor)QuestionWhat is an example of a loop variant for the distributed algorithm?AnswerLetfbe the tuple(N, M)whereNis the sum ofX.nfor all agentsX, andMis the total number of messages in
channels.Comparisons of tuples are made lexicographically.
For example (2, 1) > (1, 10), and (2, 1) < (2, 2).fis bounded below by (0, 0).An induction can be carried out on a tuple of integers.
Next we show that the execution of each event reducesf.When an agentXreceives a messagem,ifX.n\(\neq\)mthenX.nbecomesgcd(X.n, m), and so the new value ofX.nis less than its previous value, and so execution
  of the event decreasesNwhich decreasesf.ifX.n\(=\)mthen the event removes
  messagemfrom a channel and does not add messages to
  channels. So the event decreasesMwhich decreasesf.Termination Condition of a While LoopWhat can we say about the state at termination of the while loop in
the gcd example:x, y = X, Y
while x != y:
  # assertion: gcd(x, y) = gcd(X, Y)
  if x > y: x = x - y
  else: y = y - xThe while loop terminates whenx = y.
From the invariant, at termination:gcd(x, y) = gcd(x, x) = x = gcd(X, Y)Thereforexandyaregcd(X, Y)at termination of the while loop.Termination Condition of a Distributed AlgorithmWhat is the termination condition of the distributed gcd algorithm in
which thereceiveof each agent is as follows?def receive(message, sender):
   if n != message:
      n = gcd(n, message)
      for successor in successors:
         send(n, successor)At termination all channels are empty.
From the invariant, if channel (X, Y) is empty then X.n is a multiple
of Y.n.Therefore when all channels are empty, for every channel (X, Y) in the
system X.n is a multiple of Y.n.
Because the directed graph of agents and edges specified assuccessorsof agents is strongly
connected:for all agents X, Y in the system: X.n is a multiple of Y.nTherefore X.n = Y.n for all agents X, Y.
From the invariant, X.n is GCD.NextThe next webpage discussesdata flow in computations.Frequenty Asked QuestionsK. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

(Here we are abusing notation using (X, Y) for a channel and also for
the state of the channel.)QuestionHow do you prove that this condition is invariant?AnswerTo prove that this predicate (Boolean condition) is an invariant we
prove that (1) it holds initially 
and (2) and for all state transitions, if the invariant holds in the
state before the transition then it holds in the state after the transition.Initially:(X, Y) = 1and(Y, X) = 1So, the predicate holds initially.A state transition in which a balloon is returned does not change the value
of(X, Y) + (Y, X).A state transition in which a balloon is popped decreases(X, Y)
+ (Y, X).Therefore if the predicate holds before a state
transition then it holds after the transition.QuestionIs the following an invariant of the system?((X, Y) + (Y, X) = 2) or (nX = 0) or (nY = 0)AnswerYes, this predicate is an invariant.To prove that this predicate is an invariant we
prove that (1) it holds initially 
and (2) and for all state transitions, if the invariant holds in the
state before the transition then it holds in the state after the
transition.Initially,(X, Y) = 1and(Y, X) = 1.
Therefore the predicate holds initially.Let S be the state before a transition to a state S'.
We will prove that if the predicate holds in S then the predicate
holds in S'.Consider two cases:nX = 0 or nY = 0 in S.Because nX and nY do not increase,
  and nX and nY do not become negative,
  in the execution of an event, it follows that nX = 0 or nY = 0 in S'.
  Therefore the predicate holds in S'.nX > 0 and nY > 0 in S.Because the invariant holds in S it follows that(X, Y) + (Y,
  X) = 2in S.Because nY > 0 the execution of an event in which Y receives a
  balloon leaves(X, Y) + (Y, X)unchanged.Because nX > 0 the execution of an event in which X receives a
  balloon leaves(X, Y) + (Y, X)unchanged.Therefore(X, Y) + (Y, X) = 2in S'.
  So, the predicate holds in S'.Example of a Loop VariantQuestionHow do you prove that the algorithm terminates?AnswerTo prove that the algorithm terminates we use the loop variant:nX + nY + (X, Y) + (Y, X)The loop variant is a function of the state (i.e. variables) of the system.
In this example, the loop variant maps states of the program to
integers.We must show that  (1) the loop variant is bounded below and we cancarry
out inductionon the values of the loop variant, i.e., it can
decrease only a finite number of times before it reaches a lower
bound, and (2) the executions of all steps in all computations that
start at initial states reduce the value of the loop variant.
The specific lower bound is irrelevant for the proof of termination.In this example, 0 is (obviously) a lower bound.
Next we show that the execution of any event in any state reduces the
value of the loop variant.When Y receives a balloon if nY is positive then nY
decreases which the loop variant, and if nY is 0 then
(X, Y) which decreases the loop variant.Similarly every event in which X receives a balloon also reduces the
loop variant. Thus the executions of all events reduce the loop variant.Therefore the algorithm terminates execution.QuestionWhat can you prove about the state of the system when the computation
terminates? 
We have shown that computations terminate and we given examples of
invariants.AnswerAt termination all channels are empty.
Let's prove that nX or nY is 0 at termination.From the invariants if:(X, Y) + (Y, X)\(\neq\) 2 then nX
or nY is 0.At termination,(X, Y) + (Y, X)\(=\) 0, and the result follows.More Examples of Invariants and Loop VariantsWe reason about the correctness of many distributed algorithms in
  in the same way that we reason about sequentialwhileloops, by using invariants and loop variants.Example: GCD using a While LoopAloop invariantis an assertion about the state of the program that holds before and
  after each iteration of the loop.
  You can look upmany examplesof loop invariants on the web.Example: InvariantA loop invariant in the following greatest common
  divisor (gcd) program is shown in the body of the loop as the
  assertiongcd(x, y) = gcd(X, Y).x, y = X, Y
while x != y:
  # assertion: gcd(x, y) = gcd(X, Y)
  if x > y: x = x - y
  else: y = y - xInvariant of a Distributed System for GCDAn invariant of a distributed system is a
  predicates that holds in all states reachable from initial states.
  An invariant of a distributed system is the loop invariant of the
  following while loop.
  An invariantInvis shown
  as an assertion in the body of the loop.while there exists a nonempty input channel in the system:
   # assertion: Inv
   select a nonempty channel (u, v) in the system
   let the head of channel (u, v) be msg
   v executes receive(msg, u)Example of a Distributed GCDWe are given a strongly connected network of agents -- i.e.,
  there is a path from each agent to every other agent.
  Each agent has a local variablen, which is initialized
  as a positive integer.
  
  Let GCD be the gcd (greatest commond divisor) of the
  initial values ofX.nof all agentsX.n.
  The following distributed algorithm terminates and at terminationX.n = GCDfor all agentsX.n.
In the code,successorsis the list of successor agents
in a network of agents, and the agent only sends messages to its
successors.def receive(message, sender):
   if n != message:
      n = gcd(n, message)
      for successor in successors:
         send(n, successor)An invariant of the distributed algorithm is as follows.
The gcd of:all messages in channels andX.nfor all agentsXis GCD.ProofThe proof that the assertion holds initially is trivial.
Prove that if the assertion holds before any event 
then it continues to hold after the event.Let the event be agentAreceiving a messagem.
After the event,mis no longer in the channel;n = gcd(n, m); and messages with the new value ofnare in the output channels fromA.
The proof that the assertion continues to hold in the post-event state
is straightforward.Example of an InvariantAnother invariant for the gcd example, given earlier, is as follows.
For all channels(X, Y)in the network:Channel(X, Y)is empty andX.nis a
  multiple ofY.n, orThe last message in channel(X, Y)ismwherem = X.n.ProofThe assertion holds initially because the second condition holds. 
Prove that if the assertion holds before any event
then it continues to hold after the event.When agentXexecutes a receive that changesX.nthe agent sends a messagemtoYwherem = X.n, and so the second condition
holds.When agentYreceivesmthe agent setsY.n = gcd(Y.n, m), and somis a multiple of
the new value ofY.n.
If channel(X, Y)remains nonempty then the second
condition holds.
If(X, Y)becomes empty then the first condition holds
becausem = X.nandmis a multiple ofY.n.Loop VariantQuestion about Loop Variant in the Sequential
GCDWhat is an example of a loop variant for the following loop?x, y = X, Y
while x != y:
  # assertion: gcd(x, y) = gcd(X, Y)
  if x > y: x = x - y
  else: y = y - xAnswerf(x, y) = x + yis an example of a loop variant for the
following reasons.fis a function of the state of the program.fhas integer values.fis bounded below. The bound is 2 becausexandyare bounded below because of the invariantsx\(\geq\) 1, andy\(\geq\) 1.
  The precise bounds don't matter.Each execution of the loop decreasesf.We can carry out an induction onfbecausefhas integer values and is bounded below.Question about Loop Variants in Distributed AlgorithmsLet's look at the example of the distributed algorithm given
earlier? The example is repeated below.We are given a strongly connected network of agents -- i.e.,
  there is a path from each agent to every other agent.
  Each agent has a local variablen, which is initialized
  as a positive integer.
  
  Let GCD be the gcd (greatest commond divisor) of the
  initial values ofX.nof all agentsX.n.
In the code,successorsis the list of successor agents
in a network of agents, and the agent only sends messages to its
successors.def receive(message, sender):
   if n != message):
      n = gcd(n, message)
      for successor in successors:
         send(n, successor)QuestionWhat is an example of a loop variant for the distributed algorithm?AnswerLetfbe the tuple(N, M)whereNis the sum ofX.nfor all agentsX, andMis the total number of messages in
channels.Comparisons of tuples are made lexicographically.
For example (2, 1) > (1, 10), and (2, 1) < (2, 2).fis bounded below by (0, 0).An induction can be carried out on a tuple of integers.
Next we show that the execution of each event reducesf.When an agentXreceives a messagem,ifX.n\(\neq\)mthenX.nbecomesgcd(X.n, m), and so the new value ofX.nis less than its previous value, and so execution
  of the event decreasesNwhich decreasesf.ifX.n\(=\)mthen the event removes
  messagemfrom a channel and does not add messages to
  channels. So the event decreasesMwhich decreasesf.Termination Condition of a While LoopWhat can we say about the state at termination of the while loop in
the gcd example:x, y = X, Y
while x != y:
  # assertion: gcd(x, y) = gcd(X, Y)
  if x > y: x = x - y
  else: y = y - xThe while loop terminates whenx = y.
From the invariant, at termination:gcd(x, y) = gcd(x, x) = x = gcd(X, Y)Thereforexandyaregcd(X, Y)at termination of the while loop.Termination Condition of a Distributed AlgorithmWhat is the termination condition of the distributed gcd algorithm in
which thereceiveof each agent is as follows?def receive(message, sender):
   if n != message:
      n = gcd(n, message)
      for successor in successors:
         send(n, successor)At termination all channels are empty.
From the invariant, if channel (X, Y) is empty then X.n is a multiple
of Y.n.Therefore when all channels are empty, for every channel (X, Y) in the
system X.n is a multiple of Y.n.
Because the directed graph of agents and edges specified assuccessorsof agents is strongly
connected:for all agents X, Y in the system: X.n is a multiple of Y.nTherefore X.n = Y.n for all agents X, Y.
From the invariant, X.n is GCD.NextThe next webpage discussesdata flow in computations.Frequenty Asked QuestionsK. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

Initially:(X, Y) = 1and(Y, X) = 1So, the predicate holds initially.A state transition in which a balloon is returned does not change the value
of(X, Y) + (Y, X).A state transition in which a balloon is popped decreases(X, Y)
+ (Y, X).Therefore if the predicate holds before a state
transition then it holds after the transition.QuestionIs the following an invariant of the system?((X, Y) + (Y, X) = 2) or (nX = 0) or (nY = 0)AnswerYes, this predicate is an invariant.To prove that this predicate is an invariant we
prove that (1) it holds initially 
and (2) and for all state transitions, if the invariant holds in the
state before the transition then it holds in the state after the
transition.Initially,(X, Y) = 1and(Y, X) = 1.
Therefore the predicate holds initially.Let S be the state before a transition to a state S'.
We will prove that if the predicate holds in S then the predicate
holds in S'.Consider two cases:nX = 0 or nY = 0 in S.Because nX and nY do not increase,
  and nX and nY do not become negative,
  in the execution of an event, it follows that nX = 0 or nY = 0 in S'.
  Therefore the predicate holds in S'.nX > 0 and nY > 0 in S.Because the invariant holds in S it follows that(X, Y) + (Y,
  X) = 2in S.Because nY > 0 the execution of an event in which Y receives a
  balloon leaves(X, Y) + (Y, X)unchanged.Because nX > 0 the execution of an event in which X receives a
  balloon leaves(X, Y) + (Y, X)unchanged.Therefore(X, Y) + (Y, X) = 2in S'.
  So, the predicate holds in S'.Example of a Loop VariantQuestionHow do you prove that the algorithm terminates?AnswerTo prove that the algorithm terminates we use the loop variant:nX + nY + (X, Y) + (Y, X)The loop variant is a function of the state (i.e. variables) of the system.
In this example, the loop variant maps states of the program to
integers.We must show that  (1) the loop variant is bounded below and we cancarry
out inductionon the values of the loop variant, i.e., it can
decrease only a finite number of times before it reaches a lower
bound, and (2) the executions of all steps in all computations that
start at initial states reduce the value of the loop variant.
The specific lower bound is irrelevant for the proof of termination.In this example, 0 is (obviously) a lower bound.
Next we show that the execution of any event in any state reduces the
value of the loop variant.When Y receives a balloon if nY is positive then nY
decreases which the loop variant, and if nY is 0 then
(X, Y) which decreases the loop variant.Similarly every event in which X receives a balloon also reduces the
loop variant. Thus the executions of all events reduce the loop variant.Therefore the algorithm terminates execution.QuestionWhat can you prove about the state of the system when the computation
terminates? 
We have shown that computations terminate and we given examples of
invariants.AnswerAt termination all channels are empty.
Let's prove that nX or nY is 0 at termination.From the invariants if:(X, Y) + (Y, X)\(\neq\) 2 then nX
or nY is 0.At termination,(X, Y) + (Y, X)\(=\) 0, and the result follows.More Examples of Invariants and Loop VariantsWe reason about the correctness of many distributed algorithms in
  in the same way that we reason about sequentialwhileloops, by using invariants and loop variants.Example: GCD using a While LoopAloop invariantis an assertion about the state of the program that holds before and
  after each iteration of the loop.
  You can look upmany examplesof loop invariants on the web.Example: InvariantA loop invariant in the following greatest common
  divisor (gcd) program is shown in the body of the loop as the
  assertiongcd(x, y) = gcd(X, Y).x, y = X, Y
while x != y:
  # assertion: gcd(x, y) = gcd(X, Y)
  if x > y: x = x - y
  else: y = y - xInvariant of a Distributed System for GCDAn invariant of a distributed system is a
  predicates that holds in all states reachable from initial states.
  An invariant of a distributed system is the loop invariant of the
  following while loop.
  An invariantInvis shown
  as an assertion in the body of the loop.while there exists a nonempty input channel in the system:
   # assertion: Inv
   select a nonempty channel (u, v) in the system
   let the head of channel (u, v) be msg
   v executes receive(msg, u)Example of a Distributed GCDWe are given a strongly connected network of agents -- i.e.,
  there is a path from each agent to every other agent.
  Each agent has a local variablen, which is initialized
  as a positive integer.
  
  Let GCD be the gcd (greatest commond divisor) of the
  initial values ofX.nof all agentsX.n.
  The following distributed algorithm terminates and at terminationX.n = GCDfor all agentsX.n.
In the code,successorsis the list of successor agents
in a network of agents, and the agent only sends messages to its
successors.def receive(message, sender):
   if n != message:
      n = gcd(n, message)
      for successor in successors:
         send(n, successor)An invariant of the distributed algorithm is as follows.
The gcd of:all messages in channels andX.nfor all agentsXis GCD.ProofThe proof that the assertion holds initially is trivial.
Prove that if the assertion holds before any event 
then it continues to hold after the event.Let the event be agentAreceiving a messagem.
After the event,mis no longer in the channel;n = gcd(n, m); and messages with the new value ofnare in the output channels fromA.
The proof that the assertion continues to hold in the post-event state
is straightforward.Example of an InvariantAnother invariant for the gcd example, given earlier, is as follows.
For all channels(X, Y)in the network:Channel(X, Y)is empty andX.nis a
  multiple ofY.n, orThe last message in channel(X, Y)ismwherem = X.n.ProofThe assertion holds initially because the second condition holds. 
Prove that if the assertion holds before any event
then it continues to hold after the event.When agentXexecutes a receive that changesX.nthe agent sends a messagemtoYwherem = X.n, and so the second condition
holds.When agentYreceivesmthe agent setsY.n = gcd(Y.n, m), and somis a multiple of
the new value ofY.n.
If channel(X, Y)remains nonempty then the second
condition holds.
If(X, Y)becomes empty then the first condition holds
becausem = X.nandmis a multiple ofY.n.Loop VariantQuestion about Loop Variant in the Sequential
GCDWhat is an example of a loop variant for the following loop?x, y = X, Y
while x != y:
  # assertion: gcd(x, y) = gcd(X, Y)
  if x > y: x = x - y
  else: y = y - xAnswerf(x, y) = x + yis an example of a loop variant for the
following reasons.fis a function of the state of the program.fhas integer values.fis bounded below. The bound is 2 becausexandyare bounded below because of the invariantsx\(\geq\) 1, andy\(\geq\) 1.
  The precise bounds don't matter.Each execution of the loop decreasesf.We can carry out an induction onfbecausefhas integer values and is bounded below.Question about Loop Variants in Distributed AlgorithmsLet's look at the example of the distributed algorithm given
earlier? The example is repeated below.We are given a strongly connected network of agents -- i.e.,
  there is a path from each agent to every other agent.
  Each agent has a local variablen, which is initialized
  as a positive integer.
  
  Let GCD be the gcd (greatest commond divisor) of the
  initial values ofX.nof all agentsX.n.
In the code,successorsis the list of successor agents
in a network of agents, and the agent only sends messages to its
successors.def receive(message, sender):
   if n != message):
      n = gcd(n, message)
      for successor in successors:
         send(n, successor)QuestionWhat is an example of a loop variant for the distributed algorithm?AnswerLetfbe the tuple(N, M)whereNis the sum ofX.nfor all agentsX, andMis the total number of messages in
channels.Comparisons of tuples are made lexicographically.
For example (2, 1) > (1, 10), and (2, 1) < (2, 2).fis bounded below by (0, 0).An induction can be carried out on a tuple of integers.
Next we show that the execution of each event reducesf.When an agentXreceives a messagem,ifX.n\(\neq\)mthenX.nbecomesgcd(X.n, m), and so the new value ofX.nis less than its previous value, and so execution
  of the event decreasesNwhich decreasesf.ifX.n\(=\)mthen the event removes
  messagemfrom a channel and does not add messages to
  channels. So the event decreasesMwhich decreasesf.Termination Condition of a While LoopWhat can we say about the state at termination of the while loop in
the gcd example:x, y = X, Y
while x != y:
  # assertion: gcd(x, y) = gcd(X, Y)
  if x > y: x = x - y
  else: y = y - xThe while loop terminates whenx = y.
From the invariant, at termination:gcd(x, y) = gcd(x, x) = x = gcd(X, Y)Thereforexandyaregcd(X, Y)at termination of the while loop.Termination Condition of a Distributed AlgorithmWhat is the termination condition of the distributed gcd algorithm in
which thereceiveof each agent is as follows?def receive(message, sender):
   if n != message:
      n = gcd(n, message)
      for successor in successors:
         send(n, successor)At termination all channels are empty.
From the invariant, if channel (X, Y) is empty then X.n is a multiple
of Y.n.Therefore when all channels are empty, for every channel (X, Y) in the
system X.n is a multiple of Y.n.
Because the directed graph of agents and edges specified assuccessorsof agents is strongly
connected:for all agents X, Y in the system: X.n is a multiple of Y.nTherefore X.n = Y.n for all agents X, Y.
From the invariant, X.n is GCD.NextThe next webpage discussesdata flow in computations.Frequenty Asked QuestionsK. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

A state transition in which a balloon is returned does not change the value
of(X, Y) + (Y, X).A state transition in which a balloon is popped decreases(X, Y)
+ (Y, X).Therefore if the predicate holds before a state
transition then it holds after the transition.QuestionIs the following an invariant of the system?((X, Y) + (Y, X) = 2) or (nX = 0) or (nY = 0)AnswerYes, this predicate is an invariant.To prove that this predicate is an invariant we
prove that (1) it holds initially 
and (2) and for all state transitions, if the invariant holds in the
state before the transition then it holds in the state after the
transition.Initially,(X, Y) = 1and(Y, X) = 1.
Therefore the predicate holds initially.Let S be the state before a transition to a state S'.
We will prove that if the predicate holds in S then the predicate
holds in S'.Consider two cases:nX = 0 or nY = 0 in S.Because nX and nY do not increase,
  and nX and nY do not become negative,
  in the execution of an event, it follows that nX = 0 or nY = 0 in S'.
  Therefore the predicate holds in S'.nX > 0 and nY > 0 in S.Because the invariant holds in S it follows that(X, Y) + (Y,
  X) = 2in S.Because nY > 0 the execution of an event in which Y receives a
  balloon leaves(X, Y) + (Y, X)unchanged.Because nX > 0 the execution of an event in which X receives a
  balloon leaves(X, Y) + (Y, X)unchanged.Therefore(X, Y) + (Y, X) = 2in S'.
  So, the predicate holds in S'.Example of a Loop VariantQuestionHow do you prove that the algorithm terminates?AnswerTo prove that the algorithm terminates we use the loop variant:nX + nY + (X, Y) + (Y, X)The loop variant is a function of the state (i.e. variables) of the system.
In this example, the loop variant maps states of the program to
integers.We must show that  (1) the loop variant is bounded below and we cancarry
out inductionon the values of the loop variant, i.e., it can
decrease only a finite number of times before it reaches a lower
bound, and (2) the executions of all steps in all computations that
start at initial states reduce the value of the loop variant.
The specific lower bound is irrelevant for the proof of termination.In this example, 0 is (obviously) a lower bound.
Next we show that the execution of any event in any state reduces the
value of the loop variant.When Y receives a balloon if nY is positive then nY
decreases which the loop variant, and if nY is 0 then
(X, Y) which decreases the loop variant.Similarly every event in which X receives a balloon also reduces the
loop variant. Thus the executions of all events reduce the loop variant.Therefore the algorithm terminates execution.QuestionWhat can you prove about the state of the system when the computation
terminates? 
We have shown that computations terminate and we given examples of
invariants.AnswerAt termination all channels are empty.
Let's prove that nX or nY is 0 at termination.From the invariants if:(X, Y) + (Y, X)\(\neq\) 2 then nX
or nY is 0.At termination,(X, Y) + (Y, X)\(=\) 0, and the result follows.More Examples of Invariants and Loop VariantsWe reason about the correctness of many distributed algorithms in
  in the same way that we reason about sequentialwhileloops, by using invariants and loop variants.Example: GCD using a While LoopAloop invariantis an assertion about the state of the program that holds before and
  after each iteration of the loop.
  You can look upmany examplesof loop invariants on the web.Example: InvariantA loop invariant in the following greatest common
  divisor (gcd) program is shown in the body of the loop as the
  assertiongcd(x, y) = gcd(X, Y).x, y = X, Y
while x != y:
  # assertion: gcd(x, y) = gcd(X, Y)
  if x > y: x = x - y
  else: y = y - xInvariant of a Distributed System for GCDAn invariant of a distributed system is a
  predicates that holds in all states reachable from initial states.
  An invariant of a distributed system is the loop invariant of the
  following while loop.
  An invariantInvis shown
  as an assertion in the body of the loop.while there exists a nonempty input channel in the system:
   # assertion: Inv
   select a nonempty channel (u, v) in the system
   let the head of channel (u, v) be msg
   v executes receive(msg, u)Example of a Distributed GCDWe are given a strongly connected network of agents -- i.e.,
  there is a path from each agent to every other agent.
  Each agent has a local variablen, which is initialized
  as a positive integer.
  
  Let GCD be the gcd (greatest commond divisor) of the
  initial values ofX.nof all agentsX.n.
  The following distributed algorithm terminates and at terminationX.n = GCDfor all agentsX.n.
In the code,successorsis the list of successor agents
in a network of agents, and the agent only sends messages to its
successors.def receive(message, sender):
   if n != message:
      n = gcd(n, message)
      for successor in successors:
         send(n, successor)An invariant of the distributed algorithm is as follows.
The gcd of:all messages in channels andX.nfor all agentsXis GCD.ProofThe proof that the assertion holds initially is trivial.
Prove that if the assertion holds before any event 
then it continues to hold after the event.Let the event be agentAreceiving a messagem.
After the event,mis no longer in the channel;n = gcd(n, m); and messages with the new value ofnare in the output channels fromA.
The proof that the assertion continues to hold in the post-event state
is straightforward.Example of an InvariantAnother invariant for the gcd example, given earlier, is as follows.
For all channels(X, Y)in the network:Channel(X, Y)is empty andX.nis a
  multiple ofY.n, orThe last message in channel(X, Y)ismwherem = X.n.ProofThe assertion holds initially because the second condition holds. 
Prove that if the assertion holds before any event
then it continues to hold after the event.When agentXexecutes a receive that changesX.nthe agent sends a messagemtoYwherem = X.n, and so the second condition
holds.When agentYreceivesmthe agent setsY.n = gcd(Y.n, m), and somis a multiple of
the new value ofY.n.
If channel(X, Y)remains nonempty then the second
condition holds.
If(X, Y)becomes empty then the first condition holds
becausem = X.nandmis a multiple ofY.n.Loop VariantQuestion about Loop Variant in the Sequential
GCDWhat is an example of a loop variant for the following loop?x, y = X, Y
while x != y:
  # assertion: gcd(x, y) = gcd(X, Y)
  if x > y: x = x - y
  else: y = y - xAnswerf(x, y) = x + yis an example of a loop variant for the
following reasons.fis a function of the state of the program.fhas integer values.fis bounded below. The bound is 2 becausexandyare bounded below because of the invariantsx\(\geq\) 1, andy\(\geq\) 1.
  The precise bounds don't matter.Each execution of the loop decreasesf.We can carry out an induction onfbecausefhas integer values and is bounded below.Question about Loop Variants in Distributed AlgorithmsLet's look at the example of the distributed algorithm given
earlier? The example is repeated below.We are given a strongly connected network of agents -- i.e.,
  there is a path from each agent to every other agent.
  Each agent has a local variablen, which is initialized
  as a positive integer.
  
  Let GCD be the gcd (greatest commond divisor) of the
  initial values ofX.nof all agentsX.n.
In the code,successorsis the list of successor agents
in a network of agents, and the agent only sends messages to its
successors.def receive(message, sender):
   if n != message):
      n = gcd(n, message)
      for successor in successors:
         send(n, successor)QuestionWhat is an example of a loop variant for the distributed algorithm?AnswerLetfbe the tuple(N, M)whereNis the sum ofX.nfor all agentsX, andMis the total number of messages in
channels.Comparisons of tuples are made lexicographically.
For example (2, 1) > (1, 10), and (2, 1) < (2, 2).fis bounded below by (0, 0).An induction can be carried out on a tuple of integers.
Next we show that the execution of each event reducesf.When an agentXreceives a messagem,ifX.n\(\neq\)mthenX.nbecomesgcd(X.n, m), and so the new value ofX.nis less than its previous value, and so execution
  of the event decreasesNwhich decreasesf.ifX.n\(=\)mthen the event removes
  messagemfrom a channel and does not add messages to
  channels. So the event decreasesMwhich decreasesf.Termination Condition of a While LoopWhat can we say about the state at termination of the while loop in
the gcd example:x, y = X, Y
while x != y:
  # assertion: gcd(x, y) = gcd(X, Y)
  if x > y: x = x - y
  else: y = y - xThe while loop terminates whenx = y.
From the invariant, at termination:gcd(x, y) = gcd(x, x) = x = gcd(X, Y)Thereforexandyaregcd(X, Y)at termination of the while loop.Termination Condition of a Distributed AlgorithmWhat is the termination condition of the distributed gcd algorithm in
which thereceiveof each agent is as follows?def receive(message, sender):
   if n != message:
      n = gcd(n, message)
      for successor in successors:
         send(n, successor)At termination all channels are empty.
From the invariant, if channel (X, Y) is empty then X.n is a multiple
of Y.n.Therefore when all channels are empty, for every channel (X, Y) in the
system X.n is a multiple of Y.n.
Because the directed graph of agents and edges specified assuccessorsof agents is strongly
connected:for all agents X, Y in the system: X.n is a multiple of Y.nTherefore X.n = Y.n for all agents X, Y.
From the invariant, X.n is GCD.NextThe next webpage discussesdata flow in computations.Frequenty Asked QuestionsK. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

A state transition in which a balloon is popped decreases(X, Y)
+ (Y, X).Therefore if the predicate holds before a state
transition then it holds after the transition.QuestionIs the following an invariant of the system?((X, Y) + (Y, X) = 2) or (nX = 0) or (nY = 0)AnswerYes, this predicate is an invariant.To prove that this predicate is an invariant we
prove that (1) it holds initially 
and (2) and for all state transitions, if the invariant holds in the
state before the transition then it holds in the state after the
transition.Initially,(X, Y) = 1and(Y, X) = 1.
Therefore the predicate holds initially.Let S be the state before a transition to a state S'.
We will prove that if the predicate holds in S then the predicate
holds in S'.Consider two cases:nX = 0 or nY = 0 in S.Because nX and nY do not increase,
  and nX and nY do not become negative,
  in the execution of an event, it follows that nX = 0 or nY = 0 in S'.
  Therefore the predicate holds in S'.nX > 0 and nY > 0 in S.Because the invariant holds in S it follows that(X, Y) + (Y,
  X) = 2in S.Because nY > 0 the execution of an event in which Y receives a
  balloon leaves(X, Y) + (Y, X)unchanged.Because nX > 0 the execution of an event in which X receives a
  balloon leaves(X, Y) + (Y, X)unchanged.Therefore(X, Y) + (Y, X) = 2in S'.
  So, the predicate holds in S'.Example of a Loop VariantQuestionHow do you prove that the algorithm terminates?AnswerTo prove that the algorithm terminates we use the loop variant:nX + nY + (X, Y) + (Y, X)The loop variant is a function of the state (i.e. variables) of the system.
In this example, the loop variant maps states of the program to
integers.We must show that  (1) the loop variant is bounded below and we cancarry
out inductionon the values of the loop variant, i.e., it can
decrease only a finite number of times before it reaches a lower
bound, and (2) the executions of all steps in all computations that
start at initial states reduce the value of the loop variant.
The specific lower bound is irrelevant for the proof of termination.In this example, 0 is (obviously) a lower bound.
Next we show that the execution of any event in any state reduces the
value of the loop variant.When Y receives a balloon if nY is positive then nY
decreases which the loop variant, and if nY is 0 then
(X, Y) which decreases the loop variant.Similarly every event in which X receives a balloon also reduces the
loop variant. Thus the executions of all events reduce the loop variant.Therefore the algorithm terminates execution.QuestionWhat can you prove about the state of the system when the computation
terminates? 
We have shown that computations terminate and we given examples of
invariants.AnswerAt termination all channels are empty.
Let's prove that nX or nY is 0 at termination.From the invariants if:(X, Y) + (Y, X)\(\neq\) 2 then nX
or nY is 0.At termination,(X, Y) + (Y, X)\(=\) 0, and the result follows.More Examples of Invariants and Loop VariantsWe reason about the correctness of many distributed algorithms in
  in the same way that we reason about sequentialwhileloops, by using invariants and loop variants.Example: GCD using a While LoopAloop invariantis an assertion about the state of the program that holds before and
  after each iteration of the loop.
  You can look upmany examplesof loop invariants on the web.Example: InvariantA loop invariant in the following greatest common
  divisor (gcd) program is shown in the body of the loop as the
  assertiongcd(x, y) = gcd(X, Y).x, y = X, Y
while x != y:
  # assertion: gcd(x, y) = gcd(X, Y)
  if x > y: x = x - y
  else: y = y - xInvariant of a Distributed System for GCDAn invariant of a distributed system is a
  predicates that holds in all states reachable from initial states.
  An invariant of a distributed system is the loop invariant of the
  following while loop.
  An invariantInvis shown
  as an assertion in the body of the loop.while there exists a nonempty input channel in the system:
   # assertion: Inv
   select a nonempty channel (u, v) in the system
   let the head of channel (u, v) be msg
   v executes receive(msg, u)Example of a Distributed GCDWe are given a strongly connected network of agents -- i.e.,
  there is a path from each agent to every other agent.
  Each agent has a local variablen, which is initialized
  as a positive integer.
  
  Let GCD be the gcd (greatest commond divisor) of the
  initial values ofX.nof all agentsX.n.
  The following distributed algorithm terminates and at terminationX.n = GCDfor all agentsX.n.
In the code,successorsis the list of successor agents
in a network of agents, and the agent only sends messages to its
successors.def receive(message, sender):
   if n != message:
      n = gcd(n, message)
      for successor in successors:
         send(n, successor)An invariant of the distributed algorithm is as follows.
The gcd of:all messages in channels andX.nfor all agentsXis GCD.ProofThe proof that the assertion holds initially is trivial.
Prove that if the assertion holds before any event 
then it continues to hold after the event.Let the event be agentAreceiving a messagem.
After the event,mis no longer in the channel;n = gcd(n, m); and messages with the new value ofnare in the output channels fromA.
The proof that the assertion continues to hold in the post-event state
is straightforward.Example of an InvariantAnother invariant for the gcd example, given earlier, is as follows.
For all channels(X, Y)in the network:Channel(X, Y)is empty andX.nis a
  multiple ofY.n, orThe last message in channel(X, Y)ismwherem = X.n.ProofThe assertion holds initially because the second condition holds. 
Prove that if the assertion holds before any event
then it continues to hold after the event.When agentXexecutes a receive that changesX.nthe agent sends a messagemtoYwherem = X.n, and so the second condition
holds.When agentYreceivesmthe agent setsY.n = gcd(Y.n, m), and somis a multiple of
the new value ofY.n.
If channel(X, Y)remains nonempty then the second
condition holds.
If(X, Y)becomes empty then the first condition holds
becausem = X.nandmis a multiple ofY.n.Loop VariantQuestion about Loop Variant in the Sequential
GCDWhat is an example of a loop variant for the following loop?x, y = X, Y
while x != y:
  # assertion: gcd(x, y) = gcd(X, Y)
  if x > y: x = x - y
  else: y = y - xAnswerf(x, y) = x + yis an example of a loop variant for the
following reasons.fis a function of the state of the program.fhas integer values.fis bounded below. The bound is 2 becausexandyare bounded below because of the invariantsx\(\geq\) 1, andy\(\geq\) 1.
  The precise bounds don't matter.Each execution of the loop decreasesf.We can carry out an induction onfbecausefhas integer values and is bounded below.Question about Loop Variants in Distributed AlgorithmsLet's look at the example of the distributed algorithm given
earlier? The example is repeated below.We are given a strongly connected network of agents -- i.e.,
  there is a path from each agent to every other agent.
  Each agent has a local variablen, which is initialized
  as a positive integer.
  
  Let GCD be the gcd (greatest commond divisor) of the
  initial values ofX.nof all agentsX.n.
In the code,successorsis the list of successor agents
in a network of agents, and the agent only sends messages to its
successors.def receive(message, sender):
   if n != message):
      n = gcd(n, message)
      for successor in successors:
         send(n, successor)QuestionWhat is an example of a loop variant for the distributed algorithm?AnswerLetfbe the tuple(N, M)whereNis the sum ofX.nfor all agentsX, andMis the total number of messages in
channels.Comparisons of tuples are made lexicographically.
For example (2, 1) > (1, 10), and (2, 1) < (2, 2).fis bounded below by (0, 0).An induction can be carried out on a tuple of integers.
Next we show that the execution of each event reducesf.When an agentXreceives a messagem,ifX.n\(\neq\)mthenX.nbecomesgcd(X.n, m), and so the new value ofX.nis less than its previous value, and so execution
  of the event decreasesNwhich decreasesf.ifX.n\(=\)mthen the event removes
  messagemfrom a channel and does not add messages to
  channels. So the event decreasesMwhich decreasesf.Termination Condition of a While LoopWhat can we say about the state at termination of the while loop in
the gcd example:x, y = X, Y
while x != y:
  # assertion: gcd(x, y) = gcd(X, Y)
  if x > y: x = x - y
  else: y = y - xThe while loop terminates whenx = y.
From the invariant, at termination:gcd(x, y) = gcd(x, x) = x = gcd(X, Y)Thereforexandyaregcd(X, Y)at termination of the while loop.Termination Condition of a Distributed AlgorithmWhat is the termination condition of the distributed gcd algorithm in
which thereceiveof each agent is as follows?def receive(message, sender):
   if n != message:
      n = gcd(n, message)
      for successor in successors:
         send(n, successor)At termination all channels are empty.
From the invariant, if channel (X, Y) is empty then X.n is a multiple
of Y.n.Therefore when all channels are empty, for every channel (X, Y) in the
system X.n is a multiple of Y.n.
Because the directed graph of agents and edges specified assuccessorsof agents is strongly
connected:for all agents X, Y in the system: X.n is a multiple of Y.nTherefore X.n = Y.n for all agents X, Y.
From the invariant, X.n is GCD.NextThe next webpage discussesdata flow in computations.Frequenty Asked QuestionsK. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

Therefore if the predicate holds before a state
transition then it holds after the transition.QuestionIs the following an invariant of the system?((X, Y) + (Y, X) = 2) or (nX = 0) or (nY = 0)AnswerYes, this predicate is an invariant.To prove that this predicate is an invariant we
prove that (1) it holds initially 
and (2) and for all state transitions, if the invariant holds in the
state before the transition then it holds in the state after the
transition.Initially,(X, Y) = 1and(Y, X) = 1.
Therefore the predicate holds initially.Let S be the state before a transition to a state S'.
We will prove that if the predicate holds in S then the predicate
holds in S'.Consider two cases:nX = 0 or nY = 0 in S.Because nX and nY do not increase,
  and nX and nY do not become negative,
  in the execution of an event, it follows that nX = 0 or nY = 0 in S'.
  Therefore the predicate holds in S'.nX > 0 and nY > 0 in S.Because the invariant holds in S it follows that(X, Y) + (Y,
  X) = 2in S.Because nY > 0 the execution of an event in which Y receives a
  balloon leaves(X, Y) + (Y, X)unchanged.Because nX > 0 the execution of an event in which X receives a
  balloon leaves(X, Y) + (Y, X)unchanged.Therefore(X, Y) + (Y, X) = 2in S'.
  So, the predicate holds in S'.Example of a Loop VariantQuestionHow do you prove that the algorithm terminates?AnswerTo prove that the algorithm terminates we use the loop variant:nX + nY + (X, Y) + (Y, X)The loop variant is a function of the state (i.e. variables) of the system.
In this example, the loop variant maps states of the program to
integers.We must show that  (1) the loop variant is bounded below and we cancarry
out inductionon the values of the loop variant, i.e., it can
decrease only a finite number of times before it reaches a lower
bound, and (2) the executions of all steps in all computations that
start at initial states reduce the value of the loop variant.
The specific lower bound is irrelevant for the proof of termination.In this example, 0 is (obviously) a lower bound.
Next we show that the execution of any event in any state reduces the
value of the loop variant.When Y receives a balloon if nY is positive then nY
decreases which the loop variant, and if nY is 0 then
(X, Y) which decreases the loop variant.Similarly every event in which X receives a balloon also reduces the
loop variant. Thus the executions of all events reduce the loop variant.Therefore the algorithm terminates execution.QuestionWhat can you prove about the state of the system when the computation
terminates? 
We have shown that computations terminate and we given examples of
invariants.AnswerAt termination all channels are empty.
Let's prove that nX or nY is 0 at termination.From the invariants if:(X, Y) + (Y, X)\(\neq\) 2 then nX
or nY is 0.At termination,(X, Y) + (Y, X)\(=\) 0, and the result follows.More Examples of Invariants and Loop VariantsWe reason about the correctness of many distributed algorithms in
  in the same way that we reason about sequentialwhileloops, by using invariants and loop variants.Example: GCD using a While LoopAloop invariantis an assertion about the state of the program that holds before and
  after each iteration of the loop.
  You can look upmany examplesof loop invariants on the web.Example: InvariantA loop invariant in the following greatest common
  divisor (gcd) program is shown in the body of the loop as the
  assertiongcd(x, y) = gcd(X, Y).x, y = X, Y
while x != y:
  # assertion: gcd(x, y) = gcd(X, Y)
  if x > y: x = x - y
  else: y = y - xInvariant of a Distributed System for GCDAn invariant of a distributed system is a
  predicates that holds in all states reachable from initial states.
  An invariant of a distributed system is the loop invariant of the
  following while loop.
  An invariantInvis shown
  as an assertion in the body of the loop.while there exists a nonempty input channel in the system:
   # assertion: Inv
   select a nonempty channel (u, v) in the system
   let the head of channel (u, v) be msg
   v executes receive(msg, u)Example of a Distributed GCDWe are given a strongly connected network of agents -- i.e.,
  there is a path from each agent to every other agent.
  Each agent has a local variablen, which is initialized
  as a positive integer.
  
  Let GCD be the gcd (greatest commond divisor) of the
  initial values ofX.nof all agentsX.n.
  The following distributed algorithm terminates and at terminationX.n = GCDfor all agentsX.n.
In the code,successorsis the list of successor agents
in a network of agents, and the agent only sends messages to its
successors.def receive(message, sender):
   if n != message:
      n = gcd(n, message)
      for successor in successors:
         send(n, successor)An invariant of the distributed algorithm is as follows.
The gcd of:all messages in channels andX.nfor all agentsXis GCD.ProofThe proof that the assertion holds initially is trivial.
Prove that if the assertion holds before any event 
then it continues to hold after the event.Let the event be agentAreceiving a messagem.
After the event,mis no longer in the channel;n = gcd(n, m); and messages with the new value ofnare in the output channels fromA.
The proof that the assertion continues to hold in the post-event state
is straightforward.Example of an InvariantAnother invariant for the gcd example, given earlier, is as follows.
For all channels(X, Y)in the network:Channel(X, Y)is empty andX.nis a
  multiple ofY.n, orThe last message in channel(X, Y)ismwherem = X.n.ProofThe assertion holds initially because the second condition holds. 
Prove that if the assertion holds before any event
then it continues to hold after the event.When agentXexecutes a receive that changesX.nthe agent sends a messagemtoYwherem = X.n, and so the second condition
holds.When agentYreceivesmthe agent setsY.n = gcd(Y.n, m), and somis a multiple of
the new value ofY.n.
If channel(X, Y)remains nonempty then the second
condition holds.
If(X, Y)becomes empty then the first condition holds
becausem = X.nandmis a multiple ofY.n.Loop VariantQuestion about Loop Variant in the Sequential
GCDWhat is an example of a loop variant for the following loop?x, y = X, Y
while x != y:
  # assertion: gcd(x, y) = gcd(X, Y)
  if x > y: x = x - y
  else: y = y - xAnswerf(x, y) = x + yis an example of a loop variant for the
following reasons.fis a function of the state of the program.fhas integer values.fis bounded below. The bound is 2 becausexandyare bounded below because of the invariantsx\(\geq\) 1, andy\(\geq\) 1.
  The precise bounds don't matter.Each execution of the loop decreasesf.We can carry out an induction onfbecausefhas integer values and is bounded below.Question about Loop Variants in Distributed AlgorithmsLet's look at the example of the distributed algorithm given
earlier? The example is repeated below.We are given a strongly connected network of agents -- i.e.,
  there is a path from each agent to every other agent.
  Each agent has a local variablen, which is initialized
  as a positive integer.
  
  Let GCD be the gcd (greatest commond divisor) of the
  initial values ofX.nof all agentsX.n.
In the code,successorsis the list of successor agents
in a network of agents, and the agent only sends messages to its
successors.def receive(message, sender):
   if n != message):
      n = gcd(n, message)
      for successor in successors:
         send(n, successor)QuestionWhat is an example of a loop variant for the distributed algorithm?AnswerLetfbe the tuple(N, M)whereNis the sum ofX.nfor all agentsX, andMis the total number of messages in
channels.Comparisons of tuples are made lexicographically.
For example (2, 1) > (1, 10), and (2, 1) < (2, 2).fis bounded below by (0, 0).An induction can be carried out on a tuple of integers.
Next we show that the execution of each event reducesf.When an agentXreceives a messagem,ifX.n\(\neq\)mthenX.nbecomesgcd(X.n, m), and so the new value ofX.nis less than its previous value, and so execution
  of the event decreasesNwhich decreasesf.ifX.n\(=\)mthen the event removes
  messagemfrom a channel and does not add messages to
  channels. So the event decreasesMwhich decreasesf.Termination Condition of a While LoopWhat can we say about the state at termination of the while loop in
the gcd example:x, y = X, Y
while x != y:
  # assertion: gcd(x, y) = gcd(X, Y)
  if x > y: x = x - y
  else: y = y - xThe while loop terminates whenx = y.
From the invariant, at termination:gcd(x, y) = gcd(x, x) = x = gcd(X, Y)Thereforexandyaregcd(X, Y)at termination of the while loop.Termination Condition of a Distributed AlgorithmWhat is the termination condition of the distributed gcd algorithm in
which thereceiveof each agent is as follows?def receive(message, sender):
   if n != message:
      n = gcd(n, message)
      for successor in successors:
         send(n, successor)At termination all channels are empty.
From the invariant, if channel (X, Y) is empty then X.n is a multiple
of Y.n.Therefore when all channels are empty, for every channel (X, Y) in the
system X.n is a multiple of Y.n.
Because the directed graph of agents and edges specified assuccessorsof agents is strongly
connected:for all agents X, Y in the system: X.n is a multiple of Y.nTherefore X.n = Y.n for all agents X, Y.
From the invariant, X.n is GCD.NextThe next webpage discussesdata flow in computations.Frequenty Asked QuestionsK. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

((X, Y) + (Y, X) = 2) or (nX = 0) or (nY = 0)AnswerYes, this predicate is an invariant.To prove that this predicate is an invariant we
prove that (1) it holds initially 
and (2) and for all state transitions, if the invariant holds in the
state before the transition then it holds in the state after the
transition.Initially,(X, Y) = 1and(Y, X) = 1.
Therefore the predicate holds initially.Let S be the state before a transition to a state S'.
We will prove that if the predicate holds in S then the predicate
holds in S'.Consider two cases:nX = 0 or nY = 0 in S.Because nX and nY do not increase,
  and nX and nY do not become negative,
  in the execution of an event, it follows that nX = 0 or nY = 0 in S'.
  Therefore the predicate holds in S'.nX > 0 and nY > 0 in S.Because the invariant holds in S it follows that(X, Y) + (Y,
  X) = 2in S.Because nY > 0 the execution of an event in which Y receives a
  balloon leaves(X, Y) + (Y, X)unchanged.Because nX > 0 the execution of an event in which X receives a
  balloon leaves(X, Y) + (Y, X)unchanged.Therefore(X, Y) + (Y, X) = 2in S'.
  So, the predicate holds in S'.Example of a Loop VariantQuestionHow do you prove that the algorithm terminates?AnswerTo prove that the algorithm terminates we use the loop variant:nX + nY + (X, Y) + (Y, X)The loop variant is a function of the state (i.e. variables) of the system.
In this example, the loop variant maps states of the program to
integers.We must show that  (1) the loop variant is bounded below and we cancarry
out inductionon the values of the loop variant, i.e., it can
decrease only a finite number of times before it reaches a lower
bound, and (2) the executions of all steps in all computations that
start at initial states reduce the value of the loop variant.
The specific lower bound is irrelevant for the proof of termination.In this example, 0 is (obviously) a lower bound.
Next we show that the execution of any event in any state reduces the
value of the loop variant.When Y receives a balloon if nY is positive then nY
decreases which the loop variant, and if nY is 0 then
(X, Y) which decreases the loop variant.Similarly every event in which X receives a balloon also reduces the
loop variant. Thus the executions of all events reduce the loop variant.Therefore the algorithm terminates execution.QuestionWhat can you prove about the state of the system when the computation
terminates? 
We have shown that computations terminate and we given examples of
invariants.AnswerAt termination all channels are empty.
Let's prove that nX or nY is 0 at termination.From the invariants if:(X, Y) + (Y, X)\(\neq\) 2 then nX
or nY is 0.At termination,(X, Y) + (Y, X)\(=\) 0, and the result follows.More Examples of Invariants and Loop VariantsWe reason about the correctness of many distributed algorithms in
  in the same way that we reason about sequentialwhileloops, by using invariants and loop variants.Example: GCD using a While LoopAloop invariantis an assertion about the state of the program that holds before and
  after each iteration of the loop.
  You can look upmany examplesof loop invariants on the web.Example: InvariantA loop invariant in the following greatest common
  divisor (gcd) program is shown in the body of the loop as the
  assertiongcd(x, y) = gcd(X, Y).x, y = X, Y
while x != y:
  # assertion: gcd(x, y) = gcd(X, Y)
  if x > y: x = x - y
  else: y = y - xInvariant of a Distributed System for GCDAn invariant of a distributed system is a
  predicates that holds in all states reachable from initial states.
  An invariant of a distributed system is the loop invariant of the
  following while loop.
  An invariantInvis shown
  as an assertion in the body of the loop.while there exists a nonempty input channel in the system:
   # assertion: Inv
   select a nonempty channel (u, v) in the system
   let the head of channel (u, v) be msg
   v executes receive(msg, u)Example of a Distributed GCDWe are given a strongly connected network of agents -- i.e.,
  there is a path from each agent to every other agent.
  Each agent has a local variablen, which is initialized
  as a positive integer.
  
  Let GCD be the gcd (greatest commond divisor) of the
  initial values ofX.nof all agentsX.n.
  The following distributed algorithm terminates and at terminationX.n = GCDfor all agentsX.n.
In the code,successorsis the list of successor agents
in a network of agents, and the agent only sends messages to its
successors.def receive(message, sender):
   if n != message:
      n = gcd(n, message)
      for successor in successors:
         send(n, successor)An invariant of the distributed algorithm is as follows.
The gcd of:all messages in channels andX.nfor all agentsXis GCD.ProofThe proof that the assertion holds initially is trivial.
Prove that if the assertion holds before any event 
then it continues to hold after the event.Let the event be agentAreceiving a messagem.
After the event,mis no longer in the channel;n = gcd(n, m); and messages with the new value ofnare in the output channels fromA.
The proof that the assertion continues to hold in the post-event state
is straightforward.Example of an InvariantAnother invariant for the gcd example, given earlier, is as follows.
For all channels(X, Y)in the network:Channel(X, Y)is empty andX.nis a
  multiple ofY.n, orThe last message in channel(X, Y)ismwherem = X.n.ProofThe assertion holds initially because the second condition holds. 
Prove that if the assertion holds before any event
then it continues to hold after the event.When agentXexecutes a receive that changesX.nthe agent sends a messagemtoYwherem = X.n, and so the second condition
holds.When agentYreceivesmthe agent setsY.n = gcd(Y.n, m), and somis a multiple of
the new value ofY.n.
If channel(X, Y)remains nonempty then the second
condition holds.
If(X, Y)becomes empty then the first condition holds
becausem = X.nandmis a multiple ofY.n.Loop VariantQuestion about Loop Variant in the Sequential
GCDWhat is an example of a loop variant for the following loop?x, y = X, Y
while x != y:
  # assertion: gcd(x, y) = gcd(X, Y)
  if x > y: x = x - y
  else: y = y - xAnswerf(x, y) = x + yis an example of a loop variant for the
following reasons.fis a function of the state of the program.fhas integer values.fis bounded below. The bound is 2 becausexandyare bounded below because of the invariantsx\(\geq\) 1, andy\(\geq\) 1.
  The precise bounds don't matter.Each execution of the loop decreasesf.We can carry out an induction onfbecausefhas integer values and is bounded below.Question about Loop Variants in Distributed AlgorithmsLet's look at the example of the distributed algorithm given
earlier? The example is repeated below.We are given a strongly connected network of agents -- i.e.,
  there is a path from each agent to every other agent.
  Each agent has a local variablen, which is initialized
  as a positive integer.
  
  Let GCD be the gcd (greatest commond divisor) of the
  initial values ofX.nof all agentsX.n.
In the code,successorsis the list of successor agents
in a network of agents, and the agent only sends messages to its
successors.def receive(message, sender):
   if n != message):
      n = gcd(n, message)
      for successor in successors:
         send(n, successor)QuestionWhat is an example of a loop variant for the distributed algorithm?AnswerLetfbe the tuple(N, M)whereNis the sum ofX.nfor all agentsX, andMis the total number of messages in
channels.Comparisons of tuples are made lexicographically.
For example (2, 1) > (1, 10), and (2, 1) < (2, 2).fis bounded below by (0, 0).An induction can be carried out on a tuple of integers.
Next we show that the execution of each event reducesf.When an agentXreceives a messagem,ifX.n\(\neq\)mthenX.nbecomesgcd(X.n, m), and so the new value ofX.nis less than its previous value, and so execution
  of the event decreasesNwhich decreasesf.ifX.n\(=\)mthen the event removes
  messagemfrom a channel and does not add messages to
  channels. So the event decreasesMwhich decreasesf.Termination Condition of a While LoopWhat can we say about the state at termination of the while loop in
the gcd example:x, y = X, Y
while x != y:
  # assertion: gcd(x, y) = gcd(X, Y)
  if x > y: x = x - y
  else: y = y - xThe while loop terminates whenx = y.
From the invariant, at termination:gcd(x, y) = gcd(x, x) = x = gcd(X, Y)Thereforexandyaregcd(X, Y)at termination of the while loop.Termination Condition of a Distributed AlgorithmWhat is the termination condition of the distributed gcd algorithm in
which thereceiveof each agent is as follows?def receive(message, sender):
   if n != message:
      n = gcd(n, message)
      for successor in successors:
         send(n, successor)At termination all channels are empty.
From the invariant, if channel (X, Y) is empty then X.n is a multiple
of Y.n.Therefore when all channels are empty, for every channel (X, Y) in the
system X.n is a multiple of Y.n.
Because the directed graph of agents and edges specified assuccessorsof agents is strongly
connected:for all agents X, Y in the system: X.n is a multiple of Y.nTherefore X.n = Y.n for all agents X, Y.
From the invariant, X.n is GCD.NextThe next webpage discussesdata flow in computations.Frequenty Asked QuestionsK. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

To prove that this predicate is an invariant we
prove that (1) it holds initially 
and (2) and for all state transitions, if the invariant holds in the
state before the transition then it holds in the state after the
transition.Initially,(X, Y) = 1and(Y, X) = 1.
Therefore the predicate holds initially.Let S be the state before a transition to a state S'.
We will prove that if the predicate holds in S then the predicate
holds in S'.Consider two cases:nX = 0 or nY = 0 in S.Because nX and nY do not increase,
  and nX and nY do not become negative,
  in the execution of an event, it follows that nX = 0 or nY = 0 in S'.
  Therefore the predicate holds in S'.nX > 0 and nY > 0 in S.Because the invariant holds in S it follows that(X, Y) + (Y,
  X) = 2in S.Because nY > 0 the execution of an event in which Y receives a
  balloon leaves(X, Y) + (Y, X)unchanged.Because nX > 0 the execution of an event in which X receives a
  balloon leaves(X, Y) + (Y, X)unchanged.Therefore(X, Y) + (Y, X) = 2in S'.
  So, the predicate holds in S'.Example of a Loop VariantQuestionHow do you prove that the algorithm terminates?AnswerTo prove that the algorithm terminates we use the loop variant:nX + nY + (X, Y) + (Y, X)The loop variant is a function of the state (i.e. variables) of the system.
In this example, the loop variant maps states of the program to
integers.We must show that  (1) the loop variant is bounded below and we cancarry
out inductionon the values of the loop variant, i.e., it can
decrease only a finite number of times before it reaches a lower
bound, and (2) the executions of all steps in all computations that
start at initial states reduce the value of the loop variant.
The specific lower bound is irrelevant for the proof of termination.In this example, 0 is (obviously) a lower bound.
Next we show that the execution of any event in any state reduces the
value of the loop variant.When Y receives a balloon if nY is positive then nY
decreases which the loop variant, and if nY is 0 then
(X, Y) which decreases the loop variant.Similarly every event in which X receives a balloon also reduces the
loop variant. Thus the executions of all events reduce the loop variant.Therefore the algorithm terminates execution.QuestionWhat can you prove about the state of the system when the computation
terminates? 
We have shown that computations terminate and we given examples of
invariants.AnswerAt termination all channels are empty.
Let's prove that nX or nY is 0 at termination.From the invariants if:(X, Y) + (Y, X)\(\neq\) 2 then nX
or nY is 0.At termination,(X, Y) + (Y, X)\(=\) 0, and the result follows.More Examples of Invariants and Loop VariantsWe reason about the correctness of many distributed algorithms in
  in the same way that we reason about sequentialwhileloops, by using invariants and loop variants.Example: GCD using a While LoopAloop invariantis an assertion about the state of the program that holds before and
  after each iteration of the loop.
  You can look upmany examplesof loop invariants on the web.Example: InvariantA loop invariant in the following greatest common
  divisor (gcd) program is shown in the body of the loop as the
  assertiongcd(x, y) = gcd(X, Y).x, y = X, Y
while x != y:
  # assertion: gcd(x, y) = gcd(X, Y)
  if x > y: x = x - y
  else: y = y - xInvariant of a Distributed System for GCDAn invariant of a distributed system is a
  predicates that holds in all states reachable from initial states.
  An invariant of a distributed system is the loop invariant of the
  following while loop.
  An invariantInvis shown
  as an assertion in the body of the loop.while there exists a nonempty input channel in the system:
   # assertion: Inv
   select a nonempty channel (u, v) in the system
   let the head of channel (u, v) be msg
   v executes receive(msg, u)Example of a Distributed GCDWe are given a strongly connected network of agents -- i.e.,
  there is a path from each agent to every other agent.
  Each agent has a local variablen, which is initialized
  as a positive integer.
  
  Let GCD be the gcd (greatest commond divisor) of the
  initial values ofX.nof all agentsX.n.
  The following distributed algorithm terminates and at terminationX.n = GCDfor all agentsX.n.
In the code,successorsis the list of successor agents
in a network of agents, and the agent only sends messages to its
successors.def receive(message, sender):
   if n != message:
      n = gcd(n, message)
      for successor in successors:
         send(n, successor)An invariant of the distributed algorithm is as follows.
The gcd of:all messages in channels andX.nfor all agentsXis GCD.ProofThe proof that the assertion holds initially is trivial.
Prove that if the assertion holds before any event 
then it continues to hold after the event.Let the event be agentAreceiving a messagem.
After the event,mis no longer in the channel;n = gcd(n, m); and messages with the new value ofnare in the output channels fromA.
The proof that the assertion continues to hold in the post-event state
is straightforward.Example of an InvariantAnother invariant for the gcd example, given earlier, is as follows.
For all channels(X, Y)in the network:Channel(X, Y)is empty andX.nis a
  multiple ofY.n, orThe last message in channel(X, Y)ismwherem = X.n.ProofThe assertion holds initially because the second condition holds. 
Prove that if the assertion holds before any event
then it continues to hold after the event.When agentXexecutes a receive that changesX.nthe agent sends a messagemtoYwherem = X.n, and so the second condition
holds.When agentYreceivesmthe agent setsY.n = gcd(Y.n, m), and somis a multiple of
the new value ofY.n.
If channel(X, Y)remains nonempty then the second
condition holds.
If(X, Y)becomes empty then the first condition holds
becausem = X.nandmis a multiple ofY.n.Loop VariantQuestion about Loop Variant in the Sequential
GCDWhat is an example of a loop variant for the following loop?x, y = X, Y
while x != y:
  # assertion: gcd(x, y) = gcd(X, Y)
  if x > y: x = x - y
  else: y = y - xAnswerf(x, y) = x + yis an example of a loop variant for the
following reasons.fis a function of the state of the program.fhas integer values.fis bounded below. The bound is 2 becausexandyare bounded below because of the invariantsx\(\geq\) 1, andy\(\geq\) 1.
  The precise bounds don't matter.Each execution of the loop decreasesf.We can carry out an induction onfbecausefhas integer values and is bounded below.Question about Loop Variants in Distributed AlgorithmsLet's look at the example of the distributed algorithm given
earlier? The example is repeated below.We are given a strongly connected network of agents -- i.e.,
  there is a path from each agent to every other agent.
  Each agent has a local variablen, which is initialized
  as a positive integer.
  
  Let GCD be the gcd (greatest commond divisor) of the
  initial values ofX.nof all agentsX.n.
In the code,successorsis the list of successor agents
in a network of agents, and the agent only sends messages to its
successors.def receive(message, sender):
   if n != message):
      n = gcd(n, message)
      for successor in successors:
         send(n, successor)QuestionWhat is an example of a loop variant for the distributed algorithm?AnswerLetfbe the tuple(N, M)whereNis the sum ofX.nfor all agentsX, andMis the total number of messages in
channels.Comparisons of tuples are made lexicographically.
For example (2, 1) > (1, 10), and (2, 1) < (2, 2).fis bounded below by (0, 0).An induction can be carried out on a tuple of integers.
Next we show that the execution of each event reducesf.When an agentXreceives a messagem,ifX.n\(\neq\)mthenX.nbecomesgcd(X.n, m), and so the new value ofX.nis less than its previous value, and so execution
  of the event decreasesNwhich decreasesf.ifX.n\(=\)mthen the event removes
  messagemfrom a channel and does not add messages to
  channels. So the event decreasesMwhich decreasesf.Termination Condition of a While LoopWhat can we say about the state at termination of the while loop in
the gcd example:x, y = X, Y
while x != y:
  # assertion: gcd(x, y) = gcd(X, Y)
  if x > y: x = x - y
  else: y = y - xThe while loop terminates whenx = y.
From the invariant, at termination:gcd(x, y) = gcd(x, x) = x = gcd(X, Y)Thereforexandyaregcd(X, Y)at termination of the while loop.Termination Condition of a Distributed AlgorithmWhat is the termination condition of the distributed gcd algorithm in
which thereceiveof each agent is as follows?def receive(message, sender):
   if n != message:
      n = gcd(n, message)
      for successor in successors:
         send(n, successor)At termination all channels are empty.
From the invariant, if channel (X, Y) is empty then X.n is a multiple
of Y.n.Therefore when all channels are empty, for every channel (X, Y) in the
system X.n is a multiple of Y.n.
Because the directed graph of agents and edges specified assuccessorsof agents is strongly
connected:for all agents X, Y in the system: X.n is a multiple of Y.nTherefore X.n = Y.n for all agents X, Y.
From the invariant, X.n is GCD.NextThe next webpage discussesdata flow in computations.Frequenty Asked QuestionsK. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

Initially,(X, Y) = 1and(Y, X) = 1.
Therefore the predicate holds initially.Let S be the state before a transition to a state S'.
We will prove that if the predicate holds in S then the predicate
holds in S'.Consider two cases:nX = 0 or nY = 0 in S.Because nX and nY do not increase,
  and nX and nY do not become negative,
  in the execution of an event, it follows that nX = 0 or nY = 0 in S'.
  Therefore the predicate holds in S'.nX > 0 and nY > 0 in S.Because the invariant holds in S it follows that(X, Y) + (Y,
  X) = 2in S.Because nY > 0 the execution of an event in which Y receives a
  balloon leaves(X, Y) + (Y, X)unchanged.Because nX > 0 the execution of an event in which X receives a
  balloon leaves(X, Y) + (Y, X)unchanged.Therefore(X, Y) + (Y, X) = 2in S'.
  So, the predicate holds in S'.Example of a Loop VariantQuestionHow do you prove that the algorithm terminates?AnswerTo prove that the algorithm terminates we use the loop variant:nX + nY + (X, Y) + (Y, X)The loop variant is a function of the state (i.e. variables) of the system.
In this example, the loop variant maps states of the program to
integers.We must show that  (1) the loop variant is bounded below and we cancarry
out inductionon the values of the loop variant, i.e., it can
decrease only a finite number of times before it reaches a lower
bound, and (2) the executions of all steps in all computations that
start at initial states reduce the value of the loop variant.
The specific lower bound is irrelevant for the proof of termination.In this example, 0 is (obviously) a lower bound.
Next we show that the execution of any event in any state reduces the
value of the loop variant.When Y receives a balloon if nY is positive then nY
decreases which the loop variant, and if nY is 0 then
(X, Y) which decreases the loop variant.Similarly every event in which X receives a balloon also reduces the
loop variant. Thus the executions of all events reduce the loop variant.Therefore the algorithm terminates execution.QuestionWhat can you prove about the state of the system when the computation
terminates? 
We have shown that computations terminate and we given examples of
invariants.AnswerAt termination all channels are empty.
Let's prove that nX or nY is 0 at termination.From the invariants if:(X, Y) + (Y, X)\(\neq\) 2 then nX
or nY is 0.At termination,(X, Y) + (Y, X)\(=\) 0, and the result follows.More Examples of Invariants and Loop VariantsWe reason about the correctness of many distributed algorithms in
  in the same way that we reason about sequentialwhileloops, by using invariants and loop variants.Example: GCD using a While LoopAloop invariantis an assertion about the state of the program that holds before and
  after each iteration of the loop.
  You can look upmany examplesof loop invariants on the web.Example: InvariantA loop invariant in the following greatest common
  divisor (gcd) program is shown in the body of the loop as the
  assertiongcd(x, y) = gcd(X, Y).x, y = X, Y
while x != y:
  # assertion: gcd(x, y) = gcd(X, Y)
  if x > y: x = x - y
  else: y = y - xInvariant of a Distributed System for GCDAn invariant of a distributed system is a
  predicates that holds in all states reachable from initial states.
  An invariant of a distributed system is the loop invariant of the
  following while loop.
  An invariantInvis shown
  as an assertion in the body of the loop.while there exists a nonempty input channel in the system:
   # assertion: Inv
   select a nonempty channel (u, v) in the system
   let the head of channel (u, v) be msg
   v executes receive(msg, u)Example of a Distributed GCDWe are given a strongly connected network of agents -- i.e.,
  there is a path from each agent to every other agent.
  Each agent has a local variablen, which is initialized
  as a positive integer.
  
  Let GCD be the gcd (greatest commond divisor) of the
  initial values ofX.nof all agentsX.n.
  The following distributed algorithm terminates and at terminationX.n = GCDfor all agentsX.n.
In the code,successorsis the list of successor agents
in a network of agents, and the agent only sends messages to its
successors.def receive(message, sender):
   if n != message:
      n = gcd(n, message)
      for successor in successors:
         send(n, successor)An invariant of the distributed algorithm is as follows.
The gcd of:all messages in channels andX.nfor all agentsXis GCD.ProofThe proof that the assertion holds initially is trivial.
Prove that if the assertion holds before any event 
then it continues to hold after the event.Let the event be agentAreceiving a messagem.
After the event,mis no longer in the channel;n = gcd(n, m); and messages with the new value ofnare in the output channels fromA.
The proof that the assertion continues to hold in the post-event state
is straightforward.Example of an InvariantAnother invariant for the gcd example, given earlier, is as follows.
For all channels(X, Y)in the network:Channel(X, Y)is empty andX.nis a
  multiple ofY.n, orThe last message in channel(X, Y)ismwherem = X.n.ProofThe assertion holds initially because the second condition holds. 
Prove that if the assertion holds before any event
then it continues to hold after the event.When agentXexecutes a receive that changesX.nthe agent sends a messagemtoYwherem = X.n, and so the second condition
holds.When agentYreceivesmthe agent setsY.n = gcd(Y.n, m), and somis a multiple of
the new value ofY.n.
If channel(X, Y)remains nonempty then the second
condition holds.
If(X, Y)becomes empty then the first condition holds
becausem = X.nandmis a multiple ofY.n.Loop VariantQuestion about Loop Variant in the Sequential
GCDWhat is an example of a loop variant for the following loop?x, y = X, Y
while x != y:
  # assertion: gcd(x, y) = gcd(X, Y)
  if x > y: x = x - y
  else: y = y - xAnswerf(x, y) = x + yis an example of a loop variant for the
following reasons.fis a function of the state of the program.fhas integer values.fis bounded below. The bound is 2 becausexandyare bounded below because of the invariantsx\(\geq\) 1, andy\(\geq\) 1.
  The precise bounds don't matter.Each execution of the loop decreasesf.We can carry out an induction onfbecausefhas integer values and is bounded below.Question about Loop Variants in Distributed AlgorithmsLet's look at the example of the distributed algorithm given
earlier? The example is repeated below.We are given a strongly connected network of agents -- i.e.,
  there is a path from each agent to every other agent.
  Each agent has a local variablen, which is initialized
  as a positive integer.
  
  Let GCD be the gcd (greatest commond divisor) of the
  initial values ofX.nof all agentsX.n.
In the code,successorsis the list of successor agents
in a network of agents, and the agent only sends messages to its
successors.def receive(message, sender):
   if n != message):
      n = gcd(n, message)
      for successor in successors:
         send(n, successor)QuestionWhat is an example of a loop variant for the distributed algorithm?AnswerLetfbe the tuple(N, M)whereNis the sum ofX.nfor all agentsX, andMis the total number of messages in
channels.Comparisons of tuples are made lexicographically.
For example (2, 1) > (1, 10), and (2, 1) < (2, 2).fis bounded below by (0, 0).An induction can be carried out on a tuple of integers.
Next we show that the execution of each event reducesf.When an agentXreceives a messagem,ifX.n\(\neq\)mthenX.nbecomesgcd(X.n, m), and so the new value ofX.nis less than its previous value, and so execution
  of the event decreasesNwhich decreasesf.ifX.n\(=\)mthen the event removes
  messagemfrom a channel and does not add messages to
  channels. So the event decreasesMwhich decreasesf.Termination Condition of a While LoopWhat can we say about the state at termination of the while loop in
the gcd example:x, y = X, Y
while x != y:
  # assertion: gcd(x, y) = gcd(X, Y)
  if x > y: x = x - y
  else: y = y - xThe while loop terminates whenx = y.
From the invariant, at termination:gcd(x, y) = gcd(x, x) = x = gcd(X, Y)Thereforexandyaregcd(X, Y)at termination of the while loop.Termination Condition of a Distributed AlgorithmWhat is the termination condition of the distributed gcd algorithm in
which thereceiveof each agent is as follows?def receive(message, sender):
   if n != message:
      n = gcd(n, message)
      for successor in successors:
         send(n, successor)At termination all channels are empty.
From the invariant, if channel (X, Y) is empty then X.n is a multiple
of Y.n.Therefore when all channels are empty, for every channel (X, Y) in the
system X.n is a multiple of Y.n.
Because the directed graph of agents and edges specified assuccessorsof agents is strongly
connected:for all agents X, Y in the system: X.n is a multiple of Y.nTherefore X.n = Y.n for all agents X, Y.
From the invariant, X.n is GCD.NextThe next webpage discussesdata flow in computations.Frequenty Asked QuestionsK. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

Let S be the state before a transition to a state S'.
We will prove that if the predicate holds in S then the predicate
holds in S'.Consider two cases:nX = 0 or nY = 0 in S.Because nX and nY do not increase,
  and nX and nY do not become negative,
  in the execution of an event, it follows that nX = 0 or nY = 0 in S'.
  Therefore the predicate holds in S'.nX > 0 and nY > 0 in S.Because the invariant holds in S it follows that(X, Y) + (Y,
  X) = 2in S.Because nY > 0 the execution of an event in which Y receives a
  balloon leaves(X, Y) + (Y, X)unchanged.Because nX > 0 the execution of an event in which X receives a
  balloon leaves(X, Y) + (Y, X)unchanged.Therefore(X, Y) + (Y, X) = 2in S'.
  So, the predicate holds in S'.Example of a Loop VariantQuestionHow do you prove that the algorithm terminates?AnswerTo prove that the algorithm terminates we use the loop variant:nX + nY + (X, Y) + (Y, X)The loop variant is a function of the state (i.e. variables) of the system.
In this example, the loop variant maps states of the program to
integers.We must show that  (1) the loop variant is bounded below and we cancarry
out inductionon the values of the loop variant, i.e., it can
decrease only a finite number of times before it reaches a lower
bound, and (2) the executions of all steps in all computations that
start at initial states reduce the value of the loop variant.
The specific lower bound is irrelevant for the proof of termination.In this example, 0 is (obviously) a lower bound.
Next we show that the execution of any event in any state reduces the
value of the loop variant.When Y receives a balloon if nY is positive then nY
decreases which the loop variant, and if nY is 0 then
(X, Y) which decreases the loop variant.Similarly every event in which X receives a balloon also reduces the
loop variant. Thus the executions of all events reduce the loop variant.Therefore the algorithm terminates execution.QuestionWhat can you prove about the state of the system when the computation
terminates? 
We have shown that computations terminate and we given examples of
invariants.AnswerAt termination all channels are empty.
Let's prove that nX or nY is 0 at termination.From the invariants if:(X, Y) + (Y, X)\(\neq\) 2 then nX
or nY is 0.At termination,(X, Y) + (Y, X)\(=\) 0, and the result follows.More Examples of Invariants and Loop VariantsWe reason about the correctness of many distributed algorithms in
  in the same way that we reason about sequentialwhileloops, by using invariants and loop variants.Example: GCD using a While LoopAloop invariantis an assertion about the state of the program that holds before and
  after each iteration of the loop.
  You can look upmany examplesof loop invariants on the web.Example: InvariantA loop invariant in the following greatest common
  divisor (gcd) program is shown in the body of the loop as the
  assertiongcd(x, y) = gcd(X, Y).x, y = X, Y
while x != y:
  # assertion: gcd(x, y) = gcd(X, Y)
  if x > y: x = x - y
  else: y = y - xInvariant of a Distributed System for GCDAn invariant of a distributed system is a
  predicates that holds in all states reachable from initial states.
  An invariant of a distributed system is the loop invariant of the
  following while loop.
  An invariantInvis shown
  as an assertion in the body of the loop.while there exists a nonempty input channel in the system:
   # assertion: Inv
   select a nonempty channel (u, v) in the system
   let the head of channel (u, v) be msg
   v executes receive(msg, u)Example of a Distributed GCDWe are given a strongly connected network of agents -- i.e.,
  there is a path from each agent to every other agent.
  Each agent has a local variablen, which is initialized
  as a positive integer.
  
  Let GCD be the gcd (greatest commond divisor) of the
  initial values ofX.nof all agentsX.n.
  The following distributed algorithm terminates and at terminationX.n = GCDfor all agentsX.n.
In the code,successorsis the list of successor agents
in a network of agents, and the agent only sends messages to its
successors.def receive(message, sender):
   if n != message:
      n = gcd(n, message)
      for successor in successors:
         send(n, successor)An invariant of the distributed algorithm is as follows.
The gcd of:all messages in channels andX.nfor all agentsXis GCD.ProofThe proof that the assertion holds initially is trivial.
Prove that if the assertion holds before any event 
then it continues to hold after the event.Let the event be agentAreceiving a messagem.
After the event,mis no longer in the channel;n = gcd(n, m); and messages with the new value ofnare in the output channels fromA.
The proof that the assertion continues to hold in the post-event state
is straightforward.Example of an InvariantAnother invariant for the gcd example, given earlier, is as follows.
For all channels(X, Y)in the network:Channel(X, Y)is empty andX.nis a
  multiple ofY.n, orThe last message in channel(X, Y)ismwherem = X.n.ProofThe assertion holds initially because the second condition holds. 
Prove that if the assertion holds before any event
then it continues to hold after the event.When agentXexecutes a receive that changesX.nthe agent sends a messagemtoYwherem = X.n, and so the second condition
holds.When agentYreceivesmthe agent setsY.n = gcd(Y.n, m), and somis a multiple of
the new value ofY.n.
If channel(X, Y)remains nonempty then the second
condition holds.
If(X, Y)becomes empty then the first condition holds
becausem = X.nandmis a multiple ofY.n.Loop VariantQuestion about Loop Variant in the Sequential
GCDWhat is an example of a loop variant for the following loop?x, y = X, Y
while x != y:
  # assertion: gcd(x, y) = gcd(X, Y)
  if x > y: x = x - y
  else: y = y - xAnswerf(x, y) = x + yis an example of a loop variant for the
following reasons.fis a function of the state of the program.fhas integer values.fis bounded below. The bound is 2 becausexandyare bounded below because of the invariantsx\(\geq\) 1, andy\(\geq\) 1.
  The precise bounds don't matter.Each execution of the loop decreasesf.We can carry out an induction onfbecausefhas integer values and is bounded below.Question about Loop Variants in Distributed AlgorithmsLet's look at the example of the distributed algorithm given
earlier? The example is repeated below.We are given a strongly connected network of agents -- i.e.,
  there is a path from each agent to every other agent.
  Each agent has a local variablen, which is initialized
  as a positive integer.
  
  Let GCD be the gcd (greatest commond divisor) of the
  initial values ofX.nof all agentsX.n.
In the code,successorsis the list of successor agents
in a network of agents, and the agent only sends messages to its
successors.def receive(message, sender):
   if n != message):
      n = gcd(n, message)
      for successor in successors:
         send(n, successor)QuestionWhat is an example of a loop variant for the distributed algorithm?AnswerLetfbe the tuple(N, M)whereNis the sum ofX.nfor all agentsX, andMis the total number of messages in
channels.Comparisons of tuples are made lexicographically.
For example (2, 1) > (1, 10), and (2, 1) < (2, 2).fis bounded below by (0, 0).An induction can be carried out on a tuple of integers.
Next we show that the execution of each event reducesf.When an agentXreceives a messagem,ifX.n\(\neq\)mthenX.nbecomesgcd(X.n, m), and so the new value ofX.nis less than its previous value, and so execution
  of the event decreasesNwhich decreasesf.ifX.n\(=\)mthen the event removes
  messagemfrom a channel and does not add messages to
  channels. So the event decreasesMwhich decreasesf.Termination Condition of a While LoopWhat can we say about the state at termination of the while loop in
the gcd example:x, y = X, Y
while x != y:
  # assertion: gcd(x, y) = gcd(X, Y)
  if x > y: x = x - y
  else: y = y - xThe while loop terminates whenx = y.
From the invariant, at termination:gcd(x, y) = gcd(x, x) = x = gcd(X, Y)Thereforexandyaregcd(X, Y)at termination of the while loop.Termination Condition of a Distributed AlgorithmWhat is the termination condition of the distributed gcd algorithm in
which thereceiveof each agent is as follows?def receive(message, sender):
   if n != message:
      n = gcd(n, message)
      for successor in successors:
         send(n, successor)At termination all channels are empty.
From the invariant, if channel (X, Y) is empty then X.n is a multiple
of Y.n.Therefore when all channels are empty, for every channel (X, Y) in the
system X.n is a multiple of Y.n.
Because the directed graph of agents and edges specified assuccessorsof agents is strongly
connected:for all agents X, Y in the system: X.n is a multiple of Y.nTherefore X.n = Y.n for all agents X, Y.
From the invariant, X.n is GCD.NextThe next webpage discussesdata flow in computations.Frequenty Asked QuestionsK. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

Consider two cases:nX = 0 or nY = 0 in S.Because nX and nY do not increase,
  and nX and nY do not become negative,
  in the execution of an event, it follows that nX = 0 or nY = 0 in S'.
  Therefore the predicate holds in S'.nX > 0 and nY > 0 in S.Because the invariant holds in S it follows that(X, Y) + (Y,
  X) = 2in S.Because nY > 0 the execution of an event in which Y receives a
  balloon leaves(X, Y) + (Y, X)unchanged.Because nX > 0 the execution of an event in which X receives a
  balloon leaves(X, Y) + (Y, X)unchanged.Therefore(X, Y) + (Y, X) = 2in S'.
  So, the predicate holds in S'.Example of a Loop VariantQuestionHow do you prove that the algorithm terminates?AnswerTo prove that the algorithm terminates we use the loop variant:nX + nY + (X, Y) + (Y, X)The loop variant is a function of the state (i.e. variables) of the system.
In this example, the loop variant maps states of the program to
integers.We must show that  (1) the loop variant is bounded below and we cancarry
out inductionon the values of the loop variant, i.e., it can
decrease only a finite number of times before it reaches a lower
bound, and (2) the executions of all steps in all computations that
start at initial states reduce the value of the loop variant.
The specific lower bound is irrelevant for the proof of termination.In this example, 0 is (obviously) a lower bound.
Next we show that the execution of any event in any state reduces the
value of the loop variant.When Y receives a balloon if nY is positive then nY
decreases which the loop variant, and if nY is 0 then
(X, Y) which decreases the loop variant.Similarly every event in which X receives a balloon also reduces the
loop variant. Thus the executions of all events reduce the loop variant.Therefore the algorithm terminates execution.QuestionWhat can you prove about the state of the system when the computation
terminates? 
We have shown that computations terminate and we given examples of
invariants.AnswerAt termination all channels are empty.
Let's prove that nX or nY is 0 at termination.From the invariants if:(X, Y) + (Y, X)\(\neq\) 2 then nX
or nY is 0.At termination,(X, Y) + (Y, X)\(=\) 0, and the result follows.More Examples of Invariants and Loop VariantsWe reason about the correctness of many distributed algorithms in
  in the same way that we reason about sequentialwhileloops, by using invariants and loop variants.Example: GCD using a While LoopAloop invariantis an assertion about the state of the program that holds before and
  after each iteration of the loop.
  You can look upmany examplesof loop invariants on the web.Example: InvariantA loop invariant in the following greatest common
  divisor (gcd) program is shown in the body of the loop as the
  assertiongcd(x, y) = gcd(X, Y).x, y = X, Y
while x != y:
  # assertion: gcd(x, y) = gcd(X, Y)
  if x > y: x = x - y
  else: y = y - xInvariant of a Distributed System for GCDAn invariant of a distributed system is a
  predicates that holds in all states reachable from initial states.
  An invariant of a distributed system is the loop invariant of the
  following while loop.
  An invariantInvis shown
  as an assertion in the body of the loop.while there exists a nonempty input channel in the system:
   # assertion: Inv
   select a nonempty channel (u, v) in the system
   let the head of channel (u, v) be msg
   v executes receive(msg, u)Example of a Distributed GCDWe are given a strongly connected network of agents -- i.e.,
  there is a path from each agent to every other agent.
  Each agent has a local variablen, which is initialized
  as a positive integer.
  
  Let GCD be the gcd (greatest commond divisor) of the
  initial values ofX.nof all agentsX.n.
  The following distributed algorithm terminates and at terminationX.n = GCDfor all agentsX.n.
In the code,successorsis the list of successor agents
in a network of agents, and the agent only sends messages to its
successors.def receive(message, sender):
   if n != message:
      n = gcd(n, message)
      for successor in successors:
         send(n, successor)An invariant of the distributed algorithm is as follows.
The gcd of:all messages in channels andX.nfor all agentsXis GCD.ProofThe proof that the assertion holds initially is trivial.
Prove that if the assertion holds before any event 
then it continues to hold after the event.Let the event be agentAreceiving a messagem.
After the event,mis no longer in the channel;n = gcd(n, m); and messages with the new value ofnare in the output channels fromA.
The proof that the assertion continues to hold in the post-event state
is straightforward.Example of an InvariantAnother invariant for the gcd example, given earlier, is as follows.
For all channels(X, Y)in the network:Channel(X, Y)is empty andX.nis a
  multiple ofY.n, orThe last message in channel(X, Y)ismwherem = X.n.ProofThe assertion holds initially because the second condition holds. 
Prove that if the assertion holds before any event
then it continues to hold after the event.When agentXexecutes a receive that changesX.nthe agent sends a messagemtoYwherem = X.n, and so the second condition
holds.When agentYreceivesmthe agent setsY.n = gcd(Y.n, m), and somis a multiple of
the new value ofY.n.
If channel(X, Y)remains nonempty then the second
condition holds.
If(X, Y)becomes empty then the first condition holds
becausem = X.nandmis a multiple ofY.n.Loop VariantQuestion about Loop Variant in the Sequential
GCDWhat is an example of a loop variant for the following loop?x, y = X, Y
while x != y:
  # assertion: gcd(x, y) = gcd(X, Y)
  if x > y: x = x - y
  else: y = y - xAnswerf(x, y) = x + yis an example of a loop variant for the
following reasons.fis a function of the state of the program.fhas integer values.fis bounded below. The bound is 2 becausexandyare bounded below because of the invariantsx\(\geq\) 1, andy\(\geq\) 1.
  The precise bounds don't matter.Each execution of the loop decreasesf.We can carry out an induction onfbecausefhas integer values and is bounded below.Question about Loop Variants in Distributed AlgorithmsLet's look at the example of the distributed algorithm given
earlier? The example is repeated below.We are given a strongly connected network of agents -- i.e.,
  there is a path from each agent to every other agent.
  Each agent has a local variablen, which is initialized
  as a positive integer.
  
  Let GCD be the gcd (greatest commond divisor) of the
  initial values ofX.nof all agentsX.n.
In the code,successorsis the list of successor agents
in a network of agents, and the agent only sends messages to its
successors.def receive(message, sender):
   if n != message):
      n = gcd(n, message)
      for successor in successors:
         send(n, successor)QuestionWhat is an example of a loop variant for the distributed algorithm?AnswerLetfbe the tuple(N, M)whereNis the sum ofX.nfor all agentsX, andMis the total number of messages in
channels.Comparisons of tuples are made lexicographically.
For example (2, 1) > (1, 10), and (2, 1) < (2, 2).fis bounded below by (0, 0).An induction can be carried out on a tuple of integers.
Next we show that the execution of each event reducesf.When an agentXreceives a messagem,ifX.n\(\neq\)mthenX.nbecomesgcd(X.n, m), and so the new value ofX.nis less than its previous value, and so execution
  of the event decreasesNwhich decreasesf.ifX.n\(=\)mthen the event removes
  messagemfrom a channel and does not add messages to
  channels. So the event decreasesMwhich decreasesf.Termination Condition of a While LoopWhat can we say about the state at termination of the while loop in
the gcd example:x, y = X, Y
while x != y:
  # assertion: gcd(x, y) = gcd(X, Y)
  if x > y: x = x - y
  else: y = y - xThe while loop terminates whenx = y.
From the invariant, at termination:gcd(x, y) = gcd(x, x) = x = gcd(X, Y)Thereforexandyaregcd(X, Y)at termination of the while loop.Termination Condition of a Distributed AlgorithmWhat is the termination condition of the distributed gcd algorithm in
which thereceiveof each agent is as follows?def receive(message, sender):
   if n != message:
      n = gcd(n, message)
      for successor in successors:
         send(n, successor)At termination all channels are empty.
From the invariant, if channel (X, Y) is empty then X.n is a multiple
of Y.n.Therefore when all channels are empty, for every channel (X, Y) in the
system X.n is a multiple of Y.n.
Because the directed graph of agents and edges specified assuccessorsof agents is strongly
connected:for all agents X, Y in the system: X.n is a multiple of Y.nTherefore X.n = Y.n for all agents X, Y.
From the invariant, X.n is GCD.NextThe next webpage discussesdata flow in computations.Frequenty Asked QuestionsK. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

Because nX and nY do not increase,
  and nX and nY do not become negative,
  in the execution of an event, it follows that nX = 0 or nY = 0 in S'.
  Therefore the predicate holds in S'.

Because the invariant holds in S it follows that(X, Y) + (Y,
  X) = 2in S.Because nY > 0 the execution of an event in which Y receives a
  balloon leaves(X, Y) + (Y, X)unchanged.Because nX > 0 the execution of an event in which X receives a
  balloon leaves(X, Y) + (Y, X)unchanged.Therefore(X, Y) + (Y, X) = 2in S'.
  So, the predicate holds in S'.

Because nY > 0 the execution of an event in which Y receives a
  balloon leaves(X, Y) + (Y, X)unchanged.Because nX > 0 the execution of an event in which X receives a
  balloon leaves(X, Y) + (Y, X)unchanged.Therefore(X, Y) + (Y, X) = 2in S'.
  So, the predicate holds in S'.

Because nX > 0 the execution of an event in which X receives a
  balloon leaves(X, Y) + (Y, X)unchanged.Therefore(X, Y) + (Y, X) = 2in S'.
  So, the predicate holds in S'.

Therefore(X, Y) + (Y, X) = 2in S'.
  So, the predicate holds in S'.

The loop variant is a function of the state (i.e. variables) of the system.
In this example, the loop variant maps states of the program to
integers.We must show that  (1) the loop variant is bounded below and we cancarry
out inductionon the values of the loop variant, i.e., it can
decrease only a finite number of times before it reaches a lower
bound, and (2) the executions of all steps in all computations that
start at initial states reduce the value of the loop variant.
The specific lower bound is irrelevant for the proof of termination.In this example, 0 is (obviously) a lower bound.
Next we show that the execution of any event in any state reduces the
value of the loop variant.When Y receives a balloon if nY is positive then nY
decreases which the loop variant, and if nY is 0 then
(X, Y) which decreases the loop variant.Similarly every event in which X receives a balloon also reduces the
loop variant. Thus the executions of all events reduce the loop variant.Therefore the algorithm terminates execution.QuestionWhat can you prove about the state of the system when the computation
terminates? 
We have shown that computations terminate and we given examples of
invariants.AnswerAt termination all channels are empty.
Let's prove that nX or nY is 0 at termination.From the invariants if:(X, Y) + (Y, X)\(\neq\) 2 then nX
or nY is 0.At termination,(X, Y) + (Y, X)\(=\) 0, and the result follows.More Examples of Invariants and Loop VariantsWe reason about the correctness of many distributed algorithms in
  in the same way that we reason about sequentialwhileloops, by using invariants and loop variants.Example: GCD using a While LoopAloop invariantis an assertion about the state of the program that holds before and
  after each iteration of the loop.
  You can look upmany examplesof loop invariants on the web.Example: InvariantA loop invariant in the following greatest common
  divisor (gcd) program is shown in the body of the loop as the
  assertiongcd(x, y) = gcd(X, Y).x, y = X, Y
while x != y:
  # assertion: gcd(x, y) = gcd(X, Y)
  if x > y: x = x - y
  else: y = y - xInvariant of a Distributed System for GCDAn invariant of a distributed system is a
  predicates that holds in all states reachable from initial states.
  An invariant of a distributed system is the loop invariant of the
  following while loop.
  An invariantInvis shown
  as an assertion in the body of the loop.while there exists a nonempty input channel in the system:
   # assertion: Inv
   select a nonempty channel (u, v) in the system
   let the head of channel (u, v) be msg
   v executes receive(msg, u)Example of a Distributed GCDWe are given a strongly connected network of agents -- i.e.,
  there is a path from each agent to every other agent.
  Each agent has a local variablen, which is initialized
  as a positive integer.
  
  Let GCD be the gcd (greatest commond divisor) of the
  initial values ofX.nof all agentsX.n.
  The following distributed algorithm terminates and at terminationX.n = GCDfor all agentsX.n.
In the code,successorsis the list of successor agents
in a network of agents, and the agent only sends messages to its
successors.def receive(message, sender):
   if n != message:
      n = gcd(n, message)
      for successor in successors:
         send(n, successor)An invariant of the distributed algorithm is as follows.
The gcd of:all messages in channels andX.nfor all agentsXis GCD.ProofThe proof that the assertion holds initially is trivial.
Prove that if the assertion holds before any event 
then it continues to hold after the event.Let the event be agentAreceiving a messagem.
After the event,mis no longer in the channel;n = gcd(n, m); and messages with the new value ofnare in the output channels fromA.
The proof that the assertion continues to hold in the post-event state
is straightforward.Example of an InvariantAnother invariant for the gcd example, given earlier, is as follows.
For all channels(X, Y)in the network:Channel(X, Y)is empty andX.nis a
  multiple ofY.n, orThe last message in channel(X, Y)ismwherem = X.n.ProofThe assertion holds initially because the second condition holds. 
Prove that if the assertion holds before any event
then it continues to hold after the event.When agentXexecutes a receive that changesX.nthe agent sends a messagemtoYwherem = X.n, and so the second condition
holds.When agentYreceivesmthe agent setsY.n = gcd(Y.n, m), and somis a multiple of
the new value ofY.n.
If channel(X, Y)remains nonempty then the second
condition holds.
If(X, Y)becomes empty then the first condition holds
becausem = X.nandmis a multiple ofY.n.Loop VariantQuestion about Loop Variant in the Sequential
GCDWhat is an example of a loop variant for the following loop?x, y = X, Y
while x != y:
  # assertion: gcd(x, y) = gcd(X, Y)
  if x > y: x = x - y
  else: y = y - xAnswerf(x, y) = x + yis an example of a loop variant for the
following reasons.fis a function of the state of the program.fhas integer values.fis bounded below. The bound is 2 becausexandyare bounded below because of the invariantsx\(\geq\) 1, andy\(\geq\) 1.
  The precise bounds don't matter.Each execution of the loop decreasesf.We can carry out an induction onfbecausefhas integer values and is bounded below.Question about Loop Variants in Distributed AlgorithmsLet's look at the example of the distributed algorithm given
earlier? The example is repeated below.We are given a strongly connected network of agents -- i.e.,
  there is a path from each agent to every other agent.
  Each agent has a local variablen, which is initialized
  as a positive integer.
  
  Let GCD be the gcd (greatest commond divisor) of the
  initial values ofX.nof all agentsX.n.
In the code,successorsis the list of successor agents
in a network of agents, and the agent only sends messages to its
successors.def receive(message, sender):
   if n != message):
      n = gcd(n, message)
      for successor in successors:
         send(n, successor)QuestionWhat is an example of a loop variant for the distributed algorithm?AnswerLetfbe the tuple(N, M)whereNis the sum ofX.nfor all agentsX, andMis the total number of messages in
channels.Comparisons of tuples are made lexicographically.
For example (2, 1) > (1, 10), and (2, 1) < (2, 2).fis bounded below by (0, 0).An induction can be carried out on a tuple of integers.
Next we show that the execution of each event reducesf.When an agentXreceives a messagem,ifX.n\(\neq\)mthenX.nbecomesgcd(X.n, m), and so the new value ofX.nis less than its previous value, and so execution
  of the event decreasesNwhich decreasesf.ifX.n\(=\)mthen the event removes
  messagemfrom a channel and does not add messages to
  channels. So the event decreasesMwhich decreasesf.Termination Condition of a While LoopWhat can we say about the state at termination of the while loop in
the gcd example:x, y = X, Y
while x != y:
  # assertion: gcd(x, y) = gcd(X, Y)
  if x > y: x = x - y
  else: y = y - xThe while loop terminates whenx = y.
From the invariant, at termination:gcd(x, y) = gcd(x, x) = x = gcd(X, Y)Thereforexandyaregcd(X, Y)at termination of the while loop.Termination Condition of a Distributed AlgorithmWhat is the termination condition of the distributed gcd algorithm in
which thereceiveof each agent is as follows?def receive(message, sender):
   if n != message:
      n = gcd(n, message)
      for successor in successors:
         send(n, successor)At termination all channels are empty.
From the invariant, if channel (X, Y) is empty then X.n is a multiple
of Y.n.Therefore when all channels are empty, for every channel (X, Y) in the
system X.n is a multiple of Y.n.
Because the directed graph of agents and edges specified assuccessorsof agents is strongly
connected:for all agents X, Y in the system: X.n is a multiple of Y.nTherefore X.n = Y.n for all agents X, Y.
From the invariant, X.n is GCD.NextThe next webpage discussesdata flow in computations.Frequenty Asked QuestionsK. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

We must show that  (1) the loop variant is bounded below and we cancarry
out inductionon the values of the loop variant, i.e., it can
decrease only a finite number of times before it reaches a lower
bound, and (2) the executions of all steps in all computations that
start at initial states reduce the value of the loop variant.
The specific lower bound is irrelevant for the proof of termination.In this example, 0 is (obviously) a lower bound.
Next we show that the execution of any event in any state reduces the
value of the loop variant.When Y receives a balloon if nY is positive then nY
decreases which the loop variant, and if nY is 0 then
(X, Y) which decreases the loop variant.Similarly every event in which X receives a balloon also reduces the
loop variant. Thus the executions of all events reduce the loop variant.Therefore the algorithm terminates execution.QuestionWhat can you prove about the state of the system when the computation
terminates? 
We have shown that computations terminate and we given examples of
invariants.AnswerAt termination all channels are empty.
Let's prove that nX or nY is 0 at termination.From the invariants if:(X, Y) + (Y, X)\(\neq\) 2 then nX
or nY is 0.At termination,(X, Y) + (Y, X)\(=\) 0, and the result follows.More Examples of Invariants and Loop VariantsWe reason about the correctness of many distributed algorithms in
  in the same way that we reason about sequentialwhileloops, by using invariants and loop variants.Example: GCD using a While LoopAloop invariantis an assertion about the state of the program that holds before and
  after each iteration of the loop.
  You can look upmany examplesof loop invariants on the web.Example: InvariantA loop invariant in the following greatest common
  divisor (gcd) program is shown in the body of the loop as the
  assertiongcd(x, y) = gcd(X, Y).x, y = X, Y
while x != y:
  # assertion: gcd(x, y) = gcd(X, Y)
  if x > y: x = x - y
  else: y = y - xInvariant of a Distributed System for GCDAn invariant of a distributed system is a
  predicates that holds in all states reachable from initial states.
  An invariant of a distributed system is the loop invariant of the
  following while loop.
  An invariantInvis shown
  as an assertion in the body of the loop.while there exists a nonempty input channel in the system:
   # assertion: Inv
   select a nonempty channel (u, v) in the system
   let the head of channel (u, v) be msg
   v executes receive(msg, u)Example of a Distributed GCDWe are given a strongly connected network of agents -- i.e.,
  there is a path from each agent to every other agent.
  Each agent has a local variablen, which is initialized
  as a positive integer.
  
  Let GCD be the gcd (greatest commond divisor) of the
  initial values ofX.nof all agentsX.n.
  The following distributed algorithm terminates and at terminationX.n = GCDfor all agentsX.n.
In the code,successorsis the list of successor agents
in a network of agents, and the agent only sends messages to its
successors.def receive(message, sender):
   if n != message:
      n = gcd(n, message)
      for successor in successors:
         send(n, successor)An invariant of the distributed algorithm is as follows.
The gcd of:all messages in channels andX.nfor all agentsXis GCD.ProofThe proof that the assertion holds initially is trivial.
Prove that if the assertion holds before any event 
then it continues to hold after the event.Let the event be agentAreceiving a messagem.
After the event,mis no longer in the channel;n = gcd(n, m); and messages with the new value ofnare in the output channels fromA.
The proof that the assertion continues to hold in the post-event state
is straightforward.Example of an InvariantAnother invariant for the gcd example, given earlier, is as follows.
For all channels(X, Y)in the network:Channel(X, Y)is empty andX.nis a
  multiple ofY.n, orThe last message in channel(X, Y)ismwherem = X.n.ProofThe assertion holds initially because the second condition holds. 
Prove that if the assertion holds before any event
then it continues to hold after the event.When agentXexecutes a receive that changesX.nthe agent sends a messagemtoYwherem = X.n, and so the second condition
holds.When agentYreceivesmthe agent setsY.n = gcd(Y.n, m), and somis a multiple of
the new value ofY.n.
If channel(X, Y)remains nonempty then the second
condition holds.
If(X, Y)becomes empty then the first condition holds
becausem = X.nandmis a multiple ofY.n.Loop VariantQuestion about Loop Variant in the Sequential
GCDWhat is an example of a loop variant for the following loop?x, y = X, Y
while x != y:
  # assertion: gcd(x, y) = gcd(X, Y)
  if x > y: x = x - y
  else: y = y - xAnswerf(x, y) = x + yis an example of a loop variant for the
following reasons.fis a function of the state of the program.fhas integer values.fis bounded below. The bound is 2 becausexandyare bounded below because of the invariantsx\(\geq\) 1, andy\(\geq\) 1.
  The precise bounds don't matter.Each execution of the loop decreasesf.We can carry out an induction onfbecausefhas integer values and is bounded below.Question about Loop Variants in Distributed AlgorithmsLet's look at the example of the distributed algorithm given
earlier? The example is repeated below.We are given a strongly connected network of agents -- i.e.,
  there is a path from each agent to every other agent.
  Each agent has a local variablen, which is initialized
  as a positive integer.
  
  Let GCD be the gcd (greatest commond divisor) of the
  initial values ofX.nof all agentsX.n.
In the code,successorsis the list of successor agents
in a network of agents, and the agent only sends messages to its
successors.def receive(message, sender):
   if n != message):
      n = gcd(n, message)
      for successor in successors:
         send(n, successor)QuestionWhat is an example of a loop variant for the distributed algorithm?AnswerLetfbe the tuple(N, M)whereNis the sum ofX.nfor all agentsX, andMis the total number of messages in
channels.Comparisons of tuples are made lexicographically.
For example (2, 1) > (1, 10), and (2, 1) < (2, 2).fis bounded below by (0, 0).An induction can be carried out on a tuple of integers.
Next we show that the execution of each event reducesf.When an agentXreceives a messagem,ifX.n\(\neq\)mthenX.nbecomesgcd(X.n, m), and so the new value ofX.nis less than its previous value, and so execution
  of the event decreasesNwhich decreasesf.ifX.n\(=\)mthen the event removes
  messagemfrom a channel and does not add messages to
  channels. So the event decreasesMwhich decreasesf.Termination Condition of a While LoopWhat can we say about the state at termination of the while loop in
the gcd example:x, y = X, Y
while x != y:
  # assertion: gcd(x, y) = gcd(X, Y)
  if x > y: x = x - y
  else: y = y - xThe while loop terminates whenx = y.
From the invariant, at termination:gcd(x, y) = gcd(x, x) = x = gcd(X, Y)Thereforexandyaregcd(X, Y)at termination of the while loop.Termination Condition of a Distributed AlgorithmWhat is the termination condition of the distributed gcd algorithm in
which thereceiveof each agent is as follows?def receive(message, sender):
   if n != message:
      n = gcd(n, message)
      for successor in successors:
         send(n, successor)At termination all channels are empty.
From the invariant, if channel (X, Y) is empty then X.n is a multiple
of Y.n.Therefore when all channels are empty, for every channel (X, Y) in the
system X.n is a multiple of Y.n.
Because the directed graph of agents and edges specified assuccessorsof agents is strongly
connected:for all agents X, Y in the system: X.n is a multiple of Y.nTherefore X.n = Y.n for all agents X, Y.
From the invariant, X.n is GCD.NextThe next webpage discussesdata flow in computations.Frequenty Asked QuestionsK. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

In this example, 0 is (obviously) a lower bound.
Next we show that the execution of any event in any state reduces the
value of the loop variant.When Y receives a balloon if nY is positive then nY
decreases which the loop variant, and if nY is 0 then
(X, Y) which decreases the loop variant.Similarly every event in which X receives a balloon also reduces the
loop variant. Thus the executions of all events reduce the loop variant.Therefore the algorithm terminates execution.QuestionWhat can you prove about the state of the system when the computation
terminates? 
We have shown that computations terminate and we given examples of
invariants.AnswerAt termination all channels are empty.
Let's prove that nX or nY is 0 at termination.From the invariants if:(X, Y) + (Y, X)\(\neq\) 2 then nX
or nY is 0.At termination,(X, Y) + (Y, X)\(=\) 0, and the result follows.More Examples of Invariants and Loop VariantsWe reason about the correctness of many distributed algorithms in
  in the same way that we reason about sequentialwhileloops, by using invariants and loop variants.Example: GCD using a While LoopAloop invariantis an assertion about the state of the program that holds before and
  after each iteration of the loop.
  You can look upmany examplesof loop invariants on the web.Example: InvariantA loop invariant in the following greatest common
  divisor (gcd) program is shown in the body of the loop as the
  assertiongcd(x, y) = gcd(X, Y).x, y = X, Y
while x != y:
  # assertion: gcd(x, y) = gcd(X, Y)
  if x > y: x = x - y
  else: y = y - xInvariant of a Distributed System for GCDAn invariant of a distributed system is a
  predicates that holds in all states reachable from initial states.
  An invariant of a distributed system is the loop invariant of the
  following while loop.
  An invariantInvis shown
  as an assertion in the body of the loop.while there exists a nonempty input channel in the system:
   # assertion: Inv
   select a nonempty channel (u, v) in the system
   let the head of channel (u, v) be msg
   v executes receive(msg, u)Example of a Distributed GCDWe are given a strongly connected network of agents -- i.e.,
  there is a path from each agent to every other agent.
  Each agent has a local variablen, which is initialized
  as a positive integer.
  
  Let GCD be the gcd (greatest commond divisor) of the
  initial values ofX.nof all agentsX.n.
  The following distributed algorithm terminates and at terminationX.n = GCDfor all agentsX.n.
In the code,successorsis the list of successor agents
in a network of agents, and the agent only sends messages to its
successors.def receive(message, sender):
   if n != message:
      n = gcd(n, message)
      for successor in successors:
         send(n, successor)An invariant of the distributed algorithm is as follows.
The gcd of:all messages in channels andX.nfor all agentsXis GCD.ProofThe proof that the assertion holds initially is trivial.
Prove that if the assertion holds before any event 
then it continues to hold after the event.Let the event be agentAreceiving a messagem.
After the event,mis no longer in the channel;n = gcd(n, m); and messages with the new value ofnare in the output channels fromA.
The proof that the assertion continues to hold in the post-event state
is straightforward.Example of an InvariantAnother invariant for the gcd example, given earlier, is as follows.
For all channels(X, Y)in the network:Channel(X, Y)is empty andX.nis a
  multiple ofY.n, orThe last message in channel(X, Y)ismwherem = X.n.ProofThe assertion holds initially because the second condition holds. 
Prove that if the assertion holds before any event
then it continues to hold after the event.When agentXexecutes a receive that changesX.nthe agent sends a messagemtoYwherem = X.n, and so the second condition
holds.When agentYreceivesmthe agent setsY.n = gcd(Y.n, m), and somis a multiple of
the new value ofY.n.
If channel(X, Y)remains nonempty then the second
condition holds.
If(X, Y)becomes empty then the first condition holds
becausem = X.nandmis a multiple ofY.n.Loop VariantQuestion about Loop Variant in the Sequential
GCDWhat is an example of a loop variant for the following loop?x, y = X, Y
while x != y:
  # assertion: gcd(x, y) = gcd(X, Y)
  if x > y: x = x - y
  else: y = y - xAnswerf(x, y) = x + yis an example of a loop variant for the
following reasons.fis a function of the state of the program.fhas integer values.fis bounded below. The bound is 2 becausexandyare bounded below because of the invariantsx\(\geq\) 1, andy\(\geq\) 1.
  The precise bounds don't matter.Each execution of the loop decreasesf.We can carry out an induction onfbecausefhas integer values and is bounded below.Question about Loop Variants in Distributed AlgorithmsLet's look at the example of the distributed algorithm given
earlier? The example is repeated below.We are given a strongly connected network of agents -- i.e.,
  there is a path from each agent to every other agent.
  Each agent has a local variablen, which is initialized
  as a positive integer.
  
  Let GCD be the gcd (greatest commond divisor) of the
  initial values ofX.nof all agentsX.n.
In the code,successorsis the list of successor agents
in a network of agents, and the agent only sends messages to its
successors.def receive(message, sender):
   if n != message):
      n = gcd(n, message)
      for successor in successors:
         send(n, successor)QuestionWhat is an example of a loop variant for the distributed algorithm?AnswerLetfbe the tuple(N, M)whereNis the sum ofX.nfor all agentsX, andMis the total number of messages in
channels.Comparisons of tuples are made lexicographically.
For example (2, 1) > (1, 10), and (2, 1) < (2, 2).fis bounded below by (0, 0).An induction can be carried out on a tuple of integers.
Next we show that the execution of each event reducesf.When an agentXreceives a messagem,ifX.n\(\neq\)mthenX.nbecomesgcd(X.n, m), and so the new value ofX.nis less than its previous value, and so execution
  of the event decreasesNwhich decreasesf.ifX.n\(=\)mthen the event removes
  messagemfrom a channel and does not add messages to
  channels. So the event decreasesMwhich decreasesf.Termination Condition of a While LoopWhat can we say about the state at termination of the while loop in
the gcd example:x, y = X, Y
while x != y:
  # assertion: gcd(x, y) = gcd(X, Y)
  if x > y: x = x - y
  else: y = y - xThe while loop terminates whenx = y.
From the invariant, at termination:gcd(x, y) = gcd(x, x) = x = gcd(X, Y)Thereforexandyaregcd(X, Y)at termination of the while loop.Termination Condition of a Distributed AlgorithmWhat is the termination condition of the distributed gcd algorithm in
which thereceiveof each agent is as follows?def receive(message, sender):
   if n != message:
      n = gcd(n, message)
      for successor in successors:
         send(n, successor)At termination all channels are empty.
From the invariant, if channel (X, Y) is empty then X.n is a multiple
of Y.n.Therefore when all channels are empty, for every channel (X, Y) in the
system X.n is a multiple of Y.n.
Because the directed graph of agents and edges specified assuccessorsof agents is strongly
connected:for all agents X, Y in the system: X.n is a multiple of Y.nTherefore X.n = Y.n for all agents X, Y.
From the invariant, X.n is GCD.NextThe next webpage discussesdata flow in computations.Frequenty Asked QuestionsK. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

When Y receives a balloon if nY is positive then nY
decreases which the loop variant, and if nY is 0 then
(X, Y) which decreases the loop variant.Similarly every event in which X receives a balloon also reduces the
loop variant. Thus the executions of all events reduce the loop variant.Therefore the algorithm terminates execution.QuestionWhat can you prove about the state of the system when the computation
terminates? 
We have shown that computations terminate and we given examples of
invariants.AnswerAt termination all channels are empty.
Let's prove that nX or nY is 0 at termination.From the invariants if:(X, Y) + (Y, X)\(\neq\) 2 then nX
or nY is 0.At termination,(X, Y) + (Y, X)\(=\) 0, and the result follows.More Examples of Invariants and Loop VariantsWe reason about the correctness of many distributed algorithms in
  in the same way that we reason about sequentialwhileloops, by using invariants and loop variants.Example: GCD using a While LoopAloop invariantis an assertion about the state of the program that holds before and
  after each iteration of the loop.
  You can look upmany examplesof loop invariants on the web.Example: InvariantA loop invariant in the following greatest common
  divisor (gcd) program is shown in the body of the loop as the
  assertiongcd(x, y) = gcd(X, Y).x, y = X, Y
while x != y:
  # assertion: gcd(x, y) = gcd(X, Y)
  if x > y: x = x - y
  else: y = y - xInvariant of a Distributed System for GCDAn invariant of a distributed system is a
  predicates that holds in all states reachable from initial states.
  An invariant of a distributed system is the loop invariant of the
  following while loop.
  An invariantInvis shown
  as an assertion in the body of the loop.while there exists a nonempty input channel in the system:
   # assertion: Inv
   select a nonempty channel (u, v) in the system
   let the head of channel (u, v) be msg
   v executes receive(msg, u)Example of a Distributed GCDWe are given a strongly connected network of agents -- i.e.,
  there is a path from each agent to every other agent.
  Each agent has a local variablen, which is initialized
  as a positive integer.
  
  Let GCD be the gcd (greatest commond divisor) of the
  initial values ofX.nof all agentsX.n.
  The following distributed algorithm terminates and at terminationX.n = GCDfor all agentsX.n.
In the code,successorsis the list of successor agents
in a network of agents, and the agent only sends messages to its
successors.def receive(message, sender):
   if n != message:
      n = gcd(n, message)
      for successor in successors:
         send(n, successor)An invariant of the distributed algorithm is as follows.
The gcd of:all messages in channels andX.nfor all agentsXis GCD.ProofThe proof that the assertion holds initially is trivial.
Prove that if the assertion holds before any event 
then it continues to hold after the event.Let the event be agentAreceiving a messagem.
After the event,mis no longer in the channel;n = gcd(n, m); and messages with the new value ofnare in the output channels fromA.
The proof that the assertion continues to hold in the post-event state
is straightforward.Example of an InvariantAnother invariant for the gcd example, given earlier, is as follows.
For all channels(X, Y)in the network:Channel(X, Y)is empty andX.nis a
  multiple ofY.n, orThe last message in channel(X, Y)ismwherem = X.n.ProofThe assertion holds initially because the second condition holds. 
Prove that if the assertion holds before any event
then it continues to hold after the event.When agentXexecutes a receive that changesX.nthe agent sends a messagemtoYwherem = X.n, and so the second condition
holds.When agentYreceivesmthe agent setsY.n = gcd(Y.n, m), and somis a multiple of
the new value ofY.n.
If channel(X, Y)remains nonempty then the second
condition holds.
If(X, Y)becomes empty then the first condition holds
becausem = X.nandmis a multiple ofY.n.Loop VariantQuestion about Loop Variant in the Sequential
GCDWhat is an example of a loop variant for the following loop?x, y = X, Y
while x != y:
  # assertion: gcd(x, y) = gcd(X, Y)
  if x > y: x = x - y
  else: y = y - xAnswerf(x, y) = x + yis an example of a loop variant for the
following reasons.fis a function of the state of the program.fhas integer values.fis bounded below. The bound is 2 becausexandyare bounded below because of the invariantsx\(\geq\) 1, andy\(\geq\) 1.
  The precise bounds don't matter.Each execution of the loop decreasesf.We can carry out an induction onfbecausefhas integer values and is bounded below.Question about Loop Variants in Distributed AlgorithmsLet's look at the example of the distributed algorithm given
earlier? The example is repeated below.We are given a strongly connected network of agents -- i.e.,
  there is a path from each agent to every other agent.
  Each agent has a local variablen, which is initialized
  as a positive integer.
  
  Let GCD be the gcd (greatest commond divisor) of the
  initial values ofX.nof all agentsX.n.
In the code,successorsis the list of successor agents
in a network of agents, and the agent only sends messages to its
successors.def receive(message, sender):
   if n != message):
      n = gcd(n, message)
      for successor in successors:
         send(n, successor)QuestionWhat is an example of a loop variant for the distributed algorithm?AnswerLetfbe the tuple(N, M)whereNis the sum ofX.nfor all agentsX, andMis the total number of messages in
channels.Comparisons of tuples are made lexicographically.
For example (2, 1) > (1, 10), and (2, 1) < (2, 2).fis bounded below by (0, 0).An induction can be carried out on a tuple of integers.
Next we show that the execution of each event reducesf.When an agentXreceives a messagem,ifX.n\(\neq\)mthenX.nbecomesgcd(X.n, m), and so the new value ofX.nis less than its previous value, and so execution
  of the event decreasesNwhich decreasesf.ifX.n\(=\)mthen the event removes
  messagemfrom a channel and does not add messages to
  channels. So the event decreasesMwhich decreasesf.Termination Condition of a While LoopWhat can we say about the state at termination of the while loop in
the gcd example:x, y = X, Y
while x != y:
  # assertion: gcd(x, y) = gcd(X, Y)
  if x > y: x = x - y
  else: y = y - xThe while loop terminates whenx = y.
From the invariant, at termination:gcd(x, y) = gcd(x, x) = x = gcd(X, Y)Thereforexandyaregcd(X, Y)at termination of the while loop.Termination Condition of a Distributed AlgorithmWhat is the termination condition of the distributed gcd algorithm in
which thereceiveof each agent is as follows?def receive(message, sender):
   if n != message:
      n = gcd(n, message)
      for successor in successors:
         send(n, successor)At termination all channels are empty.
From the invariant, if channel (X, Y) is empty then X.n is a multiple
of Y.n.Therefore when all channels are empty, for every channel (X, Y) in the
system X.n is a multiple of Y.n.
Because the directed graph of agents and edges specified assuccessorsof agents is strongly
connected:for all agents X, Y in the system: X.n is a multiple of Y.nTherefore X.n = Y.n for all agents X, Y.
From the invariant, X.n is GCD.NextThe next webpage discussesdata flow in computations.Frequenty Asked QuestionsK. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

Similarly every event in which X receives a balloon also reduces the
loop variant. Thus the executions of all events reduce the loop variant.Therefore the algorithm terminates execution.QuestionWhat can you prove about the state of the system when the computation
terminates? 
We have shown that computations terminate and we given examples of
invariants.AnswerAt termination all channels are empty.
Let's prove that nX or nY is 0 at termination.From the invariants if:(X, Y) + (Y, X)\(\neq\) 2 then nX
or nY is 0.At termination,(X, Y) + (Y, X)\(=\) 0, and the result follows.More Examples of Invariants and Loop VariantsWe reason about the correctness of many distributed algorithms in
  in the same way that we reason about sequentialwhileloops, by using invariants and loop variants.Example: GCD using a While LoopAloop invariantis an assertion about the state of the program that holds before and
  after each iteration of the loop.
  You can look upmany examplesof loop invariants on the web.Example: InvariantA loop invariant in the following greatest common
  divisor (gcd) program is shown in the body of the loop as the
  assertiongcd(x, y) = gcd(X, Y).x, y = X, Y
while x != y:
  # assertion: gcd(x, y) = gcd(X, Y)
  if x > y: x = x - y
  else: y = y - xInvariant of a Distributed System for GCDAn invariant of a distributed system is a
  predicates that holds in all states reachable from initial states.
  An invariant of a distributed system is the loop invariant of the
  following while loop.
  An invariantInvis shown
  as an assertion in the body of the loop.while there exists a nonempty input channel in the system:
   # assertion: Inv
   select a nonempty channel (u, v) in the system
   let the head of channel (u, v) be msg
   v executes receive(msg, u)Example of a Distributed GCDWe are given a strongly connected network of agents -- i.e.,
  there is a path from each agent to every other agent.
  Each agent has a local variablen, which is initialized
  as a positive integer.
  
  Let GCD be the gcd (greatest commond divisor) of the
  initial values ofX.nof all agentsX.n.
  The following distributed algorithm terminates and at terminationX.n = GCDfor all agentsX.n.
In the code,successorsis the list of successor agents
in a network of agents, and the agent only sends messages to its
successors.def receive(message, sender):
   if n != message:
      n = gcd(n, message)
      for successor in successors:
         send(n, successor)An invariant of the distributed algorithm is as follows.
The gcd of:all messages in channels andX.nfor all agentsXis GCD.ProofThe proof that the assertion holds initially is trivial.
Prove that if the assertion holds before any event 
then it continues to hold after the event.Let the event be agentAreceiving a messagem.
After the event,mis no longer in the channel;n = gcd(n, m); and messages with the new value ofnare in the output channels fromA.
The proof that the assertion continues to hold in the post-event state
is straightforward.Example of an InvariantAnother invariant for the gcd example, given earlier, is as follows.
For all channels(X, Y)in the network:Channel(X, Y)is empty andX.nis a
  multiple ofY.n, orThe last message in channel(X, Y)ismwherem = X.n.ProofThe assertion holds initially because the second condition holds. 
Prove that if the assertion holds before any event
then it continues to hold after the event.When agentXexecutes a receive that changesX.nthe agent sends a messagemtoYwherem = X.n, and so the second condition
holds.When agentYreceivesmthe agent setsY.n = gcd(Y.n, m), and somis a multiple of
the new value ofY.n.
If channel(X, Y)remains nonempty then the second
condition holds.
If(X, Y)becomes empty then the first condition holds
becausem = X.nandmis a multiple ofY.n.Loop VariantQuestion about Loop Variant in the Sequential
GCDWhat is an example of a loop variant for the following loop?x, y = X, Y
while x != y:
  # assertion: gcd(x, y) = gcd(X, Y)
  if x > y: x = x - y
  else: y = y - xAnswerf(x, y) = x + yis an example of a loop variant for the
following reasons.fis a function of the state of the program.fhas integer values.fis bounded below. The bound is 2 becausexandyare bounded below because of the invariantsx\(\geq\) 1, andy\(\geq\) 1.
  The precise bounds don't matter.Each execution of the loop decreasesf.We can carry out an induction onfbecausefhas integer values and is bounded below.Question about Loop Variants in Distributed AlgorithmsLet's look at the example of the distributed algorithm given
earlier? The example is repeated below.We are given a strongly connected network of agents -- i.e.,
  there is a path from each agent to every other agent.
  Each agent has a local variablen, which is initialized
  as a positive integer.
  
  Let GCD be the gcd (greatest commond divisor) of the
  initial values ofX.nof all agentsX.n.
In the code,successorsis the list of successor agents
in a network of agents, and the agent only sends messages to its
successors.def receive(message, sender):
   if n != message):
      n = gcd(n, message)
      for successor in successors:
         send(n, successor)QuestionWhat is an example of a loop variant for the distributed algorithm?AnswerLetfbe the tuple(N, M)whereNis the sum ofX.nfor all agentsX, andMis the total number of messages in
channels.Comparisons of tuples are made lexicographically.
For example (2, 1) > (1, 10), and (2, 1) < (2, 2).fis bounded below by (0, 0).An induction can be carried out on a tuple of integers.
Next we show that the execution of each event reducesf.When an agentXreceives a messagem,ifX.n\(\neq\)mthenX.nbecomesgcd(X.n, m), and so the new value ofX.nis less than its previous value, and so execution
  of the event decreasesNwhich decreasesf.ifX.n\(=\)mthen the event removes
  messagemfrom a channel and does not add messages to
  channels. So the event decreasesMwhich decreasesf.Termination Condition of a While LoopWhat can we say about the state at termination of the while loop in
the gcd example:x, y = X, Y
while x != y:
  # assertion: gcd(x, y) = gcd(X, Y)
  if x > y: x = x - y
  else: y = y - xThe while loop terminates whenx = y.
From the invariant, at termination:gcd(x, y) = gcd(x, x) = x = gcd(X, Y)Thereforexandyaregcd(X, Y)at termination of the while loop.Termination Condition of a Distributed AlgorithmWhat is the termination condition of the distributed gcd algorithm in
which thereceiveof each agent is as follows?def receive(message, sender):
   if n != message:
      n = gcd(n, message)
      for successor in successors:
         send(n, successor)At termination all channels are empty.
From the invariant, if channel (X, Y) is empty then X.n is a multiple
of Y.n.Therefore when all channels are empty, for every channel (X, Y) in the
system X.n is a multiple of Y.n.
Because the directed graph of agents and edges specified assuccessorsof agents is strongly
connected:for all agents X, Y in the system: X.n is a multiple of Y.nTherefore X.n = Y.n for all agents X, Y.
From the invariant, X.n is GCD.NextThe next webpage discussesdata flow in computations.Frequenty Asked QuestionsK. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

Therefore the algorithm terminates execution.QuestionWhat can you prove about the state of the system when the computation
terminates? 
We have shown that computations terminate and we given examples of
invariants.AnswerAt termination all channels are empty.
Let's prove that nX or nY is 0 at termination.From the invariants if:(X, Y) + (Y, X)\(\neq\) 2 then nX
or nY is 0.At termination,(X, Y) + (Y, X)\(=\) 0, and the result follows.More Examples of Invariants and Loop VariantsWe reason about the correctness of many distributed algorithms in
  in the same way that we reason about sequentialwhileloops, by using invariants and loop variants.Example: GCD using a While LoopAloop invariantis an assertion about the state of the program that holds before and
  after each iteration of the loop.
  You can look upmany examplesof loop invariants on the web.Example: InvariantA loop invariant in the following greatest common
  divisor (gcd) program is shown in the body of the loop as the
  assertiongcd(x, y) = gcd(X, Y).x, y = X, Y
while x != y:
  # assertion: gcd(x, y) = gcd(X, Y)
  if x > y: x = x - y
  else: y = y - xInvariant of a Distributed System for GCDAn invariant of a distributed system is a
  predicates that holds in all states reachable from initial states.
  An invariant of a distributed system is the loop invariant of the
  following while loop.
  An invariantInvis shown
  as an assertion in the body of the loop.while there exists a nonempty input channel in the system:
   # assertion: Inv
   select a nonempty channel (u, v) in the system
   let the head of channel (u, v) be msg
   v executes receive(msg, u)Example of a Distributed GCDWe are given a strongly connected network of agents -- i.e.,
  there is a path from each agent to every other agent.
  Each agent has a local variablen, which is initialized
  as a positive integer.
  
  Let GCD be the gcd (greatest commond divisor) of the
  initial values ofX.nof all agentsX.n.
  The following distributed algorithm terminates and at terminationX.n = GCDfor all agentsX.n.
In the code,successorsis the list of successor agents
in a network of agents, and the agent only sends messages to its
successors.def receive(message, sender):
   if n != message:
      n = gcd(n, message)
      for successor in successors:
         send(n, successor)An invariant of the distributed algorithm is as follows.
The gcd of:all messages in channels andX.nfor all agentsXis GCD.ProofThe proof that the assertion holds initially is trivial.
Prove that if the assertion holds before any event 
then it continues to hold after the event.Let the event be agentAreceiving a messagem.
After the event,mis no longer in the channel;n = gcd(n, m); and messages with the new value ofnare in the output channels fromA.
The proof that the assertion continues to hold in the post-event state
is straightforward.Example of an InvariantAnother invariant for the gcd example, given earlier, is as follows.
For all channels(X, Y)in the network:Channel(X, Y)is empty andX.nis a
  multiple ofY.n, orThe last message in channel(X, Y)ismwherem = X.n.ProofThe assertion holds initially because the second condition holds. 
Prove that if the assertion holds before any event
then it continues to hold after the event.When agentXexecutes a receive that changesX.nthe agent sends a messagemtoYwherem = X.n, and so the second condition
holds.When agentYreceivesmthe agent setsY.n = gcd(Y.n, m), and somis a multiple of
the new value ofY.n.
If channel(X, Y)remains nonempty then the second
condition holds.
If(X, Y)becomes empty then the first condition holds
becausem = X.nandmis a multiple ofY.n.Loop VariantQuestion about Loop Variant in the Sequential
GCDWhat is an example of a loop variant for the following loop?x, y = X, Y
while x != y:
  # assertion: gcd(x, y) = gcd(X, Y)
  if x > y: x = x - y
  else: y = y - xAnswerf(x, y) = x + yis an example of a loop variant for the
following reasons.fis a function of the state of the program.fhas integer values.fis bounded below. The bound is 2 becausexandyare bounded below because of the invariantsx\(\geq\) 1, andy\(\geq\) 1.
  The precise bounds don't matter.Each execution of the loop decreasesf.We can carry out an induction onfbecausefhas integer values and is bounded below.Question about Loop Variants in Distributed AlgorithmsLet's look at the example of the distributed algorithm given
earlier? The example is repeated below.We are given a strongly connected network of agents -- i.e.,
  there is a path from each agent to every other agent.
  Each agent has a local variablen, which is initialized
  as a positive integer.
  
  Let GCD be the gcd (greatest commond divisor) of the
  initial values ofX.nof all agentsX.n.
In the code,successorsis the list of successor agents
in a network of agents, and the agent only sends messages to its
successors.def receive(message, sender):
   if n != message):
      n = gcd(n, message)
      for successor in successors:
         send(n, successor)QuestionWhat is an example of a loop variant for the distributed algorithm?AnswerLetfbe the tuple(N, M)whereNis the sum ofX.nfor all agentsX, andMis the total number of messages in
channels.Comparisons of tuples are made lexicographically.
For example (2, 1) > (1, 10), and (2, 1) < (2, 2).fis bounded below by (0, 0).An induction can be carried out on a tuple of integers.
Next we show that the execution of each event reducesf.When an agentXreceives a messagem,ifX.n\(\neq\)mthenX.nbecomesgcd(X.n, m), and so the new value ofX.nis less than its previous value, and so execution
  of the event decreasesNwhich decreasesf.ifX.n\(=\)mthen the event removes
  messagemfrom a channel and does not add messages to
  channels. So the event decreasesMwhich decreasesf.Termination Condition of a While LoopWhat can we say about the state at termination of the while loop in
the gcd example:x, y = X, Y
while x != y:
  # assertion: gcd(x, y) = gcd(X, Y)
  if x > y: x = x - y
  else: y = y - xThe while loop terminates whenx = y.
From the invariant, at termination:gcd(x, y) = gcd(x, x) = x = gcd(X, Y)Thereforexandyaregcd(X, Y)at termination of the while loop.Termination Condition of a Distributed AlgorithmWhat is the termination condition of the distributed gcd algorithm in
which thereceiveof each agent is as follows?def receive(message, sender):
   if n != message:
      n = gcd(n, message)
      for successor in successors:
         send(n, successor)At termination all channels are empty.
From the invariant, if channel (X, Y) is empty then X.n is a multiple
of Y.n.Therefore when all channels are empty, for every channel (X, Y) in the
system X.n is a multiple of Y.n.
Because the directed graph of agents and edges specified assuccessorsof agents is strongly
connected:for all agents X, Y in the system: X.n is a multiple of Y.nTherefore X.n = Y.n for all agents X, Y.
From the invariant, X.n is GCD.NextThe next webpage discussesdata flow in computations.Frequenty Asked QuestionsK. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

At termination all channels are empty.
Let's prove that nX or nY is 0 at termination.From the invariants if:(X, Y) + (Y, X)\(\neq\) 2 then nX
or nY is 0.At termination,(X, Y) + (Y, X)\(=\) 0, and the result follows.More Examples of Invariants and Loop VariantsWe reason about the correctness of many distributed algorithms in
  in the same way that we reason about sequentialwhileloops, by using invariants and loop variants.Example: GCD using a While LoopAloop invariantis an assertion about the state of the program that holds before and
  after each iteration of the loop.
  You can look upmany examplesof loop invariants on the web.Example: InvariantA loop invariant in the following greatest common
  divisor (gcd) program is shown in the body of the loop as the
  assertiongcd(x, y) = gcd(X, Y).x, y = X, Y
while x != y:
  # assertion: gcd(x, y) = gcd(X, Y)
  if x > y: x = x - y
  else: y = y - xInvariant of a Distributed System for GCDAn invariant of a distributed system is a
  predicates that holds in all states reachable from initial states.
  An invariant of a distributed system is the loop invariant of the
  following while loop.
  An invariantInvis shown
  as an assertion in the body of the loop.while there exists a nonempty input channel in the system:
   # assertion: Inv
   select a nonempty channel (u, v) in the system
   let the head of channel (u, v) be msg
   v executes receive(msg, u)Example of a Distributed GCDWe are given a strongly connected network of agents -- i.e.,
  there is a path from each agent to every other agent.
  Each agent has a local variablen, which is initialized
  as a positive integer.
  
  Let GCD be the gcd (greatest commond divisor) of the
  initial values ofX.nof all agentsX.n.
  The following distributed algorithm terminates and at terminationX.n = GCDfor all agentsX.n.
In the code,successorsis the list of successor agents
in a network of agents, and the agent only sends messages to its
successors.def receive(message, sender):
   if n != message:
      n = gcd(n, message)
      for successor in successors:
         send(n, successor)An invariant of the distributed algorithm is as follows.
The gcd of:all messages in channels andX.nfor all agentsXis GCD.ProofThe proof that the assertion holds initially is trivial.
Prove that if the assertion holds before any event 
then it continues to hold after the event.Let the event be agentAreceiving a messagem.
After the event,mis no longer in the channel;n = gcd(n, m); and messages with the new value ofnare in the output channels fromA.
The proof that the assertion continues to hold in the post-event state
is straightforward.Example of an InvariantAnother invariant for the gcd example, given earlier, is as follows.
For all channels(X, Y)in the network:Channel(X, Y)is empty andX.nis a
  multiple ofY.n, orThe last message in channel(X, Y)ismwherem = X.n.ProofThe assertion holds initially because the second condition holds. 
Prove that if the assertion holds before any event
then it continues to hold after the event.When agentXexecutes a receive that changesX.nthe agent sends a messagemtoYwherem = X.n, and so the second condition
holds.When agentYreceivesmthe agent setsY.n = gcd(Y.n, m), and somis a multiple of
the new value ofY.n.
If channel(X, Y)remains nonempty then the second
condition holds.
If(X, Y)becomes empty then the first condition holds
becausem = X.nandmis a multiple ofY.n.Loop VariantQuestion about Loop Variant in the Sequential
GCDWhat is an example of a loop variant for the following loop?x, y = X, Y
while x != y:
  # assertion: gcd(x, y) = gcd(X, Y)
  if x > y: x = x - y
  else: y = y - xAnswerf(x, y) = x + yis an example of a loop variant for the
following reasons.fis a function of the state of the program.fhas integer values.fis bounded below. The bound is 2 becausexandyare bounded below because of the invariantsx\(\geq\) 1, andy\(\geq\) 1.
  The precise bounds don't matter.Each execution of the loop decreasesf.We can carry out an induction onfbecausefhas integer values and is bounded below.Question about Loop Variants in Distributed AlgorithmsLet's look at the example of the distributed algorithm given
earlier? The example is repeated below.We are given a strongly connected network of agents -- i.e.,
  there is a path from each agent to every other agent.
  Each agent has a local variablen, which is initialized
  as a positive integer.
  
  Let GCD be the gcd (greatest commond divisor) of the
  initial values ofX.nof all agentsX.n.
In the code,successorsis the list of successor agents
in a network of agents, and the agent only sends messages to its
successors.def receive(message, sender):
   if n != message):
      n = gcd(n, message)
      for successor in successors:
         send(n, successor)QuestionWhat is an example of a loop variant for the distributed algorithm?AnswerLetfbe the tuple(N, M)whereNis the sum ofX.nfor all agentsX, andMis the total number of messages in
channels.Comparisons of tuples are made lexicographically.
For example (2, 1) > (1, 10), and (2, 1) < (2, 2).fis bounded below by (0, 0).An induction can be carried out on a tuple of integers.
Next we show that the execution of each event reducesf.When an agentXreceives a messagem,ifX.n\(\neq\)mthenX.nbecomesgcd(X.n, m), and so the new value ofX.nis less than its previous value, and so execution
  of the event decreasesNwhich decreasesf.ifX.n\(=\)mthen the event removes
  messagemfrom a channel and does not add messages to
  channels. So the event decreasesMwhich decreasesf.Termination Condition of a While LoopWhat can we say about the state at termination of the while loop in
the gcd example:x, y = X, Y
while x != y:
  # assertion: gcd(x, y) = gcd(X, Y)
  if x > y: x = x - y
  else: y = y - xThe while loop terminates whenx = y.
From the invariant, at termination:gcd(x, y) = gcd(x, x) = x = gcd(X, Y)Thereforexandyaregcd(X, Y)at termination of the while loop.Termination Condition of a Distributed AlgorithmWhat is the termination condition of the distributed gcd algorithm in
which thereceiveof each agent is as follows?def receive(message, sender):
   if n != message:
      n = gcd(n, message)
      for successor in successors:
         send(n, successor)At termination all channels are empty.
From the invariant, if channel (X, Y) is empty then X.n is a multiple
of Y.n.Therefore when all channels are empty, for every channel (X, Y) in the
system X.n is a multiple of Y.n.
Because the directed graph of agents and edges specified assuccessorsof agents is strongly
connected:for all agents X, Y in the system: X.n is a multiple of Y.nTherefore X.n = Y.n for all agents X, Y.
From the invariant, X.n is GCD.NextThe next webpage discussesdata flow in computations.Frequenty Asked QuestionsK. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

From the invariants if:(X, Y) + (Y, X)\(\neq\) 2 then nX
or nY is 0.At termination,(X, Y) + (Y, X)\(=\) 0, and the result follows.More Examples of Invariants and Loop VariantsWe reason about the correctness of many distributed algorithms in
  in the same way that we reason about sequentialwhileloops, by using invariants and loop variants.Example: GCD using a While LoopAloop invariantis an assertion about the state of the program that holds before and
  after each iteration of the loop.
  You can look upmany examplesof loop invariants on the web.Example: InvariantA loop invariant in the following greatest common
  divisor (gcd) program is shown in the body of the loop as the
  assertiongcd(x, y) = gcd(X, Y).x, y = X, Y
while x != y:
  # assertion: gcd(x, y) = gcd(X, Y)
  if x > y: x = x - y
  else: y = y - xInvariant of a Distributed System for GCDAn invariant of a distributed system is a
  predicates that holds in all states reachable from initial states.
  An invariant of a distributed system is the loop invariant of the
  following while loop.
  An invariantInvis shown
  as an assertion in the body of the loop.while there exists a nonempty input channel in the system:
   # assertion: Inv
   select a nonempty channel (u, v) in the system
   let the head of channel (u, v) be msg
   v executes receive(msg, u)Example of a Distributed GCDWe are given a strongly connected network of agents -- i.e.,
  there is a path from each agent to every other agent.
  Each agent has a local variablen, which is initialized
  as a positive integer.
  
  Let GCD be the gcd (greatest commond divisor) of the
  initial values ofX.nof all agentsX.n.
  The following distributed algorithm terminates and at terminationX.n = GCDfor all agentsX.n.
In the code,successorsis the list of successor agents
in a network of agents, and the agent only sends messages to its
successors.def receive(message, sender):
   if n != message:
      n = gcd(n, message)
      for successor in successors:
         send(n, successor)An invariant of the distributed algorithm is as follows.
The gcd of:all messages in channels andX.nfor all agentsXis GCD.ProofThe proof that the assertion holds initially is trivial.
Prove that if the assertion holds before any event 
then it continues to hold after the event.Let the event be agentAreceiving a messagem.
After the event,mis no longer in the channel;n = gcd(n, m); and messages with the new value ofnare in the output channels fromA.
The proof that the assertion continues to hold in the post-event state
is straightforward.Example of an InvariantAnother invariant for the gcd example, given earlier, is as follows.
For all channels(X, Y)in the network:Channel(X, Y)is empty andX.nis a
  multiple ofY.n, orThe last message in channel(X, Y)ismwherem = X.n.ProofThe assertion holds initially because the second condition holds. 
Prove that if the assertion holds before any event
then it continues to hold after the event.When agentXexecutes a receive that changesX.nthe agent sends a messagemtoYwherem = X.n, and so the second condition
holds.When agentYreceivesmthe agent setsY.n = gcd(Y.n, m), and somis a multiple of
the new value ofY.n.
If channel(X, Y)remains nonempty then the second
condition holds.
If(X, Y)becomes empty then the first condition holds
becausem = X.nandmis a multiple ofY.n.Loop VariantQuestion about Loop Variant in the Sequential
GCDWhat is an example of a loop variant for the following loop?x, y = X, Y
while x != y:
  # assertion: gcd(x, y) = gcd(X, Y)
  if x > y: x = x - y
  else: y = y - xAnswerf(x, y) = x + yis an example of a loop variant for the
following reasons.fis a function of the state of the program.fhas integer values.fis bounded below. The bound is 2 becausexandyare bounded below because of the invariantsx\(\geq\) 1, andy\(\geq\) 1.
  The precise bounds don't matter.Each execution of the loop decreasesf.We can carry out an induction onfbecausefhas integer values and is bounded below.Question about Loop Variants in Distributed AlgorithmsLet's look at the example of the distributed algorithm given
earlier? The example is repeated below.We are given a strongly connected network of agents -- i.e.,
  there is a path from each agent to every other agent.
  Each agent has a local variablen, which is initialized
  as a positive integer.
  
  Let GCD be the gcd (greatest commond divisor) of the
  initial values ofX.nof all agentsX.n.
In the code,successorsis the list of successor agents
in a network of agents, and the agent only sends messages to its
successors.def receive(message, sender):
   if n != message):
      n = gcd(n, message)
      for successor in successors:
         send(n, successor)QuestionWhat is an example of a loop variant for the distributed algorithm?AnswerLetfbe the tuple(N, M)whereNis the sum ofX.nfor all agentsX, andMis the total number of messages in
channels.Comparisons of tuples are made lexicographically.
For example (2, 1) > (1, 10), and (2, 1) < (2, 2).fis bounded below by (0, 0).An induction can be carried out on a tuple of integers.
Next we show that the execution of each event reducesf.When an agentXreceives a messagem,ifX.n\(\neq\)mthenX.nbecomesgcd(X.n, m), and so the new value ofX.nis less than its previous value, and so execution
  of the event decreasesNwhich decreasesf.ifX.n\(=\)mthen the event removes
  messagemfrom a channel and does not add messages to
  channels. So the event decreasesMwhich decreasesf.Termination Condition of a While LoopWhat can we say about the state at termination of the while loop in
the gcd example:x, y = X, Y
while x != y:
  # assertion: gcd(x, y) = gcd(X, Y)
  if x > y: x = x - y
  else: y = y - xThe while loop terminates whenx = y.
From the invariant, at termination:gcd(x, y) = gcd(x, x) = x = gcd(X, Y)Thereforexandyaregcd(X, Y)at termination of the while loop.Termination Condition of a Distributed AlgorithmWhat is the termination condition of the distributed gcd algorithm in
which thereceiveof each agent is as follows?def receive(message, sender):
   if n != message:
      n = gcd(n, message)
      for successor in successors:
         send(n, successor)At termination all channels are empty.
From the invariant, if channel (X, Y) is empty then X.n is a multiple
of Y.n.Therefore when all channels are empty, for every channel (X, Y) in the
system X.n is a multiple of Y.n.
Because the directed graph of agents and edges specified assuccessorsof agents is strongly
connected:for all agents X, Y in the system: X.n is a multiple of Y.nTherefore X.n = Y.n for all agents X, Y.
From the invariant, X.n is GCD.NextThe next webpage discussesdata flow in computations.Frequenty Asked QuestionsK. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

At termination,(X, Y) + (Y, X)\(=\) 0, and the result follows.More Examples of Invariants and Loop VariantsWe reason about the correctness of many distributed algorithms in
  in the same way that we reason about sequentialwhileloops, by using invariants and loop variants.Example: GCD using a While LoopAloop invariantis an assertion about the state of the program that holds before and
  after each iteration of the loop.
  You can look upmany examplesof loop invariants on the web.Example: InvariantA loop invariant in the following greatest common
  divisor (gcd) program is shown in the body of the loop as the
  assertiongcd(x, y) = gcd(X, Y).x, y = X, Y
while x != y:
  # assertion: gcd(x, y) = gcd(X, Y)
  if x > y: x = x - y
  else: y = y - xInvariant of a Distributed System for GCDAn invariant of a distributed system is a
  predicates that holds in all states reachable from initial states.
  An invariant of a distributed system is the loop invariant of the
  following while loop.
  An invariantInvis shown
  as an assertion in the body of the loop.while there exists a nonempty input channel in the system:
   # assertion: Inv
   select a nonempty channel (u, v) in the system
   let the head of channel (u, v) be msg
   v executes receive(msg, u)Example of a Distributed GCDWe are given a strongly connected network of agents -- i.e.,
  there is a path from each agent to every other agent.
  Each agent has a local variablen, which is initialized
  as a positive integer.
  
  Let GCD be the gcd (greatest commond divisor) of the
  initial values ofX.nof all agentsX.n.
  The following distributed algorithm terminates and at terminationX.n = GCDfor all agentsX.n.
In the code,successorsis the list of successor agents
in a network of agents, and the agent only sends messages to its
successors.def receive(message, sender):
   if n != message:
      n = gcd(n, message)
      for successor in successors:
         send(n, successor)An invariant of the distributed algorithm is as follows.
The gcd of:all messages in channels andX.nfor all agentsXis GCD.ProofThe proof that the assertion holds initially is trivial.
Prove that if the assertion holds before any event 
then it continues to hold after the event.Let the event be agentAreceiving a messagem.
After the event,mis no longer in the channel;n = gcd(n, m); and messages with the new value ofnare in the output channels fromA.
The proof that the assertion continues to hold in the post-event state
is straightforward.Example of an InvariantAnother invariant for the gcd example, given earlier, is as follows.
For all channels(X, Y)in the network:Channel(X, Y)is empty andX.nis a
  multiple ofY.n, orThe last message in channel(X, Y)ismwherem = X.n.ProofThe assertion holds initially because the second condition holds. 
Prove that if the assertion holds before any event
then it continues to hold after the event.When agentXexecutes a receive that changesX.nthe agent sends a messagemtoYwherem = X.n, and so the second condition
holds.When agentYreceivesmthe agent setsY.n = gcd(Y.n, m), and somis a multiple of
the new value ofY.n.
If channel(X, Y)remains nonempty then the second
condition holds.
If(X, Y)becomes empty then the first condition holds
becausem = X.nandmis a multiple ofY.n.Loop VariantQuestion about Loop Variant in the Sequential
GCDWhat is an example of a loop variant for the following loop?x, y = X, Y
while x != y:
  # assertion: gcd(x, y) = gcd(X, Y)
  if x > y: x = x - y
  else: y = y - xAnswerf(x, y) = x + yis an example of a loop variant for the
following reasons.fis a function of the state of the program.fhas integer values.fis bounded below. The bound is 2 becausexandyare bounded below because of the invariantsx\(\geq\) 1, andy\(\geq\) 1.
  The precise bounds don't matter.Each execution of the loop decreasesf.We can carry out an induction onfbecausefhas integer values and is bounded below.Question about Loop Variants in Distributed AlgorithmsLet's look at the example of the distributed algorithm given
earlier? The example is repeated below.We are given a strongly connected network of agents -- i.e.,
  there is a path from each agent to every other agent.
  Each agent has a local variablen, which is initialized
  as a positive integer.
  
  Let GCD be the gcd (greatest commond divisor) of the
  initial values ofX.nof all agentsX.n.
In the code,successorsis the list of successor agents
in a network of agents, and the agent only sends messages to its
successors.def receive(message, sender):
   if n != message):
      n = gcd(n, message)
      for successor in successors:
         send(n, successor)QuestionWhat is an example of a loop variant for the distributed algorithm?AnswerLetfbe the tuple(N, M)whereNis the sum ofX.nfor all agentsX, andMis the total number of messages in
channels.Comparisons of tuples are made lexicographically.
For example (2, 1) > (1, 10), and (2, 1) < (2, 2).fis bounded below by (0, 0).An induction can be carried out on a tuple of integers.
Next we show that the execution of each event reducesf.When an agentXreceives a messagem,ifX.n\(\neq\)mthenX.nbecomesgcd(X.n, m), and so the new value ofX.nis less than its previous value, and so execution
  of the event decreasesNwhich decreasesf.ifX.n\(=\)mthen the event removes
  messagemfrom a channel and does not add messages to
  channels. So the event decreasesMwhich decreasesf.Termination Condition of a While LoopWhat can we say about the state at termination of the while loop in
the gcd example:x, y = X, Y
while x != y:
  # assertion: gcd(x, y) = gcd(X, Y)
  if x > y: x = x - y
  else: y = y - xThe while loop terminates whenx = y.
From the invariant, at termination:gcd(x, y) = gcd(x, x) = x = gcd(X, Y)Thereforexandyaregcd(X, Y)at termination of the while loop.Termination Condition of a Distributed AlgorithmWhat is the termination condition of the distributed gcd algorithm in
which thereceiveof each agent is as follows?def receive(message, sender):
   if n != message:
      n = gcd(n, message)
      for successor in successors:
         send(n, successor)At termination all channels are empty.
From the invariant, if channel (X, Y) is empty then X.n is a multiple
of Y.n.Therefore when all channels are empty, for every channel (X, Y) in the
system X.n is a multiple of Y.n.
Because the directed graph of agents and edges specified assuccessorsof agents is strongly
connected:for all agents X, Y in the system: X.n is a multiple of Y.nTherefore X.n = Y.n for all agents X, Y.
From the invariant, X.n is GCD.NextThe next webpage discussesdata flow in computations.Frequenty Asked QuestionsK. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

Proof

Let the event be agentAreceiving a messagem.
After the event,mis no longer in the channel;n = gcd(n, m); and messages with the new value ofnare in the output channels fromA.
The proof that the assertion continues to hold in the post-event state
is straightforward.Example of an InvariantAnother invariant for the gcd example, given earlier, is as follows.
For all channels(X, Y)in the network:Channel(X, Y)is empty andX.nis a
  multiple ofY.n, orThe last message in channel(X, Y)ismwherem = X.n.ProofThe assertion holds initially because the second condition holds. 
Prove that if the assertion holds before any event
then it continues to hold after the event.When agentXexecutes a receive that changesX.nthe agent sends a messagemtoYwherem = X.n, and so the second condition
holds.When agentYreceivesmthe agent setsY.n = gcd(Y.n, m), and somis a multiple of
the new value ofY.n.
If channel(X, Y)remains nonempty then the second
condition holds.
If(X, Y)becomes empty then the first condition holds
becausem = X.nandmis a multiple ofY.n.Loop VariantQuestion about Loop Variant in the Sequential
GCDWhat is an example of a loop variant for the following loop?x, y = X, Y
while x != y:
  # assertion: gcd(x, y) = gcd(X, Y)
  if x > y: x = x - y
  else: y = y - xAnswerf(x, y) = x + yis an example of a loop variant for the
following reasons.fis a function of the state of the program.fhas integer values.fis bounded below. The bound is 2 becausexandyare bounded below because of the invariantsx\(\geq\) 1, andy\(\geq\) 1.
  The precise bounds don't matter.Each execution of the loop decreasesf.We can carry out an induction onfbecausefhas integer values and is bounded below.Question about Loop Variants in Distributed AlgorithmsLet's look at the example of the distributed algorithm given
earlier? The example is repeated below.We are given a strongly connected network of agents -- i.e.,
  there is a path from each agent to every other agent.
  Each agent has a local variablen, which is initialized
  as a positive integer.
  
  Let GCD be the gcd (greatest commond divisor) of the
  initial values ofX.nof all agentsX.n.
In the code,successorsis the list of successor agents
in a network of agents, and the agent only sends messages to its
successors.def receive(message, sender):
   if n != message):
      n = gcd(n, message)
      for successor in successors:
         send(n, successor)QuestionWhat is an example of a loop variant for the distributed algorithm?AnswerLetfbe the tuple(N, M)whereNis the sum ofX.nfor all agentsX, andMis the total number of messages in
channels.Comparisons of tuples are made lexicographically.
For example (2, 1) > (1, 10), and (2, 1) < (2, 2).fis bounded below by (0, 0).An induction can be carried out on a tuple of integers.
Next we show that the execution of each event reducesf.When an agentXreceives a messagem,ifX.n\(\neq\)mthenX.nbecomesgcd(X.n, m), and so the new value ofX.nis less than its previous value, and so execution
  of the event decreasesNwhich decreasesf.ifX.n\(=\)mthen the event removes
  messagemfrom a channel and does not add messages to
  channels. So the event decreasesMwhich decreasesf.Termination Condition of a While LoopWhat can we say about the state at termination of the while loop in
the gcd example:x, y = X, Y
while x != y:
  # assertion: gcd(x, y) = gcd(X, Y)
  if x > y: x = x - y
  else: y = y - xThe while loop terminates whenx = y.
From the invariant, at termination:gcd(x, y) = gcd(x, x) = x = gcd(X, Y)Thereforexandyaregcd(X, Y)at termination of the while loop.Termination Condition of a Distributed AlgorithmWhat is the termination condition of the distributed gcd algorithm in
which thereceiveof each agent is as follows?def receive(message, sender):
   if n != message:
      n = gcd(n, message)
      for successor in successors:
         send(n, successor)At termination all channels are empty.
From the invariant, if channel (X, Y) is empty then X.n is a multiple
of Y.n.Therefore when all channels are empty, for every channel (X, Y) in the
system X.n is a multiple of Y.n.
Because the directed graph of agents and edges specified assuccessorsof agents is strongly
connected:for all agents X, Y in the system: X.n is a multiple of Y.nTherefore X.n = Y.n for all agents X, Y.
From the invariant, X.n is GCD.NextThe next webpage discussesdata flow in computations.Frequenty Asked QuestionsK. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

Proof

When agentXexecutes a receive that changesX.nthe agent sends a messagemtoYwherem = X.n, and so the second condition
holds.When agentYreceivesmthe agent setsY.n = gcd(Y.n, m), and somis a multiple of
the new value ofY.n.
If channel(X, Y)remains nonempty then the second
condition holds.
If(X, Y)becomes empty then the first condition holds
becausem = X.nandmis a multiple ofY.n.Loop VariantQuestion about Loop Variant in the Sequential
GCDWhat is an example of a loop variant for the following loop?x, y = X, Y
while x != y:
  # assertion: gcd(x, y) = gcd(X, Y)
  if x > y: x = x - y
  else: y = y - xAnswerf(x, y) = x + yis an example of a loop variant for the
following reasons.fis a function of the state of the program.fhas integer values.fis bounded below. The bound is 2 becausexandyare bounded below because of the invariantsx\(\geq\) 1, andy\(\geq\) 1.
  The precise bounds don't matter.Each execution of the loop decreasesf.We can carry out an induction onfbecausefhas integer values and is bounded below.Question about Loop Variants in Distributed AlgorithmsLet's look at the example of the distributed algorithm given
earlier? The example is repeated below.We are given a strongly connected network of agents -- i.e.,
  there is a path from each agent to every other agent.
  Each agent has a local variablen, which is initialized
  as a positive integer.
  
  Let GCD be the gcd (greatest commond divisor) of the
  initial values ofX.nof all agentsX.n.
In the code,successorsis the list of successor agents
in a network of agents, and the agent only sends messages to its
successors.def receive(message, sender):
   if n != message):
      n = gcd(n, message)
      for successor in successors:
         send(n, successor)QuestionWhat is an example of a loop variant for the distributed algorithm?AnswerLetfbe the tuple(N, M)whereNis the sum ofX.nfor all agentsX, andMis the total number of messages in
channels.Comparisons of tuples are made lexicographically.
For example (2, 1) > (1, 10), and (2, 1) < (2, 2).fis bounded below by (0, 0).An induction can be carried out on a tuple of integers.
Next we show that the execution of each event reducesf.When an agentXreceives a messagem,ifX.n\(\neq\)mthenX.nbecomesgcd(X.n, m), and so the new value ofX.nis less than its previous value, and so execution
  of the event decreasesNwhich decreasesf.ifX.n\(=\)mthen the event removes
  messagemfrom a channel and does not add messages to
  channels. So the event decreasesMwhich decreasesf.Termination Condition of a While LoopWhat can we say about the state at termination of the while loop in
the gcd example:x, y = X, Y
while x != y:
  # assertion: gcd(x, y) = gcd(X, Y)
  if x > y: x = x - y
  else: y = y - xThe while loop terminates whenx = y.
From the invariant, at termination:gcd(x, y) = gcd(x, x) = x = gcd(X, Y)Thereforexandyaregcd(X, Y)at termination of the while loop.Termination Condition of a Distributed AlgorithmWhat is the termination condition of the distributed gcd algorithm in
which thereceiveof each agent is as follows?def receive(message, sender):
   if n != message:
      n = gcd(n, message)
      for successor in successors:
         send(n, successor)At termination all channels are empty.
From the invariant, if channel (X, Y) is empty then X.n is a multiple
of Y.n.Therefore when all channels are empty, for every channel (X, Y) in the
system X.n is a multiple of Y.n.
Because the directed graph of agents and edges specified assuccessorsof agents is strongly
connected:for all agents X, Y in the system: X.n is a multiple of Y.nTherefore X.n = Y.n for all agents X, Y.
From the invariant, X.n is GCD.NextThe next webpage discussesdata flow in computations.Frequenty Asked QuestionsK. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

When agentYreceivesmthe agent setsY.n = gcd(Y.n, m), and somis a multiple of
the new value ofY.n.
If channel(X, Y)remains nonempty then the second
condition holds.
If(X, Y)becomes empty then the first condition holds
becausem = X.nandmis a multiple ofY.n.Loop VariantQuestion about Loop Variant in the Sequential
GCDWhat is an example of a loop variant for the following loop?x, y = X, Y
while x != y:
  # assertion: gcd(x, y) = gcd(X, Y)
  if x > y: x = x - y
  else: y = y - xAnswerf(x, y) = x + yis an example of a loop variant for the
following reasons.fis a function of the state of the program.fhas integer values.fis bounded below. The bound is 2 becausexandyare bounded below because of the invariantsx\(\geq\) 1, andy\(\geq\) 1.
  The precise bounds don't matter.Each execution of the loop decreasesf.We can carry out an induction onfbecausefhas integer values and is bounded below.Question about Loop Variants in Distributed AlgorithmsLet's look at the example of the distributed algorithm given
earlier? The example is repeated below.We are given a strongly connected network of agents -- i.e.,
  there is a path from each agent to every other agent.
  Each agent has a local variablen, which is initialized
  as a positive integer.
  
  Let GCD be the gcd (greatest commond divisor) of the
  initial values ofX.nof all agentsX.n.
In the code,successorsis the list of successor agents
in a network of agents, and the agent only sends messages to its
successors.def receive(message, sender):
   if n != message):
      n = gcd(n, message)
      for successor in successors:
         send(n, successor)QuestionWhat is an example of a loop variant for the distributed algorithm?AnswerLetfbe the tuple(N, M)whereNis the sum ofX.nfor all agentsX, andMis the total number of messages in
channels.Comparisons of tuples are made lexicographically.
For example (2, 1) > (1, 10), and (2, 1) < (2, 2).fis bounded below by (0, 0).An induction can be carried out on a tuple of integers.
Next we show that the execution of each event reducesf.When an agentXreceives a messagem,ifX.n\(\neq\)mthenX.nbecomesgcd(X.n, m), and so the new value ofX.nis less than its previous value, and so execution
  of the event decreasesNwhich decreasesf.ifX.n\(=\)mthen the event removes
  messagemfrom a channel and does not add messages to
  channels. So the event decreasesMwhich decreasesf.Termination Condition of a While LoopWhat can we say about the state at termination of the while loop in
the gcd example:x, y = X, Y
while x != y:
  # assertion: gcd(x, y) = gcd(X, Y)
  if x > y: x = x - y
  else: y = y - xThe while loop terminates whenx = y.
From the invariant, at termination:gcd(x, y) = gcd(x, x) = x = gcd(X, Y)Thereforexandyaregcd(X, Y)at termination of the while loop.Termination Condition of a Distributed AlgorithmWhat is the termination condition of the distributed gcd algorithm in
which thereceiveof each agent is as follows?def receive(message, sender):
   if n != message:
      n = gcd(n, message)
      for successor in successors:
         send(n, successor)At termination all channels are empty.
From the invariant, if channel (X, Y) is empty then X.n is a multiple
of Y.n.Therefore when all channels are empty, for every channel (X, Y) in the
system X.n is a multiple of Y.n.
Because the directed graph of agents and edges specified assuccessorsof agents is strongly
connected:for all agents X, Y in the system: X.n is a multiple of Y.nTherefore X.n = Y.n for all agents X, Y.
From the invariant, X.n is GCD.NextThe next webpage discussesdata flow in computations.Frequenty Asked QuestionsK. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

We are given a strongly connected network of agents -- i.e.,
  there is a path from each agent to every other agent.
  Each agent has a local variablen, which is initialized
  as a positive integer.
  
  Let GCD be the gcd (greatest commond divisor) of the
  initial values ofX.nof all agentsX.n.
In the code,successorsis the list of successor agents
in a network of agents, and the agent only sends messages to its
successors.def receive(message, sender):
   if n != message):
      n = gcd(n, message)
      for successor in successors:
         send(n, successor)QuestionWhat is an example of a loop variant for the distributed algorithm?AnswerLetfbe the tuple(N, M)whereNis the sum ofX.nfor all agentsX, andMis the total number of messages in
channels.Comparisons of tuples are made lexicographically.
For example (2, 1) > (1, 10), and (2, 1) < (2, 2).fis bounded below by (0, 0).An induction can be carried out on a tuple of integers.
Next we show that the execution of each event reducesf.When an agentXreceives a messagem,ifX.n\(\neq\)mthenX.nbecomesgcd(X.n, m), and so the new value ofX.nis less than its previous value, and so execution
  of the event decreasesNwhich decreasesf.ifX.n\(=\)mthen the event removes
  messagemfrom a channel and does not add messages to
  channels. So the event decreasesMwhich decreasesf.Termination Condition of a While LoopWhat can we say about the state at termination of the while loop in
the gcd example:x, y = X, Y
while x != y:
  # assertion: gcd(x, y) = gcd(X, Y)
  if x > y: x = x - y
  else: y = y - xThe while loop terminates whenx = y.
From the invariant, at termination:gcd(x, y) = gcd(x, x) = x = gcd(X, Y)Thereforexandyaregcd(X, Y)at termination of the while loop.Termination Condition of a Distributed AlgorithmWhat is the termination condition of the distributed gcd algorithm in
which thereceiveof each agent is as follows?def receive(message, sender):
   if n != message:
      n = gcd(n, message)
      for successor in successors:
         send(n, successor)At termination all channels are empty.
From the invariant, if channel (X, Y) is empty then X.n is a multiple
of Y.n.Therefore when all channels are empty, for every channel (X, Y) in the
system X.n is a multiple of Y.n.
Because the directed graph of agents and edges specified assuccessorsof agents is strongly
connected:for all agents X, Y in the system: X.n is a multiple of Y.nTherefore X.n = Y.n for all agents X, Y.
From the invariant, X.n is GCD.NextThe next webpage discussesdata flow in computations.Frequenty Asked QuestionsK. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

Comparisons of tuples are made lexicographically.
For example (2, 1) > (1, 10), and (2, 1) < (2, 2).fis bounded below by (0, 0).An induction can be carried out on a tuple of integers.
Next we show that the execution of each event reducesf.When an agentXreceives a messagem,ifX.n\(\neq\)mthenX.nbecomesgcd(X.n, m), and so the new value ofX.nis less than its previous value, and so execution
  of the event decreasesNwhich decreasesf.ifX.n\(=\)mthen the event removes
  messagemfrom a channel and does not add messages to
  channels. So the event decreasesMwhich decreasesf.Termination Condition of a While LoopWhat can we say about the state at termination of the while loop in
the gcd example:x, y = X, Y
while x != y:
  # assertion: gcd(x, y) = gcd(X, Y)
  if x > y: x = x - y
  else: y = y - xThe while loop terminates whenx = y.
From the invariant, at termination:gcd(x, y) = gcd(x, x) = x = gcd(X, Y)Thereforexandyaregcd(X, Y)at termination of the while loop.Termination Condition of a Distributed AlgorithmWhat is the termination condition of the distributed gcd algorithm in
which thereceiveof each agent is as follows?def receive(message, sender):
   if n != message:
      n = gcd(n, message)
      for successor in successors:
         send(n, successor)At termination all channels are empty.
From the invariant, if channel (X, Y) is empty then X.n is a multiple
of Y.n.Therefore when all channels are empty, for every channel (X, Y) in the
system X.n is a multiple of Y.n.
Because the directed graph of agents and edges specified assuccessorsof agents is strongly
connected:for all agents X, Y in the system: X.n is a multiple of Y.nTherefore X.n = Y.n for all agents X, Y.
From the invariant, X.n is GCD.NextThe next webpage discussesdata flow in computations.Frequenty Asked QuestionsK. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

fis bounded below by (0, 0).An induction can be carried out on a tuple of integers.
Next we show that the execution of each event reducesf.When an agentXreceives a messagem,ifX.n\(\neq\)mthenX.nbecomesgcd(X.n, m), and so the new value ofX.nis less than its previous value, and so execution
  of the event decreasesNwhich decreasesf.ifX.n\(=\)mthen the event removes
  messagemfrom a channel and does not add messages to
  channels. So the event decreasesMwhich decreasesf.Termination Condition of a While LoopWhat can we say about the state at termination of the while loop in
the gcd example:x, y = X, Y
while x != y:
  # assertion: gcd(x, y) = gcd(X, Y)
  if x > y: x = x - y
  else: y = y - xThe while loop terminates whenx = y.
From the invariant, at termination:gcd(x, y) = gcd(x, x) = x = gcd(X, Y)Thereforexandyaregcd(X, Y)at termination of the while loop.Termination Condition of a Distributed AlgorithmWhat is the termination condition of the distributed gcd algorithm in
which thereceiveof each agent is as follows?def receive(message, sender):
   if n != message:
      n = gcd(n, message)
      for successor in successors:
         send(n, successor)At termination all channels are empty.
From the invariant, if channel (X, Y) is empty then X.n is a multiple
of Y.n.Therefore when all channels are empty, for every channel (X, Y) in the
system X.n is a multiple of Y.n.
Because the directed graph of agents and edges specified assuccessorsof agents is strongly
connected:for all agents X, Y in the system: X.n is a multiple of Y.nTherefore X.n = Y.n for all agents X, Y.
From the invariant, X.n is GCD.NextThe next webpage discussesdata flow in computations.Frequenty Asked QuestionsK. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

An induction can be carried out on a tuple of integers.
Next we show that the execution of each event reducesf.When an agentXreceives a messagem,ifX.n\(\neq\)mthenX.nbecomesgcd(X.n, m), and so the new value ofX.nis less than its previous value, and so execution
  of the event decreasesNwhich decreasesf.ifX.n\(=\)mthen the event removes
  messagemfrom a channel and does not add messages to
  channels. So the event decreasesMwhich decreasesf.Termination Condition of a While LoopWhat can we say about the state at termination of the while loop in
the gcd example:x, y = X, Y
while x != y:
  # assertion: gcd(x, y) = gcd(X, Y)
  if x > y: x = x - y
  else: y = y - xThe while loop terminates whenx = y.
From the invariant, at termination:gcd(x, y) = gcd(x, x) = x = gcd(X, Y)Thereforexandyaregcd(X, Y)at termination of the while loop.Termination Condition of a Distributed AlgorithmWhat is the termination condition of the distributed gcd algorithm in
which thereceiveof each agent is as follows?def receive(message, sender):
   if n != message:
      n = gcd(n, message)
      for successor in successors:
         send(n, successor)At termination all channels are empty.
From the invariant, if channel (X, Y) is empty then X.n is a multiple
of Y.n.Therefore when all channels are empty, for every channel (X, Y) in the
system X.n is a multiple of Y.n.
Because the directed graph of agents and edges specified assuccessorsof agents is strongly
connected:for all agents X, Y in the system: X.n is a multiple of Y.nTherefore X.n = Y.n for all agents X, Y.
From the invariant, X.n is GCD.NextThe next webpage discussesdata flow in computations.Frequenty Asked QuestionsK. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

When an agentXreceives a messagem,ifX.n\(\neq\)mthenX.nbecomesgcd(X.n, m), and so the new value ofX.nis less than its previous value, and so execution
  of the event decreasesNwhich decreasesf.ifX.n\(=\)mthen the event removes
  messagemfrom a channel and does not add messages to
  channels. So the event decreasesMwhich decreasesf.Termination Condition of a While LoopWhat can we say about the state at termination of the while loop in
the gcd example:x, y = X, Y
while x != y:
  # assertion: gcd(x, y) = gcd(X, Y)
  if x > y: x = x - y
  else: y = y - xThe while loop terminates whenx = y.
From the invariant, at termination:gcd(x, y) = gcd(x, x) = x = gcd(X, Y)Thereforexandyaregcd(X, Y)at termination of the while loop.Termination Condition of a Distributed AlgorithmWhat is the termination condition of the distributed gcd algorithm in
which thereceiveof each agent is as follows?def receive(message, sender):
   if n != message:
      n = gcd(n, message)
      for successor in successors:
         send(n, successor)At termination all channels are empty.
From the invariant, if channel (X, Y) is empty then X.n is a multiple
of Y.n.Therefore when all channels are empty, for every channel (X, Y) in the
system X.n is a multiple of Y.n.
Because the directed graph of agents and edges specified assuccessorsof agents is strongly
connected:for all agents X, Y in the system: X.n is a multiple of Y.nTherefore X.n = Y.n for all agents X, Y.
From the invariant, X.n is GCD.NextThe next webpage discussesdata flow in computations.Frequenty Asked QuestionsK. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

Therefore when all channels are empty, for every channel (X, Y) in the
system X.n is a multiple of Y.n.
Because the directed graph of agents and edges specified assuccessorsof agents is strongly
connected:for all agents X, Y in the system: X.n is a multiple of Y.nTherefore X.n = Y.n for all agents X, Y.
From the invariant, X.n is GCD.NextThe next webpage discussesdata flow in computations.Frequenty Asked QuestionsK. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

for all agents X, Y in the system: X.n is a multiple of Y.nTherefore X.n = Y.n for all agents X, Y.
From the invariant, X.n is GCD.NextThe next webpage discussesdata flow in computations.Frequenty Asked QuestionsK. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

Therefore X.n = Y.n for all agents X, Y.
From the invariant, X.n is GCD.NextThe next webpage discussesdata flow in computations.Frequenty Asked QuestionsK. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

NextThe next webpage discussesdata flow in computations.Frequenty Asked QuestionsK. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

The next webpage discussesdata flow in computations.Frequenty Asked QuestionsK. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

Frequenty Asked QuestionsK. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology
--- END Content from https://kmchandy.github.io/DISTRIBUTED_SYSTEM_MODELS/ComputationsExamples.html ---


--- START Content from https://kmchandy.github.io/DISTRIBUTED_SYSTEM_MODELS/ComputationsFAQ.html ---
TITLE: DistributedSystemModels/BasicsFAQ.html

An event is a 4-tuple which specifies a state transition.
  The same event can occur multiple times in a computation.
  Each occurrence of the event is a separate step of the computation.
  The following example illustrates the difference between a step and
  an event.Consider an agent that receives numbers on an input
  channel and sends squares of the numbers on an output channel.
  The agent may receive the message 3 and send the message 9 multiple
  times in a computation.
  The specification of an execution of areceivefunction
  which receives 3 and sends 9 is specified by an event -- a 4-tuple.
  Each execution in which the agent receives 3 and sends 9 is the
  execution of the same event.
  Each execution in which the agent receives 3 and sends 9 is a
  different step of the computation.Don't computations have to start in an
  initial state of the system?We define computations as a sequence of state transitions that can
  start inanystate.
  When we prove properties of a system we prove properties of
  computations that start in the initial state of the system.
  Some papers on distributed systems restrict computations to start in
  an initial state of the system but we don't do so in this course.What is a loop invariant?Read 
SeeEffective Theories in Programming Practiceby Jayadev Misra for
many beautiful examples.
See the Wikipedia page and links from the page to sources on the web for a definition and
many examples.A loop invariant is a predicate (a boolean formula) that holds at the
beginning and end of each step of a computation.
The following example illustrates the concept.The algorithm computes the GCD (greatest common divisor) of positive
integersXandY.x, y = X, Y
# gcd(x, y) = gcd(X, Y)
while x != y:
   # gcd(x, y) = gcd(X, Y)
   if x > y: x = x- y
   if y > x: y = y - x
    # gcd(x, y) = gcd(X, Y)
return xThe invariant is:gcd(x, y) = gcd(X, Y)It holds before each step.If the algorithm terminates then we havex == yand so at termination:gcd(x, y) = gcd(x, x) = x = gcd(X, Y)How does the idea of loop invariant work
  for a distributed computation when we don't know which event will be
  executed at each step?The loop invariant for a distributed computation has the formset up initial state of the system
# loop invariant holds
while there exists an event that can be executed:
   # loop invariant holds
   execute any executable event
   # loop invariant holdsSo, we have to prove that the invariant holds in all initial states,
and if the invariant holds beforeanyevent that is executable then it
continues to hold after the event is executed.What is a loop variant?As with loop invariants please read the link to Wikipedia and links in
the Wikipedia page.
SeeEffective Theories in Programming Practiceby Jayadev Misra for
  many beautiful examples.
A loop variant is used to prove that execution of a loop terminates.
More generally, we use the idea of loop variants to prove that a
computation eventually reaches a goal.A loop variant is a function of the state (variables) of a program.
The value of the function decreases at each iteration of a loop.
The function is bounded below, and so the value of the function cannot
decrease forever. 
Therefore the loop terminates.Let's look at the example given earlier to computegcd(X,
Y).A loop variant isx + y.
Each iteration of the loop decreasesx + y.
It is bounded below by2-- the bound doesn't matter as
long as there is a bound.Because it is bounded below and cannot decrease forever the loop
terminates.NextComputationsshows how a while loop of a sequential program can be used to analyze
distributed algorithms.K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

Consider an agent that receives numbers on an input
  channel and sends squares of the numbers on an output channel.
  The agent may receive the message 3 and send the message 9 multiple
  times in a computation.
  The specification of an execution of areceivefunction
  which receives 3 and sends 9 is specified by an event -- a 4-tuple.
  Each execution in which the agent receives 3 and sends 9 is the
  execution of the same event.
  Each execution in which the agent receives 3 and sends 9 is a
  different step of the computation.Don't computations have to start in an
  initial state of the system?We define computations as a sequence of state transitions that can
  start inanystate.
  When we prove properties of a system we prove properties of
  computations that start in the initial state of the system.
  Some papers on distributed systems restrict computations to start in
  an initial state of the system but we don't do so in this course.What is a loop invariant?Read 
SeeEffective Theories in Programming Practiceby Jayadev Misra for
many beautiful examples.
See the Wikipedia page and links from the page to sources on the web for a definition and
many examples.A loop invariant is a predicate (a boolean formula) that holds at the
beginning and end of each step of a computation.
The following example illustrates the concept.The algorithm computes the GCD (greatest common divisor) of positive
integersXandY.x, y = X, Y
# gcd(x, y) = gcd(X, Y)
while x != y:
   # gcd(x, y) = gcd(X, Y)
   if x > y: x = x- y
   if y > x: y = y - x
    # gcd(x, y) = gcd(X, Y)
return xThe invariant is:gcd(x, y) = gcd(X, Y)It holds before each step.If the algorithm terminates then we havex == yand so at termination:gcd(x, y) = gcd(x, x) = x = gcd(X, Y)How does the idea of loop invariant work
  for a distributed computation when we don't know which event will be
  executed at each step?The loop invariant for a distributed computation has the formset up initial state of the system
# loop invariant holds
while there exists an event that can be executed:
   # loop invariant holds
   execute any executable event
   # loop invariant holdsSo, we have to prove that the invariant holds in all initial states,
and if the invariant holds beforeanyevent that is executable then it
continues to hold after the event is executed.What is a loop variant?As with loop invariants please read the link to Wikipedia and links in
the Wikipedia page.
SeeEffective Theories in Programming Practiceby Jayadev Misra for
  many beautiful examples.
A loop variant is used to prove that execution of a loop terminates.
More generally, we use the idea of loop variants to prove that a
computation eventually reaches a goal.A loop variant is a function of the state (variables) of a program.
The value of the function decreases at each iteration of a loop.
The function is bounded below, and so the value of the function cannot
decrease forever. 
Therefore the loop terminates.Let's look at the example given earlier to computegcd(X,
Y).A loop variant isx + y.
Each iteration of the loop decreasesx + y.
It is bounded below by2-- the bound doesn't matter as
long as there is a bound.Because it is bounded below and cannot decrease forever the loop
terminates.NextComputationsshows how a while loop of a sequential program can be used to analyze
distributed algorithms.K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

A loop invariant is a predicate (a boolean formula) that holds at the
beginning and end of each step of a computation.
The following example illustrates the concept.The algorithm computes the GCD (greatest common divisor) of positive
integersXandY.x, y = X, Y
# gcd(x, y) = gcd(X, Y)
while x != y:
   # gcd(x, y) = gcd(X, Y)
   if x > y: x = x- y
   if y > x: y = y - x
    # gcd(x, y) = gcd(X, Y)
return xThe invariant is:gcd(x, y) = gcd(X, Y)It holds before each step.If the algorithm terminates then we havex == yand so at termination:gcd(x, y) = gcd(x, x) = x = gcd(X, Y)How does the idea of loop invariant work
  for a distributed computation when we don't know which event will be
  executed at each step?The loop invariant for a distributed computation has the formset up initial state of the system
# loop invariant holds
while there exists an event that can be executed:
   # loop invariant holds
   execute any executable event
   # loop invariant holdsSo, we have to prove that the invariant holds in all initial states,
and if the invariant holds beforeanyevent that is executable then it
continues to hold after the event is executed.What is a loop variant?As with loop invariants please read the link to Wikipedia and links in
the Wikipedia page.
SeeEffective Theories in Programming Practiceby Jayadev Misra for
  many beautiful examples.
A loop variant is used to prove that execution of a loop terminates.
More generally, we use the idea of loop variants to prove that a
computation eventually reaches a goal.A loop variant is a function of the state (variables) of a program.
The value of the function decreases at each iteration of a loop.
The function is bounded below, and so the value of the function cannot
decrease forever. 
Therefore the loop terminates.Let's look at the example given earlier to computegcd(X,
Y).A loop variant isx + y.
Each iteration of the loop decreasesx + y.
It is bounded below by2-- the bound doesn't matter as
long as there is a bound.Because it is bounded below and cannot decrease forever the loop
terminates.NextComputationsshows how a while loop of a sequential program can be used to analyze
distributed algorithms.K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

The algorithm computes the GCD (greatest common divisor) of positive
integersXandY.x, y = X, Y
# gcd(x, y) = gcd(X, Y)
while x != y:
   # gcd(x, y) = gcd(X, Y)
   if x > y: x = x- y
   if y > x: y = y - x
    # gcd(x, y) = gcd(X, Y)
return xThe invariant is:gcd(x, y) = gcd(X, Y)It holds before each step.If the algorithm terminates then we havex == yand so at termination:gcd(x, y) = gcd(x, x) = x = gcd(X, Y)How does the idea of loop invariant work
  for a distributed computation when we don't know which event will be
  executed at each step?The loop invariant for a distributed computation has the formset up initial state of the system
# loop invariant holds
while there exists an event that can be executed:
   # loop invariant holds
   execute any executable event
   # loop invariant holdsSo, we have to prove that the invariant holds in all initial states,
and if the invariant holds beforeanyevent that is executable then it
continues to hold after the event is executed.What is a loop variant?As with loop invariants please read the link to Wikipedia and links in
the Wikipedia page.
SeeEffective Theories in Programming Practiceby Jayadev Misra for
  many beautiful examples.
A loop variant is used to prove that execution of a loop terminates.
More generally, we use the idea of loop variants to prove that a
computation eventually reaches a goal.A loop variant is a function of the state (variables) of a program.
The value of the function decreases at each iteration of a loop.
The function is bounded below, and so the value of the function cannot
decrease forever. 
Therefore the loop terminates.Let's look at the example given earlier to computegcd(X,
Y).A loop variant isx + y.
Each iteration of the loop decreasesx + y.
It is bounded below by2-- the bound doesn't matter as
long as there is a bound.Because it is bounded below and cannot decrease forever the loop
terminates.NextComputationsshows how a while loop of a sequential program can be used to analyze
distributed algorithms.K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

If the algorithm terminates then we havex == yand so at termination:gcd(x, y) = gcd(x, x) = x = gcd(X, Y)How does the idea of loop invariant work
  for a distributed computation when we don't know which event will be
  executed at each step?The loop invariant for a distributed computation has the formset up initial state of the system
# loop invariant holds
while there exists an event that can be executed:
   # loop invariant holds
   execute any executable event
   # loop invariant holdsSo, we have to prove that the invariant holds in all initial states,
and if the invariant holds beforeanyevent that is executable then it
continues to hold after the event is executed.What is a loop variant?As with loop invariants please read the link to Wikipedia and links in
the Wikipedia page.
SeeEffective Theories in Programming Practiceby Jayadev Misra for
  many beautiful examples.
A loop variant is used to prove that execution of a loop terminates.
More generally, we use the idea of loop variants to prove that a
computation eventually reaches a goal.A loop variant is a function of the state (variables) of a program.
The value of the function decreases at each iteration of a loop.
The function is bounded below, and so the value of the function cannot
decrease forever. 
Therefore the loop terminates.Let's look at the example given earlier to computegcd(X,
Y).A loop variant isx + y.
Each iteration of the loop decreasesx + y.
It is bounded below by2-- the bound doesn't matter as
long as there is a bound.Because it is bounded below and cannot decrease forever the loop
terminates.NextComputationsshows how a while loop of a sequential program can be used to analyze
distributed algorithms.K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

A loop variant is a function of the state (variables) of a program.
The value of the function decreases at each iteration of a loop.
The function is bounded below, and so the value of the function cannot
decrease forever. 
Therefore the loop terminates.Let's look at the example given earlier to computegcd(X,
Y).A loop variant isx + y.
Each iteration of the loop decreasesx + y.
It is bounded below by2-- the bound doesn't matter as
long as there is a bound.Because it is bounded below and cannot decrease forever the loop
terminates.NextComputationsshows how a while loop of a sequential program can be used to analyze
distributed algorithms.K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

Let's look at the example given earlier to computegcd(X,
Y).A loop variant isx + y.
Each iteration of the loop decreasesx + y.
It is bounded below by2-- the bound doesn't matter as
long as there is a bound.Because it is bounded below and cannot decrease forever the loop
terminates.NextComputationsshows how a while loop of a sequential program can be used to analyze
distributed algorithms.K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

A loop variant isx + y.
Each iteration of the loop decreasesx + y.
It is bounded below by2-- the bound doesn't matter as
long as there is a bound.Because it is bounded below and cannot decrease forever the loop
terminates.NextComputationsshows how a while loop of a sequential program can be used to analyze
distributed algorithms.K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

Because it is bounded below and cannot decrease forever the loop
terminates.NextComputationsshows how a while loop of a sequential program can be used to analyze
distributed algorithms.K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

Computationsshows how a while loop of a sequential program can be used to analyze
distributed algorithms.K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology
--- END Content from https://kmchandy.github.io/DISTRIBUTED_SYSTEM_MODELS/ComputationsFAQ.html ---


--- START Content from https://kmchandy.github.io/DISTRIBUTED_SYSTEM_MODELS/ComputationsReview.html ---
TITLE: DistributedSystemModels/ComputationsReview.html

A computation is a central concept in describing and analyzing
  distributed algorithms.
  The relationship between computations and dataflow helps us
  understand many algorithms.What is the relationship between a single event and an iteration
    of the sequential program abstraction of a distributed system.What is the change in state caused by execution of a single
    iteration of the while loop representation of a distributed
    system? What is the relationship between this change in state and
    the change in state specified by an event?What is a computation?What is the relationship between a computation and the sequence of
    states of the while loop abstraction of a distributed system?Review sequential programming: What is an invariant of a while
    loop? How do you prove invariants?Review sequential programming: What is a loop variant of a while
    loop?Review sequential programming: Review your earlier courses on
    proving properties of loops. Give and prove an algorithm for
    computing the all-points shortest paths in graphs.NextThe next webpage discusses the concepts ofpast and futurein computations and dataflow.K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

The next webpage discusses the concepts ofpast and futurein computations and dataflow.K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology
--- END Content from https://kmchandy.github.io/DISTRIBUTED_SYSTEM_MODELS/ComputationsReview.html ---


--- START Content from https://kmchandy.github.io/DISTRIBUTED_SYSTEM_MODELS/Model.html ---
TITLE: DistributedSystemModels/Models.html

A dataflow graph of a computation is a labeled directed acyclic
  graph that shows the flow of data between agents during the
  computation. 
  Alltopological sortsof a dataflow graph are computations.



Figure Link: https://kmchandy.github.io/DISTRIBUTED_SYSTEM_MODELS/Dataflow.001.jpeg

Caption: Figure Example of a Vertex in a Dataflow Graph


The agent edge from this step is labeled with \(u\)'s initial state.
The message edges from this step to steps of an agent \(v\) are
labeled with the messages in the channel \((u, v)\) in the initial
state of the computation.Figure Example of an Initial Vertex of a Dataflow
    GraphThe figure shows an example of an initial vertex,u_initat agentuof a dataflow graph.
The vertex has an output agent edge labeledu1which is
the initial state of agentuin the computation.
The vertex has an output message edge labeledm2; this
edge is to a step in agentvand shows that the initial
state of the channel(u, v)consists of the single
messagem2.By symmetry, we use a fictitious step \(u_{fini}\) to represent the final
state of \(u\) and the final states of channels to \(u\).
With the introduction of fictitious initial and final steps for each
agent we can treat a computation as a sequence of steps where the
initial state is specified by initial steps.Figure Example of a Final Vertex of a Dataflow
    GraphWe adopt the convention that agent edges are horizontal.
So, all edges of the same agent are on the same horizontal line.We usestepof a computation andvertexof its dataflow
graph interchangeably, with the meaning understood by context.Example of a Dataflow GraphFigure Example of a Dataflow GraphThe figure shows the dataflow graph for a computation of a
system consisting of two agentsuandv, and
with two channels(u, v)and(v, u).
The computation is defined by its initial state and event sequence [1,
2, 3, 4]. The initial states at agentsuandvare represented by outputs of stepsu_initandv_init, respectively, and final states by inputs tou_finiandv_fini, respectively.The top horizontal edges represent data flow between steps at agentuand the lower horizontal edges represent data flow
between steps at agentv.
For example, the labelsu1, u2, u3are the initial state
of agentu, the state ofubetween steps 1
and 4, and the state ofuafter step 4, respectively.
Likewise, the labelsv1, v2, v3are the states of
agentvinitially and after steps 2 and 3, respectively.Message edges are labeledm1, m2, m3, m4, m5.
Initially channel(u, v)contains messagem2and channel(v, u)contains messagem1.
Messagem3is sent in step 1 and received in step 3, and
messagem4is sent in step 3 and received in step 4.Vertex 1 in the graph, see figure 1 -- example of a vertex in a
dataflow graph -- represents the execution of a step at agentu.
This step is the execution of an event specified by the following
4-tuple:The state of \(u\) before the event isu1.The message received in the event ism1from agentv.The state  of \(u\) after the event isu2.A single messagem3is sent in the event and this
  message is  sent to agentv.The input and output edges of the vertex are the inputs and outputs
(respectively) of the event.Dataflow Graphs are AcyclicA dataflow graph of a computation is acyclic because each edge is directed from a step
to a later step of the computation.Flow of Data in a ComputationThe edges of the dataflow graph of a computation show the flow of data
into and out of each step of a
computation.
Data -- in the form of the agent's state -- flows from a step of an
agent to the next step at that agent.
Data -- in the form of message contents -- flows from a step in which
a message is sent to the step in which the message is received.The Relation: "data flows from" between Steps
of a ComputationWe define a relation "data flows from" between steps of a
computation 
as follows: for steps \(x, y\) of a computation, data flows from
\(x\) to \(y\) exactly when there is a path in the computation's
dataflow graph from \(x\) to \(y\).The set of steps of a computation with the relation "data flows from"
is astrict partial order.
We say that steps \(x\) and \(y\) areindependentif data does
not flow from  \(x\) to \(y\) or from \(y\) to \(x\).Example of data flows from a step to a
stepThe figure "Example -- Data Flows from Step 2 to Step 4" shows
examples of data flow as red edges in the graph.
Steps 1 and 2 are independent because there is no path from step 1 to
step 2, and there is no path from step 2 to step 1.Figure Example -- Data Flows from Step 2 to Step 4Computations and Dataflow GraphsLet a computation be specified by a sequence of steps \(E\), and
let \(E'\) be any permutation of \(E\).Data flows forwards in \(E\)means that for all steps
\(x\) and \(y\) in \(E\):If data flows from \(x\) to \(y\) in \(E\)
then \(x\) appears before \(y\) in
\(E'\).Next we prove properties about sequences of steps in which data flows forwards.
We start with the following lemma.Lemma: Switching Order of Adjacent
Independent Steps in a ComputationLet a computation be specified by  a sequence of
steps \(E\), and let \(E'\) be any permutation of \(E\)
obtained by switching the order of a pair of adjacent elements in
\(E\). 
If data flows forward in \(E'\) then \(E'\) is a computation.ProofAssume that the order of adjacent steps \(e_{i}\) and \(e_{i+1}\) are
switched in \(E\) to get \(E'\).
Because \(e_{i}\) precedes \(e_{i+1}\) in \(E\) there is no data flow
from \(e_{i+1}\) to \(e_{i}\).
Because data flows forwards in \(E'\) there is no data flow from
\(e_{i}\) to \(e_{i+1}\).Therefore \(e_{i}\) and \(e_{i+1}\) are adjacent independent steps.
So, the inputs to \(e_{i}\) and \(e_{i+1}\) remain the same
regardless of the order in which they are executed, and so \(E'\) is
also a computation.Example of Switching Adjacent Independent
StepsThe Figure "Example of Switching Adjacent Independent Steps" shows two
diagrams with identical dataflow graphs. The upper and diagrams are 
representations of computations starting at the same state and step
sequences [1, 2, 3, 4], and [2, 1, 3, 4].
The order in which vertices 1 and 2 appear, from left to right, in the
diagrams have been switched.
The graphs, however, are identical.The inputs to step 1 remain the same whether step 2 is executed
before or after step 1.
Likewise, the inputs to step 2 remain the same whether step 1 is executed
before or after step 2.Figure Example of Switching Adjacent Independent StepsTheorem: Permutations of Computations in which
Data Flows ForwardsGiven a computation specifed by a sequence of
steps \(E\), all permutations of \(E\) in which data flows forwards
are also computations.ProofLet \(E'\) be a permutation of \(E\) where data flows forwards in
\(E'\).
The theorem holds trivially if \(E\) and \(E'\) are identical.If \(E\) and \(E'\) are not identical then there exists an adjacent
pair of steps \(x, y\) in \(E\) where \(x\) occurs before \(y\) in
\(E\), and \(y\) occurs before \(x\) in \(E'\).Because \(E\) is a computation, data flows forwards in \(E\) and
therefore there is no data flow from \(y\) to \(x\).We are given that data flows forwards in \(E'\). So there is no
data flow from \(x\) to \(y\).Therefore \(x\) and \(y\) are independent.
Let \(H\) be the sequence of steps obtained from \(E\) by switching
the order of \(x\) and \(y\).
From the lemma "Switching Order of Independent Adjacent Steps" it
follows 
that \(H\) is also a computation.The number of out of order pairs between \(H\) and \(E'\) is less than
that between \(E\) and \(E'\).
By repeatedly switching of independent adjacent out of order pairs
the sequence \(E'\) is reached.Relationship to Topological SortsThe theorem on permutations of event sequences can be expressed in
terms of topological sorts.
Atopological sortof a directed acyclic graph is a sequence of
  vertices of the graph where for every edge \((x, y)\) in the graph,
\(x\) appears before \(y\) in the sequence.Theorem: Topological Sorts of DataflowAll topological sorts of a dataflow graph are computations.NextThe next pages describesCuts in Dataflow Graphswhich are used in developing detection algorithms such as termination
detection and deadlock detection.ExamplesFrequenty Asked QuestionsReviewK. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology



Figure Link: https://kmchandy.github.io/DISTRIBUTED_SYSTEM_MODELS/Dataflow.003.jpeg

Caption: Figure Example of an Initial Vertex of a Dataflow
    Graph


By symmetry, we use a fictitious step \(u_{fini}\) to represent the final
state of \(u\) and the final states of channels to \(u\).
With the introduction of fictitious initial and final steps for each
agent we can treat a computation as a sequence of steps where the
initial state is specified by initial steps.Figure Example of a Final Vertex of a Dataflow
    GraphWe adopt the convention that agent edges are horizontal.
So, all edges of the same agent are on the same horizontal line.We usestepof a computation andvertexof its dataflow
graph interchangeably, with the meaning understood by context.Example of a Dataflow GraphFigure Example of a Dataflow GraphThe figure shows the dataflow graph for a computation of a
system consisting of two agentsuandv, and
with two channels(u, v)and(v, u).
The computation is defined by its initial state and event sequence [1,
2, 3, 4]. The initial states at agentsuandvare represented by outputs of stepsu_initandv_init, respectively, and final states by inputs tou_finiandv_fini, respectively.The top horizontal edges represent data flow between steps at agentuand the lower horizontal edges represent data flow
between steps at agentv.
For example, the labelsu1, u2, u3are the initial state
of agentu, the state ofubetween steps 1
and 4, and the state ofuafter step 4, respectively.
Likewise, the labelsv1, v2, v3are the states of
agentvinitially and after steps 2 and 3, respectively.Message edges are labeledm1, m2, m3, m4, m5.
Initially channel(u, v)contains messagem2and channel(v, u)contains messagem1.
Messagem3is sent in step 1 and received in step 3, and
messagem4is sent in step 3 and received in step 4.Vertex 1 in the graph, see figure 1 -- example of a vertex in a
dataflow graph -- represents the execution of a step at agentu.
This step is the execution of an event specified by the following
4-tuple:The state of \(u\) before the event isu1.The message received in the event ism1from agentv.The state  of \(u\) after the event isu2.A single messagem3is sent in the event and this
  message is  sent to agentv.The input and output edges of the vertex are the inputs and outputs
(respectively) of the event.Dataflow Graphs are AcyclicA dataflow graph of a computation is acyclic because each edge is directed from a step
to a later step of the computation.Flow of Data in a ComputationThe edges of the dataflow graph of a computation show the flow of data
into and out of each step of a
computation.
Data -- in the form of the agent's state -- flows from a step of an
agent to the next step at that agent.
Data -- in the form of message contents -- flows from a step in which
a message is sent to the step in which the message is received.The Relation: "data flows from" between Steps
of a ComputationWe define a relation "data flows from" between steps of a
computation 
as follows: for steps \(x, y\) of a computation, data flows from
\(x\) to \(y\) exactly when there is a path in the computation's
dataflow graph from \(x\) to \(y\).The set of steps of a computation with the relation "data flows from"
is astrict partial order.
We say that steps \(x\) and \(y\) areindependentif data does
not flow from  \(x\) to \(y\) or from \(y\) to \(x\).Example of data flows from a step to a
stepThe figure "Example -- Data Flows from Step 2 to Step 4" shows
examples of data flow as red edges in the graph.
Steps 1 and 2 are independent because there is no path from step 1 to
step 2, and there is no path from step 2 to step 1.Figure Example -- Data Flows from Step 2 to Step 4Computations and Dataflow GraphsLet a computation be specified by a sequence of steps \(E\), and
let \(E'\) be any permutation of \(E\).Data flows forwards in \(E\)means that for all steps
\(x\) and \(y\) in \(E\):If data flows from \(x\) to \(y\) in \(E\)
then \(x\) appears before \(y\) in
\(E'\).Next we prove properties about sequences of steps in which data flows forwards.
We start with the following lemma.Lemma: Switching Order of Adjacent
Independent Steps in a ComputationLet a computation be specified by  a sequence of
steps \(E\), and let \(E'\) be any permutation of \(E\)
obtained by switching the order of a pair of adjacent elements in
\(E\). 
If data flows forward in \(E'\) then \(E'\) is a computation.ProofAssume that the order of adjacent steps \(e_{i}\) and \(e_{i+1}\) are
switched in \(E\) to get \(E'\).
Because \(e_{i}\) precedes \(e_{i+1}\) in \(E\) there is no data flow
from \(e_{i+1}\) to \(e_{i}\).
Because data flows forwards in \(E'\) there is no data flow from
\(e_{i}\) to \(e_{i+1}\).Therefore \(e_{i}\) and \(e_{i+1}\) are adjacent independent steps.
So, the inputs to \(e_{i}\) and \(e_{i+1}\) remain the same
regardless of the order in which they are executed, and so \(E'\) is
also a computation.Example of Switching Adjacent Independent
StepsThe Figure "Example of Switching Adjacent Independent Steps" shows two
diagrams with identical dataflow graphs. The upper and diagrams are 
representations of computations starting at the same state and step
sequences [1, 2, 3, 4], and [2, 1, 3, 4].
The order in which vertices 1 and 2 appear, from left to right, in the
diagrams have been switched.
The graphs, however, are identical.The inputs to step 1 remain the same whether step 2 is executed
before or after step 1.
Likewise, the inputs to step 2 remain the same whether step 1 is executed
before or after step 2.Figure Example of Switching Adjacent Independent StepsTheorem: Permutations of Computations in which
Data Flows ForwardsGiven a computation specifed by a sequence of
steps \(E\), all permutations of \(E\) in which data flows forwards
are also computations.ProofLet \(E'\) be a permutation of \(E\) where data flows forwards in
\(E'\).
The theorem holds trivially if \(E\) and \(E'\) are identical.If \(E\) and \(E'\) are not identical then there exists an adjacent
pair of steps \(x, y\) in \(E\) where \(x\) occurs before \(y\) in
\(E\), and \(y\) occurs before \(x\) in \(E'\).Because \(E\) is a computation, data flows forwards in \(E\) and
therefore there is no data flow from \(y\) to \(x\).We are given that data flows forwards in \(E'\). So there is no
data flow from \(x\) to \(y\).Therefore \(x\) and \(y\) are independent.
Let \(H\) be the sequence of steps obtained from \(E\) by switching
the order of \(x\) and \(y\).
From the lemma "Switching Order of Independent Adjacent Steps" it
follows 
that \(H\) is also a computation.The number of out of order pairs between \(H\) and \(E'\) is less than
that between \(E\) and \(E'\).
By repeatedly switching of independent adjacent out of order pairs
the sequence \(E'\) is reached.Relationship to Topological SortsThe theorem on permutations of event sequences can be expressed in
terms of topological sorts.
Atopological sortof a directed acyclic graph is a sequence of
  vertices of the graph where for every edge \((x, y)\) in the graph,
\(x\) appears before \(y\) in the sequence.Theorem: Topological Sorts of DataflowAll topological sorts of a dataflow graph are computations.NextThe next pages describesCuts in Dataflow Graphswhich are used in developing detection algorithms such as termination
detection and deadlock detection.ExamplesFrequenty Asked QuestionsReviewK. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology



Figure Link: https://kmchandy.github.io/DISTRIBUTED_SYSTEM_MODELS/Dataflow.004.jpeg

Caption: Figure Example of a Final Vertex of a Dataflow
    Graph


We adopt the convention that agent edges are horizontal.
So, all edges of the same agent are on the same horizontal line.We usestepof a computation andvertexof its dataflow
graph interchangeably, with the meaning understood by context.Example of a Dataflow GraphFigure Example of a Dataflow GraphThe figure shows the dataflow graph for a computation of a
system consisting of two agentsuandv, and
with two channels(u, v)and(v, u).
The computation is defined by its initial state and event sequence [1,
2, 3, 4]. The initial states at agentsuandvare represented by outputs of stepsu_initandv_init, respectively, and final states by inputs tou_finiandv_fini, respectively.The top horizontal edges represent data flow between steps at agentuand the lower horizontal edges represent data flow
between steps at agentv.
For example, the labelsu1, u2, u3are the initial state
of agentu, the state ofubetween steps 1
and 4, and the state ofuafter step 4, respectively.
Likewise, the labelsv1, v2, v3are the states of
agentvinitially and after steps 2 and 3, respectively.Message edges are labeledm1, m2, m3, m4, m5.
Initially channel(u, v)contains messagem2and channel(v, u)contains messagem1.
Messagem3is sent in step 1 and received in step 3, and
messagem4is sent in step 3 and received in step 4.Vertex 1 in the graph, see figure 1 -- example of a vertex in a
dataflow graph -- represents the execution of a step at agentu.
This step is the execution of an event specified by the following
4-tuple:The state of \(u\) before the event isu1.The message received in the event ism1from agentv.The state  of \(u\) after the event isu2.A single messagem3is sent in the event and this
  message is  sent to agentv.The input and output edges of the vertex are the inputs and outputs
(respectively) of the event.Dataflow Graphs are AcyclicA dataflow graph of a computation is acyclic because each edge is directed from a step
to a later step of the computation.Flow of Data in a ComputationThe edges of the dataflow graph of a computation show the flow of data
into and out of each step of a
computation.
Data -- in the form of the agent's state -- flows from a step of an
agent to the next step at that agent.
Data -- in the form of message contents -- flows from a step in which
a message is sent to the step in which the message is received.The Relation: "data flows from" between Steps
of a ComputationWe define a relation "data flows from" between steps of a
computation 
as follows: for steps \(x, y\) of a computation, data flows from
\(x\) to \(y\) exactly when there is a path in the computation's
dataflow graph from \(x\) to \(y\).The set of steps of a computation with the relation "data flows from"
is astrict partial order.
We say that steps \(x\) and \(y\) areindependentif data does
not flow from  \(x\) to \(y\) or from \(y\) to \(x\).Example of data flows from a step to a
stepThe figure "Example -- Data Flows from Step 2 to Step 4" shows
examples of data flow as red edges in the graph.
Steps 1 and 2 are independent because there is no path from step 1 to
step 2, and there is no path from step 2 to step 1.Figure Example -- Data Flows from Step 2 to Step 4Computations and Dataflow GraphsLet a computation be specified by a sequence of steps \(E\), and
let \(E'\) be any permutation of \(E\).Data flows forwards in \(E\)means that for all steps
\(x\) and \(y\) in \(E\):If data flows from \(x\) to \(y\) in \(E\)
then \(x\) appears before \(y\) in
\(E'\).Next we prove properties about sequences of steps in which data flows forwards.
We start with the following lemma.Lemma: Switching Order of Adjacent
Independent Steps in a ComputationLet a computation be specified by  a sequence of
steps \(E\), and let \(E'\) be any permutation of \(E\)
obtained by switching the order of a pair of adjacent elements in
\(E\). 
If data flows forward in \(E'\) then \(E'\) is a computation.ProofAssume that the order of adjacent steps \(e_{i}\) and \(e_{i+1}\) are
switched in \(E\) to get \(E'\).
Because \(e_{i}\) precedes \(e_{i+1}\) in \(E\) there is no data flow
from \(e_{i+1}\) to \(e_{i}\).
Because data flows forwards in \(E'\) there is no data flow from
\(e_{i}\) to \(e_{i+1}\).Therefore \(e_{i}\) and \(e_{i+1}\) are adjacent independent steps.
So, the inputs to \(e_{i}\) and \(e_{i+1}\) remain the same
regardless of the order in which they are executed, and so \(E'\) is
also a computation.Example of Switching Adjacent Independent
StepsThe Figure "Example of Switching Adjacent Independent Steps" shows two
diagrams with identical dataflow graphs. The upper and diagrams are 
representations of computations starting at the same state and step
sequences [1, 2, 3, 4], and [2, 1, 3, 4].
The order in which vertices 1 and 2 appear, from left to right, in the
diagrams have been switched.
The graphs, however, are identical.The inputs to step 1 remain the same whether step 2 is executed
before or after step 1.
Likewise, the inputs to step 2 remain the same whether step 1 is executed
before or after step 2.Figure Example of Switching Adjacent Independent StepsTheorem: Permutations of Computations in which
Data Flows ForwardsGiven a computation specifed by a sequence of
steps \(E\), all permutations of \(E\) in which data flows forwards
are also computations.ProofLet \(E'\) be a permutation of \(E\) where data flows forwards in
\(E'\).
The theorem holds trivially if \(E\) and \(E'\) are identical.If \(E\) and \(E'\) are not identical then there exists an adjacent
pair of steps \(x, y\) in \(E\) where \(x\) occurs before \(y\) in
\(E\), and \(y\) occurs before \(x\) in \(E'\).Because \(E\) is a computation, data flows forwards in \(E\) and
therefore there is no data flow from \(y\) to \(x\).We are given that data flows forwards in \(E'\). So there is no
data flow from \(x\) to \(y\).Therefore \(x\) and \(y\) are independent.
Let \(H\) be the sequence of steps obtained from \(E\) by switching
the order of \(x\) and \(y\).
From the lemma "Switching Order of Independent Adjacent Steps" it
follows 
that \(H\) is also a computation.The number of out of order pairs between \(H\) and \(E'\) is less than
that between \(E\) and \(E'\).
By repeatedly switching of independent adjacent out of order pairs
the sequence \(E'\) is reached.Relationship to Topological SortsThe theorem on permutations of event sequences can be expressed in
terms of topological sorts.
Atopological sortof a directed acyclic graph is a sequence of
  vertices of the graph where for every edge \((x, y)\) in the graph,
\(x\) appears before \(y\) in the sequence.Theorem: Topological Sorts of DataflowAll topological sorts of a dataflow graph are computations.NextThe next pages describesCuts in Dataflow Graphswhich are used in developing detection algorithms such as termination
detection and deadlock detection.ExamplesFrequenty Asked QuestionsReviewK. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology



Figure Link: https://kmchandy.github.io/DISTRIBUTED_SYSTEM_MODELS/Dataflow.002.jpeg

Caption: Figure Example of a Dataflow Graph


We usestepof a computation andvertexof its dataflow
graph interchangeably, with the meaning understood by context.Example of a Dataflow GraphFigure Example of a Dataflow GraphThe figure shows the dataflow graph for a computation of a
system consisting of two agentsuandv, and
with two channels(u, v)and(v, u).
The computation is defined by its initial state and event sequence [1,
2, 3, 4]. The initial states at agentsuandvare represented by outputs of stepsu_initandv_init, respectively, and final states by inputs tou_finiandv_fini, respectively.The top horizontal edges represent data flow between steps at agentuand the lower horizontal edges represent data flow
between steps at agentv.
For example, the labelsu1, u2, u3are the initial state
of agentu, the state ofubetween steps 1
and 4, and the state ofuafter step 4, respectively.
Likewise, the labelsv1, v2, v3are the states of
agentvinitially and after steps 2 and 3, respectively.Message edges are labeledm1, m2, m3, m4, m5.
Initially channel(u, v)contains messagem2and channel(v, u)contains messagem1.
Messagem3is sent in step 1 and received in step 3, and
messagem4is sent in step 3 and received in step 4.Vertex 1 in the graph, see figure 1 -- example of a vertex in a
dataflow graph -- represents the execution of a step at agentu.
This step is the execution of an event specified by the following
4-tuple:The state of \(u\) before the event isu1.The message received in the event ism1from agentv.The state  of \(u\) after the event isu2.A single messagem3is sent in the event and this
  message is  sent to agentv.The input and output edges of the vertex are the inputs and outputs
(respectively) of the event.Dataflow Graphs are AcyclicA dataflow graph of a computation is acyclic because each edge is directed from a step
to a later step of the computation.Flow of Data in a ComputationThe edges of the dataflow graph of a computation show the flow of data
into and out of each step of a
computation.
Data -- in the form of the agent's state -- flows from a step of an
agent to the next step at that agent.
Data -- in the form of message contents -- flows from a step in which
a message is sent to the step in which the message is received.The Relation: "data flows from" between Steps
of a ComputationWe define a relation "data flows from" between steps of a
computation 
as follows: for steps \(x, y\) of a computation, data flows from
\(x\) to \(y\) exactly when there is a path in the computation's
dataflow graph from \(x\) to \(y\).The set of steps of a computation with the relation "data flows from"
is astrict partial order.
We say that steps \(x\) and \(y\) areindependentif data does
not flow from  \(x\) to \(y\) or from \(y\) to \(x\).Example of data flows from a step to a
stepThe figure "Example -- Data Flows from Step 2 to Step 4" shows
examples of data flow as red edges in the graph.
Steps 1 and 2 are independent because there is no path from step 1 to
step 2, and there is no path from step 2 to step 1.Figure Example -- Data Flows from Step 2 to Step 4Computations and Dataflow GraphsLet a computation be specified by a sequence of steps \(E\), and
let \(E'\) be any permutation of \(E\).Data flows forwards in \(E\)means that for all steps
\(x\) and \(y\) in \(E\):If data flows from \(x\) to \(y\) in \(E\)
then \(x\) appears before \(y\) in
\(E'\).Next we prove properties about sequences of steps in which data flows forwards.
We start with the following lemma.Lemma: Switching Order of Adjacent
Independent Steps in a ComputationLet a computation be specified by  a sequence of
steps \(E\), and let \(E'\) be any permutation of \(E\)
obtained by switching the order of a pair of adjacent elements in
\(E\). 
If data flows forward in \(E'\) then \(E'\) is a computation.ProofAssume that the order of adjacent steps \(e_{i}\) and \(e_{i+1}\) are
switched in \(E\) to get \(E'\).
Because \(e_{i}\) precedes \(e_{i+1}\) in \(E\) there is no data flow
from \(e_{i+1}\) to \(e_{i}\).
Because data flows forwards in \(E'\) there is no data flow from
\(e_{i}\) to \(e_{i+1}\).Therefore \(e_{i}\) and \(e_{i+1}\) are adjacent independent steps.
So, the inputs to \(e_{i}\) and \(e_{i+1}\) remain the same
regardless of the order in which they are executed, and so \(E'\) is
also a computation.Example of Switching Adjacent Independent
StepsThe Figure "Example of Switching Adjacent Independent Steps" shows two
diagrams with identical dataflow graphs. The upper and diagrams are 
representations of computations starting at the same state and step
sequences [1, 2, 3, 4], and [2, 1, 3, 4].
The order in which vertices 1 and 2 appear, from left to right, in the
diagrams have been switched.
The graphs, however, are identical.The inputs to step 1 remain the same whether step 2 is executed
before or after step 1.
Likewise, the inputs to step 2 remain the same whether step 1 is executed
before or after step 2.Figure Example of Switching Adjacent Independent StepsTheorem: Permutations of Computations in which
Data Flows ForwardsGiven a computation specifed by a sequence of
steps \(E\), all permutations of \(E\) in which data flows forwards
are also computations.ProofLet \(E'\) be a permutation of \(E\) where data flows forwards in
\(E'\).
The theorem holds trivially if \(E\) and \(E'\) are identical.If \(E\) and \(E'\) are not identical then there exists an adjacent
pair of steps \(x, y\) in \(E\) where \(x\) occurs before \(y\) in
\(E\), and \(y\) occurs before \(x\) in \(E'\).Because \(E\) is a computation, data flows forwards in \(E\) and
therefore there is no data flow from \(y\) to \(x\).We are given that data flows forwards in \(E'\). So there is no
data flow from \(x\) to \(y\).Therefore \(x\) and \(y\) are independent.
Let \(H\) be the sequence of steps obtained from \(E\) by switching
the order of \(x\) and \(y\).
From the lemma "Switching Order of Independent Adjacent Steps" it
follows 
that \(H\) is also a computation.The number of out of order pairs between \(H\) and \(E'\) is less than
that between \(E\) and \(E'\).
By repeatedly switching of independent adjacent out of order pairs
the sequence \(E'\) is reached.Relationship to Topological SortsThe theorem on permutations of event sequences can be expressed in
terms of topological sorts.
Atopological sortof a directed acyclic graph is a sequence of
  vertices of the graph where for every edge \((x, y)\) in the graph,
\(x\) appears before \(y\) in the sequence.Theorem: Topological Sorts of DataflowAll topological sorts of a dataflow graph are computations.NextThe next pages describesCuts in Dataflow Graphswhich are used in developing detection algorithms such as termination
detection and deadlock detection.ExamplesFrequenty Asked QuestionsReviewK. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology



Figure Link: https://kmchandy.github.io/DISTRIBUTED_SYSTEM_MODELS/Dataflow.002.jpeg

Caption: Figure Example of a Dataflow Graph


The figure shows the dataflow graph for a computation of a
system consisting of two agentsuandv, and
with two channels(u, v)and(v, u).
The computation is defined by its initial state and event sequence [1,
2, 3, 4]. The initial states at agentsuandvare represented by outputs of stepsu_initandv_init, respectively, and final states by inputs tou_finiandv_fini, respectively.The top horizontal edges represent data flow between steps at agentuand the lower horizontal edges represent data flow
between steps at agentv.
For example, the labelsu1, u2, u3are the initial state
of agentu, the state ofubetween steps 1
and 4, and the state ofuafter step 4, respectively.
Likewise, the labelsv1, v2, v3are the states of
agentvinitially and after steps 2 and 3, respectively.Message edges are labeledm1, m2, m3, m4, m5.
Initially channel(u, v)contains messagem2and channel(v, u)contains messagem1.
Messagem3is sent in step 1 and received in step 3, and
messagem4is sent in step 3 and received in step 4.Vertex 1 in the graph, see figure 1 -- example of a vertex in a
dataflow graph -- represents the execution of a step at agentu.
This step is the execution of an event specified by the following
4-tuple:The state of \(u\) before the event isu1.The message received in the event ism1from agentv.The state  of \(u\) after the event isu2.A single messagem3is sent in the event and this
  message is  sent to agentv.The input and output edges of the vertex are the inputs and outputs
(respectively) of the event.Dataflow Graphs are AcyclicA dataflow graph of a computation is acyclic because each edge is directed from a step
to a later step of the computation.Flow of Data in a ComputationThe edges of the dataflow graph of a computation show the flow of data
into and out of each step of a
computation.
Data -- in the form of the agent's state -- flows from a step of an
agent to the next step at that agent.
Data -- in the form of message contents -- flows from a step in which
a message is sent to the step in which the message is received.The Relation: "data flows from" between Steps
of a ComputationWe define a relation "data flows from" between steps of a
computation 
as follows: for steps \(x, y\) of a computation, data flows from
\(x\) to \(y\) exactly when there is a path in the computation's
dataflow graph from \(x\) to \(y\).The set of steps of a computation with the relation "data flows from"
is astrict partial order.
We say that steps \(x\) and \(y\) areindependentif data does
not flow from  \(x\) to \(y\) or from \(y\) to \(x\).Example of data flows from a step to a
stepThe figure "Example -- Data Flows from Step 2 to Step 4" shows
examples of data flow as red edges in the graph.
Steps 1 and 2 are independent because there is no path from step 1 to
step 2, and there is no path from step 2 to step 1.Figure Example -- Data Flows from Step 2 to Step 4Computations and Dataflow GraphsLet a computation be specified by a sequence of steps \(E\), and
let \(E'\) be any permutation of \(E\).Data flows forwards in \(E\)means that for all steps
\(x\) and \(y\) in \(E\):If data flows from \(x\) to \(y\) in \(E\)
then \(x\) appears before \(y\) in
\(E'\).Next we prove properties about sequences of steps in which data flows forwards.
We start with the following lemma.Lemma: Switching Order of Adjacent
Independent Steps in a ComputationLet a computation be specified by  a sequence of
steps \(E\), and let \(E'\) be any permutation of \(E\)
obtained by switching the order of a pair of adjacent elements in
\(E\). 
If data flows forward in \(E'\) then \(E'\) is a computation.ProofAssume that the order of adjacent steps \(e_{i}\) and \(e_{i+1}\) are
switched in \(E\) to get \(E'\).
Because \(e_{i}\) precedes \(e_{i+1}\) in \(E\) there is no data flow
from \(e_{i+1}\) to \(e_{i}\).
Because data flows forwards in \(E'\) there is no data flow from
\(e_{i}\) to \(e_{i+1}\).Therefore \(e_{i}\) and \(e_{i+1}\) are adjacent independent steps.
So, the inputs to \(e_{i}\) and \(e_{i+1}\) remain the same
regardless of the order in which they are executed, and so \(E'\) is
also a computation.Example of Switching Adjacent Independent
StepsThe Figure "Example of Switching Adjacent Independent Steps" shows two
diagrams with identical dataflow graphs. The upper and diagrams are 
representations of computations starting at the same state and step
sequences [1, 2, 3, 4], and [2, 1, 3, 4].
The order in which vertices 1 and 2 appear, from left to right, in the
diagrams have been switched.
The graphs, however, are identical.The inputs to step 1 remain the same whether step 2 is executed
before or after step 1.
Likewise, the inputs to step 2 remain the same whether step 1 is executed
before or after step 2.Figure Example of Switching Adjacent Independent StepsTheorem: Permutations of Computations in which
Data Flows ForwardsGiven a computation specifed by a sequence of
steps \(E\), all permutations of \(E\) in which data flows forwards
are also computations.ProofLet \(E'\) be a permutation of \(E\) where data flows forwards in
\(E'\).
The theorem holds trivially if \(E\) and \(E'\) are identical.If \(E\) and \(E'\) are not identical then there exists an adjacent
pair of steps \(x, y\) in \(E\) where \(x\) occurs before \(y\) in
\(E\), and \(y\) occurs before \(x\) in \(E'\).Because \(E\) is a computation, data flows forwards in \(E\) and
therefore there is no data flow from \(y\) to \(x\).We are given that data flows forwards in \(E'\). So there is no
data flow from \(x\) to \(y\).Therefore \(x\) and \(y\) are independent.
Let \(H\) be the sequence of steps obtained from \(E\) by switching
the order of \(x\) and \(y\).
From the lemma "Switching Order of Independent Adjacent Steps" it
follows 
that \(H\) is also a computation.The number of out of order pairs between \(H\) and \(E'\) is less than
that between \(E\) and \(E'\).
By repeatedly switching of independent adjacent out of order pairs
the sequence \(E'\) is reached.Relationship to Topological SortsThe theorem on permutations of event sequences can be expressed in
terms of topological sorts.
Atopological sortof a directed acyclic graph is a sequence of
  vertices of the graph where for every edge \((x, y)\) in the graph,
\(x\) appears before \(y\) in the sequence.Theorem: Topological Sorts of DataflowAll topological sorts of a dataflow graph are computations.NextThe next pages describesCuts in Dataflow Graphswhich are used in developing detection algorithms such as termination
detection and deadlock detection.ExamplesFrequenty Asked QuestionsReviewK. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology



Figure Link: https://kmchandy.github.io/DISTRIBUTED_SYSTEM_MODELS/Dataflow.005.jpeg

Caption: Figure Example -- Data Flows from Step 2 to Step 4


The top horizontal edges represent data flow between steps at agentuand the lower horizontal edges represent data flow
between steps at agentv.
For example, the labelsu1, u2, u3are the initial state
of agentu, the state ofubetween steps 1
and 4, and the state ofuafter step 4, respectively.
Likewise, the labelsv1, v2, v3are the states of
agentvinitially and after steps 2 and 3, respectively.Message edges are labeledm1, m2, m3, m4, m5.
Initially channel(u, v)contains messagem2and channel(v, u)contains messagem1.
Messagem3is sent in step 1 and received in step 3, and
messagem4is sent in step 3 and received in step 4.Vertex 1 in the graph, see figure 1 -- example of a vertex in a
dataflow graph -- represents the execution of a step at agentu.
This step is the execution of an event specified by the following
4-tuple:The state of \(u\) before the event isu1.The message received in the event ism1from agentv.The state  of \(u\) after the event isu2.A single messagem3is sent in the event and this
  message is  sent to agentv.The input and output edges of the vertex are the inputs and outputs
(respectively) of the event.Dataflow Graphs are AcyclicA dataflow graph of a computation is acyclic because each edge is directed from a step
to a later step of the computation.Flow of Data in a ComputationThe edges of the dataflow graph of a computation show the flow of data
into and out of each step of a
computation.
Data -- in the form of the agent's state -- flows from a step of an
agent to the next step at that agent.
Data -- in the form of message contents -- flows from a step in which
a message is sent to the step in which the message is received.The Relation: "data flows from" between Steps
of a ComputationWe define a relation "data flows from" between steps of a
computation 
as follows: for steps \(x, y\) of a computation, data flows from
\(x\) to \(y\) exactly when there is a path in the computation's
dataflow graph from \(x\) to \(y\).The set of steps of a computation with the relation "data flows from"
is astrict partial order.
We say that steps \(x\) and \(y\) areindependentif data does
not flow from  \(x\) to \(y\) or from \(y\) to \(x\).Example of data flows from a step to a
stepThe figure "Example -- Data Flows from Step 2 to Step 4" shows
examples of data flow as red edges in the graph.
Steps 1 and 2 are independent because there is no path from step 1 to
step 2, and there is no path from step 2 to step 1.Figure Example -- Data Flows from Step 2 to Step 4Computations and Dataflow GraphsLet a computation be specified by a sequence of steps \(E\), and
let \(E'\) be any permutation of \(E\).Data flows forwards in \(E\)means that for all steps
\(x\) and \(y\) in \(E\):If data flows from \(x\) to \(y\) in \(E\)
then \(x\) appears before \(y\) in
\(E'\).Next we prove properties about sequences of steps in which data flows forwards.
We start with the following lemma.Lemma: Switching Order of Adjacent
Independent Steps in a ComputationLet a computation be specified by  a sequence of
steps \(E\), and let \(E'\) be any permutation of \(E\)
obtained by switching the order of a pair of adjacent elements in
\(E\). 
If data flows forward in \(E'\) then \(E'\) is a computation.ProofAssume that the order of adjacent steps \(e_{i}\) and \(e_{i+1}\) are
switched in \(E\) to get \(E'\).
Because \(e_{i}\) precedes \(e_{i+1}\) in \(E\) there is no data flow
from \(e_{i+1}\) to \(e_{i}\).
Because data flows forwards in \(E'\) there is no data flow from
\(e_{i}\) to \(e_{i+1}\).Therefore \(e_{i}\) and \(e_{i+1}\) are adjacent independent steps.
So, the inputs to \(e_{i}\) and \(e_{i+1}\) remain the same
regardless of the order in which they are executed, and so \(E'\) is
also a computation.Example of Switching Adjacent Independent
StepsThe Figure "Example of Switching Adjacent Independent Steps" shows two
diagrams with identical dataflow graphs. The upper and diagrams are 
representations of computations starting at the same state and step
sequences [1, 2, 3, 4], and [2, 1, 3, 4].
The order in which vertices 1 and 2 appear, from left to right, in the
diagrams have been switched.
The graphs, however, are identical.The inputs to step 1 remain the same whether step 2 is executed
before or after step 1.
Likewise, the inputs to step 2 remain the same whether step 1 is executed
before or after step 2.Figure Example of Switching Adjacent Independent StepsTheorem: Permutations of Computations in which
Data Flows ForwardsGiven a computation specifed by a sequence of
steps \(E\), all permutations of \(E\) in which data flows forwards
are also computations.ProofLet \(E'\) be a permutation of \(E\) where data flows forwards in
\(E'\).
The theorem holds trivially if \(E\) and \(E'\) are identical.If \(E\) and \(E'\) are not identical then there exists an adjacent
pair of steps \(x, y\) in \(E\) where \(x\) occurs before \(y\) in
\(E\), and \(y\) occurs before \(x\) in \(E'\).Because \(E\) is a computation, data flows forwards in \(E\) and
therefore there is no data flow from \(y\) to \(x\).We are given that data flows forwards in \(E'\). So there is no
data flow from \(x\) to \(y\).Therefore \(x\) and \(y\) are independent.
Let \(H\) be the sequence of steps obtained from \(E\) by switching
the order of \(x\) and \(y\).
From the lemma "Switching Order of Independent Adjacent Steps" it
follows 
that \(H\) is also a computation.The number of out of order pairs between \(H\) and \(E'\) is less than
that between \(E\) and \(E'\).
By repeatedly switching of independent adjacent out of order pairs
the sequence \(E'\) is reached.Relationship to Topological SortsThe theorem on permutations of event sequences can be expressed in
terms of topological sorts.
Atopological sortof a directed acyclic graph is a sequence of
  vertices of the graph where for every edge \((x, y)\) in the graph,
\(x\) appears before \(y\) in the sequence.Theorem: Topological Sorts of DataflowAll topological sorts of a dataflow graph are computations.NextThe next pages describesCuts in Dataflow Graphswhich are used in developing detection algorithms such as termination
detection and deadlock detection.ExamplesFrequenty Asked QuestionsReviewK. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology



Figure Link: https://kmchandy.github.io/DISTRIBUTED_SYSTEM_MODELS/Dataflow.005.jpeg

Caption: Figure Example -- Data Flows from Step 2 to Step 4


Message edges are labeledm1, m2, m3, m4, m5.
Initially channel(u, v)contains messagem2and channel(v, u)contains messagem1.
Messagem3is sent in step 1 and received in step 3, and
messagem4is sent in step 3 and received in step 4.Vertex 1 in the graph, see figure 1 -- example of a vertex in a
dataflow graph -- represents the execution of a step at agentu.
This step is the execution of an event specified by the following
4-tuple:The state of \(u\) before the event isu1.The message received in the event ism1from agentv.The state  of \(u\) after the event isu2.A single messagem3is sent in the event and this
  message is  sent to agentv.The input and output edges of the vertex are the inputs and outputs
(respectively) of the event.Dataflow Graphs are AcyclicA dataflow graph of a computation is acyclic because each edge is directed from a step
to a later step of the computation.Flow of Data in a ComputationThe edges of the dataflow graph of a computation show the flow of data
into and out of each step of a
computation.
Data -- in the form of the agent's state -- flows from a step of an
agent to the next step at that agent.
Data -- in the form of message contents -- flows from a step in which
a message is sent to the step in which the message is received.The Relation: "data flows from" between Steps
of a ComputationWe define a relation "data flows from" between steps of a
computation 
as follows: for steps \(x, y\) of a computation, data flows from
\(x\) to \(y\) exactly when there is a path in the computation's
dataflow graph from \(x\) to \(y\).The set of steps of a computation with the relation "data flows from"
is astrict partial order.
We say that steps \(x\) and \(y\) areindependentif data does
not flow from  \(x\) to \(y\) or from \(y\) to \(x\).Example of data flows from a step to a
stepThe figure "Example -- Data Flows from Step 2 to Step 4" shows
examples of data flow as red edges in the graph.
Steps 1 and 2 are independent because there is no path from step 1 to
step 2, and there is no path from step 2 to step 1.Figure Example -- Data Flows from Step 2 to Step 4Computations and Dataflow GraphsLet a computation be specified by a sequence of steps \(E\), and
let \(E'\) be any permutation of \(E\).Data flows forwards in \(E\)means that for all steps
\(x\) and \(y\) in \(E\):If data flows from \(x\) to \(y\) in \(E\)
then \(x\) appears before \(y\) in
\(E'\).Next we prove properties about sequences of steps in which data flows forwards.
We start with the following lemma.Lemma: Switching Order of Adjacent
Independent Steps in a ComputationLet a computation be specified by  a sequence of
steps \(E\), and let \(E'\) be any permutation of \(E\)
obtained by switching the order of a pair of adjacent elements in
\(E\). 
If data flows forward in \(E'\) then \(E'\) is a computation.ProofAssume that the order of adjacent steps \(e_{i}\) and \(e_{i+1}\) are
switched in \(E\) to get \(E'\).
Because \(e_{i}\) precedes \(e_{i+1}\) in \(E\) there is no data flow
from \(e_{i+1}\) to \(e_{i}\).
Because data flows forwards in \(E'\) there is no data flow from
\(e_{i}\) to \(e_{i+1}\).Therefore \(e_{i}\) and \(e_{i+1}\) are adjacent independent steps.
So, the inputs to \(e_{i}\) and \(e_{i+1}\) remain the same
regardless of the order in which they are executed, and so \(E'\) is
also a computation.Example of Switching Adjacent Independent
StepsThe Figure "Example of Switching Adjacent Independent Steps" shows two
diagrams with identical dataflow graphs. The upper and diagrams are 
representations of computations starting at the same state and step
sequences [1, 2, 3, 4], and [2, 1, 3, 4].
The order in which vertices 1 and 2 appear, from left to right, in the
diagrams have been switched.
The graphs, however, are identical.The inputs to step 1 remain the same whether step 2 is executed
before or after step 1.
Likewise, the inputs to step 2 remain the same whether step 1 is executed
before or after step 2.Figure Example of Switching Adjacent Independent StepsTheorem: Permutations of Computations in which
Data Flows ForwardsGiven a computation specifed by a sequence of
steps \(E\), all permutations of \(E\) in which data flows forwards
are also computations.ProofLet \(E'\) be a permutation of \(E\) where data flows forwards in
\(E'\).
The theorem holds trivially if \(E\) and \(E'\) are identical.If \(E\) and \(E'\) are not identical then there exists an adjacent
pair of steps \(x, y\) in \(E\) where \(x\) occurs before \(y\) in
\(E\), and \(y\) occurs before \(x\) in \(E'\).Because \(E\) is a computation, data flows forwards in \(E\) and
therefore there is no data flow from \(y\) to \(x\).We are given that data flows forwards in \(E'\). So there is no
data flow from \(x\) to \(y\).Therefore \(x\) and \(y\) are independent.
Let \(H\) be the sequence of steps obtained from \(E\) by switching
the order of \(x\) and \(y\).
From the lemma "Switching Order of Independent Adjacent Steps" it
follows 
that \(H\) is also a computation.The number of out of order pairs between \(H\) and \(E'\) is less than
that between \(E\) and \(E'\).
By repeatedly switching of independent adjacent out of order pairs
the sequence \(E'\) is reached.Relationship to Topological SortsThe theorem on permutations of event sequences can be expressed in
terms of topological sorts.
Atopological sortof a directed acyclic graph is a sequence of
  vertices of the graph where for every edge \((x, y)\) in the graph,
\(x\) appears before \(y\) in the sequence.Theorem: Topological Sorts of DataflowAll topological sorts of a dataflow graph are computations.NextThe next pages describesCuts in Dataflow Graphswhich are used in developing detection algorithms such as termination
detection and deadlock detection.ExamplesFrequenty Asked QuestionsReviewK. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology



Figure Link: https://kmchandy.github.io/DISTRIBUTED_SYSTEM_MODELS/Dataflow.005.jpeg

Caption: Figure Example -- Data Flows from Step 2 to Step 4


Vertex 1 in the graph, see figure 1 -- example of a vertex in a
dataflow graph -- represents the execution of a step at agentu.
This step is the execution of an event specified by the following
4-tuple:The state of \(u\) before the event isu1.The message received in the event ism1from agentv.The state  of \(u\) after the event isu2.A single messagem3is sent in the event and this
  message is  sent to agentv.The input and output edges of the vertex are the inputs and outputs
(respectively) of the event.Dataflow Graphs are AcyclicA dataflow graph of a computation is acyclic because each edge is directed from a step
to a later step of the computation.Flow of Data in a ComputationThe edges of the dataflow graph of a computation show the flow of data
into and out of each step of a
computation.
Data -- in the form of the agent's state -- flows from a step of an
agent to the next step at that agent.
Data -- in the form of message contents -- flows from a step in which
a message is sent to the step in which the message is received.The Relation: "data flows from" between Steps
of a ComputationWe define a relation "data flows from" between steps of a
computation 
as follows: for steps \(x, y\) of a computation, data flows from
\(x\) to \(y\) exactly when there is a path in the computation's
dataflow graph from \(x\) to \(y\).The set of steps of a computation with the relation "data flows from"
is astrict partial order.
We say that steps \(x\) and \(y\) areindependentif data does
not flow from  \(x\) to \(y\) or from \(y\) to \(x\).Example of data flows from a step to a
stepThe figure "Example -- Data Flows from Step 2 to Step 4" shows
examples of data flow as red edges in the graph.
Steps 1 and 2 are independent because there is no path from step 1 to
step 2, and there is no path from step 2 to step 1.Figure Example -- Data Flows from Step 2 to Step 4Computations and Dataflow GraphsLet a computation be specified by a sequence of steps \(E\), and
let \(E'\) be any permutation of \(E\).Data flows forwards in \(E\)means that for all steps
\(x\) and \(y\) in \(E\):If data flows from \(x\) to \(y\) in \(E\)
then \(x\) appears before \(y\) in
\(E'\).Next we prove properties about sequences of steps in which data flows forwards.
We start with the following lemma.Lemma: Switching Order of Adjacent
Independent Steps in a ComputationLet a computation be specified by  a sequence of
steps \(E\), and let \(E'\) be any permutation of \(E\)
obtained by switching the order of a pair of adjacent elements in
\(E\). 
If data flows forward in \(E'\) then \(E'\) is a computation.ProofAssume that the order of adjacent steps \(e_{i}\) and \(e_{i+1}\) are
switched in \(E\) to get \(E'\).
Because \(e_{i}\) precedes \(e_{i+1}\) in \(E\) there is no data flow
from \(e_{i+1}\) to \(e_{i}\).
Because data flows forwards in \(E'\) there is no data flow from
\(e_{i}\) to \(e_{i+1}\).Therefore \(e_{i}\) and \(e_{i+1}\) are adjacent independent steps.
So, the inputs to \(e_{i}\) and \(e_{i+1}\) remain the same
regardless of the order in which they are executed, and so \(E'\) is
also a computation.Example of Switching Adjacent Independent
StepsThe Figure "Example of Switching Adjacent Independent Steps" shows two
diagrams with identical dataflow graphs. The upper and diagrams are 
representations of computations starting at the same state and step
sequences [1, 2, 3, 4], and [2, 1, 3, 4].
The order in which vertices 1 and 2 appear, from left to right, in the
diagrams have been switched.
The graphs, however, are identical.The inputs to step 1 remain the same whether step 2 is executed
before or after step 1.
Likewise, the inputs to step 2 remain the same whether step 1 is executed
before or after step 2.Figure Example of Switching Adjacent Independent StepsTheorem: Permutations of Computations in which
Data Flows ForwardsGiven a computation specifed by a sequence of
steps \(E\), all permutations of \(E\) in which data flows forwards
are also computations.ProofLet \(E'\) be a permutation of \(E\) where data flows forwards in
\(E'\).
The theorem holds trivially if \(E\) and \(E'\) are identical.If \(E\) and \(E'\) are not identical then there exists an adjacent
pair of steps \(x, y\) in \(E\) where \(x\) occurs before \(y\) in
\(E\), and \(y\) occurs before \(x\) in \(E'\).Because \(E\) is a computation, data flows forwards in \(E\) and
therefore there is no data flow from \(y\) to \(x\).We are given that data flows forwards in \(E'\). So there is no
data flow from \(x\) to \(y\).Therefore \(x\) and \(y\) are independent.
Let \(H\) be the sequence of steps obtained from \(E\) by switching
the order of \(x\) and \(y\).
From the lemma "Switching Order of Independent Adjacent Steps" it
follows 
that \(H\) is also a computation.The number of out of order pairs between \(H\) and \(E'\) is less than
that between \(E\) and \(E'\).
By repeatedly switching of independent adjacent out of order pairs
the sequence \(E'\) is reached.Relationship to Topological SortsThe theorem on permutations of event sequences can be expressed in
terms of topological sorts.
Atopological sortof a directed acyclic graph is a sequence of
  vertices of the graph where for every edge \((x, y)\) in the graph,
\(x\) appears before \(y\) in the sequence.Theorem: Topological Sorts of DataflowAll topological sorts of a dataflow graph are computations.NextThe next pages describesCuts in Dataflow Graphswhich are used in developing detection algorithms such as termination
detection and deadlock detection.ExamplesFrequenty Asked QuestionsReviewK. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology



Figure Link: https://kmchandy.github.io/DISTRIBUTED_SYSTEM_MODELS/Dataflow.005.jpeg

Caption: Figure Example -- Data Flows from Step 2 to Step 4


The set of steps of a computation with the relation "data flows from"
is astrict partial order.
We say that steps \(x\) and \(y\) areindependentif data does
not flow from  \(x\) to \(y\) or from \(y\) to \(x\).Example of data flows from a step to a
stepThe figure "Example -- Data Flows from Step 2 to Step 4" shows
examples of data flow as red edges in the graph.
Steps 1 and 2 are independent because there is no path from step 1 to
step 2, and there is no path from step 2 to step 1.Figure Example -- Data Flows from Step 2 to Step 4Computations and Dataflow GraphsLet a computation be specified by a sequence of steps \(E\), and
let \(E'\) be any permutation of \(E\).Data flows forwards in \(E\)means that for all steps
\(x\) and \(y\) in \(E\):If data flows from \(x\) to \(y\) in \(E\)
then \(x\) appears before \(y\) in
\(E'\).Next we prove properties about sequences of steps in which data flows forwards.
We start with the following lemma.Lemma: Switching Order of Adjacent
Independent Steps in a ComputationLet a computation be specified by  a sequence of
steps \(E\), and let \(E'\) be any permutation of \(E\)
obtained by switching the order of a pair of adjacent elements in
\(E\). 
If data flows forward in \(E'\) then \(E'\) is a computation.ProofAssume that the order of adjacent steps \(e_{i}\) and \(e_{i+1}\) are
switched in \(E\) to get \(E'\).
Because \(e_{i}\) precedes \(e_{i+1}\) in \(E\) there is no data flow
from \(e_{i+1}\) to \(e_{i}\).
Because data flows forwards in \(E'\) there is no data flow from
\(e_{i}\) to \(e_{i+1}\).Therefore \(e_{i}\) and \(e_{i+1}\) are adjacent independent steps.
So, the inputs to \(e_{i}\) and \(e_{i+1}\) remain the same
regardless of the order in which they are executed, and so \(E'\) is
also a computation.Example of Switching Adjacent Independent
StepsThe Figure "Example of Switching Adjacent Independent Steps" shows two
diagrams with identical dataflow graphs. The upper and diagrams are 
representations of computations starting at the same state and step
sequences [1, 2, 3, 4], and [2, 1, 3, 4].
The order in which vertices 1 and 2 appear, from left to right, in the
diagrams have been switched.
The graphs, however, are identical.The inputs to step 1 remain the same whether step 2 is executed
before or after step 1.
Likewise, the inputs to step 2 remain the same whether step 1 is executed
before or after step 2.Figure Example of Switching Adjacent Independent StepsTheorem: Permutations of Computations in which
Data Flows ForwardsGiven a computation specifed by a sequence of
steps \(E\), all permutations of \(E\) in which data flows forwards
are also computations.ProofLet \(E'\) be a permutation of \(E\) where data flows forwards in
\(E'\).
The theorem holds trivially if \(E\) and \(E'\) are identical.If \(E\) and \(E'\) are not identical then there exists an adjacent
pair of steps \(x, y\) in \(E\) where \(x\) occurs before \(y\) in
\(E\), and \(y\) occurs before \(x\) in \(E'\).Because \(E\) is a computation, data flows forwards in \(E\) and
therefore there is no data flow from \(y\) to \(x\).We are given that data flows forwards in \(E'\). So there is no
data flow from \(x\) to \(y\).Therefore \(x\) and \(y\) are independent.
Let \(H\) be the sequence of steps obtained from \(E\) by switching
the order of \(x\) and \(y\).
From the lemma "Switching Order of Independent Adjacent Steps" it
follows 
that \(H\) is also a computation.The number of out of order pairs between \(H\) and \(E'\) is less than
that between \(E\) and \(E'\).
By repeatedly switching of independent adjacent out of order pairs
the sequence \(E'\) is reached.Relationship to Topological SortsThe theorem on permutations of event sequences can be expressed in
terms of topological sorts.
Atopological sortof a directed acyclic graph is a sequence of
  vertices of the graph where for every edge \((x, y)\) in the graph,
\(x\) appears before \(y\) in the sequence.Theorem: Topological Sorts of DataflowAll topological sorts of a dataflow graph are computations.NextThe next pages describesCuts in Dataflow Graphswhich are used in developing detection algorithms such as termination
detection and deadlock detection.ExamplesFrequenty Asked QuestionsReviewK. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology



Figure Link: https://kmchandy.github.io/DISTRIBUTED_SYSTEM_MODELS/Dataflow.005.jpeg

Caption: Figure Example -- Data Flows from Step 2 to Step 4


Next we prove properties about sequences of steps in which data flows forwards.
We start with the following lemma.Lemma: Switching Order of Adjacent
Independent Steps in a ComputationLet a computation be specified by  a sequence of
steps \(E\), and let \(E'\) be any permutation of \(E\)
obtained by switching the order of a pair of adjacent elements in
\(E\). 
If data flows forward in \(E'\) then \(E'\) is a computation.ProofAssume that the order of adjacent steps \(e_{i}\) and \(e_{i+1}\) are
switched in \(E\) to get \(E'\).
Because \(e_{i}\) precedes \(e_{i+1}\) in \(E\) there is no data flow
from \(e_{i+1}\) to \(e_{i}\).
Because data flows forwards in \(E'\) there is no data flow from
\(e_{i}\) to \(e_{i+1}\).Therefore \(e_{i}\) and \(e_{i+1}\) are adjacent independent steps.
So, the inputs to \(e_{i}\) and \(e_{i+1}\) remain the same
regardless of the order in which they are executed, and so \(E'\) is
also a computation.Example of Switching Adjacent Independent
StepsThe Figure "Example of Switching Adjacent Independent Steps" shows two
diagrams with identical dataflow graphs. The upper and diagrams are 
representations of computations starting at the same state and step
sequences [1, 2, 3, 4], and [2, 1, 3, 4].
The order in which vertices 1 and 2 appear, from left to right, in the
diagrams have been switched.
The graphs, however, are identical.The inputs to step 1 remain the same whether step 2 is executed
before or after step 1.
Likewise, the inputs to step 2 remain the same whether step 1 is executed
before or after step 2.Figure Example of Switching Adjacent Independent StepsTheorem: Permutations of Computations in which
Data Flows ForwardsGiven a computation specifed by a sequence of
steps \(E\), all permutations of \(E\) in which data flows forwards
are also computations.ProofLet \(E'\) be a permutation of \(E\) where data flows forwards in
\(E'\).
The theorem holds trivially if \(E\) and \(E'\) are identical.If \(E\) and \(E'\) are not identical then there exists an adjacent
pair of steps \(x, y\) in \(E\) where \(x\) occurs before \(y\) in
\(E\), and \(y\) occurs before \(x\) in \(E'\).Because \(E\) is a computation, data flows forwards in \(E\) and
therefore there is no data flow from \(y\) to \(x\).We are given that data flows forwards in \(E'\). So there is no
data flow from \(x\) to \(y\).Therefore \(x\) and \(y\) are independent.
Let \(H\) be the sequence of steps obtained from \(E\) by switching
the order of \(x\) and \(y\).
From the lemma "Switching Order of Independent Adjacent Steps" it
follows 
that \(H\) is also a computation.The number of out of order pairs between \(H\) and \(E'\) is less than
that between \(E\) and \(E'\).
By repeatedly switching of independent adjacent out of order pairs
the sequence \(E'\) is reached.Relationship to Topological SortsThe theorem on permutations of event sequences can be expressed in
terms of topological sorts.
Atopological sortof a directed acyclic graph is a sequence of
  vertices of the graph where for every edge \((x, y)\) in the graph,
\(x\) appears before \(y\) in the sequence.Theorem: Topological Sorts of DataflowAll topological sorts of a dataflow graph are computations.NextThe next pages describesCuts in Dataflow Graphswhich are used in developing detection algorithms such as termination
detection and deadlock detection.ExamplesFrequenty Asked QuestionsReviewK. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology



Figure Link: https://kmchandy.github.io/DISTRIBUTED_SYSTEM_MODELS/Dataflow.006.jpeg

Caption: Figure Example of Switching Adjacent Independent Steps


Proof



Figure Link: https://kmchandy.github.io/DISTRIBUTED_SYSTEM_MODELS/Dataflow.006.jpeg

Caption: Figure Example of Switching Adjacent Independent Steps


Therefore \(e_{i}\) and \(e_{i+1}\) are adjacent independent steps.
So, the inputs to \(e_{i}\) and \(e_{i+1}\) remain the same
regardless of the order in which they are executed, and so \(E'\) is
also a computation.Example of Switching Adjacent Independent
StepsThe Figure "Example of Switching Adjacent Independent Steps" shows two
diagrams with identical dataflow graphs. The upper and diagrams are 
representations of computations starting at the same state and step
sequences [1, 2, 3, 4], and [2, 1, 3, 4].
The order in which vertices 1 and 2 appear, from left to right, in the
diagrams have been switched.
The graphs, however, are identical.The inputs to step 1 remain the same whether step 2 is executed
before or after step 1.
Likewise, the inputs to step 2 remain the same whether step 1 is executed
before or after step 2.Figure Example of Switching Adjacent Independent StepsTheorem: Permutations of Computations in which
Data Flows ForwardsGiven a computation specifed by a sequence of
steps \(E\), all permutations of \(E\) in which data flows forwards
are also computations.ProofLet \(E'\) be a permutation of \(E\) where data flows forwards in
\(E'\).
The theorem holds trivially if \(E\) and \(E'\) are identical.If \(E\) and \(E'\) are not identical then there exists an adjacent
pair of steps \(x, y\) in \(E\) where \(x\) occurs before \(y\) in
\(E\), and \(y\) occurs before \(x\) in \(E'\).Because \(E\) is a computation, data flows forwards in \(E\) and
therefore there is no data flow from \(y\) to \(x\).We are given that data flows forwards in \(E'\). So there is no
data flow from \(x\) to \(y\).Therefore \(x\) and \(y\) are independent.
Let \(H\) be the sequence of steps obtained from \(E\) by switching
the order of \(x\) and \(y\).
From the lemma "Switching Order of Independent Adjacent Steps" it
follows 
that \(H\) is also a computation.The number of out of order pairs between \(H\) and \(E'\) is less than
that between \(E\) and \(E'\).
By repeatedly switching of independent adjacent out of order pairs
the sequence \(E'\) is reached.Relationship to Topological SortsThe theorem on permutations of event sequences can be expressed in
terms of topological sorts.
Atopological sortof a directed acyclic graph is a sequence of
  vertices of the graph where for every edge \((x, y)\) in the graph,
\(x\) appears before \(y\) in the sequence.Theorem: Topological Sorts of DataflowAll topological sorts of a dataflow graph are computations.NextThe next pages describesCuts in Dataflow Graphswhich are used in developing detection algorithms such as termination
detection and deadlock detection.ExamplesFrequenty Asked QuestionsReviewK. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology



Figure Link: https://kmchandy.github.io/DISTRIBUTED_SYSTEM_MODELS/Dataflow.006.jpeg

Caption: Figure Example of Switching Adjacent Independent Steps


The inputs to step 1 remain the same whether step 2 is executed
before or after step 1.
Likewise, the inputs to step 2 remain the same whether step 1 is executed
before or after step 2.Figure Example of Switching Adjacent Independent StepsTheorem: Permutations of Computations in which
Data Flows ForwardsGiven a computation specifed by a sequence of
steps \(E\), all permutations of \(E\) in which data flows forwards
are also computations.ProofLet \(E'\) be a permutation of \(E\) where data flows forwards in
\(E'\).
The theorem holds trivially if \(E\) and \(E'\) are identical.If \(E\) and \(E'\) are not identical then there exists an adjacent
pair of steps \(x, y\) in \(E\) where \(x\) occurs before \(y\) in
\(E\), and \(y\) occurs before \(x\) in \(E'\).Because \(E\) is a computation, data flows forwards in \(E\) and
therefore there is no data flow from \(y\) to \(x\).We are given that data flows forwards in \(E'\). So there is no
data flow from \(x\) to \(y\).Therefore \(x\) and \(y\) are independent.
Let \(H\) be the sequence of steps obtained from \(E\) by switching
the order of \(x\) and \(y\).
From the lemma "Switching Order of Independent Adjacent Steps" it
follows 
that \(H\) is also a computation.The number of out of order pairs between \(H\) and \(E'\) is less than
that between \(E\) and \(E'\).
By repeatedly switching of independent adjacent out of order pairs
the sequence \(E'\) is reached.Relationship to Topological SortsThe theorem on permutations of event sequences can be expressed in
terms of topological sorts.
Atopological sortof a directed acyclic graph is a sequence of
  vertices of the graph where for every edge \((x, y)\) in the graph,
\(x\) appears before \(y\) in the sequence.Theorem: Topological Sorts of DataflowAll topological sorts of a dataflow graph are computations.NextThe next pages describesCuts in Dataflow Graphswhich are used in developing detection algorithms such as termination
detection and deadlock detection.ExamplesFrequenty Asked QuestionsReviewK. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology



Figure Link: https://kmchandy.github.io/DISTRIBUTED_SYSTEM_MODELS/Dataflow.006.jpeg

Caption: Figure Example of Switching Adjacent Independent Steps


Given a computation specifed by a sequence of
steps \(E\), all permutations of \(E\) in which data flows forwards
are also computations.

If \(E\) and \(E'\) are not identical then there exists an adjacent
pair of steps \(x, y\) in \(E\) where \(x\) occurs before \(y\) in
\(E\), and \(y\) occurs before \(x\) in \(E'\).Because \(E\) is a computation, data flows forwards in \(E\) and
therefore there is no data flow from \(y\) to \(x\).We are given that data flows forwards in \(E'\). So there is no
data flow from \(x\) to \(y\).Therefore \(x\) and \(y\) are independent.
Let \(H\) be the sequence of steps obtained from \(E\) by switching
the order of \(x\) and \(y\).
From the lemma "Switching Order of Independent Adjacent Steps" it
follows 
that \(H\) is also a computation.The number of out of order pairs between \(H\) and \(E'\) is less than
that between \(E\) and \(E'\).
By repeatedly switching of independent adjacent out of order pairs
the sequence \(E'\) is reached.Relationship to Topological SortsThe theorem on permutations of event sequences can be expressed in
terms of topological sorts.
Atopological sortof a directed acyclic graph is a sequence of
  vertices of the graph where for every edge \((x, y)\) in the graph,
\(x\) appears before \(y\) in the sequence.Theorem: Topological Sorts of DataflowAll topological sorts of a dataflow graph are computations.NextThe next pages describesCuts in Dataflow Graphswhich are used in developing detection algorithms such as termination
detection and deadlock detection.ExamplesFrequenty Asked QuestionsReviewK. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

Because \(E\) is a computation, data flows forwards in \(E\) and
therefore there is no data flow from \(y\) to \(x\).We are given that data flows forwards in \(E'\). So there is no
data flow from \(x\) to \(y\).Therefore \(x\) and \(y\) are independent.
Let \(H\) be the sequence of steps obtained from \(E\) by switching
the order of \(x\) and \(y\).
From the lemma "Switching Order of Independent Adjacent Steps" it
follows 
that \(H\) is also a computation.The number of out of order pairs between \(H\) and \(E'\) is less than
that between \(E\) and \(E'\).
By repeatedly switching of independent adjacent out of order pairs
the sequence \(E'\) is reached.Relationship to Topological SortsThe theorem on permutations of event sequences can be expressed in
terms of topological sorts.
Atopological sortof a directed acyclic graph is a sequence of
  vertices of the graph where for every edge \((x, y)\) in the graph,
\(x\) appears before \(y\) in the sequence.Theorem: Topological Sorts of DataflowAll topological sorts of a dataflow graph are computations.NextThe next pages describesCuts in Dataflow Graphswhich are used in developing detection algorithms such as termination
detection and deadlock detection.ExamplesFrequenty Asked QuestionsReviewK. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

We are given that data flows forwards in \(E'\). So there is no
data flow from \(x\) to \(y\).Therefore \(x\) and \(y\) are independent.
Let \(H\) be the sequence of steps obtained from \(E\) by switching
the order of \(x\) and \(y\).
From the lemma "Switching Order of Independent Adjacent Steps" it
follows 
that \(H\) is also a computation.The number of out of order pairs between \(H\) and \(E'\) is less than
that between \(E\) and \(E'\).
By repeatedly switching of independent adjacent out of order pairs
the sequence \(E'\) is reached.Relationship to Topological SortsThe theorem on permutations of event sequences can be expressed in
terms of topological sorts.
Atopological sortof a directed acyclic graph is a sequence of
  vertices of the graph where for every edge \((x, y)\) in the graph,
\(x\) appears before \(y\) in the sequence.Theorem: Topological Sorts of DataflowAll topological sorts of a dataflow graph are computations.NextThe next pages describesCuts in Dataflow Graphswhich are used in developing detection algorithms such as termination
detection and deadlock detection.ExamplesFrequenty Asked QuestionsReviewK. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

Therefore \(x\) and \(y\) are independent.
Let \(H\) be the sequence of steps obtained from \(E\) by switching
the order of \(x\) and \(y\).
From the lemma "Switching Order of Independent Adjacent Steps" it
follows 
that \(H\) is also a computation.The number of out of order pairs between \(H\) and \(E'\) is less than
that between \(E\) and \(E'\).
By repeatedly switching of independent adjacent out of order pairs
the sequence \(E'\) is reached.Relationship to Topological SortsThe theorem on permutations of event sequences can be expressed in
terms of topological sorts.
Atopological sortof a directed acyclic graph is a sequence of
  vertices of the graph where for every edge \((x, y)\) in the graph,
\(x\) appears before \(y\) in the sequence.Theorem: Topological Sorts of DataflowAll topological sorts of a dataflow graph are computations.NextThe next pages describesCuts in Dataflow Graphswhich are used in developing detection algorithms such as termination
detection and deadlock detection.ExamplesFrequenty Asked QuestionsReviewK. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

The number of out of order pairs between \(H\) and \(E'\) is less than
that between \(E\) and \(E'\).
By repeatedly switching of independent adjacent out of order pairs
the sequence \(E'\) is reached.Relationship to Topological SortsThe theorem on permutations of event sequences can be expressed in
terms of topological sorts.
Atopological sortof a directed acyclic graph is a sequence of
  vertices of the graph where for every edge \((x, y)\) in the graph,
\(x\) appears before \(y\) in the sequence.Theorem: Topological Sorts of DataflowAll topological sorts of a dataflow graph are computations.NextThe next pages describesCuts in Dataflow Graphswhich are used in developing detection algorithms such as termination
detection and deadlock detection.ExamplesFrequenty Asked QuestionsReviewK. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

All topological sorts of a dataflow graph are computations.

Cuts in Dataflow Graphswhich are used in developing detection algorithms such as termination
detection and deadlock detection.ExamplesFrequenty Asked QuestionsReviewK. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

ExamplesFrequenty Asked QuestionsReviewK. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

Frequenty Asked QuestionsReviewK. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

ReviewK. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology
--- END Content from https://kmchandy.github.io/DISTRIBUTED_SYSTEM_MODELS/Model.html ---


--- START Content from https://kmchandy.github.io/DISTRIBUTED_SYSTEM_MODELS/ModelExamples.html ---
TITLE: DistributedSystemModels/ModelExamples.html

Each child has a countdown of the number of times it tosses balloons.
  Each time a child tosses a balloon it decrements its countdown value.
After a child's countdown reaches 0 the child pops balloons that it receives
(and doesn't toss popped balloons).StateThe state of a channel is the number of balloons in it.
The state of an agent (X or Y) is the countdown value nX or nY, respectively.A Computation: A Sequence of StatesAn example of a sequence of state changes is as follows.Initial State: Channel (X, Y) has one balloon and channel (Y, X) has 
  one balloon. And the countdown for X is 2 and the countdown for Y is
  3. We represent this state by:nX = 1, nY = 2, (X, Y) = 1, (Y, X) = 1The sequence of steps from the initial state are as follows.Y receives a balloon and sends it back. So the state
  becomes:nX = 1, nY = 1, (X, Y) = 0, (Y, X) = 2X receives a balloon and  sends it back. So the state
  becomesnX = 0, nY = 1, (X, Y) = 1, (Y, X) = 1Y receives a balloon and sends it back. So the state
  becomesnX = 0, nY = 0, (X, Y) = 0, (Y, X) = 2X receives a balloon and pops it. So the state
  becomesnX = 0, nY = 0, (X, Y) = 0, (Y, X) = 1X receives a balloon and pops it. So the state
  becomesnX = 0, nY = 0, (X, Y) = 0, (Y, X) = 0There are no further steps in this computation.Dataflow of the ComputationThe dataflow graph of the computation is shown in the figure.Figure The Dataflow Graph for the ExampleAgent X has a fictitious step, X_init, that initializes X's state and X's
  output channels. 
X also steps 2, 4, and 5 in the above computation.
X also has a fictitious final step, X_fini, that identifies X's final state and
the final state of X's input channels.Likewise, agent Y has a fictitious step, Y_init, that initializes Y's state and Y's
  output channels. 
Y also steps 3, and 5 in the above computation.
Y also has a fictitious final step, Y_fini, that identifies Y's final state and
the final state of Y's input channels.Agent Edges for Agent XThe horizontal line consisting of steps at X consists of the following
directed edges.X_init to step 2 with label nX = 1. The label is the initial state
  of XStep 2 to step 4 with label nx = 0.Step 4 to step 5 with label nx = 0.Step 5 to X_fini with label nx = 0. This label is the final state of
  X in the computation.Agent Edges for Agent YLikewise the horizontal line cconsisting of steps at Y and consists of the following
directed edges.Y_init to step 1 with label nY = 2. The label is the initial state of YStep 1 to step 3 with label nY = 1.Step 3 to Y_fini with label nY = 0. This label is the final state of
  Y in the computation.Message Edges for Channel (X, Y)The message edges representing messages on channel (X, Y) are as
follows.
All the messages have label "1" because each message represents a
single balloon.X_init to step 1. This is the message in the channel in the initial
  state of the computation.Step 2 to step 3.There are no message edges from steps at X to Y_fini because the channel (X, Y) is
empty in the final state.Message Edges for Channel (Y, X)The message edges representing messages on channel (Y, X) are as
follows.
All the messages have label "1" because each message represents a
single balloon.Y_init to step 2. This is the message in the channel in the initial
  state of the computation.Step 1 to step 4.Step 2 to step 5.There are no message edges to X_fini because the channel (Y, X) is
empty in the final state.Example fromPrevious PagesThe following dataflow diagram is for the computation given inexamples of computations.
The computation starting at the initial state executes stepsp1,n1,t1,p2,t2,n2,t3,X1where the steps are shown in the dataflow diagram.Figure The Dataflow DiagramIn the dataflow diagram,p1andp2are
steps at agentpos;n1andn2are
steps at agentneg,t1, t2, t3are
steps at agenttotal, andX1is an
step at agentX.Stepsp0, n0, t0, X0are the fictitious initial steps
at agentspos, neg, total, X(respectively)
that specify the initial state of the computation.
Likewise, stepsp*, n*, t*, X*are the fictitious final steps
that specify the final state of the computation.Let's look atp0as an example of an initial step.
There is an agent edge fromp0top1labeled
with the state of agentposin the initial state of the
computation.
The state ofposis given by its variablenwhich is initially 2; so the edge fromp0top1is labeledn = 2which is abbreviated to
2.Agent X has no state; it merely prints messages that it receives.
So agent edges at X have no label (or equivalently the empty label).Let's look atX*as an example of a final step.
There are message edges labeled 1, 0, and 5 from steps attotaltoX*.
This represents the state of channel(total, X*)in the
final state of the computation.Let's look at stepp1at agentpos.
The state ofposchanges fromn = 2ton = 1.
So the output agent edge fromp1is labeledn =
1.In stepp1,possends a message 3 tototaland a wakeup message to itself.
So the output message edges from stepp1are labeled
wakeup (shown as a star) and 3; these message edges are to steps stepp2and stept1, respectively, because the
messages are received in those steps.NextThe next page describesCuts in Dataflow Graphswhich are used in developing detection algorithms such as termination
detection and deadlock detection.ExamplesFrequenty Asked QuestionsK. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology



Figure Link: https://kmchandy.github.io/DISTRIBUTED_SYSTEM_MODELS/DataflowExamples.001.jpeg

Caption: Figure The Dataflow Graph for the Example


The sequence of steps from the initial state are as follows.Y receives a balloon and sends it back. So the state
  becomes:nX = 1, nY = 1, (X, Y) = 0, (Y, X) = 2X receives a balloon and  sends it back. So the state
  becomesnX = 0, nY = 1, (X, Y) = 1, (Y, X) = 1Y receives a balloon and sends it back. So the state
  becomesnX = 0, nY = 0, (X, Y) = 0, (Y, X) = 2X receives a balloon and pops it. So the state
  becomesnX = 0, nY = 0, (X, Y) = 0, (Y, X) = 1X receives a balloon and pops it. So the state
  becomesnX = 0, nY = 0, (X, Y) = 0, (Y, X) = 0There are no further steps in this computation.Dataflow of the ComputationThe dataflow graph of the computation is shown in the figure.Figure The Dataflow Graph for the ExampleAgent X has a fictitious step, X_init, that initializes X's state and X's
  output channels. 
X also steps 2, 4, and 5 in the above computation.
X also has a fictitious final step, X_fini, that identifies X's final state and
the final state of X's input channels.Likewise, agent Y has a fictitious step, Y_init, that initializes Y's state and Y's
  output channels. 
Y also steps 3, and 5 in the above computation.
Y also has a fictitious final step, Y_fini, that identifies Y's final state and
the final state of Y's input channels.Agent Edges for Agent XThe horizontal line consisting of steps at X consists of the following
directed edges.X_init to step 2 with label nX = 1. The label is the initial state
  of XStep 2 to step 4 with label nx = 0.Step 4 to step 5 with label nx = 0.Step 5 to X_fini with label nx = 0. This label is the final state of
  X in the computation.Agent Edges for Agent YLikewise the horizontal line cconsisting of steps at Y and consists of the following
directed edges.Y_init to step 1 with label nY = 2. The label is the initial state of YStep 1 to step 3 with label nY = 1.Step 3 to Y_fini with label nY = 0. This label is the final state of
  Y in the computation.Message Edges for Channel (X, Y)The message edges representing messages on channel (X, Y) are as
follows.
All the messages have label "1" because each message represents a
single balloon.X_init to step 1. This is the message in the channel in the initial
  state of the computation.Step 2 to step 3.There are no message edges from steps at X to Y_fini because the channel (X, Y) is
empty in the final state.Message Edges for Channel (Y, X)The message edges representing messages on channel (Y, X) are as
follows.
All the messages have label "1" because each message represents a
single balloon.Y_init to step 2. This is the message in the channel in the initial
  state of the computation.Step 1 to step 4.Step 2 to step 5.There are no message edges to X_fini because the channel (Y, X) is
empty in the final state.Example fromPrevious PagesThe following dataflow diagram is for the computation given inexamples of computations.
The computation starting at the initial state executes stepsp1,n1,t1,p2,t2,n2,t3,X1where the steps are shown in the dataflow diagram.Figure The Dataflow DiagramIn the dataflow diagram,p1andp2are
steps at agentpos;n1andn2are
steps at agentneg,t1, t2, t3are
steps at agenttotal, andX1is an
step at agentX.Stepsp0, n0, t0, X0are the fictitious initial steps
at agentspos, neg, total, X(respectively)
that specify the initial state of the computation.
Likewise, stepsp*, n*, t*, X*are the fictitious final steps
that specify the final state of the computation.Let's look atp0as an example of an initial step.
There is an agent edge fromp0top1labeled
with the state of agentposin the initial state of the
computation.
The state ofposis given by its variablenwhich is initially 2; so the edge fromp0top1is labeledn = 2which is abbreviated to
2.Agent X has no state; it merely prints messages that it receives.
So agent edges at X have no label (or equivalently the empty label).Let's look atX*as an example of a final step.
There are message edges labeled 1, 0, and 5 from steps attotaltoX*.
This represents the state of channel(total, X*)in the
final state of the computation.Let's look at stepp1at agentpos.
The state ofposchanges fromn = 2ton = 1.
So the output agent edge fromp1is labeledn =
1.In stepp1,possends a message 3 tototaland a wakeup message to itself.
So the output message edges from stepp1are labeled
wakeup (shown as a star) and 3; these message edges are to steps stepp2and stept1, respectively, because the
messages are received in those steps.NextThe next page describesCuts in Dataflow Graphswhich are used in developing detection algorithms such as termination
detection and deadlock detection.ExamplesFrequenty Asked QuestionsK. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology



Figure Link: https://kmchandy.github.io/DISTRIBUTED_SYSTEM_MODELS/DataflowExamples.001.jpeg

Caption: Figure The Dataflow Graph for the Example


Likewise, agent Y has a fictitious step, Y_init, that initializes Y's state and Y's
  output channels. 
Y also steps 3, and 5 in the above computation.
Y also has a fictitious final step, Y_fini, that identifies Y's final state and
the final state of Y's input channels.Agent Edges for Agent XThe horizontal line consisting of steps at X consists of the following
directed edges.X_init to step 2 with label nX = 1. The label is the initial state
  of XStep 2 to step 4 with label nx = 0.Step 4 to step 5 with label nx = 0.Step 5 to X_fini with label nx = 0. This label is the final state of
  X in the computation.Agent Edges for Agent YLikewise the horizontal line cconsisting of steps at Y and consists of the following
directed edges.Y_init to step 1 with label nY = 2. The label is the initial state of YStep 1 to step 3 with label nY = 1.Step 3 to Y_fini with label nY = 0. This label is the final state of
  Y in the computation.Message Edges for Channel (X, Y)The message edges representing messages on channel (X, Y) are as
follows.
All the messages have label "1" because each message represents a
single balloon.X_init to step 1. This is the message in the channel in the initial
  state of the computation.Step 2 to step 3.There are no message edges from steps at X to Y_fini because the channel (X, Y) is
empty in the final state.Message Edges for Channel (Y, X)The message edges representing messages on channel (Y, X) are as
follows.
All the messages have label "1" because each message represents a
single balloon.Y_init to step 2. This is the message in the channel in the initial
  state of the computation.Step 1 to step 4.Step 2 to step 5.There are no message edges to X_fini because the channel (Y, X) is
empty in the final state.Example fromPrevious PagesThe following dataflow diagram is for the computation given inexamples of computations.
The computation starting at the initial state executes stepsp1,n1,t1,p2,t2,n2,t3,X1where the steps are shown in the dataflow diagram.Figure The Dataflow DiagramIn the dataflow diagram,p1andp2are
steps at agentpos;n1andn2are
steps at agentneg,t1, t2, t3are
steps at agenttotal, andX1is an
step at agentX.Stepsp0, n0, t0, X0are the fictitious initial steps
at agentspos, neg, total, X(respectively)
that specify the initial state of the computation.
Likewise, stepsp*, n*, t*, X*are the fictitious final steps
that specify the final state of the computation.Let's look atp0as an example of an initial step.
There is an agent edge fromp0top1labeled
with the state of agentposin the initial state of the
computation.
The state ofposis given by its variablenwhich is initially 2; so the edge fromp0top1is labeledn = 2which is abbreviated to
2.Agent X has no state; it merely prints messages that it receives.
So agent edges at X have no label (or equivalently the empty label).Let's look atX*as an example of a final step.
There are message edges labeled 1, 0, and 5 from steps attotaltoX*.
This represents the state of channel(total, X*)in the
final state of the computation.Let's look at stepp1at agentpos.
The state ofposchanges fromn = 2ton = 1.
So the output agent edge fromp1is labeledn =
1.In stepp1,possends a message 3 tototaland a wakeup message to itself.
So the output message edges from stepp1are labeled
wakeup (shown as a star) and 3; these message edges are to steps stepp2and stept1, respectively, because the
messages are received in those steps.NextThe next page describesCuts in Dataflow Graphswhich are used in developing detection algorithms such as termination
detection and deadlock detection.ExamplesFrequenty Asked QuestionsK. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology



Figure Link: https://kmchandy.github.io/DISTRIBUTED_SYSTEM_MODELS/DataflowExamples.002.jpeg

Caption: Figure The Dataflow Diagram


The horizontal line consisting of steps at X consists of the following
directed edges.X_init to step 2 with label nX = 1. The label is the initial state
  of XStep 2 to step 4 with label nx = 0.Step 4 to step 5 with label nx = 0.Step 5 to X_fini with label nx = 0. This label is the final state of
  X in the computation.Agent Edges for Agent YLikewise the horizontal line cconsisting of steps at Y and consists of the following
directed edges.Y_init to step 1 with label nY = 2. The label is the initial state of YStep 1 to step 3 with label nY = 1.Step 3 to Y_fini with label nY = 0. This label is the final state of
  Y in the computation.Message Edges for Channel (X, Y)The message edges representing messages on channel (X, Y) are as
follows.
All the messages have label "1" because each message represents a
single balloon.X_init to step 1. This is the message in the channel in the initial
  state of the computation.Step 2 to step 3.There are no message edges from steps at X to Y_fini because the channel (X, Y) is
empty in the final state.Message Edges for Channel (Y, X)The message edges representing messages on channel (Y, X) are as
follows.
All the messages have label "1" because each message represents a
single balloon.Y_init to step 2. This is the message in the channel in the initial
  state of the computation.Step 1 to step 4.Step 2 to step 5.There are no message edges to X_fini because the channel (Y, X) is
empty in the final state.Example fromPrevious PagesThe following dataflow diagram is for the computation given inexamples of computations.
The computation starting at the initial state executes stepsp1,n1,t1,p2,t2,n2,t3,X1where the steps are shown in the dataflow diagram.Figure The Dataflow DiagramIn the dataflow diagram,p1andp2are
steps at agentpos;n1andn2are
steps at agentneg,t1, t2, t3are
steps at agenttotal, andX1is an
step at agentX.Stepsp0, n0, t0, X0are the fictitious initial steps
at agentspos, neg, total, X(respectively)
that specify the initial state of the computation.
Likewise, stepsp*, n*, t*, X*are the fictitious final steps
that specify the final state of the computation.Let's look atp0as an example of an initial step.
There is an agent edge fromp0top1labeled
with the state of agentposin the initial state of the
computation.
The state ofposis given by its variablenwhich is initially 2; so the edge fromp0top1is labeledn = 2which is abbreviated to
2.Agent X has no state; it merely prints messages that it receives.
So agent edges at X have no label (or equivalently the empty label).Let's look atX*as an example of a final step.
There are message edges labeled 1, 0, and 5 from steps attotaltoX*.
This represents the state of channel(total, X*)in the
final state of the computation.Let's look at stepp1at agentpos.
The state ofposchanges fromn = 2ton = 1.
So the output agent edge fromp1is labeledn =
1.In stepp1,possends a message 3 tototaland a wakeup message to itself.
So the output message edges from stepp1are labeled
wakeup (shown as a star) and 3; these message edges are to steps stepp2and stept1, respectively, because the
messages are received in those steps.NextThe next page describesCuts in Dataflow Graphswhich are used in developing detection algorithms such as termination
detection and deadlock detection.ExamplesFrequenty Asked QuestionsK. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology



Figure Link: https://kmchandy.github.io/DISTRIBUTED_SYSTEM_MODELS/DataflowExamples.002.jpeg

Caption: Figure The Dataflow Diagram


The message edges representing messages on channel (X, Y) are as
follows.
All the messages have label "1" because each message represents a
single balloon.X_init to step 1. This is the message in the channel in the initial
  state of the computation.Step 2 to step 3.There are no message edges from steps at X to Y_fini because the channel (X, Y) is
empty in the final state.Message Edges for Channel (Y, X)The message edges representing messages on channel (Y, X) are as
follows.
All the messages have label "1" because each message represents a
single balloon.Y_init to step 2. This is the message in the channel in the initial
  state of the computation.Step 1 to step 4.Step 2 to step 5.There are no message edges to X_fini because the channel (Y, X) is
empty in the final state.Example fromPrevious PagesThe following dataflow diagram is for the computation given inexamples of computations.
The computation starting at the initial state executes stepsp1,n1,t1,p2,t2,n2,t3,X1where the steps are shown in the dataflow diagram.Figure The Dataflow DiagramIn the dataflow diagram,p1andp2are
steps at agentpos;n1andn2are
steps at agentneg,t1, t2, t3are
steps at agenttotal, andX1is an
step at agentX.Stepsp0, n0, t0, X0are the fictitious initial steps
at agentspos, neg, total, X(respectively)
that specify the initial state of the computation.
Likewise, stepsp*, n*, t*, X*are the fictitious final steps
that specify the final state of the computation.Let's look atp0as an example of an initial step.
There is an agent edge fromp0top1labeled
with the state of agentposin the initial state of the
computation.
The state ofposis given by its variablenwhich is initially 2; so the edge fromp0top1is labeledn = 2which is abbreviated to
2.Agent X has no state; it merely prints messages that it receives.
So agent edges at X have no label (or equivalently the empty label).Let's look atX*as an example of a final step.
There are message edges labeled 1, 0, and 5 from steps attotaltoX*.
This represents the state of channel(total, X*)in the
final state of the computation.Let's look at stepp1at agentpos.
The state ofposchanges fromn = 2ton = 1.
So the output agent edge fromp1is labeledn =
1.In stepp1,possends a message 3 tototaland a wakeup message to itself.
So the output message edges from stepp1are labeled
wakeup (shown as a star) and 3; these message edges are to steps stepp2and stept1, respectively, because the
messages are received in those steps.NextThe next page describesCuts in Dataflow Graphswhich are used in developing detection algorithms such as termination
detection and deadlock detection.ExamplesFrequenty Asked QuestionsK. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology



Figure Link: https://kmchandy.github.io/DISTRIBUTED_SYSTEM_MODELS/DataflowExamples.002.jpeg

Caption: Figure The Dataflow Diagram


The message edges representing messages on channel (Y, X) are as
follows.
All the messages have label "1" because each message represents a
single balloon.Y_init to step 2. This is the message in the channel in the initial
  state of the computation.Step 1 to step 4.Step 2 to step 5.There are no message edges to X_fini because the channel (Y, X) is
empty in the final state.Example fromPrevious PagesThe following dataflow diagram is for the computation given inexamples of computations.
The computation starting at the initial state executes stepsp1,n1,t1,p2,t2,n2,t3,X1where the steps are shown in the dataflow diagram.Figure The Dataflow DiagramIn the dataflow diagram,p1andp2are
steps at agentpos;n1andn2are
steps at agentneg,t1, t2, t3are
steps at agenttotal, andX1is an
step at agentX.Stepsp0, n0, t0, X0are the fictitious initial steps
at agentspos, neg, total, X(respectively)
that specify the initial state of the computation.
Likewise, stepsp*, n*, t*, X*are the fictitious final steps
that specify the final state of the computation.Let's look atp0as an example of an initial step.
There is an agent edge fromp0top1labeled
with the state of agentposin the initial state of the
computation.
The state ofposis given by its variablenwhich is initially 2; so the edge fromp0top1is labeledn = 2which is abbreviated to
2.Agent X has no state; it merely prints messages that it receives.
So agent edges at X have no label (or equivalently the empty label).Let's look atX*as an example of a final step.
There are message edges labeled 1, 0, and 5 from steps attotaltoX*.
This represents the state of channel(total, X*)in the
final state of the computation.Let's look at stepp1at agentpos.
The state ofposchanges fromn = 2ton = 1.
So the output agent edge fromp1is labeledn =
1.In stepp1,possends a message 3 tototaland a wakeup message to itself.
So the output message edges from stepp1are labeled
wakeup (shown as a star) and 3; these message edges are to steps stepp2and stept1, respectively, because the
messages are received in those steps.NextThe next page describesCuts in Dataflow Graphswhich are used in developing detection algorithms such as termination
detection and deadlock detection.ExamplesFrequenty Asked QuestionsK. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology



Figure Link: https://kmchandy.github.io/DISTRIBUTED_SYSTEM_MODELS/DataflowExamples.002.jpeg

Caption: Figure The Dataflow Diagram


Stepsp0, n0, t0, X0are the fictitious initial steps
at agentspos, neg, total, X(respectively)
that specify the initial state of the computation.
Likewise, stepsp*, n*, t*, X*are the fictitious final steps
that specify the final state of the computation.Let's look atp0as an example of an initial step.
There is an agent edge fromp0top1labeled
with the state of agentposin the initial state of the
computation.
The state ofposis given by its variablenwhich is initially 2; so the edge fromp0top1is labeledn = 2which is abbreviated to
2.Agent X has no state; it merely prints messages that it receives.
So agent edges at X have no label (or equivalently the empty label).Let's look atX*as an example of a final step.
There are message edges labeled 1, 0, and 5 from steps attotaltoX*.
This represents the state of channel(total, X*)in the
final state of the computation.Let's look at stepp1at agentpos.
The state ofposchanges fromn = 2ton = 1.
So the output agent edge fromp1is labeledn =
1.In stepp1,possends a message 3 tototaland a wakeup message to itself.
So the output message edges from stepp1are labeled
wakeup (shown as a star) and 3; these message edges are to steps stepp2and stept1, respectively, because the
messages are received in those steps.NextThe next page describesCuts in Dataflow Graphswhich are used in developing detection algorithms such as termination
detection and deadlock detection.ExamplesFrequenty Asked QuestionsK. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

Let's look atp0as an example of an initial step.
There is an agent edge fromp0top1labeled
with the state of agentposin the initial state of the
computation.
The state ofposis given by its variablenwhich is initially 2; so the edge fromp0top1is labeledn = 2which is abbreviated to
2.Agent X has no state; it merely prints messages that it receives.
So agent edges at X have no label (or equivalently the empty label).Let's look atX*as an example of a final step.
There are message edges labeled 1, 0, and 5 from steps attotaltoX*.
This represents the state of channel(total, X*)in the
final state of the computation.Let's look at stepp1at agentpos.
The state ofposchanges fromn = 2ton = 1.
So the output agent edge fromp1is labeledn =
1.In stepp1,possends a message 3 tototaland a wakeup message to itself.
So the output message edges from stepp1are labeled
wakeup (shown as a star) and 3; these message edges are to steps stepp2and stept1, respectively, because the
messages are received in those steps.NextThe next page describesCuts in Dataflow Graphswhich are used in developing detection algorithms such as termination
detection and deadlock detection.ExamplesFrequenty Asked QuestionsK. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

Agent X has no state; it merely prints messages that it receives.
So agent edges at X have no label (or equivalently the empty label).Let's look atX*as an example of a final step.
There are message edges labeled 1, 0, and 5 from steps attotaltoX*.
This represents the state of channel(total, X*)in the
final state of the computation.Let's look at stepp1at agentpos.
The state ofposchanges fromn = 2ton = 1.
So the output agent edge fromp1is labeledn =
1.In stepp1,possends a message 3 tototaland a wakeup message to itself.
So the output message edges from stepp1are labeled
wakeup (shown as a star) and 3; these message edges are to steps stepp2and stept1, respectively, because the
messages are received in those steps.NextThe next page describesCuts in Dataflow Graphswhich are used in developing detection algorithms such as termination
detection and deadlock detection.ExamplesFrequenty Asked QuestionsK. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

Let's look atX*as an example of a final step.
There are message edges labeled 1, 0, and 5 from steps attotaltoX*.
This represents the state of channel(total, X*)in the
final state of the computation.Let's look at stepp1at agentpos.
The state ofposchanges fromn = 2ton = 1.
So the output agent edge fromp1is labeledn =
1.In stepp1,possends a message 3 tototaland a wakeup message to itself.
So the output message edges from stepp1are labeled
wakeup (shown as a star) and 3; these message edges are to steps stepp2and stept1, respectively, because the
messages are received in those steps.NextThe next page describesCuts in Dataflow Graphswhich are used in developing detection algorithms such as termination
detection and deadlock detection.ExamplesFrequenty Asked QuestionsK. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

Let's look at stepp1at agentpos.
The state ofposchanges fromn = 2ton = 1.
So the output agent edge fromp1is labeledn =
1.In stepp1,possends a message 3 tototaland a wakeup message to itself.
So the output message edges from stepp1are labeled
wakeup (shown as a star) and 3; these message edges are to steps stepp2and stept1, respectively, because the
messages are received in those steps.NextThe next page describesCuts in Dataflow Graphswhich are used in developing detection algorithms such as termination
detection and deadlock detection.ExamplesFrequenty Asked QuestionsK. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

In stepp1,possends a message 3 tototaland a wakeup message to itself.
So the output message edges from stepp1are labeled
wakeup (shown as a star) and 3; these message edges are to steps stepp2and stept1, respectively, because the
messages are received in those steps.NextThe next page describesCuts in Dataflow Graphswhich are used in developing detection algorithms such as termination
detection and deadlock detection.ExamplesFrequenty Asked QuestionsK. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

Cuts in Dataflow Graphswhich are used in developing detection algorithms such as termination
detection and deadlock detection.ExamplesFrequenty Asked QuestionsK. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

ExamplesFrequenty Asked QuestionsK. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

Frequenty Asked QuestionsK. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology
--- END Content from https://kmchandy.github.io/DISTRIBUTED_SYSTEM_MODELS/ModelExamples.html ---


--- START Content from https://kmchandy.github.io/DISTRIBUTED_SYSTEM_MODELS/ModelFAQ.html ---
TITLE: DistributedSystemModels/ModelFAQ.html

A final vertex has no output edges; it only has input edges.
  The input edges of a final vertex are exactly the same as
  the input edges of the vertices that represent real (as opposed to
  fictional) steps.Exactly how are initial states of channels
  represented? If there are two messages m1, m2 in a channel from u to
  v and the messages are received by v in steps 1 and 2 then how is
  this represented in a dataflow graph?The initial states of channels from an agent u are represented by
  message edges from the vertex representing the fictitious
  initialization step of agent u.In the example, there are two message edges from the initialization
  vertex of u. The first message edge is to the
  vertex representing step 1 of v, and this edge is labeled m1.
  The second message is to the vertex representing step 1 of v.How many dataflow graphs are associated
  with a computation? How many computations are associated with a
  dataflow graph?A computation hasexactly onedataflow graph that shows the flow of
  data in the computation.All topological sorts of dataflow graphs are computations.
  The number of computations that a dataflow graph represents is the
  number of topological sorts of the dataflow graph.What is the number of topological sorts of a dataflow graph? That
  depends on the graph.
  You can construct a graph with 1 topological sort and a graph with
  n! topological sorts where n is the number of vertices.K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

In the example, there are two message edges from the initialization
  vertex of u. The first message edge is to the
  vertex representing step 1 of v, and this edge is labeled m1.
  The second message is to the vertex representing step 1 of v.How many dataflow graphs are associated
  with a computation? How many computations are associated with a
  dataflow graph?A computation hasexactly onedataflow graph that shows the flow of
  data in the computation.All topological sorts of dataflow graphs are computations.
  The number of computations that a dataflow graph represents is the
  number of topological sorts of the dataflow graph.What is the number of topological sorts of a dataflow graph? That
  depends on the graph.
  You can construct a graph with 1 topological sort and a graph with
  n! topological sorts where n is the number of vertices.K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

All topological sorts of dataflow graphs are computations.
  The number of computations that a dataflow graph represents is the
  number of topological sorts of the dataflow graph.What is the number of topological sorts of a dataflow graph? That
  depends on the graph.
  You can construct a graph with 1 topological sort and a graph with
  n! topological sorts where n is the number of vertices.K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

What is the number of topological sorts of a dataflow graph? That
  depends on the graph.
  You can construct a graph with 1 topological sort and a graph with
  n! topological sorts where n is the number of vertices.K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology
--- END Content from https://kmchandy.github.io/DISTRIBUTED_SYSTEM_MODELS/ModelFAQ.html ---


--- START Content from https://kmchandy.github.io/DISTRIBUTED_SYSTEM_MODELS/ModelReview.html ---
TITLE: DistributedSystemModels/ModelReview.html

We use dataflow in analyzing several algorithms.
  
  We use timelines to discuss performance of a few algorithms.
  This review emphasizes dataflow.What is an event?How is an event related to an execution of areceivestatement by an agent?What are the inputs and outputs of an event?What are the vertices of a dataflow graph?What are the edges of a dataflow graph?How do edges of a dataflow graph of a computation represent
    the flow of data between steps of the computation?What is the concept of time in dataflow?What is the difference betweeneventandstep?What is the relationship between computations and dataflow
    graphs?What is a topological sort of a directed acyclic graph?What is the key theorem relating computations and dataflow graphs?NextThe next pages describesCuts in Dataflow Graphswhich are used in developing detection algorithms such as termination
detection and deadlock detection.ExamplesFrequenty Asked QuestionsReviewK. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

Cuts in Dataflow Graphswhich are used in developing detection algorithms such as termination
detection and deadlock detection.ExamplesFrequenty Asked QuestionsReviewK. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

ExamplesFrequenty Asked QuestionsReviewK. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

Frequenty Asked QuestionsReviewK. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

ReviewK. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology
--- END Content from https://kmchandy.github.io/DISTRIBUTED_SYSTEM_MODELS/ModelReview.html ---


--- START Content from https://kmchandy.github.io/DISTRIBUTED_SYSTEM_MODELS/Timelines.html ---
TITLE: DistributedSystemModels/Timelines.html

This page introduces the concept of acutof a dataflow
  graph and relates a cut to the only notions of time -- past and
  future -- in dataflow.
  A cut separates past from future.
  Cuts of dataflow are central to understanding detection algorithms
  such as deadlock detection and for global snapshot algorithms that
  determine states of distributed systems.



Figure Link: https://kmchandy.github.io/DISTRIBUTED_SYSTEM_MODELS/Model/Model.005.jpeg

Caption: Fig.1: Example - A Cut of a Dataflow Graph


Equivalently, acut of a computationis a subset of steps such that
  for every step \(e\) in the cut, all steps from which data flows to
  \(e\) are also in the cut.A cut of a dataflow graph is an instance of acut of a
  flow network.ExampleThe top diagram of figure 1 shows a cut in which vertices incutare 
  colored red and vertices not incutare green.
  The curved black line is the boundary separatingcutfrom vertices outside it.Fig.1: Example - A Cut of a Dataflow GraphTheorem: Computations and CutsLet \(E\) be a computation and \(C\) be a cut of the computation.
There exists a computation \(E'\), which is a permutation of \(E\), in
which all steps in the cut are executed before any step that is not in
the cut.ProofObtain \(E'\) from \(E\) as follows.
Letprebe the sequence of steps obtained from \(E\) by
deleting all steps in the cut.
Letpostbe the sequence of steps obtained from \(E\) by
deleting all steps not in the cut.
Then \(E'\) consists ofprefollowed bypost.The proof that data flows forwards in \(E'\) is straightforward.Theorem: State at a CutThe state after execution of steps in a
  cut and before execution of steps that are not in 
  cut steps is given by the labels of
edges from vertices in the cut to vertices outside the cut.See the lower diagram in figure 1.
The stateS*after the cut is shown as final vertices (which are labeledN) ofcutandS*is shown as as initial vertices (which
  are labeled0) of steps outsidecut.NextThe properties of cuts in dataflow, and the concepts of before - after,
past - future are central foralgorithms by
which agents record the 
states of distributed systemsdiscussed in the next chapter.K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology



Figure Link: https://kmchandy.github.io/DISTRIBUTED_SYSTEM_MODELS/Model/Model.005.jpeg

Caption: Fig.1: Example - A Cut of a Dataflow Graph


A cut of a dataflow graph is an instance of acut of a
  flow network.ExampleThe top diagram of figure 1 shows a cut in which vertices incutare 
  colored red and vertices not incutare green.
  The curved black line is the boundary separatingcutfrom vertices outside it.Fig.1: Example - A Cut of a Dataflow GraphTheorem: Computations and CutsLet \(E\) be a computation and \(C\) be a cut of the computation.
There exists a computation \(E'\), which is a permutation of \(E\), in
which all steps in the cut are executed before any step that is not in
the cut.ProofObtain \(E'\) from \(E\) as follows.
Letprebe the sequence of steps obtained from \(E\) by
deleting all steps in the cut.
Letpostbe the sequence of steps obtained from \(E\) by
deleting all steps not in the cut.
Then \(E'\) consists ofprefollowed bypost.The proof that data flows forwards in \(E'\) is straightforward.Theorem: State at a CutThe state after execution of steps in a
  cut and before execution of steps that are not in 
  cut steps is given by the labels of
edges from vertices in the cut to vertices outside the cut.See the lower diagram in figure 1.
The stateS*after the cut is shown as final vertices (which are labeledN) ofcutandS*is shown as as initial vertices (which
  are labeled0) of steps outsidecut.NextThe properties of cuts in dataflow, and the concepts of before - after,
past - future are central foralgorithms by
which agents record the 
states of distributed systemsdiscussed in the next chapter.K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology



Figure Link: https://kmchandy.github.io/DISTRIBUTED_SYSTEM_MODELS/Model/Model.005.jpeg

Caption: Fig.1: Example - A Cut of a Dataflow Graph


Example



Figure Link: https://kmchandy.github.io/DISTRIBUTED_SYSTEM_MODELS/Model/Model.005.jpeg

Caption: Fig.1: Example - A Cut of a Dataflow Graph


Proof

The proof that data flows forwards in \(E'\) is straightforward.Theorem: State at a CutThe state after execution of steps in a
  cut and before execution of steps that are not in 
  cut steps is given by the labels of
edges from vertices in the cut to vertices outside the cut.See the lower diagram in figure 1.
The stateS*after the cut is shown as final vertices (which are labeledN) ofcutandS*is shown as as initial vertices (which
  are labeled0) of steps outsidecut.NextThe properties of cuts in dataflow, and the concepts of before - after,
past - future are central foralgorithms by
which agents record the 
states of distributed systemsdiscussed in the next chapter.K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

The state after execution of steps in a
  cut and before execution of steps that are not in 
  cut steps is given by the labels of
edges from vertices in the cut to vertices outside the cut.See the lower diagram in figure 1.
The stateS*after the cut is shown as final vertices (which are labeledN) ofcutandS*is shown as as initial vertices (which
  are labeled0) of steps outsidecut.NextThe properties of cuts in dataflow, and the concepts of before - after,
past - future are central foralgorithms by
which agents record the 
states of distributed systemsdiscussed in the next chapter.K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

See the lower diagram in figure 1.
The stateS*after the cut is shown as final vertices (which are labeledN) ofcutandS*is shown as as initial vertices (which
  are labeled0) of steps outsidecut.NextThe properties of cuts in dataflow, and the concepts of before - after,
past - future are central foralgorithms by
which agents record the 
states of distributed systemsdiscussed in the next chapter.K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology
--- END Content from https://kmchandy.github.io/DISTRIBUTED_SYSTEM_MODELS/Timelines.html ---


--- START Content from https://kmchandy.github.io/DISTRIBUTED_SYSTEM_MODELS/TimelinesExamples.html ---
TITLE: DistributedSystemModels/TimelinesExamples.html

Each child has a countdown of the number of times it tosses balloons.
  Each time a child tosses a balloon it decrements its countdown value.
After a child's countdown reaches 0 the child pops balloons that it receives
(and doesn't toss popped balloons).StateThe state of a channel is the number of balloons in it.
The state of an agent (X or Y) is the countdown value nX or nY, respectively.A Computation: A Sequence of StatesAn example of a sequence of state changes is as follows.Initial State: Channel (X, Y) has one balloon and channel (Y, X) has
  one balloon. And the countdown for X is 2 and the countdown for Y is
  3. We represent this state by:nX = 1, nY = 2, (X, Y) = 1, (Y, X) = 1Y receives a balloon and sends it back. So the state
  becomes:nX = 1, nY = 1, (X, Y) = 0, (Y, X) = 2X receives a balloon and  sends it back. So the state
  becomesnX = 0, nY = 1, (X, Y) = 1, (Y, X) = 1Y receives a balloon and sends it back. So the state
  becomesnX = 0, nY = 0, (X, Y) = 0, (Y, X) = 2X receives a balloon and pops it. So the state
  becomesnX = 0, nY = 0, (X, Y) = 0, (Y, X) = 1Y receives a balloon and pops it. So the state
  becomesnX = 0, nY = 0, (X, Y) = 0, (Y, X) = 0There are no further steps in this computationAnother ComputationWhat are other computations of the system?Let's look at the event in step 3 of the above computation. The event
is as follows.Event: X receives a balloon for the first timeThis event is specified by the 4-tuple:X's state before the event is nX = 1The message that is received is 1 (balloon) and the message is from
  Y.X's state after the event is nX = 0The message that is sent in the event is 1 (balloon) and the message is to
  Y.This event can be executed in any state S which satisfies the inputs
to this event:X's state before the event is nX = 1The message at the head of channel (Y, X) is 1.So, this event can be executed in the initial state.You can now construct the following computation.Initial State: Channel (X, Y) has one balloon and channel (Y, X) has
  one balloon. And the countdown for X is 2 and the countdown for Y is
  3. We represent this state by:nX = 1, nY = 2, (X, Y) = 1, (Y, X) = 1X receives a balloon and sends it back. So the state
  becomes:nX = 0, nY = 2, (X, Y) = 2, (Y, X) = 0Y receives a balloon and  sends it back. So the state
  becomesnX = 0, nY = 1, (X, Y) = 1, (Y, X) = 1X receives a balloon and pops it. So the state
  becomesnX = 0, nY = 1, (X, Y) = 1, (Y, X) = 0Y receives a balloon sends it back. So the state
  becomesnX = 0, nY = 0, (X, Y) = 0, (Y, X) = 1X receives a balloon and pops it. So the state
  becomesnX = 0, nY = 0, (X, Y) = 0, (Y, X) = 0There are no further steps in this computationA system may have many computations.
All the states of the computations of the system are states in thewhileloop:Initial State
while there exists an executable event:
   execute any executable eventIn this example, after the initial state there were two executable
events: an event at X and an event at Y.
Either of these two events could have been selected as the next event
to be executed.InvariantAn invariant of this system is:Number of balloons is at most 2which is equivalent to:nX + nY + (X, Y) + (Y, X) <= 2(Here we are abusing notation using (X, Y) for a channel and also for
the state of the channel.)To prove that this predicate (Boolean formula on variables of the
program) is an invariant we prove that (1) this predicate holds initially
(i.e., the formula is true initially) and (2) if it holds before any
step then it holds after the step.Loop VariantTo prove that the function terminates we use the loop variant:nX + nY + (X, Y) + (Y, X)The loop variant is a function of the variables of the system.
We can carry out an induction on this function because it returns
integers and is bounded below.
The bound is not important. In our example, a bound is 0.We prove that the computation terminates by showing that each step
reduces the variant function.NextStates of a Distributed System.Review material for this pageK. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

Let's look at the event in step 3 of the above computation. The event
is as follows.Event: X receives a balloon for the first timeThis event is specified by the 4-tuple:X's state before the event is nX = 1The message that is received is 1 (balloon) and the message is from
  Y.X's state after the event is nX = 0The message that is sent in the event is 1 (balloon) and the message is to
  Y.This event can be executed in any state S which satisfies the inputs
to this event:X's state before the event is nX = 1The message at the head of channel (Y, X) is 1.So, this event can be executed in the initial state.You can now construct the following computation.Initial State: Channel (X, Y) has one balloon and channel (Y, X) has
  one balloon. And the countdown for X is 2 and the countdown for Y is
  3. We represent this state by:nX = 1, nY = 2, (X, Y) = 1, (Y, X) = 1X receives a balloon and sends it back. So the state
  becomes:nX = 0, nY = 2, (X, Y) = 2, (Y, X) = 0Y receives a balloon and  sends it back. So the state
  becomesnX = 0, nY = 1, (X, Y) = 1, (Y, X) = 1X receives a balloon and pops it. So the state
  becomesnX = 0, nY = 1, (X, Y) = 1, (Y, X) = 0Y receives a balloon sends it back. So the state
  becomesnX = 0, nY = 0, (X, Y) = 0, (Y, X) = 1X receives a balloon and pops it. So the state
  becomesnX = 0, nY = 0, (X, Y) = 0, (Y, X) = 0There are no further steps in this computationA system may have many computations.
All the states of the computations of the system are states in thewhileloop:Initial State
while there exists an executable event:
   execute any executable eventIn this example, after the initial state there were two executable
events: an event at X and an event at Y.
Either of these two events could have been selected as the next event
to be executed.InvariantAn invariant of this system is:Number of balloons is at most 2which is equivalent to:nX + nY + (X, Y) + (Y, X) <= 2(Here we are abusing notation using (X, Y) for a channel and also for
the state of the channel.)To prove that this predicate (Boolean formula on variables of the
program) is an invariant we prove that (1) this predicate holds initially
(i.e., the formula is true initially) and (2) if it holds before any
step then it holds after the step.Loop VariantTo prove that the function terminates we use the loop variant:nX + nY + (X, Y) + (Y, X)The loop variant is a function of the variables of the system.
We can carry out an induction on this function because it returns
integers and is bounded below.
The bound is not important. In our example, a bound is 0.We prove that the computation terminates by showing that each step
reduces the variant function.NextStates of a Distributed System.Review material for this pageK. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

You can now construct the following computation.Initial State: Channel (X, Y) has one balloon and channel (Y, X) has
  one balloon. And the countdown for X is 2 and the countdown for Y is
  3. We represent this state by:nX = 1, nY = 2, (X, Y) = 1, (Y, X) = 1X receives a balloon and sends it back. So the state
  becomes:nX = 0, nY = 2, (X, Y) = 2, (Y, X) = 0Y receives a balloon and  sends it back. So the state
  becomesnX = 0, nY = 1, (X, Y) = 1, (Y, X) = 1X receives a balloon and pops it. So the state
  becomesnX = 0, nY = 1, (X, Y) = 1, (Y, X) = 0Y receives a balloon sends it back. So the state
  becomesnX = 0, nY = 0, (X, Y) = 0, (Y, X) = 1X receives a balloon and pops it. So the state
  becomesnX = 0, nY = 0, (X, Y) = 0, (Y, X) = 0There are no further steps in this computationA system may have many computations.
All the states of the computations of the system are states in thewhileloop:Initial State
while there exists an executable event:
   execute any executable eventIn this example, after the initial state there were two executable
events: an event at X and an event at Y.
Either of these two events could have been selected as the next event
to be executed.InvariantAn invariant of this system is:Number of balloons is at most 2which is equivalent to:nX + nY + (X, Y) + (Y, X) <= 2(Here we are abusing notation using (X, Y) for a channel and also for
the state of the channel.)To prove that this predicate (Boolean formula on variables of the
program) is an invariant we prove that (1) this predicate holds initially
(i.e., the formula is true initially) and (2) if it holds before any
step then it holds after the step.Loop VariantTo prove that the function terminates we use the loop variant:nX + nY + (X, Y) + (Y, X)The loop variant is a function of the variables of the system.
We can carry out an induction on this function because it returns
integers and is bounded below.
The bound is not important. In our example, a bound is 0.We prove that the computation terminates by showing that each step
reduces the variant function.NextStates of a Distributed System.Review material for this pageK. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

A system may have many computations.
All the states of the computations of the system are states in thewhileloop:Initial State
while there exists an executable event:
   execute any executable eventIn this example, after the initial state there were two executable
events: an event at X and an event at Y.
Either of these two events could have been selected as the next event
to be executed.InvariantAn invariant of this system is:Number of balloons is at most 2which is equivalent to:nX + nY + (X, Y) + (Y, X) <= 2(Here we are abusing notation using (X, Y) for a channel and also for
the state of the channel.)To prove that this predicate (Boolean formula on variables of the
program) is an invariant we prove that (1) this predicate holds initially
(i.e., the formula is true initially) and (2) if it holds before any
step then it holds after the step.Loop VariantTo prove that the function terminates we use the loop variant:nX + nY + (X, Y) + (Y, X)The loop variant is a function of the variables of the system.
We can carry out an induction on this function because it returns
integers and is bounded below.
The bound is not important. In our example, a bound is 0.We prove that the computation terminates by showing that each step
reduces the variant function.NextStates of a Distributed System.Review material for this pageK. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

Number of balloons is at most 2which is equivalent to:nX + nY + (X, Y) + (Y, X) <= 2(Here we are abusing notation using (X, Y) for a channel and also for
the state of the channel.)To prove that this predicate (Boolean formula on variables of the
program) is an invariant we prove that (1) this predicate holds initially
(i.e., the formula is true initially) and (2) if it holds before any
step then it holds after the step.Loop VariantTo prove that the function terminates we use the loop variant:nX + nY + (X, Y) + (Y, X)The loop variant is a function of the variables of the system.
We can carry out an induction on this function because it returns
integers and is bounded below.
The bound is not important. In our example, a bound is 0.We prove that the computation terminates by showing that each step
reduces the variant function.NextStates of a Distributed System.Review material for this pageK. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

which is equivalent to:nX + nY + (X, Y) + (Y, X) <= 2(Here we are abusing notation using (X, Y) for a channel and also for
the state of the channel.)To prove that this predicate (Boolean formula on variables of the
program) is an invariant we prove that (1) this predicate holds initially
(i.e., the formula is true initially) and (2) if it holds before any
step then it holds after the step.Loop VariantTo prove that the function terminates we use the loop variant:nX + nY + (X, Y) + (Y, X)The loop variant is a function of the variables of the system.
We can carry out an induction on this function because it returns
integers and is bounded below.
The bound is not important. In our example, a bound is 0.We prove that the computation terminates by showing that each step
reduces the variant function.NextStates of a Distributed System.Review material for this pageK. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

(Here we are abusing notation using (X, Y) for a channel and also for
the state of the channel.)To prove that this predicate (Boolean formula on variables of the
program) is an invariant we prove that (1) this predicate holds initially
(i.e., the formula is true initially) and (2) if it holds before any
step then it holds after the step.Loop VariantTo prove that the function terminates we use the loop variant:nX + nY + (X, Y) + (Y, X)The loop variant is a function of the variables of the system.
We can carry out an induction on this function because it returns
integers and is bounded below.
The bound is not important. In our example, a bound is 0.We prove that the computation terminates by showing that each step
reduces the variant function.NextStates of a Distributed System.Review material for this pageK. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

To prove that this predicate (Boolean formula on variables of the
program) is an invariant we prove that (1) this predicate holds initially
(i.e., the formula is true initially) and (2) if it holds before any
step then it holds after the step.Loop VariantTo prove that the function terminates we use the loop variant:nX + nY + (X, Y) + (Y, X)The loop variant is a function of the variables of the system.
We can carry out an induction on this function because it returns
integers and is bounded below.
The bound is not important. In our example, a bound is 0.We prove that the computation terminates by showing that each step
reduces the variant function.NextStates of a Distributed System.Review material for this pageK. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

The loop variant is a function of the variables of the system.
We can carry out an induction on this function because it returns
integers and is bounded below.
The bound is not important. In our example, a bound is 0.We prove that the computation terminates by showing that each step
reduces the variant function.NextStates of a Distributed System.Review material for this pageK. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

We prove that the computation terminates by showing that each step
reduces the variant function.NextStates of a Distributed System.Review material for this pageK. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

Review material for this pageK. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology
--- END Content from https://kmchandy.github.io/DISTRIBUTED_SYSTEM_MODELS/TimelinesExamples.html ---


--- START Content from https://kmchandy.github.io/DISTRIBUTED_SYSTEM_MODELS/TimelinesFAQ.html ---
TITLE: DistributedSystemModels/TimelinesFAQ.html

Which agent starts a global snapshot
  algorithm?

The algorithm is started byanyone or more agents.
  The agents that start the algorithm isn't specified.The agent that should initiate a snapshot algorithm is usually self
  evident from the application of the algorithm.
  For example, an agent that has been waiting for a long time for a
  message may obtain a global snapshot to determine if the agent is in
  a deadlock.Can the snapshot algorithm be used to record
  the states and channels of a subset of agents, or does the algorithm
  have to span all agents and channels?The snapshot algorithm can be used to record the states of a
  connected subset of agents.
  An agent doesn't propagate markers to agents outside the set of
  interest.
  The recorded states of the subset of agents and channels forms a
  part of a global snapshot.
  Some algorithms only used snapshots of subsets of agents.Is "every message received inpastis sent inpast" equivalent to
  "every message sent infutureis received infuture"?YesNote that a message may be received in the final state.NextChapter on Snapshots and Clocks:Global Snapshots.Properties of DataflowK. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

The agent that should initiate a snapshot algorithm is usually self
  evident from the application of the algorithm.
  For example, an agent that has been waiting for a long time for a
  message may obtain a global snapshot to determine if the agent is in
  a deadlock.Can the snapshot algorithm be used to record
  the states and channels of a subset of agents, or does the algorithm
  have to span all agents and channels?The snapshot algorithm can be used to record the states of a
  connected subset of agents.
  An agent doesn't propagate markers to agents outside the set of
  interest.
  The recorded states of the subset of agents and channels forms a
  part of a global snapshot.
  Some algorithms only used snapshots of subsets of agents.Is "every message received inpastis sent inpast" equivalent to
  "every message sent infutureis received infuture"?YesNote that a message may be received in the final state.NextChapter on Snapshots and Clocks:Global Snapshots.Properties of DataflowK. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

Can the snapshot algorithm be used to record
  the states and channels of a subset of agents, or does the algorithm
  have to span all agents and channels?

The snapshot algorithm can be used to record the states of a
  connected subset of agents.
  An agent doesn't propagate markers to agents outside the set of
  interest.
  The recorded states of the subset of agents and channels forms a
  part of a global snapshot.
  Some algorithms only used snapshots of subsets of agents.Is "every message received inpastis sent inpast" equivalent to
  "every message sent infutureis received infuture"?YesNote that a message may be received in the final state.NextChapter on Snapshots and Clocks:Global Snapshots.Properties of DataflowK. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

Is "every message received inpastis sent inpast" equivalent to
  "every message sent infutureis received infuture"?

YesNote that a message may be received in the final state.NextChapter on Snapshots and Clocks:Global Snapshots.Properties of DataflowK. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

Note that a message may be received in the final state.NextChapter on Snapshots and Clocks:Global Snapshots.Properties of DataflowK. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

Properties of DataflowK. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology
--- END Content from https://kmchandy.github.io/DISTRIBUTED_SYSTEM_MODELS/TimelinesFAQ.html ---


--- START Content from https://kmchandy.github.io/DISTRIBUTED_SYSTEM_MODELS/TimelinesReview.html ---
TITLE: DistributedSystemModels/TimelinesReview.html

K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology
--- END Content from https://kmchandy.github.io/DISTRIBUTED_SYSTEM_MODELS/TimelinesReview.html ---


--- START Content from https://kmchandy.github.io/ChannelSnapshots/ChannelSnapshots.html ---
TITLE: ChannelSnapshots/ChannelSnapshots.html

A global snapshot algorithm records a state of the system that can
  occur during a computation.
  The state obtained by the algorithm is called a global snapshot.
  Systems are monitored by taking repeated global snapshots.
  When a transient error is detected, a rollback and recovery
  algorithm restarts the computation from the most recent snapshot
  instead of starting it from the initial state.



Figure Link: https://kmchandy.github.io/ChannelSnapshots/ChannelSnapshots/ChannelSnapshots.001.jpeg

Caption: Fig.1: OS and Clients use the same Channels


A global snapshot algorithm records a state of the system that
  occurs during a computation of the system.
  A state obtained by the algorithm is called a global snapshot.A state of the system is a tuple with an element of the tuple for each agent and
  each channel.
  A system state is also called a global state to distinguish it from
  states of agents and states of channels.An algorithm to record the state of a system is not instantaneous
  because the algorithm records the states of multiple agents and
  channels.
  The algorithm starts at some point and terminates at a later point.
  A global snapshot is a state that occurs in a computation from the
  state in which the algorithm starts to the state in which the algorithm ends.The global snapshot algorithm is an example of an algorithm that is
  executed by a distributed operating system (OS) on behalf of a
  client.
  Next, we describe features of the OS that are relevant to the
  snapshot algorithm.A Distributed Operating SystemEach client agent has an OS agent that supervises it.
  OS agents use the same processors and channels as clients do.
  
  OS agents can record, but not modify, states of their clients.
  
  OS agents can send and receive OS messages that are not seen
by clients.Figure 1 is a representation of two OS agents that manage their
client agents.
Messages sent by a client are recorded by the OS and passed through
to destination clients.
The OS sends messages on the same channels as clients, but the OS
traps these messages so that the client does not see them.Fig.1: OS and Clients use the same ChannelsExecution of an OS agent on a processor may delay a client's
  steps on the same processor, and thus change the order in which the
  client's steps are executed.The OS may change a client's computation -- the order of steps -- but
the OS must not change the client's dataflow.One way to record a global snapshot is for the OS to stop a client
  computation, then take a global snapshot, and then  restart the
client computation.
Our goal is to design an algorithm that does not stop the client.Hereafter, when we refer to an agent we mean an OS agent.
Likewise, by messages we mean those that are sent and received by the
OS.Next, let's develop OS agents and OS messages to record a global
snapshot.The ProblemLet \(S_{init}\) and \(S_{fini}\) be the states in which the
algorithm starts and finishes, respectively.
Design an algorithm that records a state \(S^{*}\) such that
there exists a computation that starts at \(S_{init}\), then visits
\(S^{*}\) and then visits \(S_{fini}\).How Should You Solve the Problem?StrategyA general strategy for designing algorithms dealing with intermediate
states is to find a helpfulproperty of cuts.
What property helps to determine \(S^{*}\)?The property "Computations of Past before Future," given below, tells
us that \(S^{*}\) can be the state at any cut.Computations of Past before Future.Let computation \(X\) start in state \(S_{init}\) and end in state
\(S_{fini}\).
Let \(S^{*}\) be the state at a cut(past, future)of \(X\).
There exists a computation \(Y\) that starts in \(S_{init}\), visits
\(S^{*}\), and ends in \(S_{fini}\).Using this strategy, our tasks reduce to (1) identifying a cut, and
(2) recording the state, \(S^{*}\), at the cut.Identifying a CutEach agent has to record its own state because an agent's state is not
accessible to other agents.
The state of an agent that it records is called alocal snapshot.Definepastas the set of steps at each agent before the
agent takes its local snapshot, and
definefutureas the set of steps at each agent after the
agent takes its local snapshot.
So, if a step \(x\) at an agent is inpastthen all steps
at that agent before \(x\) are also inpast.Let's usethe following property ofpast,futureand cuts:The partition(past, future)is a cut exactly when every
message received inpastis sent inpast.Therefore,(past, future)is a cut exactly when:Global Snapshot RuleEach message received before the receiver takes its local snapshot
is sent before the sender takes its local snapshot.Design an algorithm yourself before reading further, and compare your
algorithm with the one given below.The Global Snapshot AlgorithmA special OS message called amarkeris used to distinguish
pre-snapshot from post-snapshot messages.
Messages sent on a channel before amarkeris sent on the
channel are messages sent in thepast-- i.e. before the
sender takes its local snapshot -- and messages sent
after the marker are sent in thefuture.The algorithmThe algorithm begins by one or more agents taking their local
  snapshots.When an agent takes its local snapshot it sends a marker on each
  of its outgoing channels.When an agent receives a marker, the agent takes its local snapshot
  if it has not already done so.The snapshot of a channel is the sequence of messages received on
  the channel after the receiver takes its snapshot and before the
  receiver receives a marker on the channel.Proof of correctnessFrom rule 3, each message received by an agent \(r\) on a channel
\(c\) before \(r\) takes its 
local snapshot is a message received by \(r\) before \(r\) receives a
marker on channel \(c\).Because channels are first in first out, each message received by
 \(r\) on \(c\) before \(r\) receives a
marker on \(c\) is sent on \(c\) before a marker is
sent on \(c\).From rule 2 each message sent on \(c\) before a marker is sent on
\(c\) is sent before the sender takes its local snapshot.From the three paragraphs above it follows that the global snapshot
rule holds for the algorithm.Proof about States of ChannelsThe messages in a channel at the cut are the
messages sent inpastand received infuture.
These are messages sent before the sender takes its snapshot and
received after the receiver takes its snapshot.
So, the state of a channel is the sequence of
messages received along the channel after the receiver takes its
snapshot and before the receiver receives a marker along the channel.Note: If an agent takes its local snapshot when it receives a marker
along a channel, then the snapshot of the channel is the empty
sequence of messages.Termination of the AlgorithmAfter any agent \(v\) initiates the algorithm, all agents that are
reachable from \(v\) will receive a marker and take their local
snapshots.
If every agent is reachable from an initiator then all agents take
local snapshots.Each agent takes its local snapshot at most once.
So, a marker is sent on a channel at most once.
The computation terminates when all markers are received.Collecting Local Snapshots to form Global
SnapshotsOne way to collect local snapshots is to have an OS agent act as an
observer.
Each agent sends its local snapshots to the observer which puts the
local snapshots together to form the global snapshots.
Successive snapshots are disambiguated by using sequence numbers or
timestamps.Some algorithms carry out distributed computations on local snapshots
without using an observer to collect local snapshots.
Later, we give examples of such algorithms.Applications of Global SnapshotsSystem MonitoringSystems can be monitored by taking global snapshots repeatedly.
Let \(S_{0}, S_{1}, S_{2}, \ldots, \) be the sequence of states
recorded by the system.
From the propertyComputations through Increasing Cutsthere exists a computation that visits each state \(S_{i}\) in order
of increasing \(i\).
The system monitor checks the sequence of snapshots to determine if
some action is required.Rollback and RecoverLet \(S^{*}\) be the most recent snapshot recorded by a system monitor.
From the property,"Computations of Past before Future"there exists a computation that starts at the initial state and later
visits \(S^{*}\).
So, if an error is detected in a computation then the computation can be
restarted from \(S^{*}\) rather than rolling all the way back to the
initial state.Detecting Stable PredicatesAstable predicateis a predicate with the following property: If the
predicate holds at any point in any computation then it continues to
hold forever thereafter in that computation. Equivalently, if a stable
predicate holds in a state \(s\) then it holds in all states reachable
from \(s\).Examples of stable predicates are: "The computation has
terminated," and "The computation is deadlocked."
If a computation has terminated at some point then it remains 
terminated.
Likewise if a computation has deadlocked then it remains deadlocked.Specification of Detection AlgorithmsAn algorithm to detect a stable property \(P\) has the following
specification.If \(P\) holds when the algorithm is initiated then the algorithm
  detects that \(P\) holds.If the algorithm detects that \(P\) holds then \(P\) holds when the
  algorithm terminates.General Detection AlgorithmsA general solution is for the operating system to monitor a client
computation by taking repeated snapshots of the computation.
The OS checks whether a specified stable property holds in each
snapshot.
From the property,"Computations of Past before Future"this general solution satisfies the specification of detection
algorithms.Detection with ObserversThe OS uses an agent, the observer, to collect local snapshots and
form a global snapshot.
The observer inspects the global snapshot to determine if the property
holds in the snapshot.
The OS can also use multiple observers each of which collects
local information from subnetworks; the OS then carries out a
distributed algorithm on its collection of observers.The OS can also execute a distributed algorithm on local
snapshots without having observers collect local information, as
described next.Detection without Observers: Distributed
Algorithms on Local SnapshotsDistributed algorithms on local snapshots operate in two phases.
In the first phase a global snapshot algorithm is executed.
The local snapshot of each agent and its incoming channels are stored
locally, at the agent, without sending the information to observers.In the second phase a distributed algorithm is executed to determine
if the local information stored at agents satisfies a specified global
property, such as "computation has deadlocked."
The algorithm in the second phase operates on unchanging data.
These algorithms are often distributed graph algorithms.The two phases can be executed concurrently in many applications.NextA code skeleton of the algorithm and examples of the global snapshot
algorithm are providedhere.
Nextlogical clocks.K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology



Figure Link: https://kmchandy.github.io/ChannelSnapshots/ChannelSnapshots/ChannelSnapshots.001.jpeg

Caption: Fig.1: OS and Clients use the same Channels


A state of the system is a tuple with an element of the tuple for each agent and
  each channel.
  A system state is also called a global state to distinguish it from
  states of agents and states of channels.An algorithm to record the state of a system is not instantaneous
  because the algorithm records the states of multiple agents and
  channels.
  The algorithm starts at some point and terminates at a later point.
  A global snapshot is a state that occurs in a computation from the
  state in which the algorithm starts to the state in which the algorithm ends.The global snapshot algorithm is an example of an algorithm that is
  executed by a distributed operating system (OS) on behalf of a
  client.
  Next, we describe features of the OS that are relevant to the
  snapshot algorithm.A Distributed Operating SystemEach client agent has an OS agent that supervises it.
  OS agents use the same processors and channels as clients do.
  
  OS agents can record, but not modify, states of their clients.
  
  OS agents can send and receive OS messages that are not seen
by clients.Figure 1 is a representation of two OS agents that manage their
client agents.
Messages sent by a client are recorded by the OS and passed through
to destination clients.
The OS sends messages on the same channels as clients, but the OS
traps these messages so that the client does not see them.Fig.1: OS and Clients use the same ChannelsExecution of an OS agent on a processor may delay a client's
  steps on the same processor, and thus change the order in which the
  client's steps are executed.The OS may change a client's computation -- the order of steps -- but
the OS must not change the client's dataflow.One way to record a global snapshot is for the OS to stop a client
  computation, then take a global snapshot, and then  restart the
client computation.
Our goal is to design an algorithm that does not stop the client.Hereafter, when we refer to an agent we mean an OS agent.
Likewise, by messages we mean those that are sent and received by the
OS.Next, let's develop OS agents and OS messages to record a global
snapshot.The ProblemLet \(S_{init}\) and \(S_{fini}\) be the states in which the
algorithm starts and finishes, respectively.
Design an algorithm that records a state \(S^{*}\) such that
there exists a computation that starts at \(S_{init}\), then visits
\(S^{*}\) and then visits \(S_{fini}\).How Should You Solve the Problem?StrategyA general strategy for designing algorithms dealing with intermediate
states is to find a helpfulproperty of cuts.
What property helps to determine \(S^{*}\)?The property "Computations of Past before Future," given below, tells
us that \(S^{*}\) can be the state at any cut.Computations of Past before Future.Let computation \(X\) start in state \(S_{init}\) and end in state
\(S_{fini}\).
Let \(S^{*}\) be the state at a cut(past, future)of \(X\).
There exists a computation \(Y\) that starts in \(S_{init}\), visits
\(S^{*}\), and ends in \(S_{fini}\).Using this strategy, our tasks reduce to (1) identifying a cut, and
(2) recording the state, \(S^{*}\), at the cut.Identifying a CutEach agent has to record its own state because an agent's state is not
accessible to other agents.
The state of an agent that it records is called alocal snapshot.Definepastas the set of steps at each agent before the
agent takes its local snapshot, and
definefutureas the set of steps at each agent after the
agent takes its local snapshot.
So, if a step \(x\) at an agent is inpastthen all steps
at that agent before \(x\) are also inpast.Let's usethe following property ofpast,futureand cuts:The partition(past, future)is a cut exactly when every
message received inpastis sent inpast.Therefore,(past, future)is a cut exactly when:Global Snapshot RuleEach message received before the receiver takes its local snapshot
is sent before the sender takes its local snapshot.Design an algorithm yourself before reading further, and compare your
algorithm with the one given below.The Global Snapshot AlgorithmA special OS message called amarkeris used to distinguish
pre-snapshot from post-snapshot messages.
Messages sent on a channel before amarkeris sent on the
channel are messages sent in thepast-- i.e. before the
sender takes its local snapshot -- and messages sent
after the marker are sent in thefuture.The algorithmThe algorithm begins by one or more agents taking their local
  snapshots.When an agent takes its local snapshot it sends a marker on each
  of its outgoing channels.When an agent receives a marker, the agent takes its local snapshot
  if it has not already done so.The snapshot of a channel is the sequence of messages received on
  the channel after the receiver takes its snapshot and before the
  receiver receives a marker on the channel.Proof of correctnessFrom rule 3, each message received by an agent \(r\) on a channel
\(c\) before \(r\) takes its 
local snapshot is a message received by \(r\) before \(r\) receives a
marker on channel \(c\).Because channels are first in first out, each message received by
 \(r\) on \(c\) before \(r\) receives a
marker on \(c\) is sent on \(c\) before a marker is
sent on \(c\).From rule 2 each message sent on \(c\) before a marker is sent on
\(c\) is sent before the sender takes its local snapshot.From the three paragraphs above it follows that the global snapshot
rule holds for the algorithm.Proof about States of ChannelsThe messages in a channel at the cut are the
messages sent inpastand received infuture.
These are messages sent before the sender takes its snapshot and
received after the receiver takes its snapshot.
So, the state of a channel is the sequence of
messages received along the channel after the receiver takes its
snapshot and before the receiver receives a marker along the channel.Note: If an agent takes its local snapshot when it receives a marker
along a channel, then the snapshot of the channel is the empty
sequence of messages.Termination of the AlgorithmAfter any agent \(v\) initiates the algorithm, all agents that are
reachable from \(v\) will receive a marker and take their local
snapshots.
If every agent is reachable from an initiator then all agents take
local snapshots.Each agent takes its local snapshot at most once.
So, a marker is sent on a channel at most once.
The computation terminates when all markers are received.Collecting Local Snapshots to form Global
SnapshotsOne way to collect local snapshots is to have an OS agent act as an
observer.
Each agent sends its local snapshots to the observer which puts the
local snapshots together to form the global snapshots.
Successive snapshots are disambiguated by using sequence numbers or
timestamps.Some algorithms carry out distributed computations on local snapshots
without using an observer to collect local snapshots.
Later, we give examples of such algorithms.Applications of Global SnapshotsSystem MonitoringSystems can be monitored by taking global snapshots repeatedly.
Let \(S_{0}, S_{1}, S_{2}, \ldots, \) be the sequence of states
recorded by the system.
From the propertyComputations through Increasing Cutsthere exists a computation that visits each state \(S_{i}\) in order
of increasing \(i\).
The system monitor checks the sequence of snapshots to determine if
some action is required.Rollback and RecoverLet \(S^{*}\) be the most recent snapshot recorded by a system monitor.
From the property,"Computations of Past before Future"there exists a computation that starts at the initial state and later
visits \(S^{*}\).
So, if an error is detected in a computation then the computation can be
restarted from \(S^{*}\) rather than rolling all the way back to the
initial state.Detecting Stable PredicatesAstable predicateis a predicate with the following property: If the
predicate holds at any point in any computation then it continues to
hold forever thereafter in that computation. Equivalently, if a stable
predicate holds in a state \(s\) then it holds in all states reachable
from \(s\).Examples of stable predicates are: "The computation has
terminated," and "The computation is deadlocked."
If a computation has terminated at some point then it remains 
terminated.
Likewise if a computation has deadlocked then it remains deadlocked.Specification of Detection AlgorithmsAn algorithm to detect a stable property \(P\) has the following
specification.If \(P\) holds when the algorithm is initiated then the algorithm
  detects that \(P\) holds.If the algorithm detects that \(P\) holds then \(P\) holds when the
  algorithm terminates.General Detection AlgorithmsA general solution is for the operating system to monitor a client
computation by taking repeated snapshots of the computation.
The OS checks whether a specified stable property holds in each
snapshot.
From the property,"Computations of Past before Future"this general solution satisfies the specification of detection
algorithms.Detection with ObserversThe OS uses an agent, the observer, to collect local snapshots and
form a global snapshot.
The observer inspects the global snapshot to determine if the property
holds in the snapshot.
The OS can also use multiple observers each of which collects
local information from subnetworks; the OS then carries out a
distributed algorithm on its collection of observers.The OS can also execute a distributed algorithm on local
snapshots without having observers collect local information, as
described next.Detection without Observers: Distributed
Algorithms on Local SnapshotsDistributed algorithms on local snapshots operate in two phases.
In the first phase a global snapshot algorithm is executed.
The local snapshot of each agent and its incoming channels are stored
locally, at the agent, without sending the information to observers.In the second phase a distributed algorithm is executed to determine
if the local information stored at agents satisfies a specified global
property, such as "computation has deadlocked."
The algorithm in the second phase operates on unchanging data.
These algorithms are often distributed graph algorithms.The two phases can be executed concurrently in many applications.NextA code skeleton of the algorithm and examples of the global snapshot
algorithm are providedhere.
Nextlogical clocks.K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology



Figure Link: https://kmchandy.github.io/ChannelSnapshots/ChannelSnapshots/ChannelSnapshots.001.jpeg

Caption: Fig.1: OS and Clients use the same Channels


An algorithm to record the state of a system is not instantaneous
  because the algorithm records the states of multiple agents and
  channels.
  The algorithm starts at some point and terminates at a later point.
  A global snapshot is a state that occurs in a computation from the
  state in which the algorithm starts to the state in which the algorithm ends.The global snapshot algorithm is an example of an algorithm that is
  executed by a distributed operating system (OS) on behalf of a
  client.
  Next, we describe features of the OS that are relevant to the
  snapshot algorithm.A Distributed Operating SystemEach client agent has an OS agent that supervises it.
  OS agents use the same processors and channels as clients do.
  
  OS agents can record, but not modify, states of their clients.
  
  OS agents can send and receive OS messages that are not seen
by clients.Figure 1 is a representation of two OS agents that manage their
client agents.
Messages sent by a client are recorded by the OS and passed through
to destination clients.
The OS sends messages on the same channels as clients, but the OS
traps these messages so that the client does not see them.Fig.1: OS and Clients use the same ChannelsExecution of an OS agent on a processor may delay a client's
  steps on the same processor, and thus change the order in which the
  client's steps are executed.The OS may change a client's computation -- the order of steps -- but
the OS must not change the client's dataflow.One way to record a global snapshot is for the OS to stop a client
  computation, then take a global snapshot, and then  restart the
client computation.
Our goal is to design an algorithm that does not stop the client.Hereafter, when we refer to an agent we mean an OS agent.
Likewise, by messages we mean those that are sent and received by the
OS.Next, let's develop OS agents and OS messages to record a global
snapshot.The ProblemLet \(S_{init}\) and \(S_{fini}\) be the states in which the
algorithm starts and finishes, respectively.
Design an algorithm that records a state \(S^{*}\) such that
there exists a computation that starts at \(S_{init}\), then visits
\(S^{*}\) and then visits \(S_{fini}\).How Should You Solve the Problem?StrategyA general strategy for designing algorithms dealing with intermediate
states is to find a helpfulproperty of cuts.
What property helps to determine \(S^{*}\)?The property "Computations of Past before Future," given below, tells
us that \(S^{*}\) can be the state at any cut.Computations of Past before Future.Let computation \(X\) start in state \(S_{init}\) and end in state
\(S_{fini}\).
Let \(S^{*}\) be the state at a cut(past, future)of \(X\).
There exists a computation \(Y\) that starts in \(S_{init}\), visits
\(S^{*}\), and ends in \(S_{fini}\).Using this strategy, our tasks reduce to (1) identifying a cut, and
(2) recording the state, \(S^{*}\), at the cut.Identifying a CutEach agent has to record its own state because an agent's state is not
accessible to other agents.
The state of an agent that it records is called alocal snapshot.Definepastas the set of steps at each agent before the
agent takes its local snapshot, and
definefutureas the set of steps at each agent after the
agent takes its local snapshot.
So, if a step \(x\) at an agent is inpastthen all steps
at that agent before \(x\) are also inpast.Let's usethe following property ofpast,futureand cuts:The partition(past, future)is a cut exactly when every
message received inpastis sent inpast.Therefore,(past, future)is a cut exactly when:Global Snapshot RuleEach message received before the receiver takes its local snapshot
is sent before the sender takes its local snapshot.Design an algorithm yourself before reading further, and compare your
algorithm with the one given below.The Global Snapshot AlgorithmA special OS message called amarkeris used to distinguish
pre-snapshot from post-snapshot messages.
Messages sent on a channel before amarkeris sent on the
channel are messages sent in thepast-- i.e. before the
sender takes its local snapshot -- and messages sent
after the marker are sent in thefuture.The algorithmThe algorithm begins by one or more agents taking their local
  snapshots.When an agent takes its local snapshot it sends a marker on each
  of its outgoing channels.When an agent receives a marker, the agent takes its local snapshot
  if it has not already done so.The snapshot of a channel is the sequence of messages received on
  the channel after the receiver takes its snapshot and before the
  receiver receives a marker on the channel.Proof of correctnessFrom rule 3, each message received by an agent \(r\) on a channel
\(c\) before \(r\) takes its 
local snapshot is a message received by \(r\) before \(r\) receives a
marker on channel \(c\).Because channels are first in first out, each message received by
 \(r\) on \(c\) before \(r\) receives a
marker on \(c\) is sent on \(c\) before a marker is
sent on \(c\).From rule 2 each message sent on \(c\) before a marker is sent on
\(c\) is sent before the sender takes its local snapshot.From the three paragraphs above it follows that the global snapshot
rule holds for the algorithm.Proof about States of ChannelsThe messages in a channel at the cut are the
messages sent inpastand received infuture.
These are messages sent before the sender takes its snapshot and
received after the receiver takes its snapshot.
So, the state of a channel is the sequence of
messages received along the channel after the receiver takes its
snapshot and before the receiver receives a marker along the channel.Note: If an agent takes its local snapshot when it receives a marker
along a channel, then the snapshot of the channel is the empty
sequence of messages.Termination of the AlgorithmAfter any agent \(v\) initiates the algorithm, all agents that are
reachable from \(v\) will receive a marker and take their local
snapshots.
If every agent is reachable from an initiator then all agents take
local snapshots.Each agent takes its local snapshot at most once.
So, a marker is sent on a channel at most once.
The computation terminates when all markers are received.Collecting Local Snapshots to form Global
SnapshotsOne way to collect local snapshots is to have an OS agent act as an
observer.
Each agent sends its local snapshots to the observer which puts the
local snapshots together to form the global snapshots.
Successive snapshots are disambiguated by using sequence numbers or
timestamps.Some algorithms carry out distributed computations on local snapshots
without using an observer to collect local snapshots.
Later, we give examples of such algorithms.Applications of Global SnapshotsSystem MonitoringSystems can be monitored by taking global snapshots repeatedly.
Let \(S_{0}, S_{1}, S_{2}, \ldots, \) be the sequence of states
recorded by the system.
From the propertyComputations through Increasing Cutsthere exists a computation that visits each state \(S_{i}\) in order
of increasing \(i\).
The system monitor checks the sequence of snapshots to determine if
some action is required.Rollback and RecoverLet \(S^{*}\) be the most recent snapshot recorded by a system monitor.
From the property,"Computations of Past before Future"there exists a computation that starts at the initial state and later
visits \(S^{*}\).
So, if an error is detected in a computation then the computation can be
restarted from \(S^{*}\) rather than rolling all the way back to the
initial state.Detecting Stable PredicatesAstable predicateis a predicate with the following property: If the
predicate holds at any point in any computation then it continues to
hold forever thereafter in that computation. Equivalently, if a stable
predicate holds in a state \(s\) then it holds in all states reachable
from \(s\).Examples of stable predicates are: "The computation has
terminated," and "The computation is deadlocked."
If a computation has terminated at some point then it remains 
terminated.
Likewise if a computation has deadlocked then it remains deadlocked.Specification of Detection AlgorithmsAn algorithm to detect a stable property \(P\) has the following
specification.If \(P\) holds when the algorithm is initiated then the algorithm
  detects that \(P\) holds.If the algorithm detects that \(P\) holds then \(P\) holds when the
  algorithm terminates.General Detection AlgorithmsA general solution is for the operating system to monitor a client
computation by taking repeated snapshots of the computation.
The OS checks whether a specified stable property holds in each
snapshot.
From the property,"Computations of Past before Future"this general solution satisfies the specification of detection
algorithms.Detection with ObserversThe OS uses an agent, the observer, to collect local snapshots and
form a global snapshot.
The observer inspects the global snapshot to determine if the property
holds in the snapshot.
The OS can also use multiple observers each of which collects
local information from subnetworks; the OS then carries out a
distributed algorithm on its collection of observers.The OS can also execute a distributed algorithm on local
snapshots without having observers collect local information, as
described next.Detection without Observers: Distributed
Algorithms on Local SnapshotsDistributed algorithms on local snapshots operate in two phases.
In the first phase a global snapshot algorithm is executed.
The local snapshot of each agent and its incoming channels are stored
locally, at the agent, without sending the information to observers.In the second phase a distributed algorithm is executed to determine
if the local information stored at agents satisfies a specified global
property, such as "computation has deadlocked."
The algorithm in the second phase operates on unchanging data.
These algorithms are often distributed graph algorithms.The two phases can be executed concurrently in many applications.NextA code skeleton of the algorithm and examples of the global snapshot
algorithm are providedhere.
Nextlogical clocks.K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology



Figure Link: https://kmchandy.github.io/ChannelSnapshots/ChannelSnapshots/ChannelSnapshots.001.jpeg

Caption: Fig.1: OS and Clients use the same Channels


The global snapshot algorithm is an example of an algorithm that is
  executed by a distributed operating system (OS) on behalf of a
  client.
  Next, we describe features of the OS that are relevant to the
  snapshot algorithm.A Distributed Operating SystemEach client agent has an OS agent that supervises it.
  OS agents use the same processors and channels as clients do.
  
  OS agents can record, but not modify, states of their clients.
  
  OS agents can send and receive OS messages that are not seen
by clients.Figure 1 is a representation of two OS agents that manage their
client agents.
Messages sent by a client are recorded by the OS and passed through
to destination clients.
The OS sends messages on the same channels as clients, but the OS
traps these messages so that the client does not see them.Fig.1: OS and Clients use the same ChannelsExecution of an OS agent on a processor may delay a client's
  steps on the same processor, and thus change the order in which the
  client's steps are executed.The OS may change a client's computation -- the order of steps -- but
the OS must not change the client's dataflow.One way to record a global snapshot is for the OS to stop a client
  computation, then take a global snapshot, and then  restart the
client computation.
Our goal is to design an algorithm that does not stop the client.Hereafter, when we refer to an agent we mean an OS agent.
Likewise, by messages we mean those that are sent and received by the
OS.Next, let's develop OS agents and OS messages to record a global
snapshot.The ProblemLet \(S_{init}\) and \(S_{fini}\) be the states in which the
algorithm starts and finishes, respectively.
Design an algorithm that records a state \(S^{*}\) such that
there exists a computation that starts at \(S_{init}\), then visits
\(S^{*}\) and then visits \(S_{fini}\).How Should You Solve the Problem?StrategyA general strategy for designing algorithms dealing with intermediate
states is to find a helpfulproperty of cuts.
What property helps to determine \(S^{*}\)?The property "Computations of Past before Future," given below, tells
us that \(S^{*}\) can be the state at any cut.Computations of Past before Future.Let computation \(X\) start in state \(S_{init}\) and end in state
\(S_{fini}\).
Let \(S^{*}\) be the state at a cut(past, future)of \(X\).
There exists a computation \(Y\) that starts in \(S_{init}\), visits
\(S^{*}\), and ends in \(S_{fini}\).Using this strategy, our tasks reduce to (1) identifying a cut, and
(2) recording the state, \(S^{*}\), at the cut.Identifying a CutEach agent has to record its own state because an agent's state is not
accessible to other agents.
The state of an agent that it records is called alocal snapshot.Definepastas the set of steps at each agent before the
agent takes its local snapshot, and
definefutureas the set of steps at each agent after the
agent takes its local snapshot.
So, if a step \(x\) at an agent is inpastthen all steps
at that agent before \(x\) are also inpast.Let's usethe following property ofpast,futureand cuts:The partition(past, future)is a cut exactly when every
message received inpastis sent inpast.Therefore,(past, future)is a cut exactly when:Global Snapshot RuleEach message received before the receiver takes its local snapshot
is sent before the sender takes its local snapshot.Design an algorithm yourself before reading further, and compare your
algorithm with the one given below.The Global Snapshot AlgorithmA special OS message called amarkeris used to distinguish
pre-snapshot from post-snapshot messages.
Messages sent on a channel before amarkeris sent on the
channel are messages sent in thepast-- i.e. before the
sender takes its local snapshot -- and messages sent
after the marker are sent in thefuture.The algorithmThe algorithm begins by one or more agents taking their local
  snapshots.When an agent takes its local snapshot it sends a marker on each
  of its outgoing channels.When an agent receives a marker, the agent takes its local snapshot
  if it has not already done so.The snapshot of a channel is the sequence of messages received on
  the channel after the receiver takes its snapshot and before the
  receiver receives a marker on the channel.Proof of correctnessFrom rule 3, each message received by an agent \(r\) on a channel
\(c\) before \(r\) takes its 
local snapshot is a message received by \(r\) before \(r\) receives a
marker on channel \(c\).Because channels are first in first out, each message received by
 \(r\) on \(c\) before \(r\) receives a
marker on \(c\) is sent on \(c\) before a marker is
sent on \(c\).From rule 2 each message sent on \(c\) before a marker is sent on
\(c\) is sent before the sender takes its local snapshot.From the three paragraphs above it follows that the global snapshot
rule holds for the algorithm.Proof about States of ChannelsThe messages in a channel at the cut are the
messages sent inpastand received infuture.
These are messages sent before the sender takes its snapshot and
received after the receiver takes its snapshot.
So, the state of a channel is the sequence of
messages received along the channel after the receiver takes its
snapshot and before the receiver receives a marker along the channel.Note: If an agent takes its local snapshot when it receives a marker
along a channel, then the snapshot of the channel is the empty
sequence of messages.Termination of the AlgorithmAfter any agent \(v\) initiates the algorithm, all agents that are
reachable from \(v\) will receive a marker and take their local
snapshots.
If every agent is reachable from an initiator then all agents take
local snapshots.Each agent takes its local snapshot at most once.
So, a marker is sent on a channel at most once.
The computation terminates when all markers are received.Collecting Local Snapshots to form Global
SnapshotsOne way to collect local snapshots is to have an OS agent act as an
observer.
Each agent sends its local snapshots to the observer which puts the
local snapshots together to form the global snapshots.
Successive snapshots are disambiguated by using sequence numbers or
timestamps.Some algorithms carry out distributed computations on local snapshots
without using an observer to collect local snapshots.
Later, we give examples of such algorithms.Applications of Global SnapshotsSystem MonitoringSystems can be monitored by taking global snapshots repeatedly.
Let \(S_{0}, S_{1}, S_{2}, \ldots, \) be the sequence of states
recorded by the system.
From the propertyComputations through Increasing Cutsthere exists a computation that visits each state \(S_{i}\) in order
of increasing \(i\).
The system monitor checks the sequence of snapshots to determine if
some action is required.Rollback and RecoverLet \(S^{*}\) be the most recent snapshot recorded by a system monitor.
From the property,"Computations of Past before Future"there exists a computation that starts at the initial state and later
visits \(S^{*}\).
So, if an error is detected in a computation then the computation can be
restarted from \(S^{*}\) rather than rolling all the way back to the
initial state.Detecting Stable PredicatesAstable predicateis a predicate with the following property: If the
predicate holds at any point in any computation then it continues to
hold forever thereafter in that computation. Equivalently, if a stable
predicate holds in a state \(s\) then it holds in all states reachable
from \(s\).Examples of stable predicates are: "The computation has
terminated," and "The computation is deadlocked."
If a computation has terminated at some point then it remains 
terminated.
Likewise if a computation has deadlocked then it remains deadlocked.Specification of Detection AlgorithmsAn algorithm to detect a stable property \(P\) has the following
specification.If \(P\) holds when the algorithm is initiated then the algorithm
  detects that \(P\) holds.If the algorithm detects that \(P\) holds then \(P\) holds when the
  algorithm terminates.General Detection AlgorithmsA general solution is for the operating system to monitor a client
computation by taking repeated snapshots of the computation.
The OS checks whether a specified stable property holds in each
snapshot.
From the property,"Computations of Past before Future"this general solution satisfies the specification of detection
algorithms.Detection with ObserversThe OS uses an agent, the observer, to collect local snapshots and
form a global snapshot.
The observer inspects the global snapshot to determine if the property
holds in the snapshot.
The OS can also use multiple observers each of which collects
local information from subnetworks; the OS then carries out a
distributed algorithm on its collection of observers.The OS can also execute a distributed algorithm on local
snapshots without having observers collect local information, as
described next.Detection without Observers: Distributed
Algorithms on Local SnapshotsDistributed algorithms on local snapshots operate in two phases.
In the first phase a global snapshot algorithm is executed.
The local snapshot of each agent and its incoming channels are stored
locally, at the agent, without sending the information to observers.In the second phase a distributed algorithm is executed to determine
if the local information stored at agents satisfies a specified global
property, such as "computation has deadlocked."
The algorithm in the second phase operates on unchanging data.
These algorithms are often distributed graph algorithms.The two phases can be executed concurrently in many applications.NextA code skeleton of the algorithm and examples of the global snapshot
algorithm are providedhere.
Nextlogical clocks.K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology



Figure Link: https://kmchandy.github.io/ChannelSnapshots/ChannelSnapshots/ChannelSnapshots.001.jpeg

Caption: Fig.1: OS and Clients use the same Channels


Figure 1 is a representation of two OS agents that manage their
client agents.
Messages sent by a client are recorded by the OS and passed through
to destination clients.
The OS sends messages on the same channels as clients, but the OS
traps these messages so that the client does not see them.Fig.1: OS and Clients use the same ChannelsExecution of an OS agent on a processor may delay a client's
  steps on the same processor, and thus change the order in which the
  client's steps are executed.The OS may change a client's computation -- the order of steps -- but
the OS must not change the client's dataflow.One way to record a global snapshot is for the OS to stop a client
  computation, then take a global snapshot, and then  restart the
client computation.
Our goal is to design an algorithm that does not stop the client.Hereafter, when we refer to an agent we mean an OS agent.
Likewise, by messages we mean those that are sent and received by the
OS.Next, let's develop OS agents and OS messages to record a global
snapshot.The ProblemLet \(S_{init}\) and \(S_{fini}\) be the states in which the
algorithm starts and finishes, respectively.
Design an algorithm that records a state \(S^{*}\) such that
there exists a computation that starts at \(S_{init}\), then visits
\(S^{*}\) and then visits \(S_{fini}\).How Should You Solve the Problem?StrategyA general strategy for designing algorithms dealing with intermediate
states is to find a helpfulproperty of cuts.
What property helps to determine \(S^{*}\)?The property "Computations of Past before Future," given below, tells
us that \(S^{*}\) can be the state at any cut.Computations of Past before Future.Let computation \(X\) start in state \(S_{init}\) and end in state
\(S_{fini}\).
Let \(S^{*}\) be the state at a cut(past, future)of \(X\).
There exists a computation \(Y\) that starts in \(S_{init}\), visits
\(S^{*}\), and ends in \(S_{fini}\).Using this strategy, our tasks reduce to (1) identifying a cut, and
(2) recording the state, \(S^{*}\), at the cut.Identifying a CutEach agent has to record its own state because an agent's state is not
accessible to other agents.
The state of an agent that it records is called alocal snapshot.Definepastas the set of steps at each agent before the
agent takes its local snapshot, and
definefutureas the set of steps at each agent after the
agent takes its local snapshot.
So, if a step \(x\) at an agent is inpastthen all steps
at that agent before \(x\) are also inpast.Let's usethe following property ofpast,futureand cuts:The partition(past, future)is a cut exactly when every
message received inpastis sent inpast.Therefore,(past, future)is a cut exactly when:Global Snapshot RuleEach message received before the receiver takes its local snapshot
is sent before the sender takes its local snapshot.Design an algorithm yourself before reading further, and compare your
algorithm with the one given below.The Global Snapshot AlgorithmA special OS message called amarkeris used to distinguish
pre-snapshot from post-snapshot messages.
Messages sent on a channel before amarkeris sent on the
channel are messages sent in thepast-- i.e. before the
sender takes its local snapshot -- and messages sent
after the marker are sent in thefuture.The algorithmThe algorithm begins by one or more agents taking their local
  snapshots.When an agent takes its local snapshot it sends a marker on each
  of its outgoing channels.When an agent receives a marker, the agent takes its local snapshot
  if it has not already done so.The snapshot of a channel is the sequence of messages received on
  the channel after the receiver takes its snapshot and before the
  receiver receives a marker on the channel.Proof of correctnessFrom rule 3, each message received by an agent \(r\) on a channel
\(c\) before \(r\) takes its 
local snapshot is a message received by \(r\) before \(r\) receives a
marker on channel \(c\).Because channels are first in first out, each message received by
 \(r\) on \(c\) before \(r\) receives a
marker on \(c\) is sent on \(c\) before a marker is
sent on \(c\).From rule 2 each message sent on \(c\) before a marker is sent on
\(c\) is sent before the sender takes its local snapshot.From the three paragraphs above it follows that the global snapshot
rule holds for the algorithm.Proof about States of ChannelsThe messages in a channel at the cut are the
messages sent inpastand received infuture.
These are messages sent before the sender takes its snapshot and
received after the receiver takes its snapshot.
So, the state of a channel is the sequence of
messages received along the channel after the receiver takes its
snapshot and before the receiver receives a marker along the channel.Note: If an agent takes its local snapshot when it receives a marker
along a channel, then the snapshot of the channel is the empty
sequence of messages.Termination of the AlgorithmAfter any agent \(v\) initiates the algorithm, all agents that are
reachable from \(v\) will receive a marker and take their local
snapshots.
If every agent is reachable from an initiator then all agents take
local snapshots.Each agent takes its local snapshot at most once.
So, a marker is sent on a channel at most once.
The computation terminates when all markers are received.Collecting Local Snapshots to form Global
SnapshotsOne way to collect local snapshots is to have an OS agent act as an
observer.
Each agent sends its local snapshots to the observer which puts the
local snapshots together to form the global snapshots.
Successive snapshots are disambiguated by using sequence numbers or
timestamps.Some algorithms carry out distributed computations on local snapshots
without using an observer to collect local snapshots.
Later, we give examples of such algorithms.Applications of Global SnapshotsSystem MonitoringSystems can be monitored by taking global snapshots repeatedly.
Let \(S_{0}, S_{1}, S_{2}, \ldots, \) be the sequence of states
recorded by the system.
From the propertyComputations through Increasing Cutsthere exists a computation that visits each state \(S_{i}\) in order
of increasing \(i\).
The system monitor checks the sequence of snapshots to determine if
some action is required.Rollback and RecoverLet \(S^{*}\) be the most recent snapshot recorded by a system monitor.
From the property,"Computations of Past before Future"there exists a computation that starts at the initial state and later
visits \(S^{*}\).
So, if an error is detected in a computation then the computation can be
restarted from \(S^{*}\) rather than rolling all the way back to the
initial state.Detecting Stable PredicatesAstable predicateis a predicate with the following property: If the
predicate holds at any point in any computation then it continues to
hold forever thereafter in that computation. Equivalently, if a stable
predicate holds in a state \(s\) then it holds in all states reachable
from \(s\).Examples of stable predicates are: "The computation has
terminated," and "The computation is deadlocked."
If a computation has terminated at some point then it remains 
terminated.
Likewise if a computation has deadlocked then it remains deadlocked.Specification of Detection AlgorithmsAn algorithm to detect a stable property \(P\) has the following
specification.If \(P\) holds when the algorithm is initiated then the algorithm
  detects that \(P\) holds.If the algorithm detects that \(P\) holds then \(P\) holds when the
  algorithm terminates.General Detection AlgorithmsA general solution is for the operating system to monitor a client
computation by taking repeated snapshots of the computation.
The OS checks whether a specified stable property holds in each
snapshot.
From the property,"Computations of Past before Future"this general solution satisfies the specification of detection
algorithms.Detection with ObserversThe OS uses an agent, the observer, to collect local snapshots and
form a global snapshot.
The observer inspects the global snapshot to determine if the property
holds in the snapshot.
The OS can also use multiple observers each of which collects
local information from subnetworks; the OS then carries out a
distributed algorithm on its collection of observers.The OS can also execute a distributed algorithm on local
snapshots without having observers collect local information, as
described next.Detection without Observers: Distributed
Algorithms on Local SnapshotsDistributed algorithms on local snapshots operate in two phases.
In the first phase a global snapshot algorithm is executed.
The local snapshot of each agent and its incoming channels are stored
locally, at the agent, without sending the information to observers.In the second phase a distributed algorithm is executed to determine
if the local information stored at agents satisfies a specified global
property, such as "computation has deadlocked."
The algorithm in the second phase operates on unchanging data.
These algorithms are often distributed graph algorithms.The two phases can be executed concurrently in many applications.NextA code skeleton of the algorithm and examples of the global snapshot
algorithm are providedhere.
Nextlogical clocks.K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology



Figure Link: https://kmchandy.github.io/ChannelSnapshots/ChannelSnapshots/ChannelSnapshots.001.jpeg

Caption: Fig.1: OS and Clients use the same Channels


Execution of an OS agent on a processor may delay a client's
  steps on the same processor, and thus change the order in which the
  client's steps are executed.The OS may change a client's computation -- the order of steps -- but
the OS must not change the client's dataflow.One way to record a global snapshot is for the OS to stop a client
  computation, then take a global snapshot, and then  restart the
client computation.
Our goal is to design an algorithm that does not stop the client.Hereafter, when we refer to an agent we mean an OS agent.
Likewise, by messages we mean those that are sent and received by the
OS.Next, let's develop OS agents and OS messages to record a global
snapshot.The ProblemLet \(S_{init}\) and \(S_{fini}\) be the states in which the
algorithm starts and finishes, respectively.
Design an algorithm that records a state \(S^{*}\) such that
there exists a computation that starts at \(S_{init}\), then visits
\(S^{*}\) and then visits \(S_{fini}\).How Should You Solve the Problem?StrategyA general strategy for designing algorithms dealing with intermediate
states is to find a helpfulproperty of cuts.
What property helps to determine \(S^{*}\)?The property "Computations of Past before Future," given below, tells
us that \(S^{*}\) can be the state at any cut.Computations of Past before Future.Let computation \(X\) start in state \(S_{init}\) and end in state
\(S_{fini}\).
Let \(S^{*}\) be the state at a cut(past, future)of \(X\).
There exists a computation \(Y\) that starts in \(S_{init}\), visits
\(S^{*}\), and ends in \(S_{fini}\).Using this strategy, our tasks reduce to (1) identifying a cut, and
(2) recording the state, \(S^{*}\), at the cut.Identifying a CutEach agent has to record its own state because an agent's state is not
accessible to other agents.
The state of an agent that it records is called alocal snapshot.Definepastas the set of steps at each agent before the
agent takes its local snapshot, and
definefutureas the set of steps at each agent after the
agent takes its local snapshot.
So, if a step \(x\) at an agent is inpastthen all steps
at that agent before \(x\) are also inpast.Let's usethe following property ofpast,futureand cuts:The partition(past, future)is a cut exactly when every
message received inpastis sent inpast.Therefore,(past, future)is a cut exactly when:Global Snapshot RuleEach message received before the receiver takes its local snapshot
is sent before the sender takes its local snapshot.Design an algorithm yourself before reading further, and compare your
algorithm with the one given below.The Global Snapshot AlgorithmA special OS message called amarkeris used to distinguish
pre-snapshot from post-snapshot messages.
Messages sent on a channel before amarkeris sent on the
channel are messages sent in thepast-- i.e. before the
sender takes its local snapshot -- and messages sent
after the marker are sent in thefuture.The algorithmThe algorithm begins by one or more agents taking their local
  snapshots.When an agent takes its local snapshot it sends a marker on each
  of its outgoing channels.When an agent receives a marker, the agent takes its local snapshot
  if it has not already done so.The snapshot of a channel is the sequence of messages received on
  the channel after the receiver takes its snapshot and before the
  receiver receives a marker on the channel.Proof of correctnessFrom rule 3, each message received by an agent \(r\) on a channel
\(c\) before \(r\) takes its 
local snapshot is a message received by \(r\) before \(r\) receives a
marker on channel \(c\).Because channels are first in first out, each message received by
 \(r\) on \(c\) before \(r\) receives a
marker on \(c\) is sent on \(c\) before a marker is
sent on \(c\).From rule 2 each message sent on \(c\) before a marker is sent on
\(c\) is sent before the sender takes its local snapshot.From the three paragraphs above it follows that the global snapshot
rule holds for the algorithm.Proof about States of ChannelsThe messages in a channel at the cut are the
messages sent inpastand received infuture.
These are messages sent before the sender takes its snapshot and
received after the receiver takes its snapshot.
So, the state of a channel is the sequence of
messages received along the channel after the receiver takes its
snapshot and before the receiver receives a marker along the channel.Note: If an agent takes its local snapshot when it receives a marker
along a channel, then the snapshot of the channel is the empty
sequence of messages.Termination of the AlgorithmAfter any agent \(v\) initiates the algorithm, all agents that are
reachable from \(v\) will receive a marker and take their local
snapshots.
If every agent is reachable from an initiator then all agents take
local snapshots.Each agent takes its local snapshot at most once.
So, a marker is sent on a channel at most once.
The computation terminates when all markers are received.Collecting Local Snapshots to form Global
SnapshotsOne way to collect local snapshots is to have an OS agent act as an
observer.
Each agent sends its local snapshots to the observer which puts the
local snapshots together to form the global snapshots.
Successive snapshots are disambiguated by using sequence numbers or
timestamps.Some algorithms carry out distributed computations on local snapshots
without using an observer to collect local snapshots.
Later, we give examples of such algorithms.Applications of Global SnapshotsSystem MonitoringSystems can be monitored by taking global snapshots repeatedly.
Let \(S_{0}, S_{1}, S_{2}, \ldots, \) be the sequence of states
recorded by the system.
From the propertyComputations through Increasing Cutsthere exists a computation that visits each state \(S_{i}\) in order
of increasing \(i\).
The system monitor checks the sequence of snapshots to determine if
some action is required.Rollback and RecoverLet \(S^{*}\) be the most recent snapshot recorded by a system monitor.
From the property,"Computations of Past before Future"there exists a computation that starts at the initial state and later
visits \(S^{*}\).
So, if an error is detected in a computation then the computation can be
restarted from \(S^{*}\) rather than rolling all the way back to the
initial state.Detecting Stable PredicatesAstable predicateis a predicate with the following property: If the
predicate holds at any point in any computation then it continues to
hold forever thereafter in that computation. Equivalently, if a stable
predicate holds in a state \(s\) then it holds in all states reachable
from \(s\).Examples of stable predicates are: "The computation has
terminated," and "The computation is deadlocked."
If a computation has terminated at some point then it remains 
terminated.
Likewise if a computation has deadlocked then it remains deadlocked.Specification of Detection AlgorithmsAn algorithm to detect a stable property \(P\) has the following
specification.If \(P\) holds when the algorithm is initiated then the algorithm
  detects that \(P\) holds.If the algorithm detects that \(P\) holds then \(P\) holds when the
  algorithm terminates.General Detection AlgorithmsA general solution is for the operating system to monitor a client
computation by taking repeated snapshots of the computation.
The OS checks whether a specified stable property holds in each
snapshot.
From the property,"Computations of Past before Future"this general solution satisfies the specification of detection
algorithms.Detection with ObserversThe OS uses an agent, the observer, to collect local snapshots and
form a global snapshot.
The observer inspects the global snapshot to determine if the property
holds in the snapshot.
The OS can also use multiple observers each of which collects
local information from subnetworks; the OS then carries out a
distributed algorithm on its collection of observers.The OS can also execute a distributed algorithm on local
snapshots without having observers collect local information, as
described next.Detection without Observers: Distributed
Algorithms on Local SnapshotsDistributed algorithms on local snapshots operate in two phases.
In the first phase a global snapshot algorithm is executed.
The local snapshot of each agent and its incoming channels are stored
locally, at the agent, without sending the information to observers.In the second phase a distributed algorithm is executed to determine
if the local information stored at agents satisfies a specified global
property, such as "computation has deadlocked."
The algorithm in the second phase operates on unchanging data.
These algorithms are often distributed graph algorithms.The two phases can be executed concurrently in many applications.NextA code skeleton of the algorithm and examples of the global snapshot
algorithm are providedhere.
Nextlogical clocks.K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

One way to record a global snapshot is for the OS to stop a client
  computation, then take a global snapshot, and then  restart the
client computation.
Our goal is to design an algorithm that does not stop the client.Hereafter, when we refer to an agent we mean an OS agent.
Likewise, by messages we mean those that are sent and received by the
OS.Next, let's develop OS agents and OS messages to record a global
snapshot.The ProblemLet \(S_{init}\) and \(S_{fini}\) be the states in which the
algorithm starts and finishes, respectively.
Design an algorithm that records a state \(S^{*}\) such that
there exists a computation that starts at \(S_{init}\), then visits
\(S^{*}\) and then visits \(S_{fini}\).How Should You Solve the Problem?StrategyA general strategy for designing algorithms dealing with intermediate
states is to find a helpfulproperty of cuts.
What property helps to determine \(S^{*}\)?The property "Computations of Past before Future," given below, tells
us that \(S^{*}\) can be the state at any cut.Computations of Past before Future.Let computation \(X\) start in state \(S_{init}\) and end in state
\(S_{fini}\).
Let \(S^{*}\) be the state at a cut(past, future)of \(X\).
There exists a computation \(Y\) that starts in \(S_{init}\), visits
\(S^{*}\), and ends in \(S_{fini}\).Using this strategy, our tasks reduce to (1) identifying a cut, and
(2) recording the state, \(S^{*}\), at the cut.Identifying a CutEach agent has to record its own state because an agent's state is not
accessible to other agents.
The state of an agent that it records is called alocal snapshot.Definepastas the set of steps at each agent before the
agent takes its local snapshot, and
definefutureas the set of steps at each agent after the
agent takes its local snapshot.
So, if a step \(x\) at an agent is inpastthen all steps
at that agent before \(x\) are also inpast.Let's usethe following property ofpast,futureand cuts:The partition(past, future)is a cut exactly when every
message received inpastis sent inpast.Therefore,(past, future)is a cut exactly when:Global Snapshot RuleEach message received before the receiver takes its local snapshot
is sent before the sender takes its local snapshot.Design an algorithm yourself before reading further, and compare your
algorithm with the one given below.The Global Snapshot AlgorithmA special OS message called amarkeris used to distinguish
pre-snapshot from post-snapshot messages.
Messages sent on a channel before amarkeris sent on the
channel are messages sent in thepast-- i.e. before the
sender takes its local snapshot -- and messages sent
after the marker are sent in thefuture.The algorithmThe algorithm begins by one or more agents taking their local
  snapshots.When an agent takes its local snapshot it sends a marker on each
  of its outgoing channels.When an agent receives a marker, the agent takes its local snapshot
  if it has not already done so.The snapshot of a channel is the sequence of messages received on
  the channel after the receiver takes its snapshot and before the
  receiver receives a marker on the channel.Proof of correctnessFrom rule 3, each message received by an agent \(r\) on a channel
\(c\) before \(r\) takes its 
local snapshot is a message received by \(r\) before \(r\) receives a
marker on channel \(c\).Because channels are first in first out, each message received by
 \(r\) on \(c\) before \(r\) receives a
marker on \(c\) is sent on \(c\) before a marker is
sent on \(c\).From rule 2 each message sent on \(c\) before a marker is sent on
\(c\) is sent before the sender takes its local snapshot.From the three paragraphs above it follows that the global snapshot
rule holds for the algorithm.Proof about States of ChannelsThe messages in a channel at the cut are the
messages sent inpastand received infuture.
These are messages sent before the sender takes its snapshot and
received after the receiver takes its snapshot.
So, the state of a channel is the sequence of
messages received along the channel after the receiver takes its
snapshot and before the receiver receives a marker along the channel.Note: If an agent takes its local snapshot when it receives a marker
along a channel, then the snapshot of the channel is the empty
sequence of messages.Termination of the AlgorithmAfter any agent \(v\) initiates the algorithm, all agents that are
reachable from \(v\) will receive a marker and take their local
snapshots.
If every agent is reachable from an initiator then all agents take
local snapshots.Each agent takes its local snapshot at most once.
So, a marker is sent on a channel at most once.
The computation terminates when all markers are received.Collecting Local Snapshots to form Global
SnapshotsOne way to collect local snapshots is to have an OS agent act as an
observer.
Each agent sends its local snapshots to the observer which puts the
local snapshots together to form the global snapshots.
Successive snapshots are disambiguated by using sequence numbers or
timestamps.Some algorithms carry out distributed computations on local snapshots
without using an observer to collect local snapshots.
Later, we give examples of such algorithms.Applications of Global SnapshotsSystem MonitoringSystems can be monitored by taking global snapshots repeatedly.
Let \(S_{0}, S_{1}, S_{2}, \ldots, \) be the sequence of states
recorded by the system.
From the propertyComputations through Increasing Cutsthere exists a computation that visits each state \(S_{i}\) in order
of increasing \(i\).
The system monitor checks the sequence of snapshots to determine if
some action is required.Rollback and RecoverLet \(S^{*}\) be the most recent snapshot recorded by a system monitor.
From the property,"Computations of Past before Future"there exists a computation that starts at the initial state and later
visits \(S^{*}\).
So, if an error is detected in a computation then the computation can be
restarted from \(S^{*}\) rather than rolling all the way back to the
initial state.Detecting Stable PredicatesAstable predicateis a predicate with the following property: If the
predicate holds at any point in any computation then it continues to
hold forever thereafter in that computation. Equivalently, if a stable
predicate holds in a state \(s\) then it holds in all states reachable
from \(s\).Examples of stable predicates are: "The computation has
terminated," and "The computation is deadlocked."
If a computation has terminated at some point then it remains 
terminated.
Likewise if a computation has deadlocked then it remains deadlocked.Specification of Detection AlgorithmsAn algorithm to detect a stable property \(P\) has the following
specification.If \(P\) holds when the algorithm is initiated then the algorithm
  detects that \(P\) holds.If the algorithm detects that \(P\) holds then \(P\) holds when the
  algorithm terminates.General Detection AlgorithmsA general solution is for the operating system to monitor a client
computation by taking repeated snapshots of the computation.
The OS checks whether a specified stable property holds in each
snapshot.
From the property,"Computations of Past before Future"this general solution satisfies the specification of detection
algorithms.Detection with ObserversThe OS uses an agent, the observer, to collect local snapshots and
form a global snapshot.
The observer inspects the global snapshot to determine if the property
holds in the snapshot.
The OS can also use multiple observers each of which collects
local information from subnetworks; the OS then carries out a
distributed algorithm on its collection of observers.The OS can also execute a distributed algorithm on local
snapshots without having observers collect local information, as
described next.Detection without Observers: Distributed
Algorithms on Local SnapshotsDistributed algorithms on local snapshots operate in two phases.
In the first phase a global snapshot algorithm is executed.
The local snapshot of each agent and its incoming channels are stored
locally, at the agent, without sending the information to observers.In the second phase a distributed algorithm is executed to determine
if the local information stored at agents satisfies a specified global
property, such as "computation has deadlocked."
The algorithm in the second phase operates on unchanging data.
These algorithms are often distributed graph algorithms.The two phases can be executed concurrently in many applications.NextA code skeleton of the algorithm and examples of the global snapshot
algorithm are providedhere.
Nextlogical clocks.K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

Hereafter, when we refer to an agent we mean an OS agent.
Likewise, by messages we mean those that are sent and received by the
OS.Next, let's develop OS agents and OS messages to record a global
snapshot.The ProblemLet \(S_{init}\) and \(S_{fini}\) be the states in which the
algorithm starts and finishes, respectively.
Design an algorithm that records a state \(S^{*}\) such that
there exists a computation that starts at \(S_{init}\), then visits
\(S^{*}\) and then visits \(S_{fini}\).How Should You Solve the Problem?StrategyA general strategy for designing algorithms dealing with intermediate
states is to find a helpfulproperty of cuts.
What property helps to determine \(S^{*}\)?The property "Computations of Past before Future," given below, tells
us that \(S^{*}\) can be the state at any cut.Computations of Past before Future.Let computation \(X\) start in state \(S_{init}\) and end in state
\(S_{fini}\).
Let \(S^{*}\) be the state at a cut(past, future)of \(X\).
There exists a computation \(Y\) that starts in \(S_{init}\), visits
\(S^{*}\), and ends in \(S_{fini}\).Using this strategy, our tasks reduce to (1) identifying a cut, and
(2) recording the state, \(S^{*}\), at the cut.Identifying a CutEach agent has to record its own state because an agent's state is not
accessible to other agents.
The state of an agent that it records is called alocal snapshot.Definepastas the set of steps at each agent before the
agent takes its local snapshot, and
definefutureas the set of steps at each agent after the
agent takes its local snapshot.
So, if a step \(x\) at an agent is inpastthen all steps
at that agent before \(x\) are also inpast.Let's usethe following property ofpast,futureand cuts:The partition(past, future)is a cut exactly when every
message received inpastis sent inpast.Therefore,(past, future)is a cut exactly when:Global Snapshot RuleEach message received before the receiver takes its local snapshot
is sent before the sender takes its local snapshot.Design an algorithm yourself before reading further, and compare your
algorithm with the one given below.The Global Snapshot AlgorithmA special OS message called amarkeris used to distinguish
pre-snapshot from post-snapshot messages.
Messages sent on a channel before amarkeris sent on the
channel are messages sent in thepast-- i.e. before the
sender takes its local snapshot -- and messages sent
after the marker are sent in thefuture.The algorithmThe algorithm begins by one or more agents taking their local
  snapshots.When an agent takes its local snapshot it sends a marker on each
  of its outgoing channels.When an agent receives a marker, the agent takes its local snapshot
  if it has not already done so.The snapshot of a channel is the sequence of messages received on
  the channel after the receiver takes its snapshot and before the
  receiver receives a marker on the channel.Proof of correctnessFrom rule 3, each message received by an agent \(r\) on a channel
\(c\) before \(r\) takes its 
local snapshot is a message received by \(r\) before \(r\) receives a
marker on channel \(c\).Because channels are first in first out, each message received by
 \(r\) on \(c\) before \(r\) receives a
marker on \(c\) is sent on \(c\) before a marker is
sent on \(c\).From rule 2 each message sent on \(c\) before a marker is sent on
\(c\) is sent before the sender takes its local snapshot.From the three paragraphs above it follows that the global snapshot
rule holds for the algorithm.Proof about States of ChannelsThe messages in a channel at the cut are the
messages sent inpastand received infuture.
These are messages sent before the sender takes its snapshot and
received after the receiver takes its snapshot.
So, the state of a channel is the sequence of
messages received along the channel after the receiver takes its
snapshot and before the receiver receives a marker along the channel.Note: If an agent takes its local snapshot when it receives a marker
along a channel, then the snapshot of the channel is the empty
sequence of messages.Termination of the AlgorithmAfter any agent \(v\) initiates the algorithm, all agents that are
reachable from \(v\) will receive a marker and take their local
snapshots.
If every agent is reachable from an initiator then all agents take
local snapshots.Each agent takes its local snapshot at most once.
So, a marker is sent on a channel at most once.
The computation terminates when all markers are received.Collecting Local Snapshots to form Global
SnapshotsOne way to collect local snapshots is to have an OS agent act as an
observer.
Each agent sends its local snapshots to the observer which puts the
local snapshots together to form the global snapshots.
Successive snapshots are disambiguated by using sequence numbers or
timestamps.Some algorithms carry out distributed computations on local snapshots
without using an observer to collect local snapshots.
Later, we give examples of such algorithms.Applications of Global SnapshotsSystem MonitoringSystems can be monitored by taking global snapshots repeatedly.
Let \(S_{0}, S_{1}, S_{2}, \ldots, \) be the sequence of states
recorded by the system.
From the propertyComputations through Increasing Cutsthere exists a computation that visits each state \(S_{i}\) in order
of increasing \(i\).
The system monitor checks the sequence of snapshots to determine if
some action is required.Rollback and RecoverLet \(S^{*}\) be the most recent snapshot recorded by a system monitor.
From the property,"Computations of Past before Future"there exists a computation that starts at the initial state and later
visits \(S^{*}\).
So, if an error is detected in a computation then the computation can be
restarted from \(S^{*}\) rather than rolling all the way back to the
initial state.Detecting Stable PredicatesAstable predicateis a predicate with the following property: If the
predicate holds at any point in any computation then it continues to
hold forever thereafter in that computation. Equivalently, if a stable
predicate holds in a state \(s\) then it holds in all states reachable
from \(s\).Examples of stable predicates are: "The computation has
terminated," and "The computation is deadlocked."
If a computation has terminated at some point then it remains 
terminated.
Likewise if a computation has deadlocked then it remains deadlocked.Specification of Detection AlgorithmsAn algorithm to detect a stable property \(P\) has the following
specification.If \(P\) holds when the algorithm is initiated then the algorithm
  detects that \(P\) holds.If the algorithm detects that \(P\) holds then \(P\) holds when the
  algorithm terminates.General Detection AlgorithmsA general solution is for the operating system to monitor a client
computation by taking repeated snapshots of the computation.
The OS checks whether a specified stable property holds in each
snapshot.
From the property,"Computations of Past before Future"this general solution satisfies the specification of detection
algorithms.Detection with ObserversThe OS uses an agent, the observer, to collect local snapshots and
form a global snapshot.
The observer inspects the global snapshot to determine if the property
holds in the snapshot.
The OS can also use multiple observers each of which collects
local information from subnetworks; the OS then carries out a
distributed algorithm on its collection of observers.The OS can also execute a distributed algorithm on local
snapshots without having observers collect local information, as
described next.Detection without Observers: Distributed
Algorithms on Local SnapshotsDistributed algorithms on local snapshots operate in two phases.
In the first phase a global snapshot algorithm is executed.
The local snapshot of each agent and its incoming channels are stored
locally, at the agent, without sending the information to observers.In the second phase a distributed algorithm is executed to determine
if the local information stored at agents satisfies a specified global
property, such as "computation has deadlocked."
The algorithm in the second phase operates on unchanging data.
These algorithms are often distributed graph algorithms.The two phases can be executed concurrently in many applications.NextA code skeleton of the algorithm and examples of the global snapshot
algorithm are providedhere.
Nextlogical clocks.K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

Next, let's develop OS agents and OS messages to record a global
snapshot.The ProblemLet \(S_{init}\) and \(S_{fini}\) be the states in which the
algorithm starts and finishes, respectively.
Design an algorithm that records a state \(S^{*}\) such that
there exists a computation that starts at \(S_{init}\), then visits
\(S^{*}\) and then visits \(S_{fini}\).How Should You Solve the Problem?StrategyA general strategy for designing algorithms dealing with intermediate
states is to find a helpfulproperty of cuts.
What property helps to determine \(S^{*}\)?The property "Computations of Past before Future," given below, tells
us that \(S^{*}\) can be the state at any cut.Computations of Past before Future.Let computation \(X\) start in state \(S_{init}\) and end in state
\(S_{fini}\).
Let \(S^{*}\) be the state at a cut(past, future)of \(X\).
There exists a computation \(Y\) that starts in \(S_{init}\), visits
\(S^{*}\), and ends in \(S_{fini}\).Using this strategy, our tasks reduce to (1) identifying a cut, and
(2) recording the state, \(S^{*}\), at the cut.Identifying a CutEach agent has to record its own state because an agent's state is not
accessible to other agents.
The state of an agent that it records is called alocal snapshot.Definepastas the set of steps at each agent before the
agent takes its local snapshot, and
definefutureas the set of steps at each agent after the
agent takes its local snapshot.
So, if a step \(x\) at an agent is inpastthen all steps
at that agent before \(x\) are also inpast.Let's usethe following property ofpast,futureand cuts:The partition(past, future)is a cut exactly when every
message received inpastis sent inpast.Therefore,(past, future)is a cut exactly when:Global Snapshot RuleEach message received before the receiver takes its local snapshot
is sent before the sender takes its local snapshot.Design an algorithm yourself before reading further, and compare your
algorithm with the one given below.The Global Snapshot AlgorithmA special OS message called amarkeris used to distinguish
pre-snapshot from post-snapshot messages.
Messages sent on a channel before amarkeris sent on the
channel are messages sent in thepast-- i.e. before the
sender takes its local snapshot -- and messages sent
after the marker are sent in thefuture.The algorithmThe algorithm begins by one or more agents taking their local
  snapshots.When an agent takes its local snapshot it sends a marker on each
  of its outgoing channels.When an agent receives a marker, the agent takes its local snapshot
  if it has not already done so.The snapshot of a channel is the sequence of messages received on
  the channel after the receiver takes its snapshot and before the
  receiver receives a marker on the channel.Proof of correctnessFrom rule 3, each message received by an agent \(r\) on a channel
\(c\) before \(r\) takes its 
local snapshot is a message received by \(r\) before \(r\) receives a
marker on channel \(c\).Because channels are first in first out, each message received by
 \(r\) on \(c\) before \(r\) receives a
marker on \(c\) is sent on \(c\) before a marker is
sent on \(c\).From rule 2 each message sent on \(c\) before a marker is sent on
\(c\) is sent before the sender takes its local snapshot.From the three paragraphs above it follows that the global snapshot
rule holds for the algorithm.Proof about States of ChannelsThe messages in a channel at the cut are the
messages sent inpastand received infuture.
These are messages sent before the sender takes its snapshot and
received after the receiver takes its snapshot.
So, the state of a channel is the sequence of
messages received along the channel after the receiver takes its
snapshot and before the receiver receives a marker along the channel.Note: If an agent takes its local snapshot when it receives a marker
along a channel, then the snapshot of the channel is the empty
sequence of messages.Termination of the AlgorithmAfter any agent \(v\) initiates the algorithm, all agents that are
reachable from \(v\) will receive a marker and take their local
snapshots.
If every agent is reachable from an initiator then all agents take
local snapshots.Each agent takes its local snapshot at most once.
So, a marker is sent on a channel at most once.
The computation terminates when all markers are received.Collecting Local Snapshots to form Global
SnapshotsOne way to collect local snapshots is to have an OS agent act as an
observer.
Each agent sends its local snapshots to the observer which puts the
local snapshots together to form the global snapshots.
Successive snapshots are disambiguated by using sequence numbers or
timestamps.Some algorithms carry out distributed computations on local snapshots
without using an observer to collect local snapshots.
Later, we give examples of such algorithms.Applications of Global SnapshotsSystem MonitoringSystems can be monitored by taking global snapshots repeatedly.
Let \(S_{0}, S_{1}, S_{2}, \ldots, \) be the sequence of states
recorded by the system.
From the propertyComputations through Increasing Cutsthere exists a computation that visits each state \(S_{i}\) in order
of increasing \(i\).
The system monitor checks the sequence of snapshots to determine if
some action is required.Rollback and RecoverLet \(S^{*}\) be the most recent snapshot recorded by a system monitor.
From the property,"Computations of Past before Future"there exists a computation that starts at the initial state and later
visits \(S^{*}\).
So, if an error is detected in a computation then the computation can be
restarted from \(S^{*}\) rather than rolling all the way back to the
initial state.Detecting Stable PredicatesAstable predicateis a predicate with the following property: If the
predicate holds at any point in any computation then it continues to
hold forever thereafter in that computation. Equivalently, if a stable
predicate holds in a state \(s\) then it holds in all states reachable
from \(s\).Examples of stable predicates are: "The computation has
terminated," and "The computation is deadlocked."
If a computation has terminated at some point then it remains 
terminated.
Likewise if a computation has deadlocked then it remains deadlocked.Specification of Detection AlgorithmsAn algorithm to detect a stable property \(P\) has the following
specification.If \(P\) holds when the algorithm is initiated then the algorithm
  detects that \(P\) holds.If the algorithm detects that \(P\) holds then \(P\) holds when the
  algorithm terminates.General Detection AlgorithmsA general solution is for the operating system to monitor a client
computation by taking repeated snapshots of the computation.
The OS checks whether a specified stable property holds in each
snapshot.
From the property,"Computations of Past before Future"this general solution satisfies the specification of detection
algorithms.Detection with ObserversThe OS uses an agent, the observer, to collect local snapshots and
form a global snapshot.
The observer inspects the global snapshot to determine if the property
holds in the snapshot.
The OS can also use multiple observers each of which collects
local information from subnetworks; the OS then carries out a
distributed algorithm on its collection of observers.The OS can also execute a distributed algorithm on local
snapshots without having observers collect local information, as
described next.Detection without Observers: Distributed
Algorithms on Local SnapshotsDistributed algorithms on local snapshots operate in two phases.
In the first phase a global snapshot algorithm is executed.
The local snapshot of each agent and its incoming channels are stored
locally, at the agent, without sending the information to observers.In the second phase a distributed algorithm is executed to determine
if the local information stored at agents satisfies a specified global
property, such as "computation has deadlocked."
The algorithm in the second phase operates on unchanging data.
These algorithms are often distributed graph algorithms.The two phases can be executed concurrently in many applications.NextA code skeleton of the algorithm and examples of the global snapshot
algorithm are providedhere.
Nextlogical clocks.K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

Let \(S_{init}\) and \(S_{fini}\) be the states in which the
algorithm starts and finishes, respectively.
Design an algorithm that records a state \(S^{*}\) such that
there exists a computation that starts at \(S_{init}\), then visits
\(S^{*}\) and then visits \(S_{fini}\).How Should You Solve the Problem?StrategyA general strategy for designing algorithms dealing with intermediate
states is to find a helpfulproperty of cuts.
What property helps to determine \(S^{*}\)?The property "Computations of Past before Future," given below, tells
us that \(S^{*}\) can be the state at any cut.Computations of Past before Future.Let computation \(X\) start in state \(S_{init}\) and end in state
\(S_{fini}\).
Let \(S^{*}\) be the state at a cut(past, future)of \(X\).
There exists a computation \(Y\) that starts in \(S_{init}\), visits
\(S^{*}\), and ends in \(S_{fini}\).Using this strategy, our tasks reduce to (1) identifying a cut, and
(2) recording the state, \(S^{*}\), at the cut.Identifying a CutEach agent has to record its own state because an agent's state is not
accessible to other agents.
The state of an agent that it records is called alocal snapshot.Definepastas the set of steps at each agent before the
agent takes its local snapshot, and
definefutureas the set of steps at each agent after the
agent takes its local snapshot.
So, if a step \(x\) at an agent is inpastthen all steps
at that agent before \(x\) are also inpast.Let's usethe following property ofpast,futureand cuts:The partition(past, future)is a cut exactly when every
message received inpastis sent inpast.Therefore,(past, future)is a cut exactly when:Global Snapshot RuleEach message received before the receiver takes its local snapshot
is sent before the sender takes its local snapshot.Design an algorithm yourself before reading further, and compare your
algorithm with the one given below.The Global Snapshot AlgorithmA special OS message called amarkeris used to distinguish
pre-snapshot from post-snapshot messages.
Messages sent on a channel before amarkeris sent on the
channel are messages sent in thepast-- i.e. before the
sender takes its local snapshot -- and messages sent
after the marker are sent in thefuture.The algorithmThe algorithm begins by one or more agents taking their local
  snapshots.When an agent takes its local snapshot it sends a marker on each
  of its outgoing channels.When an agent receives a marker, the agent takes its local snapshot
  if it has not already done so.The snapshot of a channel is the sequence of messages received on
  the channel after the receiver takes its snapshot and before the
  receiver receives a marker on the channel.Proof of correctnessFrom rule 3, each message received by an agent \(r\) on a channel
\(c\) before \(r\) takes its 
local snapshot is a message received by \(r\) before \(r\) receives a
marker on channel \(c\).Because channels are first in first out, each message received by
 \(r\) on \(c\) before \(r\) receives a
marker on \(c\) is sent on \(c\) before a marker is
sent on \(c\).From rule 2 each message sent on \(c\) before a marker is sent on
\(c\) is sent before the sender takes its local snapshot.From the three paragraphs above it follows that the global snapshot
rule holds for the algorithm.Proof about States of ChannelsThe messages in a channel at the cut are the
messages sent inpastand received infuture.
These are messages sent before the sender takes its snapshot and
received after the receiver takes its snapshot.
So, the state of a channel is the sequence of
messages received along the channel after the receiver takes its
snapshot and before the receiver receives a marker along the channel.Note: If an agent takes its local snapshot when it receives a marker
along a channel, then the snapshot of the channel is the empty
sequence of messages.Termination of the AlgorithmAfter any agent \(v\) initiates the algorithm, all agents that are
reachable from \(v\) will receive a marker and take their local
snapshots.
If every agent is reachable from an initiator then all agents take
local snapshots.Each agent takes its local snapshot at most once.
So, a marker is sent on a channel at most once.
The computation terminates when all markers are received.Collecting Local Snapshots to form Global
SnapshotsOne way to collect local snapshots is to have an OS agent act as an
observer.
Each agent sends its local snapshots to the observer which puts the
local snapshots together to form the global snapshots.
Successive snapshots are disambiguated by using sequence numbers or
timestamps.Some algorithms carry out distributed computations on local snapshots
without using an observer to collect local snapshots.
Later, we give examples of such algorithms.Applications of Global SnapshotsSystem MonitoringSystems can be monitored by taking global snapshots repeatedly.
Let \(S_{0}, S_{1}, S_{2}, \ldots, \) be the sequence of states
recorded by the system.
From the propertyComputations through Increasing Cutsthere exists a computation that visits each state \(S_{i}\) in order
of increasing \(i\).
The system monitor checks the sequence of snapshots to determine if
some action is required.Rollback and RecoverLet \(S^{*}\) be the most recent snapshot recorded by a system monitor.
From the property,"Computations of Past before Future"there exists a computation that starts at the initial state and later
visits \(S^{*}\).
So, if an error is detected in a computation then the computation can be
restarted from \(S^{*}\) rather than rolling all the way back to the
initial state.Detecting Stable PredicatesAstable predicateis a predicate with the following property: If the
predicate holds at any point in any computation then it continues to
hold forever thereafter in that computation. Equivalently, if a stable
predicate holds in a state \(s\) then it holds in all states reachable
from \(s\).Examples of stable predicates are: "The computation has
terminated," and "The computation is deadlocked."
If a computation has terminated at some point then it remains 
terminated.
Likewise if a computation has deadlocked then it remains deadlocked.Specification of Detection AlgorithmsAn algorithm to detect a stable property \(P\) has the following
specification.If \(P\) holds when the algorithm is initiated then the algorithm
  detects that \(P\) holds.If the algorithm detects that \(P\) holds then \(P\) holds when the
  algorithm terminates.General Detection AlgorithmsA general solution is for the operating system to monitor a client
computation by taking repeated snapshots of the computation.
The OS checks whether a specified stable property holds in each
snapshot.
From the property,"Computations of Past before Future"this general solution satisfies the specification of detection
algorithms.Detection with ObserversThe OS uses an agent, the observer, to collect local snapshots and
form a global snapshot.
The observer inspects the global snapshot to determine if the property
holds in the snapshot.
The OS can also use multiple observers each of which collects
local information from subnetworks; the OS then carries out a
distributed algorithm on its collection of observers.The OS can also execute a distributed algorithm on local
snapshots without having observers collect local information, as
described next.Detection without Observers: Distributed
Algorithms on Local SnapshotsDistributed algorithms on local snapshots operate in two phases.
In the first phase a global snapshot algorithm is executed.
The local snapshot of each agent and its incoming channels are stored
locally, at the agent, without sending the information to observers.In the second phase a distributed algorithm is executed to determine
if the local information stored at agents satisfies a specified global
property, such as "computation has deadlocked."
The algorithm in the second phase operates on unchanging data.
These algorithms are often distributed graph algorithms.The two phases can be executed concurrently in many applications.NextA code skeleton of the algorithm and examples of the global snapshot
algorithm are providedhere.
Nextlogical clocks.K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

Strategy

The property "Computations of Past before Future," given below, tells
us that \(S^{*}\) can be the state at any cut.Computations of Past before Future.Let computation \(X\) start in state \(S_{init}\) and end in state
\(S_{fini}\).
Let \(S^{*}\) be the state at a cut(past, future)of \(X\).
There exists a computation \(Y\) that starts in \(S_{init}\), visits
\(S^{*}\), and ends in \(S_{fini}\).Using this strategy, our tasks reduce to (1) identifying a cut, and
(2) recording the state, \(S^{*}\), at the cut.Identifying a CutEach agent has to record its own state because an agent's state is not
accessible to other agents.
The state of an agent that it records is called alocal snapshot.Definepastas the set of steps at each agent before the
agent takes its local snapshot, and
definefutureas the set of steps at each agent after the
agent takes its local snapshot.
So, if a step \(x\) at an agent is inpastthen all steps
at that agent before \(x\) are also inpast.Let's usethe following property ofpast,futureand cuts:The partition(past, future)is a cut exactly when every
message received inpastis sent inpast.Therefore,(past, future)is a cut exactly when:Global Snapshot RuleEach message received before the receiver takes its local snapshot
is sent before the sender takes its local snapshot.Design an algorithm yourself before reading further, and compare your
algorithm with the one given below.The Global Snapshot AlgorithmA special OS message called amarkeris used to distinguish
pre-snapshot from post-snapshot messages.
Messages sent on a channel before amarkeris sent on the
channel are messages sent in thepast-- i.e. before the
sender takes its local snapshot -- and messages sent
after the marker are sent in thefuture.The algorithmThe algorithm begins by one or more agents taking their local
  snapshots.When an agent takes its local snapshot it sends a marker on each
  of its outgoing channels.When an agent receives a marker, the agent takes its local snapshot
  if it has not already done so.The snapshot of a channel is the sequence of messages received on
  the channel after the receiver takes its snapshot and before the
  receiver receives a marker on the channel.Proof of correctnessFrom rule 3, each message received by an agent \(r\) on a channel
\(c\) before \(r\) takes its 
local snapshot is a message received by \(r\) before \(r\) receives a
marker on channel \(c\).Because channels are first in first out, each message received by
 \(r\) on \(c\) before \(r\) receives a
marker on \(c\) is sent on \(c\) before a marker is
sent on \(c\).From rule 2 each message sent on \(c\) before a marker is sent on
\(c\) is sent before the sender takes its local snapshot.From the three paragraphs above it follows that the global snapshot
rule holds for the algorithm.Proof about States of ChannelsThe messages in a channel at the cut are the
messages sent inpastand received infuture.
These are messages sent before the sender takes its snapshot and
received after the receiver takes its snapshot.
So, the state of a channel is the sequence of
messages received along the channel after the receiver takes its
snapshot and before the receiver receives a marker along the channel.Note: If an agent takes its local snapshot when it receives a marker
along a channel, then the snapshot of the channel is the empty
sequence of messages.Termination of the AlgorithmAfter any agent \(v\) initiates the algorithm, all agents that are
reachable from \(v\) will receive a marker and take their local
snapshots.
If every agent is reachable from an initiator then all agents take
local snapshots.Each agent takes its local snapshot at most once.
So, a marker is sent on a channel at most once.
The computation terminates when all markers are received.Collecting Local Snapshots to form Global
SnapshotsOne way to collect local snapshots is to have an OS agent act as an
observer.
Each agent sends its local snapshots to the observer which puts the
local snapshots together to form the global snapshots.
Successive snapshots are disambiguated by using sequence numbers or
timestamps.Some algorithms carry out distributed computations on local snapshots
without using an observer to collect local snapshots.
Later, we give examples of such algorithms.Applications of Global SnapshotsSystem MonitoringSystems can be monitored by taking global snapshots repeatedly.
Let \(S_{0}, S_{1}, S_{2}, \ldots, \) be the sequence of states
recorded by the system.
From the propertyComputations through Increasing Cutsthere exists a computation that visits each state \(S_{i}\) in order
of increasing \(i\).
The system monitor checks the sequence of snapshots to determine if
some action is required.Rollback and RecoverLet \(S^{*}\) be the most recent snapshot recorded by a system monitor.
From the property,"Computations of Past before Future"there exists a computation that starts at the initial state and later
visits \(S^{*}\).
So, if an error is detected in a computation then the computation can be
restarted from \(S^{*}\) rather than rolling all the way back to the
initial state.Detecting Stable PredicatesAstable predicateis a predicate with the following property: If the
predicate holds at any point in any computation then it continues to
hold forever thereafter in that computation. Equivalently, if a stable
predicate holds in a state \(s\) then it holds in all states reachable
from \(s\).Examples of stable predicates are: "The computation has
terminated," and "The computation is deadlocked."
If a computation has terminated at some point then it remains 
terminated.
Likewise if a computation has deadlocked then it remains deadlocked.Specification of Detection AlgorithmsAn algorithm to detect a stable property \(P\) has the following
specification.If \(P\) holds when the algorithm is initiated then the algorithm
  detects that \(P\) holds.If the algorithm detects that \(P\) holds then \(P\) holds when the
  algorithm terminates.General Detection AlgorithmsA general solution is for the operating system to monitor a client
computation by taking repeated snapshots of the computation.
The OS checks whether a specified stable property holds in each
snapshot.
From the property,"Computations of Past before Future"this general solution satisfies the specification of detection
algorithms.Detection with ObserversThe OS uses an agent, the observer, to collect local snapshots and
form a global snapshot.
The observer inspects the global snapshot to determine if the property
holds in the snapshot.
The OS can also use multiple observers each of which collects
local information from subnetworks; the OS then carries out a
distributed algorithm on its collection of observers.The OS can also execute a distributed algorithm on local
snapshots without having observers collect local information, as
described next.Detection without Observers: Distributed
Algorithms on Local SnapshotsDistributed algorithms on local snapshots operate in two phases.
In the first phase a global snapshot algorithm is executed.
The local snapshot of each agent and its incoming channels are stored
locally, at the agent, without sending the information to observers.In the second phase a distributed algorithm is executed to determine
if the local information stored at agents satisfies a specified global
property, such as "computation has deadlocked."
The algorithm in the second phase operates on unchanging data.
These algorithms are often distributed graph algorithms.The two phases can be executed concurrently in many applications.NextA code skeleton of the algorithm and examples of the global snapshot
algorithm are providedhere.
Nextlogical clocks.K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

Computations of Past before Future.

Let computation \(X\) start in state \(S_{init}\) and end in state
\(S_{fini}\).
Let \(S^{*}\) be the state at a cut(past, future)of \(X\).
There exists a computation \(Y\) that starts in \(S_{init}\), visits
\(S^{*}\), and ends in \(S_{fini}\).Using this strategy, our tasks reduce to (1) identifying a cut, and
(2) recording the state, \(S^{*}\), at the cut.Identifying a CutEach agent has to record its own state because an agent's state is not
accessible to other agents.
The state of an agent that it records is called alocal snapshot.Definepastas the set of steps at each agent before the
agent takes its local snapshot, and
definefutureas the set of steps at each agent after the
agent takes its local snapshot.
So, if a step \(x\) at an agent is inpastthen all steps
at that agent before \(x\) are also inpast.Let's usethe following property ofpast,futureand cuts:The partition(past, future)is a cut exactly when every
message received inpastis sent inpast.Therefore,(past, future)is a cut exactly when:Global Snapshot RuleEach message received before the receiver takes its local snapshot
is sent before the sender takes its local snapshot.Design an algorithm yourself before reading further, and compare your
algorithm with the one given below.The Global Snapshot AlgorithmA special OS message called amarkeris used to distinguish
pre-snapshot from post-snapshot messages.
Messages sent on a channel before amarkeris sent on the
channel are messages sent in thepast-- i.e. before the
sender takes its local snapshot -- and messages sent
after the marker are sent in thefuture.The algorithmThe algorithm begins by one or more agents taking their local
  snapshots.When an agent takes its local snapshot it sends a marker on each
  of its outgoing channels.When an agent receives a marker, the agent takes its local snapshot
  if it has not already done so.The snapshot of a channel is the sequence of messages received on
  the channel after the receiver takes its snapshot and before the
  receiver receives a marker on the channel.Proof of correctnessFrom rule 3, each message received by an agent \(r\) on a channel
\(c\) before \(r\) takes its 
local snapshot is a message received by \(r\) before \(r\) receives a
marker on channel \(c\).Because channels are first in first out, each message received by
 \(r\) on \(c\) before \(r\) receives a
marker on \(c\) is sent on \(c\) before a marker is
sent on \(c\).From rule 2 each message sent on \(c\) before a marker is sent on
\(c\) is sent before the sender takes its local snapshot.From the three paragraphs above it follows that the global snapshot
rule holds for the algorithm.Proof about States of ChannelsThe messages in a channel at the cut are the
messages sent inpastand received infuture.
These are messages sent before the sender takes its snapshot and
received after the receiver takes its snapshot.
So, the state of a channel is the sequence of
messages received along the channel after the receiver takes its
snapshot and before the receiver receives a marker along the channel.Note: If an agent takes its local snapshot when it receives a marker
along a channel, then the snapshot of the channel is the empty
sequence of messages.Termination of the AlgorithmAfter any agent \(v\) initiates the algorithm, all agents that are
reachable from \(v\) will receive a marker and take their local
snapshots.
If every agent is reachable from an initiator then all agents take
local snapshots.Each agent takes its local snapshot at most once.
So, a marker is sent on a channel at most once.
The computation terminates when all markers are received.Collecting Local Snapshots to form Global
SnapshotsOne way to collect local snapshots is to have an OS agent act as an
observer.
Each agent sends its local snapshots to the observer which puts the
local snapshots together to form the global snapshots.
Successive snapshots are disambiguated by using sequence numbers or
timestamps.Some algorithms carry out distributed computations on local snapshots
without using an observer to collect local snapshots.
Later, we give examples of such algorithms.Applications of Global SnapshotsSystem MonitoringSystems can be monitored by taking global snapshots repeatedly.
Let \(S_{0}, S_{1}, S_{2}, \ldots, \) be the sequence of states
recorded by the system.
From the propertyComputations through Increasing Cutsthere exists a computation that visits each state \(S_{i}\) in order
of increasing \(i\).
The system monitor checks the sequence of snapshots to determine if
some action is required.Rollback and RecoverLet \(S^{*}\) be the most recent snapshot recorded by a system monitor.
From the property,"Computations of Past before Future"there exists a computation that starts at the initial state and later
visits \(S^{*}\).
So, if an error is detected in a computation then the computation can be
restarted from \(S^{*}\) rather than rolling all the way back to the
initial state.Detecting Stable PredicatesAstable predicateis a predicate with the following property: If the
predicate holds at any point in any computation then it continues to
hold forever thereafter in that computation. Equivalently, if a stable
predicate holds in a state \(s\) then it holds in all states reachable
from \(s\).Examples of stable predicates are: "The computation has
terminated," and "The computation is deadlocked."
If a computation has terminated at some point then it remains 
terminated.
Likewise if a computation has deadlocked then it remains deadlocked.Specification of Detection AlgorithmsAn algorithm to detect a stable property \(P\) has the following
specification.If \(P\) holds when the algorithm is initiated then the algorithm
  detects that \(P\) holds.If the algorithm detects that \(P\) holds then \(P\) holds when the
  algorithm terminates.General Detection AlgorithmsA general solution is for the operating system to monitor a client
computation by taking repeated snapshots of the computation.
The OS checks whether a specified stable property holds in each
snapshot.
From the property,"Computations of Past before Future"this general solution satisfies the specification of detection
algorithms.Detection with ObserversThe OS uses an agent, the observer, to collect local snapshots and
form a global snapshot.
The observer inspects the global snapshot to determine if the property
holds in the snapshot.
The OS can also use multiple observers each of which collects
local information from subnetworks; the OS then carries out a
distributed algorithm on its collection of observers.The OS can also execute a distributed algorithm on local
snapshots without having observers collect local information, as
described next.Detection without Observers: Distributed
Algorithms on Local SnapshotsDistributed algorithms on local snapshots operate in two phases.
In the first phase a global snapshot algorithm is executed.
The local snapshot of each agent and its incoming channels are stored
locally, at the agent, without sending the information to observers.In the second phase a distributed algorithm is executed to determine
if the local information stored at agents satisfies a specified global
property, such as "computation has deadlocked."
The algorithm in the second phase operates on unchanging data.
These algorithms are often distributed graph algorithms.The two phases can be executed concurrently in many applications.NextA code skeleton of the algorithm and examples of the global snapshot
algorithm are providedhere.
Nextlogical clocks.K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

Using this strategy, our tasks reduce to (1) identifying a cut, and
(2) recording the state, \(S^{*}\), at the cut.Identifying a CutEach agent has to record its own state because an agent's state is not
accessible to other agents.
The state of an agent that it records is called alocal snapshot.Definepastas the set of steps at each agent before the
agent takes its local snapshot, and
definefutureas the set of steps at each agent after the
agent takes its local snapshot.
So, if a step \(x\) at an agent is inpastthen all steps
at that agent before \(x\) are also inpast.Let's usethe following property ofpast,futureand cuts:The partition(past, future)is a cut exactly when every
message received inpastis sent inpast.Therefore,(past, future)is a cut exactly when:Global Snapshot RuleEach message received before the receiver takes its local snapshot
is sent before the sender takes its local snapshot.Design an algorithm yourself before reading further, and compare your
algorithm with the one given below.The Global Snapshot AlgorithmA special OS message called amarkeris used to distinguish
pre-snapshot from post-snapshot messages.
Messages sent on a channel before amarkeris sent on the
channel are messages sent in thepast-- i.e. before the
sender takes its local snapshot -- and messages sent
after the marker are sent in thefuture.The algorithmThe algorithm begins by one or more agents taking their local
  snapshots.When an agent takes its local snapshot it sends a marker on each
  of its outgoing channels.When an agent receives a marker, the agent takes its local snapshot
  if it has not already done so.The snapshot of a channel is the sequence of messages received on
  the channel after the receiver takes its snapshot and before the
  receiver receives a marker on the channel.Proof of correctnessFrom rule 3, each message received by an agent \(r\) on a channel
\(c\) before \(r\) takes its 
local snapshot is a message received by \(r\) before \(r\) receives a
marker on channel \(c\).Because channels are first in first out, each message received by
 \(r\) on \(c\) before \(r\) receives a
marker on \(c\) is sent on \(c\) before a marker is
sent on \(c\).From rule 2 each message sent on \(c\) before a marker is sent on
\(c\) is sent before the sender takes its local snapshot.From the three paragraphs above it follows that the global snapshot
rule holds for the algorithm.Proof about States of ChannelsThe messages in a channel at the cut are the
messages sent inpastand received infuture.
These are messages sent before the sender takes its snapshot and
received after the receiver takes its snapshot.
So, the state of a channel is the sequence of
messages received along the channel after the receiver takes its
snapshot and before the receiver receives a marker along the channel.Note: If an agent takes its local snapshot when it receives a marker
along a channel, then the snapshot of the channel is the empty
sequence of messages.Termination of the AlgorithmAfter any agent \(v\) initiates the algorithm, all agents that are
reachable from \(v\) will receive a marker and take their local
snapshots.
If every agent is reachable from an initiator then all agents take
local snapshots.Each agent takes its local snapshot at most once.
So, a marker is sent on a channel at most once.
The computation terminates when all markers are received.Collecting Local Snapshots to form Global
SnapshotsOne way to collect local snapshots is to have an OS agent act as an
observer.
Each agent sends its local snapshots to the observer which puts the
local snapshots together to form the global snapshots.
Successive snapshots are disambiguated by using sequence numbers or
timestamps.Some algorithms carry out distributed computations on local snapshots
without using an observer to collect local snapshots.
Later, we give examples of such algorithms.Applications of Global SnapshotsSystem MonitoringSystems can be monitored by taking global snapshots repeatedly.
Let \(S_{0}, S_{1}, S_{2}, \ldots, \) be the sequence of states
recorded by the system.
From the propertyComputations through Increasing Cutsthere exists a computation that visits each state \(S_{i}\) in order
of increasing \(i\).
The system monitor checks the sequence of snapshots to determine if
some action is required.Rollback and RecoverLet \(S^{*}\) be the most recent snapshot recorded by a system monitor.
From the property,"Computations of Past before Future"there exists a computation that starts at the initial state and later
visits \(S^{*}\).
So, if an error is detected in a computation then the computation can be
restarted from \(S^{*}\) rather than rolling all the way back to the
initial state.Detecting Stable PredicatesAstable predicateis a predicate with the following property: If the
predicate holds at any point in any computation then it continues to
hold forever thereafter in that computation. Equivalently, if a stable
predicate holds in a state \(s\) then it holds in all states reachable
from \(s\).Examples of stable predicates are: "The computation has
terminated," and "The computation is deadlocked."
If a computation has terminated at some point then it remains 
terminated.
Likewise if a computation has deadlocked then it remains deadlocked.Specification of Detection AlgorithmsAn algorithm to detect a stable property \(P\) has the following
specification.If \(P\) holds when the algorithm is initiated then the algorithm
  detects that \(P\) holds.If the algorithm detects that \(P\) holds then \(P\) holds when the
  algorithm terminates.General Detection AlgorithmsA general solution is for the operating system to monitor a client
computation by taking repeated snapshots of the computation.
The OS checks whether a specified stable property holds in each
snapshot.
From the property,"Computations of Past before Future"this general solution satisfies the specification of detection
algorithms.Detection with ObserversThe OS uses an agent, the observer, to collect local snapshots and
form a global snapshot.
The observer inspects the global snapshot to determine if the property
holds in the snapshot.
The OS can also use multiple observers each of which collects
local information from subnetworks; the OS then carries out a
distributed algorithm on its collection of observers.The OS can also execute a distributed algorithm on local
snapshots without having observers collect local information, as
described next.Detection without Observers: Distributed
Algorithms on Local SnapshotsDistributed algorithms on local snapshots operate in two phases.
In the first phase a global snapshot algorithm is executed.
The local snapshot of each agent and its incoming channels are stored
locally, at the agent, without sending the information to observers.In the second phase a distributed algorithm is executed to determine
if the local information stored at agents satisfies a specified global
property, such as "computation has deadlocked."
The algorithm in the second phase operates on unchanging data.
These algorithms are often distributed graph algorithms.The two phases can be executed concurrently in many applications.NextA code skeleton of the algorithm and examples of the global snapshot
algorithm are providedhere.
Nextlogical clocks.K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

Definepastas the set of steps at each agent before the
agent takes its local snapshot, and
definefutureas the set of steps at each agent after the
agent takes its local snapshot.
So, if a step \(x\) at an agent is inpastthen all steps
at that agent before \(x\) are also inpast.Let's usethe following property ofpast,futureand cuts:The partition(past, future)is a cut exactly when every
message received inpastis sent inpast.Therefore,(past, future)is a cut exactly when:Global Snapshot RuleEach message received before the receiver takes its local snapshot
is sent before the sender takes its local snapshot.Design an algorithm yourself before reading further, and compare your
algorithm with the one given below.The Global Snapshot AlgorithmA special OS message called amarkeris used to distinguish
pre-snapshot from post-snapshot messages.
Messages sent on a channel before amarkeris sent on the
channel are messages sent in thepast-- i.e. before the
sender takes its local snapshot -- and messages sent
after the marker are sent in thefuture.The algorithmThe algorithm begins by one or more agents taking their local
  snapshots.When an agent takes its local snapshot it sends a marker on each
  of its outgoing channels.When an agent receives a marker, the agent takes its local snapshot
  if it has not already done so.The snapshot of a channel is the sequence of messages received on
  the channel after the receiver takes its snapshot and before the
  receiver receives a marker on the channel.Proof of correctnessFrom rule 3, each message received by an agent \(r\) on a channel
\(c\) before \(r\) takes its 
local snapshot is a message received by \(r\) before \(r\) receives a
marker on channel \(c\).Because channels are first in first out, each message received by
 \(r\) on \(c\) before \(r\) receives a
marker on \(c\) is sent on \(c\) before a marker is
sent on \(c\).From rule 2 each message sent on \(c\) before a marker is sent on
\(c\) is sent before the sender takes its local snapshot.From the three paragraphs above it follows that the global snapshot
rule holds for the algorithm.Proof about States of ChannelsThe messages in a channel at the cut are the
messages sent inpastand received infuture.
These are messages sent before the sender takes its snapshot and
received after the receiver takes its snapshot.
So, the state of a channel is the sequence of
messages received along the channel after the receiver takes its
snapshot and before the receiver receives a marker along the channel.Note: If an agent takes its local snapshot when it receives a marker
along a channel, then the snapshot of the channel is the empty
sequence of messages.Termination of the AlgorithmAfter any agent \(v\) initiates the algorithm, all agents that are
reachable from \(v\) will receive a marker and take their local
snapshots.
If every agent is reachable from an initiator then all agents take
local snapshots.Each agent takes its local snapshot at most once.
So, a marker is sent on a channel at most once.
The computation terminates when all markers are received.Collecting Local Snapshots to form Global
SnapshotsOne way to collect local snapshots is to have an OS agent act as an
observer.
Each agent sends its local snapshots to the observer which puts the
local snapshots together to form the global snapshots.
Successive snapshots are disambiguated by using sequence numbers or
timestamps.Some algorithms carry out distributed computations on local snapshots
without using an observer to collect local snapshots.
Later, we give examples of such algorithms.Applications of Global SnapshotsSystem MonitoringSystems can be monitored by taking global snapshots repeatedly.
Let \(S_{0}, S_{1}, S_{2}, \ldots, \) be the sequence of states
recorded by the system.
From the propertyComputations through Increasing Cutsthere exists a computation that visits each state \(S_{i}\) in order
of increasing \(i\).
The system monitor checks the sequence of snapshots to determine if
some action is required.Rollback and RecoverLet \(S^{*}\) be the most recent snapshot recorded by a system monitor.
From the property,"Computations of Past before Future"there exists a computation that starts at the initial state and later
visits \(S^{*}\).
So, if an error is detected in a computation then the computation can be
restarted from \(S^{*}\) rather than rolling all the way back to the
initial state.Detecting Stable PredicatesAstable predicateis a predicate with the following property: If the
predicate holds at any point in any computation then it continues to
hold forever thereafter in that computation. Equivalently, if a stable
predicate holds in a state \(s\) then it holds in all states reachable
from \(s\).Examples of stable predicates are: "The computation has
terminated," and "The computation is deadlocked."
If a computation has terminated at some point then it remains 
terminated.
Likewise if a computation has deadlocked then it remains deadlocked.Specification of Detection AlgorithmsAn algorithm to detect a stable property \(P\) has the following
specification.If \(P\) holds when the algorithm is initiated then the algorithm
  detects that \(P\) holds.If the algorithm detects that \(P\) holds then \(P\) holds when the
  algorithm terminates.General Detection AlgorithmsA general solution is for the operating system to monitor a client
computation by taking repeated snapshots of the computation.
The OS checks whether a specified stable property holds in each
snapshot.
From the property,"Computations of Past before Future"this general solution satisfies the specification of detection
algorithms.Detection with ObserversThe OS uses an agent, the observer, to collect local snapshots and
form a global snapshot.
The observer inspects the global snapshot to determine if the property
holds in the snapshot.
The OS can also use multiple observers each of which collects
local information from subnetworks; the OS then carries out a
distributed algorithm on its collection of observers.The OS can also execute a distributed algorithm on local
snapshots without having observers collect local information, as
described next.Detection without Observers: Distributed
Algorithms on Local SnapshotsDistributed algorithms on local snapshots operate in two phases.
In the first phase a global snapshot algorithm is executed.
The local snapshot of each agent and its incoming channels are stored
locally, at the agent, without sending the information to observers.In the second phase a distributed algorithm is executed to determine
if the local information stored at agents satisfies a specified global
property, such as "computation has deadlocked."
The algorithm in the second phase operates on unchanging data.
These algorithms are often distributed graph algorithms.The two phases can be executed concurrently in many applications.NextA code skeleton of the algorithm and examples of the global snapshot
algorithm are providedhere.
Nextlogical clocks.K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

Let's usethe following property ofpast,futureand cuts:The partition(past, future)is a cut exactly when every
message received inpastis sent inpast.Therefore,(past, future)is a cut exactly when:Global Snapshot RuleEach message received before the receiver takes its local snapshot
is sent before the sender takes its local snapshot.Design an algorithm yourself before reading further, and compare your
algorithm with the one given below.The Global Snapshot AlgorithmA special OS message called amarkeris used to distinguish
pre-snapshot from post-snapshot messages.
Messages sent on a channel before amarkeris sent on the
channel are messages sent in thepast-- i.e. before the
sender takes its local snapshot -- and messages sent
after the marker are sent in thefuture.The algorithmThe algorithm begins by one or more agents taking their local
  snapshots.When an agent takes its local snapshot it sends a marker on each
  of its outgoing channels.When an agent receives a marker, the agent takes its local snapshot
  if it has not already done so.The snapshot of a channel is the sequence of messages received on
  the channel after the receiver takes its snapshot and before the
  receiver receives a marker on the channel.Proof of correctnessFrom rule 3, each message received by an agent \(r\) on a channel
\(c\) before \(r\) takes its 
local snapshot is a message received by \(r\) before \(r\) receives a
marker on channel \(c\).Because channels are first in first out, each message received by
 \(r\) on \(c\) before \(r\) receives a
marker on \(c\) is sent on \(c\) before a marker is
sent on \(c\).From rule 2 each message sent on \(c\) before a marker is sent on
\(c\) is sent before the sender takes its local snapshot.From the three paragraphs above it follows that the global snapshot
rule holds for the algorithm.Proof about States of ChannelsThe messages in a channel at the cut are the
messages sent inpastand received infuture.
These are messages sent before the sender takes its snapshot and
received after the receiver takes its snapshot.
So, the state of a channel is the sequence of
messages received along the channel after the receiver takes its
snapshot and before the receiver receives a marker along the channel.Note: If an agent takes its local snapshot when it receives a marker
along a channel, then the snapshot of the channel is the empty
sequence of messages.Termination of the AlgorithmAfter any agent \(v\) initiates the algorithm, all agents that are
reachable from \(v\) will receive a marker and take their local
snapshots.
If every agent is reachable from an initiator then all agents take
local snapshots.Each agent takes its local snapshot at most once.
So, a marker is sent on a channel at most once.
The computation terminates when all markers are received.Collecting Local Snapshots to form Global
SnapshotsOne way to collect local snapshots is to have an OS agent act as an
observer.
Each agent sends its local snapshots to the observer which puts the
local snapshots together to form the global snapshots.
Successive snapshots are disambiguated by using sequence numbers or
timestamps.Some algorithms carry out distributed computations on local snapshots
without using an observer to collect local snapshots.
Later, we give examples of such algorithms.Applications of Global SnapshotsSystem MonitoringSystems can be monitored by taking global snapshots repeatedly.
Let \(S_{0}, S_{1}, S_{2}, \ldots, \) be the sequence of states
recorded by the system.
From the propertyComputations through Increasing Cutsthere exists a computation that visits each state \(S_{i}\) in order
of increasing \(i\).
The system monitor checks the sequence of snapshots to determine if
some action is required.Rollback and RecoverLet \(S^{*}\) be the most recent snapshot recorded by a system monitor.
From the property,"Computations of Past before Future"there exists a computation that starts at the initial state and later
visits \(S^{*}\).
So, if an error is detected in a computation then the computation can be
restarted from \(S^{*}\) rather than rolling all the way back to the
initial state.Detecting Stable PredicatesAstable predicateis a predicate with the following property: If the
predicate holds at any point in any computation then it continues to
hold forever thereafter in that computation. Equivalently, if a stable
predicate holds in a state \(s\) then it holds in all states reachable
from \(s\).Examples of stable predicates are: "The computation has
terminated," and "The computation is deadlocked."
If a computation has terminated at some point then it remains 
terminated.
Likewise if a computation has deadlocked then it remains deadlocked.Specification of Detection AlgorithmsAn algorithm to detect a stable property \(P\) has the following
specification.If \(P\) holds when the algorithm is initiated then the algorithm
  detects that \(P\) holds.If the algorithm detects that \(P\) holds then \(P\) holds when the
  algorithm terminates.General Detection AlgorithmsA general solution is for the operating system to monitor a client
computation by taking repeated snapshots of the computation.
The OS checks whether a specified stable property holds in each
snapshot.
From the property,"Computations of Past before Future"this general solution satisfies the specification of detection
algorithms.Detection with ObserversThe OS uses an agent, the observer, to collect local snapshots and
form a global snapshot.
The observer inspects the global snapshot to determine if the property
holds in the snapshot.
The OS can also use multiple observers each of which collects
local information from subnetworks; the OS then carries out a
distributed algorithm on its collection of observers.The OS can also execute a distributed algorithm on local
snapshots without having observers collect local information, as
described next.Detection without Observers: Distributed
Algorithms on Local SnapshotsDistributed algorithms on local snapshots operate in two phases.
In the first phase a global snapshot algorithm is executed.
The local snapshot of each agent and its incoming channels are stored
locally, at the agent, without sending the information to observers.In the second phase a distributed algorithm is executed to determine
if the local information stored at agents satisfies a specified global
property, such as "computation has deadlocked."
The algorithm in the second phase operates on unchanging data.
These algorithms are often distributed graph algorithms.The two phases can be executed concurrently in many applications.NextA code skeleton of the algorithm and examples of the global snapshot
algorithm are providedhere.
Nextlogical clocks.K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

The partition(past, future)is a cut exactly when every
message received inpastis sent inpast.Therefore,(past, future)is a cut exactly when:Global Snapshot RuleEach message received before the receiver takes its local snapshot
is sent before the sender takes its local snapshot.Design an algorithm yourself before reading further, and compare your
algorithm with the one given below.The Global Snapshot AlgorithmA special OS message called amarkeris used to distinguish
pre-snapshot from post-snapshot messages.
Messages sent on a channel before amarkeris sent on the
channel are messages sent in thepast-- i.e. before the
sender takes its local snapshot -- and messages sent
after the marker are sent in thefuture.The algorithmThe algorithm begins by one or more agents taking their local
  snapshots.When an agent takes its local snapshot it sends a marker on each
  of its outgoing channels.When an agent receives a marker, the agent takes its local snapshot
  if it has not already done so.The snapshot of a channel is the sequence of messages received on
  the channel after the receiver takes its snapshot and before the
  receiver receives a marker on the channel.Proof of correctnessFrom rule 3, each message received by an agent \(r\) on a channel
\(c\) before \(r\) takes its 
local snapshot is a message received by \(r\) before \(r\) receives a
marker on channel \(c\).Because channels are first in first out, each message received by
 \(r\) on \(c\) before \(r\) receives a
marker on \(c\) is sent on \(c\) before a marker is
sent on \(c\).From rule 2 each message sent on \(c\) before a marker is sent on
\(c\) is sent before the sender takes its local snapshot.From the three paragraphs above it follows that the global snapshot
rule holds for the algorithm.Proof about States of ChannelsThe messages in a channel at the cut are the
messages sent inpastand received infuture.
These are messages sent before the sender takes its snapshot and
received after the receiver takes its snapshot.
So, the state of a channel is the sequence of
messages received along the channel after the receiver takes its
snapshot and before the receiver receives a marker along the channel.Note: If an agent takes its local snapshot when it receives a marker
along a channel, then the snapshot of the channel is the empty
sequence of messages.Termination of the AlgorithmAfter any agent \(v\) initiates the algorithm, all agents that are
reachable from \(v\) will receive a marker and take their local
snapshots.
If every agent is reachable from an initiator then all agents take
local snapshots.Each agent takes its local snapshot at most once.
So, a marker is sent on a channel at most once.
The computation terminates when all markers are received.Collecting Local Snapshots to form Global
SnapshotsOne way to collect local snapshots is to have an OS agent act as an
observer.
Each agent sends its local snapshots to the observer which puts the
local snapshots together to form the global snapshots.
Successive snapshots are disambiguated by using sequence numbers or
timestamps.Some algorithms carry out distributed computations on local snapshots
without using an observer to collect local snapshots.
Later, we give examples of such algorithms.Applications of Global SnapshotsSystem MonitoringSystems can be monitored by taking global snapshots repeatedly.
Let \(S_{0}, S_{1}, S_{2}, \ldots, \) be the sequence of states
recorded by the system.
From the propertyComputations through Increasing Cutsthere exists a computation that visits each state \(S_{i}\) in order
of increasing \(i\).
The system monitor checks the sequence of snapshots to determine if
some action is required.Rollback and RecoverLet \(S^{*}\) be the most recent snapshot recorded by a system monitor.
From the property,"Computations of Past before Future"there exists a computation that starts at the initial state and later
visits \(S^{*}\).
So, if an error is detected in a computation then the computation can be
restarted from \(S^{*}\) rather than rolling all the way back to the
initial state.Detecting Stable PredicatesAstable predicateis a predicate with the following property: If the
predicate holds at any point in any computation then it continues to
hold forever thereafter in that computation. Equivalently, if a stable
predicate holds in a state \(s\) then it holds in all states reachable
from \(s\).Examples of stable predicates are: "The computation has
terminated," and "The computation is deadlocked."
If a computation has terminated at some point then it remains 
terminated.
Likewise if a computation has deadlocked then it remains deadlocked.Specification of Detection AlgorithmsAn algorithm to detect a stable property \(P\) has the following
specification.If \(P\) holds when the algorithm is initiated then the algorithm
  detects that \(P\) holds.If the algorithm detects that \(P\) holds then \(P\) holds when the
  algorithm terminates.General Detection AlgorithmsA general solution is for the operating system to monitor a client
computation by taking repeated snapshots of the computation.
The OS checks whether a specified stable property holds in each
snapshot.
From the property,"Computations of Past before Future"this general solution satisfies the specification of detection
algorithms.Detection with ObserversThe OS uses an agent, the observer, to collect local snapshots and
form a global snapshot.
The observer inspects the global snapshot to determine if the property
holds in the snapshot.
The OS can also use multiple observers each of which collects
local information from subnetworks; the OS then carries out a
distributed algorithm on its collection of observers.The OS can also execute a distributed algorithm on local
snapshots without having observers collect local information, as
described next.Detection without Observers: Distributed
Algorithms on Local SnapshotsDistributed algorithms on local snapshots operate in two phases.
In the first phase a global snapshot algorithm is executed.
The local snapshot of each agent and its incoming channels are stored
locally, at the agent, without sending the information to observers.In the second phase a distributed algorithm is executed to determine
if the local information stored at agents satisfies a specified global
property, such as "computation has deadlocked."
The algorithm in the second phase operates on unchanging data.
These algorithms are often distributed graph algorithms.The two phases can be executed concurrently in many applications.NextA code skeleton of the algorithm and examples of the global snapshot
algorithm are providedhere.
Nextlogical clocks.K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

Therefore,(past, future)is a cut exactly when:Global Snapshot RuleEach message received before the receiver takes its local snapshot
is sent before the sender takes its local snapshot.Design an algorithm yourself before reading further, and compare your
algorithm with the one given below.The Global Snapshot AlgorithmA special OS message called amarkeris used to distinguish
pre-snapshot from post-snapshot messages.
Messages sent on a channel before amarkeris sent on the
channel are messages sent in thepast-- i.e. before the
sender takes its local snapshot -- and messages sent
after the marker are sent in thefuture.The algorithmThe algorithm begins by one or more agents taking their local
  snapshots.When an agent takes its local snapshot it sends a marker on each
  of its outgoing channels.When an agent receives a marker, the agent takes its local snapshot
  if it has not already done so.The snapshot of a channel is the sequence of messages received on
  the channel after the receiver takes its snapshot and before the
  receiver receives a marker on the channel.Proof of correctnessFrom rule 3, each message received by an agent \(r\) on a channel
\(c\) before \(r\) takes its 
local snapshot is a message received by \(r\) before \(r\) receives a
marker on channel \(c\).Because channels are first in first out, each message received by
 \(r\) on \(c\) before \(r\) receives a
marker on \(c\) is sent on \(c\) before a marker is
sent on \(c\).From rule 2 each message sent on \(c\) before a marker is sent on
\(c\) is sent before the sender takes its local snapshot.From the three paragraphs above it follows that the global snapshot
rule holds for the algorithm.Proof about States of ChannelsThe messages in a channel at the cut are the
messages sent inpastand received infuture.
These are messages sent before the sender takes its snapshot and
received after the receiver takes its snapshot.
So, the state of a channel is the sequence of
messages received along the channel after the receiver takes its
snapshot and before the receiver receives a marker along the channel.Note: If an agent takes its local snapshot when it receives a marker
along a channel, then the snapshot of the channel is the empty
sequence of messages.Termination of the AlgorithmAfter any agent \(v\) initiates the algorithm, all agents that are
reachable from \(v\) will receive a marker and take their local
snapshots.
If every agent is reachable from an initiator then all agents take
local snapshots.Each agent takes its local snapshot at most once.
So, a marker is sent on a channel at most once.
The computation terminates when all markers are received.Collecting Local Snapshots to form Global
SnapshotsOne way to collect local snapshots is to have an OS agent act as an
observer.
Each agent sends its local snapshots to the observer which puts the
local snapshots together to form the global snapshots.
Successive snapshots are disambiguated by using sequence numbers or
timestamps.Some algorithms carry out distributed computations on local snapshots
without using an observer to collect local snapshots.
Later, we give examples of such algorithms.Applications of Global SnapshotsSystem MonitoringSystems can be monitored by taking global snapshots repeatedly.
Let \(S_{0}, S_{1}, S_{2}, \ldots, \) be the sequence of states
recorded by the system.
From the propertyComputations through Increasing Cutsthere exists a computation that visits each state \(S_{i}\) in order
of increasing \(i\).
The system monitor checks the sequence of snapshots to determine if
some action is required.Rollback and RecoverLet \(S^{*}\) be the most recent snapshot recorded by a system monitor.
From the property,"Computations of Past before Future"there exists a computation that starts at the initial state and later
visits \(S^{*}\).
So, if an error is detected in a computation then the computation can be
restarted from \(S^{*}\) rather than rolling all the way back to the
initial state.Detecting Stable PredicatesAstable predicateis a predicate with the following property: If the
predicate holds at any point in any computation then it continues to
hold forever thereafter in that computation. Equivalently, if a stable
predicate holds in a state \(s\) then it holds in all states reachable
from \(s\).Examples of stable predicates are: "The computation has
terminated," and "The computation is deadlocked."
If a computation has terminated at some point then it remains 
terminated.
Likewise if a computation has deadlocked then it remains deadlocked.Specification of Detection AlgorithmsAn algorithm to detect a stable property \(P\) has the following
specification.If \(P\) holds when the algorithm is initiated then the algorithm
  detects that \(P\) holds.If the algorithm detects that \(P\) holds then \(P\) holds when the
  algorithm terminates.General Detection AlgorithmsA general solution is for the operating system to monitor a client
computation by taking repeated snapshots of the computation.
The OS checks whether a specified stable property holds in each
snapshot.
From the property,"Computations of Past before Future"this general solution satisfies the specification of detection
algorithms.Detection with ObserversThe OS uses an agent, the observer, to collect local snapshots and
form a global snapshot.
The observer inspects the global snapshot to determine if the property
holds in the snapshot.
The OS can also use multiple observers each of which collects
local information from subnetworks; the OS then carries out a
distributed algorithm on its collection of observers.The OS can also execute a distributed algorithm on local
snapshots without having observers collect local information, as
described next.Detection without Observers: Distributed
Algorithms on Local SnapshotsDistributed algorithms on local snapshots operate in two phases.
In the first phase a global snapshot algorithm is executed.
The local snapshot of each agent and its incoming channels are stored
locally, at the agent, without sending the information to observers.In the second phase a distributed algorithm is executed to determine
if the local information stored at agents satisfies a specified global
property, such as "computation has deadlocked."
The algorithm in the second phase operates on unchanging data.
These algorithms are often distributed graph algorithms.The two phases can be executed concurrently in many applications.NextA code skeleton of the algorithm and examples of the global snapshot
algorithm are providedhere.
Nextlogical clocks.K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

Global Snapshot Rule

Each message received before the receiver takes its local snapshot
is sent before the sender takes its local snapshot.Design an algorithm yourself before reading further, and compare your
algorithm with the one given below.The Global Snapshot AlgorithmA special OS message called amarkeris used to distinguish
pre-snapshot from post-snapshot messages.
Messages sent on a channel before amarkeris sent on the
channel are messages sent in thepast-- i.e. before the
sender takes its local snapshot -- and messages sent
after the marker are sent in thefuture.The algorithmThe algorithm begins by one or more agents taking their local
  snapshots.When an agent takes its local snapshot it sends a marker on each
  of its outgoing channels.When an agent receives a marker, the agent takes its local snapshot
  if it has not already done so.The snapshot of a channel is the sequence of messages received on
  the channel after the receiver takes its snapshot and before the
  receiver receives a marker on the channel.Proof of correctnessFrom rule 3, each message received by an agent \(r\) on a channel
\(c\) before \(r\) takes its 
local snapshot is a message received by \(r\) before \(r\) receives a
marker on channel \(c\).Because channels are first in first out, each message received by
 \(r\) on \(c\) before \(r\) receives a
marker on \(c\) is sent on \(c\) before a marker is
sent on \(c\).From rule 2 each message sent on \(c\) before a marker is sent on
\(c\) is sent before the sender takes its local snapshot.From the three paragraphs above it follows that the global snapshot
rule holds for the algorithm.Proof about States of ChannelsThe messages in a channel at the cut are the
messages sent inpastand received infuture.
These are messages sent before the sender takes its snapshot and
received after the receiver takes its snapshot.
So, the state of a channel is the sequence of
messages received along the channel after the receiver takes its
snapshot and before the receiver receives a marker along the channel.Note: If an agent takes its local snapshot when it receives a marker
along a channel, then the snapshot of the channel is the empty
sequence of messages.Termination of the AlgorithmAfter any agent \(v\) initiates the algorithm, all agents that are
reachable from \(v\) will receive a marker and take their local
snapshots.
If every agent is reachable from an initiator then all agents take
local snapshots.Each agent takes its local snapshot at most once.
So, a marker is sent on a channel at most once.
The computation terminates when all markers are received.Collecting Local Snapshots to form Global
SnapshotsOne way to collect local snapshots is to have an OS agent act as an
observer.
Each agent sends its local snapshots to the observer which puts the
local snapshots together to form the global snapshots.
Successive snapshots are disambiguated by using sequence numbers or
timestamps.Some algorithms carry out distributed computations on local snapshots
without using an observer to collect local snapshots.
Later, we give examples of such algorithms.Applications of Global SnapshotsSystem MonitoringSystems can be monitored by taking global snapshots repeatedly.
Let \(S_{0}, S_{1}, S_{2}, \ldots, \) be the sequence of states
recorded by the system.
From the propertyComputations through Increasing Cutsthere exists a computation that visits each state \(S_{i}\) in order
of increasing \(i\).
The system monitor checks the sequence of snapshots to determine if
some action is required.Rollback and RecoverLet \(S^{*}\) be the most recent snapshot recorded by a system monitor.
From the property,"Computations of Past before Future"there exists a computation that starts at the initial state and later
visits \(S^{*}\).
So, if an error is detected in a computation then the computation can be
restarted from \(S^{*}\) rather than rolling all the way back to the
initial state.Detecting Stable PredicatesAstable predicateis a predicate with the following property: If the
predicate holds at any point in any computation then it continues to
hold forever thereafter in that computation. Equivalently, if a stable
predicate holds in a state \(s\) then it holds in all states reachable
from \(s\).Examples of stable predicates are: "The computation has
terminated," and "The computation is deadlocked."
If a computation has terminated at some point then it remains 
terminated.
Likewise if a computation has deadlocked then it remains deadlocked.Specification of Detection AlgorithmsAn algorithm to detect a stable property \(P\) has the following
specification.If \(P\) holds when the algorithm is initiated then the algorithm
  detects that \(P\) holds.If the algorithm detects that \(P\) holds then \(P\) holds when the
  algorithm terminates.General Detection AlgorithmsA general solution is for the operating system to monitor a client
computation by taking repeated snapshots of the computation.
The OS checks whether a specified stable property holds in each
snapshot.
From the property,"Computations of Past before Future"this general solution satisfies the specification of detection
algorithms.Detection with ObserversThe OS uses an agent, the observer, to collect local snapshots and
form a global snapshot.
The observer inspects the global snapshot to determine if the property
holds in the snapshot.
The OS can also use multiple observers each of which collects
local information from subnetworks; the OS then carries out a
distributed algorithm on its collection of observers.The OS can also execute a distributed algorithm on local
snapshots without having observers collect local information, as
described next.Detection without Observers: Distributed
Algorithms on Local SnapshotsDistributed algorithms on local snapshots operate in two phases.
In the first phase a global snapshot algorithm is executed.
The local snapshot of each agent and its incoming channels are stored
locally, at the agent, without sending the information to observers.In the second phase a distributed algorithm is executed to determine
if the local information stored at agents satisfies a specified global
property, such as "computation has deadlocked."
The algorithm in the second phase operates on unchanging data.
These algorithms are often distributed graph algorithms.The two phases can be executed concurrently in many applications.NextA code skeleton of the algorithm and examples of the global snapshot
algorithm are providedhere.
Nextlogical clocks.K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

Design an algorithm yourself before reading further, and compare your
algorithm with the one given below.The Global Snapshot AlgorithmA special OS message called amarkeris used to distinguish
pre-snapshot from post-snapshot messages.
Messages sent on a channel before amarkeris sent on the
channel are messages sent in thepast-- i.e. before the
sender takes its local snapshot -- and messages sent
after the marker are sent in thefuture.The algorithmThe algorithm begins by one or more agents taking their local
  snapshots.When an agent takes its local snapshot it sends a marker on each
  of its outgoing channels.When an agent receives a marker, the agent takes its local snapshot
  if it has not already done so.The snapshot of a channel is the sequence of messages received on
  the channel after the receiver takes its snapshot and before the
  receiver receives a marker on the channel.Proof of correctnessFrom rule 3, each message received by an agent \(r\) on a channel
\(c\) before \(r\) takes its 
local snapshot is a message received by \(r\) before \(r\) receives a
marker on channel \(c\).Because channels are first in first out, each message received by
 \(r\) on \(c\) before \(r\) receives a
marker on \(c\) is sent on \(c\) before a marker is
sent on \(c\).From rule 2 each message sent on \(c\) before a marker is sent on
\(c\) is sent before the sender takes its local snapshot.From the three paragraphs above it follows that the global snapshot
rule holds for the algorithm.Proof about States of ChannelsThe messages in a channel at the cut are the
messages sent inpastand received infuture.
These are messages sent before the sender takes its snapshot and
received after the receiver takes its snapshot.
So, the state of a channel is the sequence of
messages received along the channel after the receiver takes its
snapshot and before the receiver receives a marker along the channel.Note: If an agent takes its local snapshot when it receives a marker
along a channel, then the snapshot of the channel is the empty
sequence of messages.Termination of the AlgorithmAfter any agent \(v\) initiates the algorithm, all agents that are
reachable from \(v\) will receive a marker and take their local
snapshots.
If every agent is reachable from an initiator then all agents take
local snapshots.Each agent takes its local snapshot at most once.
So, a marker is sent on a channel at most once.
The computation terminates when all markers are received.Collecting Local Snapshots to form Global
SnapshotsOne way to collect local snapshots is to have an OS agent act as an
observer.
Each agent sends its local snapshots to the observer which puts the
local snapshots together to form the global snapshots.
Successive snapshots are disambiguated by using sequence numbers or
timestamps.Some algorithms carry out distributed computations on local snapshots
without using an observer to collect local snapshots.
Later, we give examples of such algorithms.Applications of Global SnapshotsSystem MonitoringSystems can be monitored by taking global snapshots repeatedly.
Let \(S_{0}, S_{1}, S_{2}, \ldots, \) be the sequence of states
recorded by the system.
From the propertyComputations through Increasing Cutsthere exists a computation that visits each state \(S_{i}\) in order
of increasing \(i\).
The system monitor checks the sequence of snapshots to determine if
some action is required.Rollback and RecoverLet \(S^{*}\) be the most recent snapshot recorded by a system monitor.
From the property,"Computations of Past before Future"there exists a computation that starts at the initial state and later
visits \(S^{*}\).
So, if an error is detected in a computation then the computation can be
restarted from \(S^{*}\) rather than rolling all the way back to the
initial state.Detecting Stable PredicatesAstable predicateis a predicate with the following property: If the
predicate holds at any point in any computation then it continues to
hold forever thereafter in that computation. Equivalently, if a stable
predicate holds in a state \(s\) then it holds in all states reachable
from \(s\).Examples of stable predicates are: "The computation has
terminated," and "The computation is deadlocked."
If a computation has terminated at some point then it remains 
terminated.
Likewise if a computation has deadlocked then it remains deadlocked.Specification of Detection AlgorithmsAn algorithm to detect a stable property \(P\) has the following
specification.If \(P\) holds when the algorithm is initiated then the algorithm
  detects that \(P\) holds.If the algorithm detects that \(P\) holds then \(P\) holds when the
  algorithm terminates.General Detection AlgorithmsA general solution is for the operating system to monitor a client
computation by taking repeated snapshots of the computation.
The OS checks whether a specified stable property holds in each
snapshot.
From the property,"Computations of Past before Future"this general solution satisfies the specification of detection
algorithms.Detection with ObserversThe OS uses an agent, the observer, to collect local snapshots and
form a global snapshot.
The observer inspects the global snapshot to determine if the property
holds in the snapshot.
The OS can also use multiple observers each of which collects
local information from subnetworks; the OS then carries out a
distributed algorithm on its collection of observers.The OS can also execute a distributed algorithm on local
snapshots without having observers collect local information, as
described next.Detection without Observers: Distributed
Algorithms on Local SnapshotsDistributed algorithms on local snapshots operate in two phases.
In the first phase a global snapshot algorithm is executed.
The local snapshot of each agent and its incoming channels are stored
locally, at the agent, without sending the information to observers.In the second phase a distributed algorithm is executed to determine
if the local information stored at agents satisfies a specified global
property, such as "computation has deadlocked."
The algorithm in the second phase operates on unchanging data.
These algorithms are often distributed graph algorithms.The two phases can be executed concurrently in many applications.NextA code skeleton of the algorithm and examples of the global snapshot
algorithm are providedhere.
Nextlogical clocks.K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

The algorithm

From rule 3, each message received by an agent \(r\) on a channel
\(c\) before \(r\) takes its 
local snapshot is a message received by \(r\) before \(r\) receives a
marker on channel \(c\).Because channels are first in first out, each message received by
 \(r\) on \(c\) before \(r\) receives a
marker on \(c\) is sent on \(c\) before a marker is
sent on \(c\).From rule 2 each message sent on \(c\) before a marker is sent on
\(c\) is sent before the sender takes its local snapshot.From the three paragraphs above it follows that the global snapshot
rule holds for the algorithm.Proof about States of ChannelsThe messages in a channel at the cut are the
messages sent inpastand received infuture.
These are messages sent before the sender takes its snapshot and
received after the receiver takes its snapshot.
So, the state of a channel is the sequence of
messages received along the channel after the receiver takes its
snapshot and before the receiver receives a marker along the channel.Note: If an agent takes its local snapshot when it receives a marker
along a channel, then the snapshot of the channel is the empty
sequence of messages.Termination of the AlgorithmAfter any agent \(v\) initiates the algorithm, all agents that are
reachable from \(v\) will receive a marker and take their local
snapshots.
If every agent is reachable from an initiator then all agents take
local snapshots.Each agent takes its local snapshot at most once.
So, a marker is sent on a channel at most once.
The computation terminates when all markers are received.Collecting Local Snapshots to form Global
SnapshotsOne way to collect local snapshots is to have an OS agent act as an
observer.
Each agent sends its local snapshots to the observer which puts the
local snapshots together to form the global snapshots.
Successive snapshots are disambiguated by using sequence numbers or
timestamps.Some algorithms carry out distributed computations on local snapshots
without using an observer to collect local snapshots.
Later, we give examples of such algorithms.Applications of Global SnapshotsSystem MonitoringSystems can be monitored by taking global snapshots repeatedly.
Let \(S_{0}, S_{1}, S_{2}, \ldots, \) be the sequence of states
recorded by the system.
From the propertyComputations through Increasing Cutsthere exists a computation that visits each state \(S_{i}\) in order
of increasing \(i\).
The system monitor checks the sequence of snapshots to determine if
some action is required.Rollback and RecoverLet \(S^{*}\) be the most recent snapshot recorded by a system monitor.
From the property,"Computations of Past before Future"there exists a computation that starts at the initial state and later
visits \(S^{*}\).
So, if an error is detected in a computation then the computation can be
restarted from \(S^{*}\) rather than rolling all the way back to the
initial state.Detecting Stable PredicatesAstable predicateis a predicate with the following property: If the
predicate holds at any point in any computation then it continues to
hold forever thereafter in that computation. Equivalently, if a stable
predicate holds in a state \(s\) then it holds in all states reachable
from \(s\).Examples of stable predicates are: "The computation has
terminated," and "The computation is deadlocked."
If a computation has terminated at some point then it remains 
terminated.
Likewise if a computation has deadlocked then it remains deadlocked.Specification of Detection AlgorithmsAn algorithm to detect a stable property \(P\) has the following
specification.If \(P\) holds when the algorithm is initiated then the algorithm
  detects that \(P\) holds.If the algorithm detects that \(P\) holds then \(P\) holds when the
  algorithm terminates.General Detection AlgorithmsA general solution is for the operating system to monitor a client
computation by taking repeated snapshots of the computation.
The OS checks whether a specified stable property holds in each
snapshot.
From the property,"Computations of Past before Future"this general solution satisfies the specification of detection
algorithms.Detection with ObserversThe OS uses an agent, the observer, to collect local snapshots and
form a global snapshot.
The observer inspects the global snapshot to determine if the property
holds in the snapshot.
The OS can also use multiple observers each of which collects
local information from subnetworks; the OS then carries out a
distributed algorithm on its collection of observers.The OS can also execute a distributed algorithm on local
snapshots without having observers collect local information, as
described next.Detection without Observers: Distributed
Algorithms on Local SnapshotsDistributed algorithms on local snapshots operate in two phases.
In the first phase a global snapshot algorithm is executed.
The local snapshot of each agent and its incoming channels are stored
locally, at the agent, without sending the information to observers.In the second phase a distributed algorithm is executed to determine
if the local information stored at agents satisfies a specified global
property, such as "computation has deadlocked."
The algorithm in the second phase operates on unchanging data.
These algorithms are often distributed graph algorithms.The two phases can be executed concurrently in many applications.NextA code skeleton of the algorithm and examples of the global snapshot
algorithm are providedhere.
Nextlogical clocks.K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

Because channels are first in first out, each message received by
 \(r\) on \(c\) before \(r\) receives a
marker on \(c\) is sent on \(c\) before a marker is
sent on \(c\).From rule 2 each message sent on \(c\) before a marker is sent on
\(c\) is sent before the sender takes its local snapshot.From the three paragraphs above it follows that the global snapshot
rule holds for the algorithm.Proof about States of ChannelsThe messages in a channel at the cut are the
messages sent inpastand received infuture.
These are messages sent before the sender takes its snapshot and
received after the receiver takes its snapshot.
So, the state of a channel is the sequence of
messages received along the channel after the receiver takes its
snapshot and before the receiver receives a marker along the channel.Note: If an agent takes its local snapshot when it receives a marker
along a channel, then the snapshot of the channel is the empty
sequence of messages.Termination of the AlgorithmAfter any agent \(v\) initiates the algorithm, all agents that are
reachable from \(v\) will receive a marker and take their local
snapshots.
If every agent is reachable from an initiator then all agents take
local snapshots.Each agent takes its local snapshot at most once.
So, a marker is sent on a channel at most once.
The computation terminates when all markers are received.Collecting Local Snapshots to form Global
SnapshotsOne way to collect local snapshots is to have an OS agent act as an
observer.
Each agent sends its local snapshots to the observer which puts the
local snapshots together to form the global snapshots.
Successive snapshots are disambiguated by using sequence numbers or
timestamps.Some algorithms carry out distributed computations on local snapshots
without using an observer to collect local snapshots.
Later, we give examples of such algorithms.Applications of Global SnapshotsSystem MonitoringSystems can be monitored by taking global snapshots repeatedly.
Let \(S_{0}, S_{1}, S_{2}, \ldots, \) be the sequence of states
recorded by the system.
From the propertyComputations through Increasing Cutsthere exists a computation that visits each state \(S_{i}\) in order
of increasing \(i\).
The system monitor checks the sequence of snapshots to determine if
some action is required.Rollback and RecoverLet \(S^{*}\) be the most recent snapshot recorded by a system monitor.
From the property,"Computations of Past before Future"there exists a computation that starts at the initial state and later
visits \(S^{*}\).
So, if an error is detected in a computation then the computation can be
restarted from \(S^{*}\) rather than rolling all the way back to the
initial state.Detecting Stable PredicatesAstable predicateis a predicate with the following property: If the
predicate holds at any point in any computation then it continues to
hold forever thereafter in that computation. Equivalently, if a stable
predicate holds in a state \(s\) then it holds in all states reachable
from \(s\).Examples of stable predicates are: "The computation has
terminated," and "The computation is deadlocked."
If a computation has terminated at some point then it remains 
terminated.
Likewise if a computation has deadlocked then it remains deadlocked.Specification of Detection AlgorithmsAn algorithm to detect a stable property \(P\) has the following
specification.If \(P\) holds when the algorithm is initiated then the algorithm
  detects that \(P\) holds.If the algorithm detects that \(P\) holds then \(P\) holds when the
  algorithm terminates.General Detection AlgorithmsA general solution is for the operating system to monitor a client
computation by taking repeated snapshots of the computation.
The OS checks whether a specified stable property holds in each
snapshot.
From the property,"Computations of Past before Future"this general solution satisfies the specification of detection
algorithms.Detection with ObserversThe OS uses an agent, the observer, to collect local snapshots and
form a global snapshot.
The observer inspects the global snapshot to determine if the property
holds in the snapshot.
The OS can also use multiple observers each of which collects
local information from subnetworks; the OS then carries out a
distributed algorithm on its collection of observers.The OS can also execute a distributed algorithm on local
snapshots without having observers collect local information, as
described next.Detection without Observers: Distributed
Algorithms on Local SnapshotsDistributed algorithms on local snapshots operate in two phases.
In the first phase a global snapshot algorithm is executed.
The local snapshot of each agent and its incoming channels are stored
locally, at the agent, without sending the information to observers.In the second phase a distributed algorithm is executed to determine
if the local information stored at agents satisfies a specified global
property, such as "computation has deadlocked."
The algorithm in the second phase operates on unchanging data.
These algorithms are often distributed graph algorithms.The two phases can be executed concurrently in many applications.NextA code skeleton of the algorithm and examples of the global snapshot
algorithm are providedhere.
Nextlogical clocks.K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

From rule 2 each message sent on \(c\) before a marker is sent on
\(c\) is sent before the sender takes its local snapshot.From the three paragraphs above it follows that the global snapshot
rule holds for the algorithm.Proof about States of ChannelsThe messages in a channel at the cut are the
messages sent inpastand received infuture.
These are messages sent before the sender takes its snapshot and
received after the receiver takes its snapshot.
So, the state of a channel is the sequence of
messages received along the channel after the receiver takes its
snapshot and before the receiver receives a marker along the channel.Note: If an agent takes its local snapshot when it receives a marker
along a channel, then the snapshot of the channel is the empty
sequence of messages.Termination of the AlgorithmAfter any agent \(v\) initiates the algorithm, all agents that are
reachable from \(v\) will receive a marker and take their local
snapshots.
If every agent is reachable from an initiator then all agents take
local snapshots.Each agent takes its local snapshot at most once.
So, a marker is sent on a channel at most once.
The computation terminates when all markers are received.Collecting Local Snapshots to form Global
SnapshotsOne way to collect local snapshots is to have an OS agent act as an
observer.
Each agent sends its local snapshots to the observer which puts the
local snapshots together to form the global snapshots.
Successive snapshots are disambiguated by using sequence numbers or
timestamps.Some algorithms carry out distributed computations on local snapshots
without using an observer to collect local snapshots.
Later, we give examples of such algorithms.Applications of Global SnapshotsSystem MonitoringSystems can be monitored by taking global snapshots repeatedly.
Let \(S_{0}, S_{1}, S_{2}, \ldots, \) be the sequence of states
recorded by the system.
From the propertyComputations through Increasing Cutsthere exists a computation that visits each state \(S_{i}\) in order
of increasing \(i\).
The system monitor checks the sequence of snapshots to determine if
some action is required.Rollback and RecoverLet \(S^{*}\) be the most recent snapshot recorded by a system monitor.
From the property,"Computations of Past before Future"there exists a computation that starts at the initial state and later
visits \(S^{*}\).
So, if an error is detected in a computation then the computation can be
restarted from \(S^{*}\) rather than rolling all the way back to the
initial state.Detecting Stable PredicatesAstable predicateis a predicate with the following property: If the
predicate holds at any point in any computation then it continues to
hold forever thereafter in that computation. Equivalently, if a stable
predicate holds in a state \(s\) then it holds in all states reachable
from \(s\).Examples of stable predicates are: "The computation has
terminated," and "The computation is deadlocked."
If a computation has terminated at some point then it remains 
terminated.
Likewise if a computation has deadlocked then it remains deadlocked.Specification of Detection AlgorithmsAn algorithm to detect a stable property \(P\) has the following
specification.If \(P\) holds when the algorithm is initiated then the algorithm
  detects that \(P\) holds.If the algorithm detects that \(P\) holds then \(P\) holds when the
  algorithm terminates.General Detection AlgorithmsA general solution is for the operating system to monitor a client
computation by taking repeated snapshots of the computation.
The OS checks whether a specified stable property holds in each
snapshot.
From the property,"Computations of Past before Future"this general solution satisfies the specification of detection
algorithms.Detection with ObserversThe OS uses an agent, the observer, to collect local snapshots and
form a global snapshot.
The observer inspects the global snapshot to determine if the property
holds in the snapshot.
The OS can also use multiple observers each of which collects
local information from subnetworks; the OS then carries out a
distributed algorithm on its collection of observers.The OS can also execute a distributed algorithm on local
snapshots without having observers collect local information, as
described next.Detection without Observers: Distributed
Algorithms on Local SnapshotsDistributed algorithms on local snapshots operate in two phases.
In the first phase a global snapshot algorithm is executed.
The local snapshot of each agent and its incoming channels are stored
locally, at the agent, without sending the information to observers.In the second phase a distributed algorithm is executed to determine
if the local information stored at agents satisfies a specified global
property, such as "computation has deadlocked."
The algorithm in the second phase operates on unchanging data.
These algorithms are often distributed graph algorithms.The two phases can be executed concurrently in many applications.NextA code skeleton of the algorithm and examples of the global snapshot
algorithm are providedhere.
Nextlogical clocks.K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

From the three paragraphs above it follows that the global snapshot
rule holds for the algorithm.Proof about States of ChannelsThe messages in a channel at the cut are the
messages sent inpastand received infuture.
These are messages sent before the sender takes its snapshot and
received after the receiver takes its snapshot.
So, the state of a channel is the sequence of
messages received along the channel after the receiver takes its
snapshot and before the receiver receives a marker along the channel.Note: If an agent takes its local snapshot when it receives a marker
along a channel, then the snapshot of the channel is the empty
sequence of messages.Termination of the AlgorithmAfter any agent \(v\) initiates the algorithm, all agents that are
reachable from \(v\) will receive a marker and take their local
snapshots.
If every agent is reachable from an initiator then all agents take
local snapshots.Each agent takes its local snapshot at most once.
So, a marker is sent on a channel at most once.
The computation terminates when all markers are received.Collecting Local Snapshots to form Global
SnapshotsOne way to collect local snapshots is to have an OS agent act as an
observer.
Each agent sends its local snapshots to the observer which puts the
local snapshots together to form the global snapshots.
Successive snapshots are disambiguated by using sequence numbers or
timestamps.Some algorithms carry out distributed computations on local snapshots
without using an observer to collect local snapshots.
Later, we give examples of such algorithms.Applications of Global SnapshotsSystem MonitoringSystems can be monitored by taking global snapshots repeatedly.
Let \(S_{0}, S_{1}, S_{2}, \ldots, \) be the sequence of states
recorded by the system.
From the propertyComputations through Increasing Cutsthere exists a computation that visits each state \(S_{i}\) in order
of increasing \(i\).
The system monitor checks the sequence of snapshots to determine if
some action is required.Rollback and RecoverLet \(S^{*}\) be the most recent snapshot recorded by a system monitor.
From the property,"Computations of Past before Future"there exists a computation that starts at the initial state and later
visits \(S^{*}\).
So, if an error is detected in a computation then the computation can be
restarted from \(S^{*}\) rather than rolling all the way back to the
initial state.Detecting Stable PredicatesAstable predicateis a predicate with the following property: If the
predicate holds at any point in any computation then it continues to
hold forever thereafter in that computation. Equivalently, if a stable
predicate holds in a state \(s\) then it holds in all states reachable
from \(s\).Examples of stable predicates are: "The computation has
terminated," and "The computation is deadlocked."
If a computation has terminated at some point then it remains 
terminated.
Likewise if a computation has deadlocked then it remains deadlocked.Specification of Detection AlgorithmsAn algorithm to detect a stable property \(P\) has the following
specification.If \(P\) holds when the algorithm is initiated then the algorithm
  detects that \(P\) holds.If the algorithm detects that \(P\) holds then \(P\) holds when the
  algorithm terminates.General Detection AlgorithmsA general solution is for the operating system to monitor a client
computation by taking repeated snapshots of the computation.
The OS checks whether a specified stable property holds in each
snapshot.
From the property,"Computations of Past before Future"this general solution satisfies the specification of detection
algorithms.Detection with ObserversThe OS uses an agent, the observer, to collect local snapshots and
form a global snapshot.
The observer inspects the global snapshot to determine if the property
holds in the snapshot.
The OS can also use multiple observers each of which collects
local information from subnetworks; the OS then carries out a
distributed algorithm on its collection of observers.The OS can also execute a distributed algorithm on local
snapshots without having observers collect local information, as
described next.Detection without Observers: Distributed
Algorithms on Local SnapshotsDistributed algorithms on local snapshots operate in two phases.
In the first phase a global snapshot algorithm is executed.
The local snapshot of each agent and its incoming channels are stored
locally, at the agent, without sending the information to observers.In the second phase a distributed algorithm is executed to determine
if the local information stored at agents satisfies a specified global
property, such as "computation has deadlocked."
The algorithm in the second phase operates on unchanging data.
These algorithms are often distributed graph algorithms.The two phases can be executed concurrently in many applications.NextA code skeleton of the algorithm and examples of the global snapshot
algorithm are providedhere.
Nextlogical clocks.K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

Proof about States of Channels

Note: If an agent takes its local snapshot when it receives a marker
along a channel, then the snapshot of the channel is the empty
sequence of messages.Termination of the AlgorithmAfter any agent \(v\) initiates the algorithm, all agents that are
reachable from \(v\) will receive a marker and take their local
snapshots.
If every agent is reachable from an initiator then all agents take
local snapshots.Each agent takes its local snapshot at most once.
So, a marker is sent on a channel at most once.
The computation terminates when all markers are received.Collecting Local Snapshots to form Global
SnapshotsOne way to collect local snapshots is to have an OS agent act as an
observer.
Each agent sends its local snapshots to the observer which puts the
local snapshots together to form the global snapshots.
Successive snapshots are disambiguated by using sequence numbers or
timestamps.Some algorithms carry out distributed computations on local snapshots
without using an observer to collect local snapshots.
Later, we give examples of such algorithms.Applications of Global SnapshotsSystem MonitoringSystems can be monitored by taking global snapshots repeatedly.
Let \(S_{0}, S_{1}, S_{2}, \ldots, \) be the sequence of states
recorded by the system.
From the propertyComputations through Increasing Cutsthere exists a computation that visits each state \(S_{i}\) in order
of increasing \(i\).
The system monitor checks the sequence of snapshots to determine if
some action is required.Rollback and RecoverLet \(S^{*}\) be the most recent snapshot recorded by a system monitor.
From the property,"Computations of Past before Future"there exists a computation that starts at the initial state and later
visits \(S^{*}\).
So, if an error is detected in a computation then the computation can be
restarted from \(S^{*}\) rather than rolling all the way back to the
initial state.Detecting Stable PredicatesAstable predicateis a predicate with the following property: If the
predicate holds at any point in any computation then it continues to
hold forever thereafter in that computation. Equivalently, if a stable
predicate holds in a state \(s\) then it holds in all states reachable
from \(s\).Examples of stable predicates are: "The computation has
terminated," and "The computation is deadlocked."
If a computation has terminated at some point then it remains 
terminated.
Likewise if a computation has deadlocked then it remains deadlocked.Specification of Detection AlgorithmsAn algorithm to detect a stable property \(P\) has the following
specification.If \(P\) holds when the algorithm is initiated then the algorithm
  detects that \(P\) holds.If the algorithm detects that \(P\) holds then \(P\) holds when the
  algorithm terminates.General Detection AlgorithmsA general solution is for the operating system to monitor a client
computation by taking repeated snapshots of the computation.
The OS checks whether a specified stable property holds in each
snapshot.
From the property,"Computations of Past before Future"this general solution satisfies the specification of detection
algorithms.Detection with ObserversThe OS uses an agent, the observer, to collect local snapshots and
form a global snapshot.
The observer inspects the global snapshot to determine if the property
holds in the snapshot.
The OS can also use multiple observers each of which collects
local information from subnetworks; the OS then carries out a
distributed algorithm on its collection of observers.The OS can also execute a distributed algorithm on local
snapshots without having observers collect local information, as
described next.Detection without Observers: Distributed
Algorithms on Local SnapshotsDistributed algorithms on local snapshots operate in two phases.
In the first phase a global snapshot algorithm is executed.
The local snapshot of each agent and its incoming channels are stored
locally, at the agent, without sending the information to observers.In the second phase a distributed algorithm is executed to determine
if the local information stored at agents satisfies a specified global
property, such as "computation has deadlocked."
The algorithm in the second phase operates on unchanging data.
These algorithms are often distributed graph algorithms.The two phases can be executed concurrently in many applications.NextA code skeleton of the algorithm and examples of the global snapshot
algorithm are providedhere.
Nextlogical clocks.K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

Each agent takes its local snapshot at most once.
So, a marker is sent on a channel at most once.
The computation terminates when all markers are received.Collecting Local Snapshots to form Global
SnapshotsOne way to collect local snapshots is to have an OS agent act as an
observer.
Each agent sends its local snapshots to the observer which puts the
local snapshots together to form the global snapshots.
Successive snapshots are disambiguated by using sequence numbers or
timestamps.Some algorithms carry out distributed computations on local snapshots
without using an observer to collect local snapshots.
Later, we give examples of such algorithms.Applications of Global SnapshotsSystem MonitoringSystems can be monitored by taking global snapshots repeatedly.
Let \(S_{0}, S_{1}, S_{2}, \ldots, \) be the sequence of states
recorded by the system.
From the propertyComputations through Increasing Cutsthere exists a computation that visits each state \(S_{i}\) in order
of increasing \(i\).
The system monitor checks the sequence of snapshots to determine if
some action is required.Rollback and RecoverLet \(S^{*}\) be the most recent snapshot recorded by a system monitor.
From the property,"Computations of Past before Future"there exists a computation that starts at the initial state and later
visits \(S^{*}\).
So, if an error is detected in a computation then the computation can be
restarted from \(S^{*}\) rather than rolling all the way back to the
initial state.Detecting Stable PredicatesAstable predicateis a predicate with the following property: If the
predicate holds at any point in any computation then it continues to
hold forever thereafter in that computation. Equivalently, if a stable
predicate holds in a state \(s\) then it holds in all states reachable
from \(s\).Examples of stable predicates are: "The computation has
terminated," and "The computation is deadlocked."
If a computation has terminated at some point then it remains 
terminated.
Likewise if a computation has deadlocked then it remains deadlocked.Specification of Detection AlgorithmsAn algorithm to detect a stable property \(P\) has the following
specification.If \(P\) holds when the algorithm is initiated then the algorithm
  detects that \(P\) holds.If the algorithm detects that \(P\) holds then \(P\) holds when the
  algorithm terminates.General Detection AlgorithmsA general solution is for the operating system to monitor a client
computation by taking repeated snapshots of the computation.
The OS checks whether a specified stable property holds in each
snapshot.
From the property,"Computations of Past before Future"this general solution satisfies the specification of detection
algorithms.Detection with ObserversThe OS uses an agent, the observer, to collect local snapshots and
form a global snapshot.
The observer inspects the global snapshot to determine if the property
holds in the snapshot.
The OS can also use multiple observers each of which collects
local information from subnetworks; the OS then carries out a
distributed algorithm on its collection of observers.The OS can also execute a distributed algorithm on local
snapshots without having observers collect local information, as
described next.Detection without Observers: Distributed
Algorithms on Local SnapshotsDistributed algorithms on local snapshots operate in two phases.
In the first phase a global snapshot algorithm is executed.
The local snapshot of each agent and its incoming channels are stored
locally, at the agent, without sending the information to observers.In the second phase a distributed algorithm is executed to determine
if the local information stored at agents satisfies a specified global
property, such as "computation has deadlocked."
The algorithm in the second phase operates on unchanging data.
These algorithms are often distributed graph algorithms.The two phases can be executed concurrently in many applications.NextA code skeleton of the algorithm and examples of the global snapshot
algorithm are providedhere.
Nextlogical clocks.K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

Some algorithms carry out distributed computations on local snapshots
without using an observer to collect local snapshots.
Later, we give examples of such algorithms.Applications of Global SnapshotsSystem MonitoringSystems can be monitored by taking global snapshots repeatedly.
Let \(S_{0}, S_{1}, S_{2}, \ldots, \) be the sequence of states
recorded by the system.
From the propertyComputations through Increasing Cutsthere exists a computation that visits each state \(S_{i}\) in order
of increasing \(i\).
The system monitor checks the sequence of snapshots to determine if
some action is required.Rollback and RecoverLet \(S^{*}\) be the most recent snapshot recorded by a system monitor.
From the property,"Computations of Past before Future"there exists a computation that starts at the initial state and later
visits \(S^{*}\).
So, if an error is detected in a computation then the computation can be
restarted from \(S^{*}\) rather than rolling all the way back to the
initial state.Detecting Stable PredicatesAstable predicateis a predicate with the following property: If the
predicate holds at any point in any computation then it continues to
hold forever thereafter in that computation. Equivalently, if a stable
predicate holds in a state \(s\) then it holds in all states reachable
from \(s\).Examples of stable predicates are: "The computation has
terminated," and "The computation is deadlocked."
If a computation has terminated at some point then it remains 
terminated.
Likewise if a computation has deadlocked then it remains deadlocked.Specification of Detection AlgorithmsAn algorithm to detect a stable property \(P\) has the following
specification.If \(P\) holds when the algorithm is initiated then the algorithm
  detects that \(P\) holds.If the algorithm detects that \(P\) holds then \(P\) holds when the
  algorithm terminates.General Detection AlgorithmsA general solution is for the operating system to monitor a client
computation by taking repeated snapshots of the computation.
The OS checks whether a specified stable property holds in each
snapshot.
From the property,"Computations of Past before Future"this general solution satisfies the specification of detection
algorithms.Detection with ObserversThe OS uses an agent, the observer, to collect local snapshots and
form a global snapshot.
The observer inspects the global snapshot to determine if the property
holds in the snapshot.
The OS can also use multiple observers each of which collects
local information from subnetworks; the OS then carries out a
distributed algorithm on its collection of observers.The OS can also execute a distributed algorithm on local
snapshots without having observers collect local information, as
described next.Detection without Observers: Distributed
Algorithms on Local SnapshotsDistributed algorithms on local snapshots operate in two phases.
In the first phase a global snapshot algorithm is executed.
The local snapshot of each agent and its incoming channels are stored
locally, at the agent, without sending the information to observers.In the second phase a distributed algorithm is executed to determine
if the local information stored at agents satisfies a specified global
property, such as "computation has deadlocked."
The algorithm in the second phase operates on unchanging data.
These algorithms are often distributed graph algorithms.The two phases can be executed concurrently in many applications.NextA code skeleton of the algorithm and examples of the global snapshot
algorithm are providedhere.
Nextlogical clocks.K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

Examples of stable predicates are: "The computation has
terminated," and "The computation is deadlocked."
If a computation has terminated at some point then it remains 
terminated.
Likewise if a computation has deadlocked then it remains deadlocked.Specification of Detection AlgorithmsAn algorithm to detect a stable property \(P\) has the following
specification.If \(P\) holds when the algorithm is initiated then the algorithm
  detects that \(P\) holds.If the algorithm detects that \(P\) holds then \(P\) holds when the
  algorithm terminates.General Detection AlgorithmsA general solution is for the operating system to monitor a client
computation by taking repeated snapshots of the computation.
The OS checks whether a specified stable property holds in each
snapshot.
From the property,"Computations of Past before Future"this general solution satisfies the specification of detection
algorithms.Detection with ObserversThe OS uses an agent, the observer, to collect local snapshots and
form a global snapshot.
The observer inspects the global snapshot to determine if the property
holds in the snapshot.
The OS can also use multiple observers each of which collects
local information from subnetworks; the OS then carries out a
distributed algorithm on its collection of observers.The OS can also execute a distributed algorithm on local
snapshots without having observers collect local information, as
described next.Detection without Observers: Distributed
Algorithms on Local SnapshotsDistributed algorithms on local snapshots operate in two phases.
In the first phase a global snapshot algorithm is executed.
The local snapshot of each agent and its incoming channels are stored
locally, at the agent, without sending the information to observers.In the second phase a distributed algorithm is executed to determine
if the local information stored at agents satisfies a specified global
property, such as "computation has deadlocked."
The algorithm in the second phase operates on unchanging data.
These algorithms are often distributed graph algorithms.The two phases can be executed concurrently in many applications.NextA code skeleton of the algorithm and examples of the global snapshot
algorithm are providedhere.
Nextlogical clocks.K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

Detection with Observers

The OS can also execute a distributed algorithm on local
snapshots without having observers collect local information, as
described next.Detection without Observers: Distributed
Algorithms on Local SnapshotsDistributed algorithms on local snapshots operate in two phases.
In the first phase a global snapshot algorithm is executed.
The local snapshot of each agent and its incoming channels are stored
locally, at the agent, without sending the information to observers.In the second phase a distributed algorithm is executed to determine
if the local information stored at agents satisfies a specified global
property, such as "computation has deadlocked."
The algorithm in the second phase operates on unchanging data.
These algorithms are often distributed graph algorithms.The two phases can be executed concurrently in many applications.NextA code skeleton of the algorithm and examples of the global snapshot
algorithm are providedhere.
Nextlogical clocks.K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

Detection without Observers: Distributed
Algorithms on Local Snapshots

In the second phase a distributed algorithm is executed to determine
if the local information stored at agents satisfies a specified global
property, such as "computation has deadlocked."
The algorithm in the second phase operates on unchanging data.
These algorithms are often distributed graph algorithms.The two phases can be executed concurrently in many applications.NextA code skeleton of the algorithm and examples of the global snapshot
algorithm are providedhere.
Nextlogical clocks.K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

The two phases can be executed concurrently in many applications.NextA code skeleton of the algorithm and examples of the global snapshot
algorithm are providedhere.
Nextlogical clocks.K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

A code skeleton of the algorithm and examples of the global snapshot
algorithm are providedhere.
Nextlogical clocks.K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology
--- END Content from https://kmchandy.github.io/ChannelSnapshots/ChannelSnapshots.html ---


--- START Content from https://kmchandy.github.io/ChannelSnapshots/ChannelSnapshotsDetails.html ---
TITLE: ChannelSnapshots/ChannelSnapshotsDetails.html

A global snapshot algorithm records a state of the system that can
  occur during a computation.
  The state obtained by the algorithm is called a global snapshot.
  Systems are monitored by taking repeated global snapshots.
  When a transient error is detected, a rollback and recovery
  algorithm restarts the computation from the most recent snapshot
  instead of starting it from the initial state.



Figure Link: https://kmchandy.github.io/ChannelSnapshots/ChannelSnapshots/ChannelSnapshots.002.jpeg

Caption: Fig.2: Representation of a Computation without Snapshots


Figure 2 is a representation of a computation with event sequence
\([0, 1, 2, \ldots, ]\) and agents \(X, Y, Z\) without a concurrent OS
algorithm, and figure 3 shows how the OS changes this computation.
Events later in the computation are placed to the
right of earlier events.Fig.2: Representation of a Computation without SnapshotsFigure 3 shows how a client's computation is changed when the OS takes
snapshots. 
The local snapshots taken by agents are shown as a yellow
circle on the agents' timelines.
The OS delays event 3 so that it occurs after events 4, 5, 6, and 7,
as shown in the figure.
The OS changes the computation, but it does not change the dataflow.Fig.3: The OS changes a Client's ComputationIn figure 3, the pre-snapshot events are 0, 1, 2, 4, 6.
There is only one message received in a pre-snapshot
event, namely the message represented by the edge (0, 2).
So, every message received in a pre-snapshot event is sent in a
pre-snapshot event.
The figure shows that the set of pre-snapshot events is closed.Example: Steps in a Global Snapshot
Algorithm: InitiationFigure 4 illustrates the first step of the algorithm.Fig.4: Agent Sends Markers when it Takes its Local
    SnapshotAgent Y takes its local snapshot shown as a yellow vertex on Y's
timeline.
When Y takes its snapshot it sends markers on its output channels.
The markers are shown as green edges in the figure.When agents X and Z each receive the markers, they take their local
snapshots because they haven't taken snapshots earlier.Fig.5: Agents Take Local Snapshots when they Receive MarkersThe actions by X and Z of taking their snapshots are shown as yellow
vertices on their timelines in figure 5.Example: Agents take Snapshots upon Receiving MarkersWhen X and Z take their snapshots they send markers out on their
output channels.
The markers sent by X are shown in figure 7.
The markers sent by Z are not shown in the figure.Fig.6: When an Agent takes its Snapshot it sends Markers.Example: Snapshot of a ChannelFigure 8 shows how agent Y determines the state of the channel from X
to Y in the global snapshot.
Y starts recording the messages it receives along this channel after Y
takes its snapshot and stops the recording when it receives a marker
on this channel
The only message in this interval is the message corresponding to edge
(6, 7).Fig.7: Example: Recording a Channel StateThe message corresponding to edge \((0, 2)\) is from X to Y but is not
in the snapshot of 
the channel because both \(0\) and \(2\) are pre-snapshot events.
Likewise, the message corresponding to edge \((12, 13)\) is from X to
Y but is not in the snapshot of
the channel because both \(12\) and \(13\) are post-snapshot events.
The message corresponding to edge \((6, 7)\) was sent in a
pre-snapshot event and received in a post-snapshot event, and so it is
in the snapshot of the channel.NextNextlogical clocks.K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology



Figure Link: https://kmchandy.github.io/ChannelSnapshots/ChannelSnapshots/ChannelSnapshots.002.jpeg

Caption: Fig.2: Representation of a Computation without Snapshots


Figure 3 shows how a client's computation is changed when the OS takes
snapshots. 
The local snapshots taken by agents are shown as a yellow
circle on the agents' timelines.
The OS delays event 3 so that it occurs after events 4, 5, 6, and 7,
as shown in the figure.
The OS changes the computation, but it does not change the dataflow.Fig.3: The OS changes a Client's ComputationIn figure 3, the pre-snapshot events are 0, 1, 2, 4, 6.
There is only one message received in a pre-snapshot
event, namely the message represented by the edge (0, 2).
So, every message received in a pre-snapshot event is sent in a
pre-snapshot event.
The figure shows that the set of pre-snapshot events is closed.Example: Steps in a Global Snapshot
Algorithm: InitiationFigure 4 illustrates the first step of the algorithm.Fig.4: Agent Sends Markers when it Takes its Local
    SnapshotAgent Y takes its local snapshot shown as a yellow vertex on Y's
timeline.
When Y takes its snapshot it sends markers on its output channels.
The markers are shown as green edges in the figure.When agents X and Z each receive the markers, they take their local
snapshots because they haven't taken snapshots earlier.Fig.5: Agents Take Local Snapshots when they Receive MarkersThe actions by X and Z of taking their snapshots are shown as yellow
vertices on their timelines in figure 5.Example: Agents take Snapshots upon Receiving MarkersWhen X and Z take their snapshots they send markers out on their
output channels.
The markers sent by X are shown in figure 7.
The markers sent by Z are not shown in the figure.Fig.6: When an Agent takes its Snapshot it sends Markers.Example: Snapshot of a ChannelFigure 8 shows how agent Y determines the state of the channel from X
to Y in the global snapshot.
Y starts recording the messages it receives along this channel after Y
takes its snapshot and stops the recording when it receives a marker
on this channel
The only message in this interval is the message corresponding to edge
(6, 7).Fig.7: Example: Recording a Channel StateThe message corresponding to edge \((0, 2)\) is from X to Y but is not
in the snapshot of 
the channel because both \(0\) and \(2\) are pre-snapshot events.
Likewise, the message corresponding to edge \((12, 13)\) is from X to
Y but is not in the snapshot of
the channel because both \(12\) and \(13\) are post-snapshot events.
The message corresponding to edge \((6, 7)\) was sent in a
pre-snapshot event and received in a post-snapshot event, and so it is
in the snapshot of the channel.NextNextlogical clocks.K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology



Figure Link: https://kmchandy.github.io/ChannelSnapshots/ChannelSnapshots/ChannelSnapshots.003.jpeg

Caption: Fig.3: The OS changes a Client's Computation


When agents X and Z each receive the markers, they take their local
snapshots because they haven't taken snapshots earlier.Fig.5: Agents Take Local Snapshots when they Receive MarkersThe actions by X and Z of taking their snapshots are shown as yellow
vertices on their timelines in figure 5.Example: Agents take Snapshots upon Receiving MarkersWhen X and Z take their snapshots they send markers out on their
output channels.
The markers sent by X are shown in figure 7.
The markers sent by Z are not shown in the figure.Fig.6: When an Agent takes its Snapshot it sends Markers.Example: Snapshot of a ChannelFigure 8 shows how agent Y determines the state of the channel from X
to Y in the global snapshot.
Y starts recording the messages it receives along this channel after Y
takes its snapshot and stops the recording when it receives a marker
on this channel
The only message in this interval is the message corresponding to edge
(6, 7).Fig.7: Example: Recording a Channel StateThe message corresponding to edge \((0, 2)\) is from X to Y but is not
in the snapshot of 
the channel because both \(0\) and \(2\) are pre-snapshot events.
Likewise, the message corresponding to edge \((12, 13)\) is from X to
Y but is not in the snapshot of
the channel because both \(12\) and \(13\) are post-snapshot events.
The message corresponding to edge \((6, 7)\) was sent in a
pre-snapshot event and received in a post-snapshot event, and so it is
in the snapshot of the channel.NextNextlogical clocks.K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology



Figure Link: https://kmchandy.github.io/ChannelSnapshots/ChannelSnapshots/ChannelSnapshots.006.jpeg

Caption: Fig.5: Agents Take Local Snapshots when they Receive Markers


When X and Z take their snapshots they send markers out on their
output channels.
The markers sent by X are shown in figure 7.
The markers sent by Z are not shown in the figure.Fig.6: When an Agent takes its Snapshot it sends Markers.Example: Snapshot of a ChannelFigure 8 shows how agent Y determines the state of the channel from X
to Y in the global snapshot.
Y starts recording the messages it receives along this channel after Y
takes its snapshot and stops the recording when it receives a marker
on this channel
The only message in this interval is the message corresponding to edge
(6, 7).Fig.7: Example: Recording a Channel StateThe message corresponding to edge \((0, 2)\) is from X to Y but is not
in the snapshot of 
the channel because both \(0\) and \(2\) are pre-snapshot events.
Likewise, the message corresponding to edge \((12, 13)\) is from X to
Y but is not in the snapshot of
the channel because both \(12\) and \(13\) are post-snapshot events.
The message corresponding to edge \((6, 7)\) was sent in a
pre-snapshot event and received in a post-snapshot event, and so it is
in the snapshot of the channel.NextNextlogical clocks.K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology



Figure Link: https://kmchandy.github.io/ChannelSnapshots/ChannelSnapshots/ChannelSnapshots.007.jpeg

Caption: Fig.6: When an Agent takes its Snapshot it sends Markers.


Figure 8 shows how agent Y determines the state of the channel from X
to Y in the global snapshot.
Y starts recording the messages it receives along this channel after Y
takes its snapshot and stops the recording when it receives a marker
on this channel
The only message in this interval is the message corresponding to edge
(6, 7).Fig.7: Example: Recording a Channel StateThe message corresponding to edge \((0, 2)\) is from X to Y but is not
in the snapshot of 
the channel because both \(0\) and \(2\) are pre-snapshot events.
Likewise, the message corresponding to edge \((12, 13)\) is from X to
Y but is not in the snapshot of
the channel because both \(12\) and \(13\) are post-snapshot events.
The message corresponding to edge \((6, 7)\) was sent in a
pre-snapshot event and received in a post-snapshot event, and so it is
in the snapshot of the channel.NextNextlogical clocks.K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology



Figure Link: https://kmchandy.github.io/ChannelSnapshots/ChannelSnapshots/ChannelSnapshots.008.jpeg

Caption: Fig.7: Example: Recording a Channel State


K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology
--- END Content from https://kmchandy.github.io/ChannelSnapshots/ChannelSnapshotsDetails.html ---


--- START Content from https://kmchandy.github.io/ChannelSnapshots/ChannelSnapshotsFAQ.html ---
TITLE: ChannelSnapshots/ChannelSnapshotsFAQ.html

Which agent starts a global snapshot algorithm?

Can the algorithm be used to detect the states of a subset of agents
  and channels, or does the algorithm necessarily have to obtain a
  snapshot that encompasses all agents?

The subset of agent and channel states that are recorded are part of
  a global state.
  Some algorithms only need states of subsets of agents.Can the algorithm be used to take repeated snapshots?Yes.Each initiation of the algorithm by an agent \(x\) at local time
  \(t\) is identified by the pair \(x, t\).
  Every local snapshot and every marker is identified in this way.
  So, different executions of the global snapshot algorithm can be
  disambiguated in this way.Nextlogical clocks.K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

Can the algorithm be used to take repeated snapshots?

Each initiation of the algorithm by an agent \(x\) at local time
  \(t\) is identified by the pair \(x, t\).
  Every local snapshot and every marker is identified in this way.
  So, different executions of the global snapshot algorithm can be
  disambiguated in this way.Nextlogical clocks.K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

Nextlogical clocks.K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology
--- END Content from https://kmchandy.github.io/ChannelSnapshots/ChannelSnapshotsFAQ.html ---


--- START Content from https://kmchandy.github.io/ChannelSnapshots/ChannelSnapshotsReview.html ---
TITLE: ChannelSnapshots/ChannelSnapshotsReview.html

K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology
--- END Content from https://kmchandy.github.io/ChannelSnapshots/ChannelSnapshotsReview.html ---


--- START Content from https://kmchandy.github.io/ChannelSnapshots/LogicalClocks.html ---
TITLE: ChannelSnapshots/LogicalClocksNew.html

A logical clock algorithm assigns a value, called the logical time,
  to each step in a computation so that all sequences of steps in
  ascending logical time are computations.



Figure Link: https://kmchandy.github.io/ChannelSnapshots/Timelines/Timelines.001.jpeg

Caption: Fig. 1: Logical Times of Steps: Edges Directed from
  Lower to Higher Logical Times


Design an algorithm that assigns a value,  called thelogical
  time, to each step in a computation so that all sequences of
  steps in ascending logical time are computations.Thisdefinition is differentfrom that given in the paper that
  introduces the concept.How Should You Solve the Problem?StrategyA strategy for assigning attributes to steps in computations is to
  find aproperty of computationsthat can help.We use the following property:
  A computation is a sequence of steps in which for all edges
  \((e, e')\) in the dataflow graph corresponding to the computation:\(e\) occurs before \(e'\) in the sequence.This property suggests the following algorithm to assign logical
  time \(t(e)\) to step \(e\).The Logical Time PropertyFor all edges \((e, e')\) of the dataflow graph:
\(t(e) < t(e')\).Example: Logical Times of StepsFigure 1 shows the dataflow graph of a computation with agents
  \(A, B, C\) and an step sequence \([0, 1, 2, \ldots, ]\).
  The numbers inside the vertices are the step ids which show the
  position of the step in the computation.
  The red numbers outside the steps are logical times assigned to
  steps.Logical times are arbitrary provided
  every edge is directed from a lower to a higer logical time.Fig. 1: Logical Times of Steps: Edges Directed from
  Lower to Higher Logical TimesVerify that every edge in figure 1 is from an step with a lower
logical time to an step with a higher logical time.Steps in Increasing Order of Logical TimeAll sequences of steps in increasing order of logical time are
computations.This result follows from the fact thatall topological sorts of dataflow graphs are computations, and
sequences of of steps in increasing 
logical time are topological sorts.For example, a sequence of steps in increasing logical time in
figure 1 is:
[1, 2, 4, 3, 5, 6, 7, 8, 9], and this sequence is a computation.A Logical Clock AlgorithmThe following algorithm is suggested by the logical time property.
Let \(t(e)\) be the logical time assigned to step \(e\).
A message sent in an step \(e\) is assigned a timestamp \(t(e)\).Let \(e'\) be the step immediately preceding an step \(e\) at an
agent, and let the timestamp of the message received in \(e\) be
\(T\).Set \(t(e)\) to be any value greater than max(t(e'), T).The correctness of the algorithm is self evident.NextLogical clocks are used to record global snapshotsas described here.K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology



Figure Link: https://kmchandy.github.io/ChannelSnapshots/Timelines/Timelines.001.jpeg

Caption: Fig. 1: Logical Times of Steps: Edges Directed from
  Lower to Higher Logical Times


Thisdefinition is differentfrom that given in the paper that
  introduces the concept.How Should You Solve the Problem?StrategyA strategy for assigning attributes to steps in computations is to
  find aproperty of computationsthat can help.We use the following property:
  A computation is a sequence of steps in which for all edges
  \((e, e')\) in the dataflow graph corresponding to the computation:\(e\) occurs before \(e'\) in the sequence.This property suggests the following algorithm to assign logical
  time \(t(e)\) to step \(e\).The Logical Time PropertyFor all edges \((e, e')\) of the dataflow graph:
\(t(e) < t(e')\).Example: Logical Times of StepsFigure 1 shows the dataflow graph of a computation with agents
  \(A, B, C\) and an step sequence \([0, 1, 2, \ldots, ]\).
  The numbers inside the vertices are the step ids which show the
  position of the step in the computation.
  The red numbers outside the steps are logical times assigned to
  steps.Logical times are arbitrary provided
  every edge is directed from a lower to a higer logical time.Fig. 1: Logical Times of Steps: Edges Directed from
  Lower to Higher Logical TimesVerify that every edge in figure 1 is from an step with a lower
logical time to an step with a higher logical time.Steps in Increasing Order of Logical TimeAll sequences of steps in increasing order of logical time are
computations.This result follows from the fact thatall topological sorts of dataflow graphs are computations, and
sequences of of steps in increasing 
logical time are topological sorts.For example, a sequence of steps in increasing logical time in
figure 1 is:
[1, 2, 4, 3, 5, 6, 7, 8, 9], and this sequence is a computation.A Logical Clock AlgorithmThe following algorithm is suggested by the logical time property.
Let \(t(e)\) be the logical time assigned to step \(e\).
A message sent in an step \(e\) is assigned a timestamp \(t(e)\).Let \(e'\) be the step immediately preceding an step \(e\) at an
agent, and let the timestamp of the message received in \(e\) be
\(T\).Set \(t(e)\) to be any value greater than max(t(e'), T).The correctness of the algorithm is self evident.NextLogical clocks are used to record global snapshotsas described here.K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology



Figure Link: https://kmchandy.github.io/ChannelSnapshots/Timelines/Timelines.001.jpeg

Caption: Fig. 1: Logical Times of Steps: Edges Directed from
  Lower to Higher Logical Times


Strategy



Figure Link: https://kmchandy.github.io/ChannelSnapshots/Timelines/Timelines.001.jpeg

Caption: Fig. 1: Logical Times of Steps: Edges Directed from
  Lower to Higher Logical Times


A strategy for assigning attributes to steps in computations is to
  find aproperty of computationsthat can help.We use the following property:
  A computation is a sequence of steps in which for all edges
  \((e, e')\) in the dataflow graph corresponding to the computation:\(e\) occurs before \(e'\) in the sequence.This property suggests the following algorithm to assign logical
  time \(t(e)\) to step \(e\).The Logical Time PropertyFor all edges \((e, e')\) of the dataflow graph:
\(t(e) < t(e')\).Example: Logical Times of StepsFigure 1 shows the dataflow graph of a computation with agents
  \(A, B, C\) and an step sequence \([0, 1, 2, \ldots, ]\).
  The numbers inside the vertices are the step ids which show the
  position of the step in the computation.
  The red numbers outside the steps are logical times assigned to
  steps.Logical times are arbitrary provided
  every edge is directed from a lower to a higer logical time.Fig. 1: Logical Times of Steps: Edges Directed from
  Lower to Higher Logical TimesVerify that every edge in figure 1 is from an step with a lower
logical time to an step with a higher logical time.Steps in Increasing Order of Logical TimeAll sequences of steps in increasing order of logical time are
computations.This result follows from the fact thatall topological sorts of dataflow graphs are computations, and
sequences of of steps in increasing 
logical time are topological sorts.For example, a sequence of steps in increasing logical time in
figure 1 is:
[1, 2, 4, 3, 5, 6, 7, 8, 9], and this sequence is a computation.A Logical Clock AlgorithmThe following algorithm is suggested by the logical time property.
Let \(t(e)\) be the logical time assigned to step \(e\).
A message sent in an step \(e\) is assigned a timestamp \(t(e)\).Let \(e'\) be the step immediately preceding an step \(e\) at an
agent, and let the timestamp of the message received in \(e\) be
\(T\).Set \(t(e)\) to be any value greater than max(t(e'), T).The correctness of the algorithm is self evident.NextLogical clocks are used to record global snapshotsas described here.K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology



Figure Link: https://kmchandy.github.io/ChannelSnapshots/Timelines/Timelines.001.jpeg

Caption: Fig. 1: Logical Times of Steps: Edges Directed from
  Lower to Higher Logical Times


We use the following property:
  A computation is a sequence of steps in which for all edges
  \((e, e')\) in the dataflow graph corresponding to the computation:\(e\) occurs before \(e'\) in the sequence.This property suggests the following algorithm to assign logical
  time \(t(e)\) to step \(e\).The Logical Time PropertyFor all edges \((e, e')\) of the dataflow graph:
\(t(e) < t(e')\).Example: Logical Times of StepsFigure 1 shows the dataflow graph of a computation with agents
  \(A, B, C\) and an step sequence \([0, 1, 2, \ldots, ]\).
  The numbers inside the vertices are the step ids which show the
  position of the step in the computation.
  The red numbers outside the steps are logical times assigned to
  steps.Logical times are arbitrary provided
  every edge is directed from a lower to a higer logical time.Fig. 1: Logical Times of Steps: Edges Directed from
  Lower to Higher Logical TimesVerify that every edge in figure 1 is from an step with a lower
logical time to an step with a higher logical time.Steps in Increasing Order of Logical TimeAll sequences of steps in increasing order of logical time are
computations.This result follows from the fact thatall topological sorts of dataflow graphs are computations, and
sequences of of steps in increasing 
logical time are topological sorts.For example, a sequence of steps in increasing logical time in
figure 1 is:
[1, 2, 4, 3, 5, 6, 7, 8, 9], and this sequence is a computation.A Logical Clock AlgorithmThe following algorithm is suggested by the logical time property.
Let \(t(e)\) be the logical time assigned to step \(e\).
A message sent in an step \(e\) is assigned a timestamp \(t(e)\).Let \(e'\) be the step immediately preceding an step \(e\) at an
agent, and let the timestamp of the message received in \(e\) be
\(T\).Set \(t(e)\) to be any value greater than max(t(e'), T).The correctness of the algorithm is self evident.NextLogical clocks are used to record global snapshotsas described here.K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology



Figure Link: https://kmchandy.github.io/ChannelSnapshots/Timelines/Timelines.001.jpeg

Caption: Fig. 1: Logical Times of Steps: Edges Directed from
  Lower to Higher Logical Times


This property suggests the following algorithm to assign logical
  time \(t(e)\) to step \(e\).The Logical Time PropertyFor all edges \((e, e')\) of the dataflow graph:
\(t(e) < t(e')\).Example: Logical Times of StepsFigure 1 shows the dataflow graph of a computation with agents
  \(A, B, C\) and an step sequence \([0, 1, 2, \ldots, ]\).
  The numbers inside the vertices are the step ids which show the
  position of the step in the computation.
  The red numbers outside the steps are logical times assigned to
  steps.Logical times are arbitrary provided
  every edge is directed from a lower to a higer logical time.Fig. 1: Logical Times of Steps: Edges Directed from
  Lower to Higher Logical TimesVerify that every edge in figure 1 is from an step with a lower
logical time to an step with a higher logical time.Steps in Increasing Order of Logical TimeAll sequences of steps in increasing order of logical time are
computations.This result follows from the fact thatall topological sorts of dataflow graphs are computations, and
sequences of of steps in increasing 
logical time are topological sorts.For example, a sequence of steps in increasing logical time in
figure 1 is:
[1, 2, 4, 3, 5, 6, 7, 8, 9], and this sequence is a computation.A Logical Clock AlgorithmThe following algorithm is suggested by the logical time property.
Let \(t(e)\) be the logical time assigned to step \(e\).
A message sent in an step \(e\) is assigned a timestamp \(t(e)\).Let \(e'\) be the step immediately preceding an step \(e\) at an
agent, and let the timestamp of the message received in \(e\) be
\(T\).Set \(t(e)\) to be any value greater than max(t(e'), T).The correctness of the algorithm is self evident.NextLogical clocks are used to record global snapshotsas described here.K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology



Figure Link: https://kmchandy.github.io/ChannelSnapshots/Timelines/Timelines.001.jpeg

Caption: Fig. 1: Logical Times of Steps: Edges Directed from
  Lower to Higher Logical Times


The Logical Time PropertyFor all edges \((e, e')\) of the dataflow graph:
\(t(e) < t(e')\).Example: Logical Times of StepsFigure 1 shows the dataflow graph of a computation with agents
  \(A, B, C\) and an step sequence \([0, 1, 2, \ldots, ]\).
  The numbers inside the vertices are the step ids which show the
  position of the step in the computation.
  The red numbers outside the steps are logical times assigned to
  steps.Logical times are arbitrary provided
  every edge is directed from a lower to a higer logical time.Fig. 1: Logical Times of Steps: Edges Directed from
  Lower to Higher Logical TimesVerify that every edge in figure 1 is from an step with a lower
logical time to an step with a higher logical time.Steps in Increasing Order of Logical TimeAll sequences of steps in increasing order of logical time are
computations.This result follows from the fact thatall topological sorts of dataflow graphs are computations, and
sequences of of steps in increasing 
logical time are topological sorts.For example, a sequence of steps in increasing logical time in
figure 1 is:
[1, 2, 4, 3, 5, 6, 7, 8, 9], and this sequence is a computation.A Logical Clock AlgorithmThe following algorithm is suggested by the logical time property.
Let \(t(e)\) be the logical time assigned to step \(e\).
A message sent in an step \(e\) is assigned a timestamp \(t(e)\).Let \(e'\) be the step immediately preceding an step \(e\) at an
agent, and let the timestamp of the message received in \(e\) be
\(T\).Set \(t(e)\) to be any value greater than max(t(e'), T).The correctness of the algorithm is self evident.NextLogical clocks are used to record global snapshotsas described here.K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology



Figure Link: https://kmchandy.github.io/ChannelSnapshots/Timelines/Timelines.001.jpeg

Caption: Fig. 1: Logical Times of Steps: Edges Directed from
  Lower to Higher Logical Times


For all edges \((e, e')\) of the dataflow graph:
\(t(e) < t(e')\).



Figure Link: https://kmchandy.github.io/ChannelSnapshots/Timelines/Timelines.001.jpeg

Caption: Fig. 1: Logical Times of Steps: Edges Directed from
  Lower to Higher Logical Times


Logical times are arbitrary provided
  every edge is directed from a lower to a higer logical time.Fig. 1: Logical Times of Steps: Edges Directed from
  Lower to Higher Logical TimesVerify that every edge in figure 1 is from an step with a lower
logical time to an step with a higher logical time.Steps in Increasing Order of Logical TimeAll sequences of steps in increasing order of logical time are
computations.This result follows from the fact thatall topological sorts of dataflow graphs are computations, and
sequences of of steps in increasing 
logical time are topological sorts.For example, a sequence of steps in increasing logical time in
figure 1 is:
[1, 2, 4, 3, 5, 6, 7, 8, 9], and this sequence is a computation.A Logical Clock AlgorithmThe following algorithm is suggested by the logical time property.
Let \(t(e)\) be the logical time assigned to step \(e\).
A message sent in an step \(e\) is assigned a timestamp \(t(e)\).Let \(e'\) be the step immediately preceding an step \(e\) at an
agent, and let the timestamp of the message received in \(e\) be
\(T\).Set \(t(e)\) to be any value greater than max(t(e'), T).The correctness of the algorithm is self evident.NextLogical clocks are used to record global snapshotsas described here.K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology



Figure Link: https://kmchandy.github.io/ChannelSnapshots/Timelines/Timelines.001.jpeg

Caption: Fig. 1: Logical Times of Steps: Edges Directed from
  Lower to Higher Logical Times


This result follows from the fact thatall topological sorts of dataflow graphs are computations, and
sequences of of steps in increasing 
logical time are topological sorts.For example, a sequence of steps in increasing logical time in
figure 1 is:
[1, 2, 4, 3, 5, 6, 7, 8, 9], and this sequence is a computation.A Logical Clock AlgorithmThe following algorithm is suggested by the logical time property.
Let \(t(e)\) be the logical time assigned to step \(e\).
A message sent in an step \(e\) is assigned a timestamp \(t(e)\).Let \(e'\) be the step immediately preceding an step \(e\) at an
agent, and let the timestamp of the message received in \(e\) be
\(T\).Set \(t(e)\) to be any value greater than max(t(e'), T).The correctness of the algorithm is self evident.NextLogical clocks are used to record global snapshotsas described here.K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

For example, a sequence of steps in increasing logical time in
figure 1 is:
[1, 2, 4, 3, 5, 6, 7, 8, 9], and this sequence is a computation.A Logical Clock AlgorithmThe following algorithm is suggested by the logical time property.
Let \(t(e)\) be the logical time assigned to step \(e\).
A message sent in an step \(e\) is assigned a timestamp \(t(e)\).Let \(e'\) be the step immediately preceding an step \(e\) at an
agent, and let the timestamp of the message received in \(e\) be
\(T\).Set \(t(e)\) to be any value greater than max(t(e'), T).The correctness of the algorithm is self evident.NextLogical clocks are used to record global snapshotsas described here.K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

Let \(e'\) be the step immediately preceding an step \(e\) at an
agent, and let the timestamp of the message received in \(e\) be
\(T\).Set \(t(e)\) to be any value greater than max(t(e'), T).The correctness of the algorithm is self evident.NextLogical clocks are used to record global snapshotsas described here.K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

Set \(t(e)\) to be any value greater than max(t(e'), T).The correctness of the algorithm is self evident.NextLogical clocks are used to record global snapshotsas described here.K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

The correctness of the algorithm is self evident.NextLogical clocks are used to record global snapshotsas described here.K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology
--- END Content from https://kmchandy.github.io/ChannelSnapshots/LogicalClocks.html ---


--- START Content from https://kmchandy.github.io/ChannelSnapshots/LogicalClocksSnapshots.html ---
TITLE: ChannelSnapshots/LogicalClocksSnapshots.html

The state at which all agents are at the same logical time \(t\) is
  a global snapshot. 
  The state when local physical clocks
  of all agents are at the same time \(t\) may not be a global snapshot.
  Combining physical and logical clocks results in clocks that tick
  forward and where the state when all local clocks are at the same
  time is a global snapshot.
  Examples of algorithms that use such clocks are given later.



Figure Link: https://kmchandy.github.io/ChannelSnapshots/Timelines/Timelines.002.jpeg

Caption: Fig. 1: Cut at Logical Time 6.5. Past is Set of Steps
    with Logical Time at most 6.5


Design an algorithm that computes global snapshots using logical
  clocks.How Should You Solve the Problem?StrategyUse properties of logical times and computations.Sequences of steps in increasing logical time are computations.
  Therefore, the sequence of steps with logical time at most  \(t\) is
  a computation, for all  \(t\).
  This suggests the following definition of the state at logical time
  \(t\).State at Logical Time \(t\)The state of an agent \(A\) at logical time \(t\) is its state after
  steps with logical time \(t\) or less and before steps with
  logical time greater than \(t\).The state of a channel at logical time \(t\) is the sequence of
  messages sent along the channel in steps with logical time at most
  \(t\), but not received in these steps.Example: State at Logical Time \(t\)Figure 1 illustrates the state at logical time 6.5 of the
computation shown in figure 1.
  The curved purple line represents the cut.
  The cut separates the past of the cut from its future.
  Past steps are colored black while future steps are colored
green.
The states of agents and channels at logical time \(t = 6.5\) are
given by the labels of the edges that cut the purple line.Fig. 1: Cut at Logical Time 6.5. Past is Set of Steps
    with Logical Time at most 6.5The point at which the purple line cuts the timeline for agent \(A\)
can be thought of as the point in \(A\)'s computation at which the
logical time is exactly 6.5.
This cut is on the edge from the step at \(A\) with
  logical time at most 6.5 to the step with logical time greater
  than 6.5.
  In this example the cut is on the edge from step 3 to step 5.The message edge from step 3 to step 7 represents a message sent
  along the channel from \(A\) to \(B\) in the past that is received
  in the future.
In this example, the state of the channel \((A, B)\) is the sequence
consisting of a 
single message which is the label of this edge.Global Snapshot Algorithm to Record the State at a Logical TimeAn algorithm to record the state at logical time \(t\) follows
directly from the definition of the state at logical time \(t\).Each agent takes its local snapshot -- i.e. records its state -- after
a step with logical time at most \(t\) and before a
  step with logical time greater than \(t\).An agent records the state of an input channel as the sequence of
messages with timestamps atmost \(t\) that the agent
  receives when its logical clock exceeds \(t\).The purple line in figure 2 represents the global snapshot at logical
time 6.5.Using Imperfect Clocks in Distributed AlgorithmsIntuitionWe will design some algorithms using 
logical time to play the role of real time.
Figure 3 shows the computation in figure 2 with the horizontal axis
representing real time.
The position of a step with logical time \(t\) is at a distance of
\(t\) units from the origin.Fig. 3: Computation with Logical Time as Real TimeThink of logical time as continuous, just as real time is continuous.
In this example, points at logical times \(6.5\) and \(6.6\), at agent
\(A\), refer to the same edge.
It helps intuition, however, to think
of the point at logical time \(6.6\) as a location to the right of the
point at logical time \(6.5\) on the same edge.
Imagine that logical time \(6.6\) is 0.1 time units to the right of
logical time \(6.5\).The cut at logical time \(6.5\) is represented by the vertical line
at time \(6.5\).
The left of the line is the past at logical time \(6.5\), and the
right side of the line is the future at that time.Physical and Logical ClocksOperating systems maintain clocks. Some have atomic clocks or other
  high-fidelity clocks that use Precision or Network Time Protocols
(PTP, NTP).
With high-fidelity clocks, a message sent when the sender's clock is
at \(t\) will almost always be received when the receiver's
clock is later than \(t\).
So physical clocks almost always obey the logical clock requirement.We cannot rule out the possibility that a message sent when the
sender's clock is at \(t\) is received when the receiver's clock is
earlier than \(t\) or equal to \(t\).
We can use physical clocks, but correct them so that messages are
received only after they are sent, where the times are determined by
corrected physical clocks.Such clocks have the following properties that we use in
designing algorithms:Clocks tick forward forever: For all \(t\), there is a point in
  an infinite computation at which clocks of all agents exceed \(t\).Sequences of steps in ascending order of time are computations.NextThe next few pages describe applications of global snapshots.
We begin withTermination Detection.K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology



Figure Link: https://kmchandy.github.io/ChannelSnapshots/Timelines/Timelines.002.jpeg

Caption: Fig. 1: Cut at Logical Time 6.5. Past is Set of Steps
    with Logical Time at most 6.5


Strategy



Figure Link: https://kmchandy.github.io/ChannelSnapshots/Timelines/Timelines.002.jpeg

Caption: Fig. 1: Cut at Logical Time 6.5. Past is Set of Steps
    with Logical Time at most 6.5


Sequences of steps in increasing logical time are computations.
  Therefore, the sequence of steps with logical time at most  \(t\) is
  a computation, for all  \(t\).
  This suggests the following definition of the state at logical time
  \(t\).State at Logical Time \(t\)The state of an agent \(A\) at logical time \(t\) is its state after
  steps with logical time \(t\) or less and before steps with
  logical time greater than \(t\).The state of a channel at logical time \(t\) is the sequence of
  messages sent along the channel in steps with logical time at most
  \(t\), but not received in these steps.Example: State at Logical Time \(t\)Figure 1 illustrates the state at logical time 6.5 of the
computation shown in figure 1.
  The curved purple line represents the cut.
  The cut separates the past of the cut from its future.
  Past steps are colored black while future steps are colored
green.
The states of agents and channels at logical time \(t = 6.5\) are
given by the labels of the edges that cut the purple line.Fig. 1: Cut at Logical Time 6.5. Past is Set of Steps
    with Logical Time at most 6.5The point at which the purple line cuts the timeline for agent \(A\)
can be thought of as the point in \(A\)'s computation at which the
logical time is exactly 6.5.
This cut is on the edge from the step at \(A\) with
  logical time at most 6.5 to the step with logical time greater
  than 6.5.
  In this example the cut is on the edge from step 3 to step 5.The message edge from step 3 to step 7 represents a message sent
  along the channel from \(A\) to \(B\) in the past that is received
  in the future.
In this example, the state of the channel \((A, B)\) is the sequence
consisting of a 
single message which is the label of this edge.Global Snapshot Algorithm to Record the State at a Logical TimeAn algorithm to record the state at logical time \(t\) follows
directly from the definition of the state at logical time \(t\).Each agent takes its local snapshot -- i.e. records its state -- after
a step with logical time at most \(t\) and before a
  step with logical time greater than \(t\).An agent records the state of an input channel as the sequence of
messages with timestamps atmost \(t\) that the agent
  receives when its logical clock exceeds \(t\).The purple line in figure 2 represents the global snapshot at logical
time 6.5.Using Imperfect Clocks in Distributed AlgorithmsIntuitionWe will design some algorithms using 
logical time to play the role of real time.
Figure 3 shows the computation in figure 2 with the horizontal axis
representing real time.
The position of a step with logical time \(t\) is at a distance of
\(t\) units from the origin.Fig. 3: Computation with Logical Time as Real TimeThink of logical time as continuous, just as real time is continuous.
In this example, points at logical times \(6.5\) and \(6.6\), at agent
\(A\), refer to the same edge.
It helps intuition, however, to think
of the point at logical time \(6.6\) as a location to the right of the
point at logical time \(6.5\) on the same edge.
Imagine that logical time \(6.6\) is 0.1 time units to the right of
logical time \(6.5\).The cut at logical time \(6.5\) is represented by the vertical line
at time \(6.5\).
The left of the line is the past at logical time \(6.5\), and the
right side of the line is the future at that time.Physical and Logical ClocksOperating systems maintain clocks. Some have atomic clocks or other
  high-fidelity clocks that use Precision or Network Time Protocols
(PTP, NTP).
With high-fidelity clocks, a message sent when the sender's clock is
at \(t\) will almost always be received when the receiver's
clock is later than \(t\).
So physical clocks almost always obey the logical clock requirement.We cannot rule out the possibility that a message sent when the
sender's clock is at \(t\) is received when the receiver's clock is
earlier than \(t\) or equal to \(t\).
We can use physical clocks, but correct them so that messages are
received only after they are sent, where the times are determined by
corrected physical clocks.Such clocks have the following properties that we use in
designing algorithms:Clocks tick forward forever: For all \(t\), there is a point in
  an infinite computation at which clocks of all agents exceed \(t\).Sequences of steps in ascending order of time are computations.NextThe next few pages describe applications of global snapshots.
We begin withTermination Detection.K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology



Figure Link: https://kmchandy.github.io/ChannelSnapshots/Timelines/Timelines.002.jpeg

Caption: Fig. 1: Cut at Logical Time 6.5. Past is Set of Steps
    with Logical Time at most 6.5


The state of a channel at logical time \(t\) is the sequence of
  messages sent along the channel in steps with logical time at most
  \(t\), but not received in these steps.Example: State at Logical Time \(t\)Figure 1 illustrates the state at logical time 6.5 of the
computation shown in figure 1.
  The curved purple line represents the cut.
  The cut separates the past of the cut from its future.
  Past steps are colored black while future steps are colored
green.
The states of agents and channels at logical time \(t = 6.5\) are
given by the labels of the edges that cut the purple line.Fig. 1: Cut at Logical Time 6.5. Past is Set of Steps
    with Logical Time at most 6.5The point at which the purple line cuts the timeline for agent \(A\)
can be thought of as the point in \(A\)'s computation at which the
logical time is exactly 6.5.
This cut is on the edge from the step at \(A\) with
  logical time at most 6.5 to the step with logical time greater
  than 6.5.
  In this example the cut is on the edge from step 3 to step 5.The message edge from step 3 to step 7 represents a message sent
  along the channel from \(A\) to \(B\) in the past that is received
  in the future.
In this example, the state of the channel \((A, B)\) is the sequence
consisting of a 
single message which is the label of this edge.Global Snapshot Algorithm to Record the State at a Logical TimeAn algorithm to record the state at logical time \(t\) follows
directly from the definition of the state at logical time \(t\).Each agent takes its local snapshot -- i.e. records its state -- after
a step with logical time at most \(t\) and before a
  step with logical time greater than \(t\).An agent records the state of an input channel as the sequence of
messages with timestamps atmost \(t\) that the agent
  receives when its logical clock exceeds \(t\).The purple line in figure 2 represents the global snapshot at logical
time 6.5.Using Imperfect Clocks in Distributed AlgorithmsIntuitionWe will design some algorithms using 
logical time to play the role of real time.
Figure 3 shows the computation in figure 2 with the horizontal axis
representing real time.
The position of a step with logical time \(t\) is at a distance of
\(t\) units from the origin.Fig. 3: Computation with Logical Time as Real TimeThink of logical time as continuous, just as real time is continuous.
In this example, points at logical times \(6.5\) and \(6.6\), at agent
\(A\), refer to the same edge.
It helps intuition, however, to think
of the point at logical time \(6.6\) as a location to the right of the
point at logical time \(6.5\) on the same edge.
Imagine that logical time \(6.6\) is 0.1 time units to the right of
logical time \(6.5\).The cut at logical time \(6.5\) is represented by the vertical line
at time \(6.5\).
The left of the line is the past at logical time \(6.5\), and the
right side of the line is the future at that time.Physical and Logical ClocksOperating systems maintain clocks. Some have atomic clocks or other
  high-fidelity clocks that use Precision or Network Time Protocols
(PTP, NTP).
With high-fidelity clocks, a message sent when the sender's clock is
at \(t\) will almost always be received when the receiver's
clock is later than \(t\).
So physical clocks almost always obey the logical clock requirement.We cannot rule out the possibility that a message sent when the
sender's clock is at \(t\) is received when the receiver's clock is
earlier than \(t\) or equal to \(t\).
We can use physical clocks, but correct them so that messages are
received only after they are sent, where the times are determined by
corrected physical clocks.Such clocks have the following properties that we use in
designing algorithms:Clocks tick forward forever: For all \(t\), there is a point in
  an infinite computation at which clocks of all agents exceed \(t\).Sequences of steps in ascending order of time are computations.NextThe next few pages describe applications of global snapshots.
We begin withTermination Detection.K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology



Figure Link: https://kmchandy.github.io/ChannelSnapshots/Timelines/Timelines.002.jpeg

Caption: Fig. 1: Cut at Logical Time 6.5. Past is Set of Steps
    with Logical Time at most 6.5


The point at which the purple line cuts the timeline for agent \(A\)
can be thought of as the point in \(A\)'s computation at which the
logical time is exactly 6.5.
This cut is on the edge from the step at \(A\) with
  logical time at most 6.5 to the step with logical time greater
  than 6.5.
  In this example the cut is on the edge from step 3 to step 5.The message edge from step 3 to step 7 represents a message sent
  along the channel from \(A\) to \(B\) in the past that is received
  in the future.
In this example, the state of the channel \((A, B)\) is the sequence
consisting of a 
single message which is the label of this edge.Global Snapshot Algorithm to Record the State at a Logical TimeAn algorithm to record the state at logical time \(t\) follows
directly from the definition of the state at logical time \(t\).Each agent takes its local snapshot -- i.e. records its state -- after
a step with logical time at most \(t\) and before a
  step with logical time greater than \(t\).An agent records the state of an input channel as the sequence of
messages with timestamps atmost \(t\) that the agent
  receives when its logical clock exceeds \(t\).The purple line in figure 2 represents the global snapshot at logical
time 6.5.Using Imperfect Clocks in Distributed AlgorithmsIntuitionWe will design some algorithms using 
logical time to play the role of real time.
Figure 3 shows the computation in figure 2 with the horizontal axis
representing real time.
The position of a step with logical time \(t\) is at a distance of
\(t\) units from the origin.Fig. 3: Computation with Logical Time as Real TimeThink of logical time as continuous, just as real time is continuous.
In this example, points at logical times \(6.5\) and \(6.6\), at agent
\(A\), refer to the same edge.
It helps intuition, however, to think
of the point at logical time \(6.6\) as a location to the right of the
point at logical time \(6.5\) on the same edge.
Imagine that logical time \(6.6\) is 0.1 time units to the right of
logical time \(6.5\).The cut at logical time \(6.5\) is represented by the vertical line
at time \(6.5\).
The left of the line is the past at logical time \(6.5\), and the
right side of the line is the future at that time.Physical and Logical ClocksOperating systems maintain clocks. Some have atomic clocks or other
  high-fidelity clocks that use Precision or Network Time Protocols
(PTP, NTP).
With high-fidelity clocks, a message sent when the sender's clock is
at \(t\) will almost always be received when the receiver's
clock is later than \(t\).
So physical clocks almost always obey the logical clock requirement.We cannot rule out the possibility that a message sent when the
sender's clock is at \(t\) is received when the receiver's clock is
earlier than \(t\) or equal to \(t\).
We can use physical clocks, but correct them so that messages are
received only after they are sent, where the times are determined by
corrected physical clocks.Such clocks have the following properties that we use in
designing algorithms:Clocks tick forward forever: For all \(t\), there is a point in
  an infinite computation at which clocks of all agents exceed \(t\).Sequences of steps in ascending order of time are computations.NextThe next few pages describe applications of global snapshots.
We begin withTermination Detection.K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology



Figure Link: https://kmchandy.github.io/ChannelSnapshots/Timelines/Timelines.003.jpeg

Caption: Fig. 3: Computation with Logical Time as Real Time


The message edge from step 3 to step 7 represents a message sent
  along the channel from \(A\) to \(B\) in the past that is received
  in the future.
In this example, the state of the channel \((A, B)\) is the sequence
consisting of a 
single message which is the label of this edge.Global Snapshot Algorithm to Record the State at a Logical TimeAn algorithm to record the state at logical time \(t\) follows
directly from the definition of the state at logical time \(t\).Each agent takes its local snapshot -- i.e. records its state -- after
a step with logical time at most \(t\) and before a
  step with logical time greater than \(t\).An agent records the state of an input channel as the sequence of
messages with timestamps atmost \(t\) that the agent
  receives when its logical clock exceeds \(t\).The purple line in figure 2 represents the global snapshot at logical
time 6.5.Using Imperfect Clocks in Distributed AlgorithmsIntuitionWe will design some algorithms using 
logical time to play the role of real time.
Figure 3 shows the computation in figure 2 with the horizontal axis
representing real time.
The position of a step with logical time \(t\) is at a distance of
\(t\) units from the origin.Fig. 3: Computation with Logical Time as Real TimeThink of logical time as continuous, just as real time is continuous.
In this example, points at logical times \(6.5\) and \(6.6\), at agent
\(A\), refer to the same edge.
It helps intuition, however, to think
of the point at logical time \(6.6\) as a location to the right of the
point at logical time \(6.5\) on the same edge.
Imagine that logical time \(6.6\) is 0.1 time units to the right of
logical time \(6.5\).The cut at logical time \(6.5\) is represented by the vertical line
at time \(6.5\).
The left of the line is the past at logical time \(6.5\), and the
right side of the line is the future at that time.Physical and Logical ClocksOperating systems maintain clocks. Some have atomic clocks or other
  high-fidelity clocks that use Precision or Network Time Protocols
(PTP, NTP).
With high-fidelity clocks, a message sent when the sender's clock is
at \(t\) will almost always be received when the receiver's
clock is later than \(t\).
So physical clocks almost always obey the logical clock requirement.We cannot rule out the possibility that a message sent when the
sender's clock is at \(t\) is received when the receiver's clock is
earlier than \(t\) or equal to \(t\).
We can use physical clocks, but correct them so that messages are
received only after they are sent, where the times are determined by
corrected physical clocks.Such clocks have the following properties that we use in
designing algorithms:Clocks tick forward forever: For all \(t\), there is a point in
  an infinite computation at which clocks of all agents exceed \(t\).Sequences of steps in ascending order of time are computations.NextThe next few pages describe applications of global snapshots.
We begin withTermination Detection.K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology



Figure Link: https://kmchandy.github.io/ChannelSnapshots/Timelines/Timelines.003.jpeg

Caption: Fig. 3: Computation with Logical Time as Real Time


Intuition



Figure Link: https://kmchandy.github.io/ChannelSnapshots/Timelines/Timelines.003.jpeg

Caption: Fig. 3: Computation with Logical Time as Real Time


Think of logical time as continuous, just as real time is continuous.
In this example, points at logical times \(6.5\) and \(6.6\), at agent
\(A\), refer to the same edge.
It helps intuition, however, to think
of the point at logical time \(6.6\) as a location to the right of the
point at logical time \(6.5\) on the same edge.
Imagine that logical time \(6.6\) is 0.1 time units to the right of
logical time \(6.5\).The cut at logical time \(6.5\) is represented by the vertical line
at time \(6.5\).
The left of the line is the past at logical time \(6.5\), and the
right side of the line is the future at that time.Physical and Logical ClocksOperating systems maintain clocks. Some have atomic clocks or other
  high-fidelity clocks that use Precision or Network Time Protocols
(PTP, NTP).
With high-fidelity clocks, a message sent when the sender's clock is
at \(t\) will almost always be received when the receiver's
clock is later than \(t\).
So physical clocks almost always obey the logical clock requirement.We cannot rule out the possibility that a message sent when the
sender's clock is at \(t\) is received when the receiver's clock is
earlier than \(t\) or equal to \(t\).
We can use physical clocks, but correct them so that messages are
received only after they are sent, where the times are determined by
corrected physical clocks.Such clocks have the following properties that we use in
designing algorithms:Clocks tick forward forever: For all \(t\), there is a point in
  an infinite computation at which clocks of all agents exceed \(t\).Sequences of steps in ascending order of time are computations.NextThe next few pages describe applications of global snapshots.
We begin withTermination Detection.K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

The cut at logical time \(6.5\) is represented by the vertical line
at time \(6.5\).
The left of the line is the past at logical time \(6.5\), and the
right side of the line is the future at that time.Physical and Logical ClocksOperating systems maintain clocks. Some have atomic clocks or other
  high-fidelity clocks that use Precision or Network Time Protocols
(PTP, NTP).
With high-fidelity clocks, a message sent when the sender's clock is
at \(t\) will almost always be received when the receiver's
clock is later than \(t\).
So physical clocks almost always obey the logical clock requirement.We cannot rule out the possibility that a message sent when the
sender's clock is at \(t\) is received when the receiver's clock is
earlier than \(t\) or equal to \(t\).
We can use physical clocks, but correct them so that messages are
received only after they are sent, where the times are determined by
corrected physical clocks.Such clocks have the following properties that we use in
designing algorithms:Clocks tick forward forever: For all \(t\), there is a point in
  an infinite computation at which clocks of all agents exceed \(t\).Sequences of steps in ascending order of time are computations.NextThe next few pages describe applications of global snapshots.
We begin withTermination Detection.K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

Physical and Logical Clocks

Operating systems maintain clocks. Some have atomic clocks or other
  high-fidelity clocks that use Precision or Network Time Protocols
(PTP, NTP).
With high-fidelity clocks, a message sent when the sender's clock is
at \(t\) will almost always be received when the receiver's
clock is later than \(t\).
So physical clocks almost always obey the logical clock requirement.We cannot rule out the possibility that a message sent when the
sender's clock is at \(t\) is received when the receiver's clock is
earlier than \(t\) or equal to \(t\).
We can use physical clocks, but correct them so that messages are
received only after they are sent, where the times are determined by
corrected physical clocks.Such clocks have the following properties that we use in
designing algorithms:Clocks tick forward forever: For all \(t\), there is a point in
  an infinite computation at which clocks of all agents exceed \(t\).Sequences of steps in ascending order of time are computations.NextThe next few pages describe applications of global snapshots.
We begin withTermination Detection.K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

We cannot rule out the possibility that a message sent when the
sender's clock is at \(t\) is received when the receiver's clock is
earlier than \(t\) or equal to \(t\).
We can use physical clocks, but correct them so that messages are
received only after they are sent, where the times are determined by
corrected physical clocks.Such clocks have the following properties that we use in
designing algorithms:Clocks tick forward forever: For all \(t\), there is a point in
  an infinite computation at which clocks of all agents exceed \(t\).Sequences of steps in ascending order of time are computations.NextThe next few pages describe applications of global snapshots.
We begin withTermination Detection.K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

Such clocks have the following properties that we use in
designing algorithms:Clocks tick forward forever: For all \(t\), there is a point in
  an infinite computation at which clocks of all agents exceed \(t\).Sequences of steps in ascending order of time are computations.NextThe next few pages describe applications of global snapshots.
We begin withTermination Detection.K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

NextThe next few pages describe applications of global snapshots.
We begin withTermination Detection.K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology
--- END Content from https://kmchandy.github.io/ChannelSnapshots/LogicalClocksSnapshots.html ---


--- START Content from https://kmchandy.github.io/ChannelSnapshots/TerminationDetection.html ---
TITLE: TerminationDetection

A distributed computation has terminated when all agents are idle
  and all channels are empty.
  A termination detection algorithm is executed by the operating
  system to determine whether a client computation has terminated.

A computation terminates in states in which all
  channels are empty and all agents are waiting to receive messages.
  An agent is said to beactivewhile it is processing a
  message andidlewhile it is waiting to process a message.
  A terminated state is one in which all agents are idle and all
  channels are empty.Let \(C_{s}\) and \(C_{r}\) be the numbers of messages 
  sent and received (respectively) on channel \(C\).
  A computation is in a terminated state exactly
  when:All agents are idle andfor all channels \(C\):  \(\; C_{s} = C_{r}\).The problem is to design an algorithm that detects whether the
  computation is in a terminated state.How Should You Solve the Problem?StrategyA strategy to solve detection problems is to start with thegeneral detection algorithmand
  then explore optimizations by usingproperties of cuts.Let's explore optimizations.The algorithm detects whether a channel is empty and it can do so
  given the numbers of messages sent and received on the channel
  without information about message contents.
  Whatproperties of cutscome to mind to help us design an algorithm based on message counts?We use"Cut based on Counts of Messages Sent and Received,"which is given again below.There exists a cut(past, future)exactly when the
following two conditions hold:For all \(C\):  \(\; C_{s} \geq C_{r}\),where
  \(C_{s}\) and \(C_{r}\) are the numbers of messages sent and
  received, respectively, on channel \(C\), inpast.If a step \(x\) of an agent is inpastthen steps at
  that agent before \(x\) are also inpast.The property suggests the following algorithm.A Termination Detection AlgorithmAgent ActionsWhen an agent changes state from active to idle the agent sends a
  message to the observer. 
  This message contains \(C_{s}\) for each output channel and
  \(C_{r}\) for each input channel of the agent.Observer ActionsThe observer keeps only the latest message that it receives from
  each agent.
  
  For each channel \(C\), let \(C_{s}^{*}\) and \(C_{r}^{*}\) be the
  latest value of \(C_{s}\) and \(C_{r}\), respectively, that the
observer has received.Initial ConditionAll agents are idle.
\(C_{s}^{*}\) and \(C_{r}^{*}\) are the numbers of messages sent and
received (respectively) on channel \(C\) for all \(C\).Termination DetectionThe observer detects computation has terminated if
  for all channels \(C\): \(C_{s}^{*} = C_{r}^{*}\).Proof of CorrectnessWe first prove that if the observer detects that the computation has
terminated then the computation has indeed terminated.Since the observer has detected termination, for each channel \(C\),
either \(C_{s}^{*} = C_{r}^{*}\) initially, or the observer received
messages containing \(C_{s}^{*}\) and \(C_{r}^{*}\) such that
\(C_{s}^{*} = C_{r}^{*}\).Let(past, future)be a partition of the steps of the
  computation wherepastconsists of steps at an agent
  before the agent sent the messages to the observer containing
\(C_{s}^{*}\) and 
\(C_{r}^{*}\) for channels \(C\) incident on the agent. 
(If the agent sends no messages to the observer 
then the agent has no steps inpast.)From the property,"Cut based on Counts of Messages Sent and
Received,"(past, future)is a cut.From the definition of terminated state it follows that the state at
this cut is a terminated state.
Termination is a stable property -- once computation has terminated it
remains terminated.
So, if the state at a cut of the computation
is a terminated state then all succeeding states are terminated states.Next we prove that if the computation terminates then the observer
detects termination.
The last message sent by each agent has counts \(C_{s}^{*}\) and
\(C_{r}^{*}\) of the numbers of messages sent and received (respectively)
for each of its output channels \(C\).
Because these are the last messages sent when the algorithm terminates
it follows that \(C_{s}^{*} = C_{r}^{*}\) for all \(C\).NextNextdatabase deadlock
detectionK. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

Let \(C_{s}\) and \(C_{r}\) be the numbers of messages 
  sent and received (respectively) on channel \(C\).
  A computation is in a terminated state exactly
  when:All agents are idle andfor all channels \(C\):  \(\; C_{s} = C_{r}\).The problem is to design an algorithm that detects whether the
  computation is in a terminated state.How Should You Solve the Problem?StrategyA strategy to solve detection problems is to start with thegeneral detection algorithmand
  then explore optimizations by usingproperties of cuts.Let's explore optimizations.The algorithm detects whether a channel is empty and it can do so
  given the numbers of messages sent and received on the channel
  without information about message contents.
  Whatproperties of cutscome to mind to help us design an algorithm based on message counts?We use"Cut based on Counts of Messages Sent and Received,"which is given again below.There exists a cut(past, future)exactly when the
following two conditions hold:For all \(C\):  \(\; C_{s} \geq C_{r}\),where
  \(C_{s}\) and \(C_{r}\) are the numbers of messages sent and
  received, respectively, on channel \(C\), inpast.If a step \(x\) of an agent is inpastthen steps at
  that agent before \(x\) are also inpast.The property suggests the following algorithm.A Termination Detection AlgorithmAgent ActionsWhen an agent changes state from active to idle the agent sends a
  message to the observer. 
  This message contains \(C_{s}\) for each output channel and
  \(C_{r}\) for each input channel of the agent.Observer ActionsThe observer keeps only the latest message that it receives from
  each agent.
  
  For each channel \(C\), let \(C_{s}^{*}\) and \(C_{r}^{*}\) be the
  latest value of \(C_{s}\) and \(C_{r}\), respectively, that the
observer has received.Initial ConditionAll agents are idle.
\(C_{s}^{*}\) and \(C_{r}^{*}\) are the numbers of messages sent and
received (respectively) on channel \(C\) for all \(C\).Termination DetectionThe observer detects computation has terminated if
  for all channels \(C\): \(C_{s}^{*} = C_{r}^{*}\).Proof of CorrectnessWe first prove that if the observer detects that the computation has
terminated then the computation has indeed terminated.Since the observer has detected termination, for each channel \(C\),
either \(C_{s}^{*} = C_{r}^{*}\) initially, or the observer received
messages containing \(C_{s}^{*}\) and \(C_{r}^{*}\) such that
\(C_{s}^{*} = C_{r}^{*}\).Let(past, future)be a partition of the steps of the
  computation wherepastconsists of steps at an agent
  before the agent sent the messages to the observer containing
\(C_{s}^{*}\) and 
\(C_{r}^{*}\) for channels \(C\) incident on the agent. 
(If the agent sends no messages to the observer 
then the agent has no steps inpast.)From the property,"Cut based on Counts of Messages Sent and
Received,"(past, future)is a cut.From the definition of terminated state it follows that the state at
this cut is a terminated state.
Termination is a stable property -- once computation has terminated it
remains terminated.
So, if the state at a cut of the computation
is a terminated state then all succeeding states are terminated states.Next we prove that if the computation terminates then the observer
detects termination.
The last message sent by each agent has counts \(C_{s}^{*}\) and
\(C_{r}^{*}\) of the numbers of messages sent and received (respectively)
for each of its output channels \(C\).
Because these are the last messages sent when the algorithm terminates
it follows that \(C_{s}^{*} = C_{r}^{*}\) for all \(C\).NextNextdatabase deadlock
detectionK. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

Strategy

The algorithm detects whether a channel is empty and it can do so
  given the numbers of messages sent and received on the channel
  without information about message contents.
  Whatproperties of cutscome to mind to help us design an algorithm based on message counts?We use"Cut based on Counts of Messages Sent and Received,"which is given again below.There exists a cut(past, future)exactly when the
following two conditions hold:For all \(C\):  \(\; C_{s} \geq C_{r}\),where
  \(C_{s}\) and \(C_{r}\) are the numbers of messages sent and
  received, respectively, on channel \(C\), inpast.If a step \(x\) of an agent is inpastthen steps at
  that agent before \(x\) are also inpast.The property suggests the following algorithm.A Termination Detection AlgorithmAgent ActionsWhen an agent changes state from active to idle the agent sends a
  message to the observer. 
  This message contains \(C_{s}\) for each output channel and
  \(C_{r}\) for each input channel of the agent.Observer ActionsThe observer keeps only the latest message that it receives from
  each agent.
  
  For each channel \(C\), let \(C_{s}^{*}\) and \(C_{r}^{*}\) be the
  latest value of \(C_{s}\) and \(C_{r}\), respectively, that the
observer has received.Initial ConditionAll agents are idle.
\(C_{s}^{*}\) and \(C_{r}^{*}\) are the numbers of messages sent and
received (respectively) on channel \(C\) for all \(C\).Termination DetectionThe observer detects computation has terminated if
  for all channels \(C\): \(C_{s}^{*} = C_{r}^{*}\).Proof of CorrectnessWe first prove that if the observer detects that the computation has
terminated then the computation has indeed terminated.Since the observer has detected termination, for each channel \(C\),
either \(C_{s}^{*} = C_{r}^{*}\) initially, or the observer received
messages containing \(C_{s}^{*}\) and \(C_{r}^{*}\) such that
\(C_{s}^{*} = C_{r}^{*}\).Let(past, future)be a partition of the steps of the
  computation wherepastconsists of steps at an agent
  before the agent sent the messages to the observer containing
\(C_{s}^{*}\) and 
\(C_{r}^{*}\) for channels \(C\) incident on the agent. 
(If the agent sends no messages to the observer 
then the agent has no steps inpast.)From the property,"Cut based on Counts of Messages Sent and
Received,"(past, future)is a cut.From the definition of terminated state it follows that the state at
this cut is a terminated state.
Termination is a stable property -- once computation has terminated it
remains terminated.
So, if the state at a cut of the computation
is a terminated state then all succeeding states are terminated states.Next we prove that if the computation terminates then the observer
detects termination.
The last message sent by each agent has counts \(C_{s}^{*}\) and
\(C_{r}^{*}\) of the numbers of messages sent and received (respectively)
for each of its output channels \(C\).
Because these are the last messages sent when the algorithm terminates
it follows that \(C_{s}^{*} = C_{r}^{*}\) for all \(C\).NextNextdatabase deadlock
detectionK. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

We use"Cut based on Counts of Messages Sent and Received,"which is given again below.There exists a cut(past, future)exactly when the
following two conditions hold:For all \(C\):  \(\; C_{s} \geq C_{r}\),where
  \(C_{s}\) and \(C_{r}\) are the numbers of messages sent and
  received, respectively, on channel \(C\), inpast.If a step \(x\) of an agent is inpastthen steps at
  that agent before \(x\) are also inpast.The property suggests the following algorithm.A Termination Detection AlgorithmAgent ActionsWhen an agent changes state from active to idle the agent sends a
  message to the observer. 
  This message contains \(C_{s}\) for each output channel and
  \(C_{r}\) for each input channel of the agent.Observer ActionsThe observer keeps only the latest message that it receives from
  each agent.
  
  For each channel \(C\), let \(C_{s}^{*}\) and \(C_{r}^{*}\) be the
  latest value of \(C_{s}\) and \(C_{r}\), respectively, that the
observer has received.Initial ConditionAll agents are idle.
\(C_{s}^{*}\) and \(C_{r}^{*}\) are the numbers of messages sent and
received (respectively) on channel \(C\) for all \(C\).Termination DetectionThe observer detects computation has terminated if
  for all channels \(C\): \(C_{s}^{*} = C_{r}^{*}\).Proof of CorrectnessWe first prove that if the observer detects that the computation has
terminated then the computation has indeed terminated.Since the observer has detected termination, for each channel \(C\),
either \(C_{s}^{*} = C_{r}^{*}\) initially, or the observer received
messages containing \(C_{s}^{*}\) and \(C_{r}^{*}\) such that
\(C_{s}^{*} = C_{r}^{*}\).Let(past, future)be a partition of the steps of the
  computation wherepastconsists of steps at an agent
  before the agent sent the messages to the observer containing
\(C_{s}^{*}\) and 
\(C_{r}^{*}\) for channels \(C\) incident on the agent. 
(If the agent sends no messages to the observer 
then the agent has no steps inpast.)From the property,"Cut based on Counts of Messages Sent and
Received,"(past, future)is a cut.From the definition of terminated state it follows that the state at
this cut is a terminated state.
Termination is a stable property -- once computation has terminated it
remains terminated.
So, if the state at a cut of the computation
is a terminated state then all succeeding states are terminated states.Next we prove that if the computation terminates then the observer
detects termination.
The last message sent by each agent has counts \(C_{s}^{*}\) and
\(C_{r}^{*}\) of the numbers of messages sent and received (respectively)
for each of its output channels \(C\).
Because these are the last messages sent when the algorithm terminates
it follows that \(C_{s}^{*} = C_{r}^{*}\) for all \(C\).NextNextdatabase deadlock
detectionK. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

There exists a cut(past, future)exactly when the
following two conditions hold:For all \(C\):  \(\; C_{s} \geq C_{r}\),where
  \(C_{s}\) and \(C_{r}\) are the numbers of messages sent and
  received, respectively, on channel \(C\), inpast.If a step \(x\) of an agent is inpastthen steps at
  that agent before \(x\) are also inpast.The property suggests the following algorithm.A Termination Detection AlgorithmAgent ActionsWhen an agent changes state from active to idle the agent sends a
  message to the observer. 
  This message contains \(C_{s}\) for each output channel and
  \(C_{r}\) for each input channel of the agent.Observer ActionsThe observer keeps only the latest message that it receives from
  each agent.
  
  For each channel \(C\), let \(C_{s}^{*}\) and \(C_{r}^{*}\) be the
  latest value of \(C_{s}\) and \(C_{r}\), respectively, that the
observer has received.Initial ConditionAll agents are idle.
\(C_{s}^{*}\) and \(C_{r}^{*}\) are the numbers of messages sent and
received (respectively) on channel \(C\) for all \(C\).Termination DetectionThe observer detects computation has terminated if
  for all channels \(C\): \(C_{s}^{*} = C_{r}^{*}\).Proof of CorrectnessWe first prove that if the observer detects that the computation has
terminated then the computation has indeed terminated.Since the observer has detected termination, for each channel \(C\),
either \(C_{s}^{*} = C_{r}^{*}\) initially, or the observer received
messages containing \(C_{s}^{*}\) and \(C_{r}^{*}\) such that
\(C_{s}^{*} = C_{r}^{*}\).Let(past, future)be a partition of the steps of the
  computation wherepastconsists of steps at an agent
  before the agent sent the messages to the observer containing
\(C_{s}^{*}\) and 
\(C_{r}^{*}\) for channels \(C\) incident on the agent. 
(If the agent sends no messages to the observer 
then the agent has no steps inpast.)From the property,"Cut based on Counts of Messages Sent and
Received,"(past, future)is a cut.From the definition of terminated state it follows that the state at
this cut is a terminated state.
Termination is a stable property -- once computation has terminated it
remains terminated.
So, if the state at a cut of the computation
is a terminated state then all succeeding states are terminated states.Next we prove that if the computation terminates then the observer
detects termination.
The last message sent by each agent has counts \(C_{s}^{*}\) and
\(C_{r}^{*}\) of the numbers of messages sent and received (respectively)
for each of its output channels \(C\).
Because these are the last messages sent when the algorithm terminates
it follows that \(C_{s}^{*} = C_{r}^{*}\) for all \(C\).NextNextdatabase deadlock
detectionK. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

where
  \(C_{s}\) and \(C_{r}\) are the numbers of messages sent and
  received, respectively, on channel \(C\), inpast.

Agent Actions

Observer Actions

Initial Condition

Termination Detection

The observer detects computation has terminated if
  for all channels \(C\): \(C_{s}^{*} = C_{r}^{*}\).

We first prove that if the observer detects that the computation has
terminated then the computation has indeed terminated.Since the observer has detected termination, for each channel \(C\),
either \(C_{s}^{*} = C_{r}^{*}\) initially, or the observer received
messages containing \(C_{s}^{*}\) and \(C_{r}^{*}\) such that
\(C_{s}^{*} = C_{r}^{*}\).Let(past, future)be a partition of the steps of the
  computation wherepastconsists of steps at an agent
  before the agent sent the messages to the observer containing
\(C_{s}^{*}\) and 
\(C_{r}^{*}\) for channels \(C\) incident on the agent. 
(If the agent sends no messages to the observer 
then the agent has no steps inpast.)From the property,"Cut based on Counts of Messages Sent and
Received,"(past, future)is a cut.From the definition of terminated state it follows that the state at
this cut is a terminated state.
Termination is a stable property -- once computation has terminated it
remains terminated.
So, if the state at a cut of the computation
is a terminated state then all succeeding states are terminated states.Next we prove that if the computation terminates then the observer
detects termination.
The last message sent by each agent has counts \(C_{s}^{*}\) and
\(C_{r}^{*}\) of the numbers of messages sent and received (respectively)
for each of its output channels \(C\).
Because these are the last messages sent when the algorithm terminates
it follows that \(C_{s}^{*} = C_{r}^{*}\) for all \(C\).NextNextdatabase deadlock
detectionK. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

Since the observer has detected termination, for each channel \(C\),
either \(C_{s}^{*} = C_{r}^{*}\) initially, or the observer received
messages containing \(C_{s}^{*}\) and \(C_{r}^{*}\) such that
\(C_{s}^{*} = C_{r}^{*}\).Let(past, future)be a partition of the steps of the
  computation wherepastconsists of steps at an agent
  before the agent sent the messages to the observer containing
\(C_{s}^{*}\) and 
\(C_{r}^{*}\) for channels \(C\) incident on the agent. 
(If the agent sends no messages to the observer 
then the agent has no steps inpast.)From the property,"Cut based on Counts of Messages Sent and
Received,"(past, future)is a cut.From the definition of terminated state it follows that the state at
this cut is a terminated state.
Termination is a stable property -- once computation has terminated it
remains terminated.
So, if the state at a cut of the computation
is a terminated state then all succeeding states are terminated states.Next we prove that if the computation terminates then the observer
detects termination.
The last message sent by each agent has counts \(C_{s}^{*}\) and
\(C_{r}^{*}\) of the numbers of messages sent and received (respectively)
for each of its output channels \(C\).
Because these are the last messages sent when the algorithm terminates
it follows that \(C_{s}^{*} = C_{r}^{*}\) for all \(C\).NextNextdatabase deadlock
detectionK. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

Let(past, future)be a partition of the steps of the
  computation wherepastconsists of steps at an agent
  before the agent sent the messages to the observer containing
\(C_{s}^{*}\) and 
\(C_{r}^{*}\) for channels \(C\) incident on the agent. 
(If the agent sends no messages to the observer 
then the agent has no steps inpast.)From the property,"Cut based on Counts of Messages Sent and
Received,"(past, future)is a cut.From the definition of terminated state it follows that the state at
this cut is a terminated state.
Termination is a stable property -- once computation has terminated it
remains terminated.
So, if the state at a cut of the computation
is a terminated state then all succeeding states are terminated states.Next we prove that if the computation terminates then the observer
detects termination.
The last message sent by each agent has counts \(C_{s}^{*}\) and
\(C_{r}^{*}\) of the numbers of messages sent and received (respectively)
for each of its output channels \(C\).
Because these are the last messages sent when the algorithm terminates
it follows that \(C_{s}^{*} = C_{r}^{*}\) for all \(C\).NextNextdatabase deadlock
detectionK. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

From the property,"Cut based on Counts of Messages Sent and
Received,"(past, future)is a cut.From the definition of terminated state it follows that the state at
this cut is a terminated state.
Termination is a stable property -- once computation has terminated it
remains terminated.
So, if the state at a cut of the computation
is a terminated state then all succeeding states are terminated states.Next we prove that if the computation terminates then the observer
detects termination.
The last message sent by each agent has counts \(C_{s}^{*}\) and
\(C_{r}^{*}\) of the numbers of messages sent and received (respectively)
for each of its output channels \(C\).
Because these are the last messages sent when the algorithm terminates
it follows that \(C_{s}^{*} = C_{r}^{*}\) for all \(C\).NextNextdatabase deadlock
detectionK. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

From the definition of terminated state it follows that the state at
this cut is a terminated state.
Termination is a stable property -- once computation has terminated it
remains terminated.
So, if the state at a cut of the computation
is a terminated state then all succeeding states are terminated states.Next we prove that if the computation terminates then the observer
detects termination.
The last message sent by each agent has counts \(C_{s}^{*}\) and
\(C_{r}^{*}\) of the numbers of messages sent and received (respectively)
for each of its output channels \(C\).
Because these are the last messages sent when the algorithm terminates
it follows that \(C_{s}^{*} = C_{r}^{*}\) for all \(C\).NextNextdatabase deadlock
detectionK. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

Next we prove that if the computation terminates then the observer
detects termination.
The last message sent by each agent has counts \(C_{s}^{*}\) and
\(C_{r}^{*}\) of the numbers of messages sent and received (respectively)
for each of its output channels \(C\).
Because these are the last messages sent when the algorithm terminates
it follows that \(C_{s}^{*} = C_{r}^{*}\) for all \(C\).NextNextdatabase deadlock
detectionK. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology
--- END Content from https://kmchandy.github.io/ChannelSnapshots/TerminationDetection.html ---


--- START Content from https://kmchandy.github.io/ChannelSnapshots/DatabaseDeadlockDetection.html ---
TITLE: No Title

A database deadlock occurs when each agent in a cycle remains
  waiting forever for a resource held and required by the next agent
  in the cycle. 
  A database deadlock detection algorithm is executed by the operating
  system to determine whether there exists a cycle of deadlocked agents.



Figure Link: https://kmchandy.github.io/ChannelSnapshots/DatabaseDeadlockDetection/DatabaseDeadlockDetection.002.jpeg

Caption: Fig.2 - An Example of a Deadlock


The problem described here is a simplification of the database
  deadlock problem.
  The simplification focuses on the essentials of the problem.Agents in a system share a set of indivisible resources.
  An example of such a resource is exclusive access to a file.A deadlock arises when there is a cycle of agents,
  \([x_{0}, x_{1}, \ldots, x_{n-1}, x_{0}]\) where for all \(i\):agent \(x_{i}\) holds a resource \(r_{i}\), andagent \(x_{i}\) requires resources \(r_{i}\) and \(r_{(i+1)}\) to
    continue executing.(Operations on indices of agents are taken mod \(n\) and \(n >
    1\).)ExampleIn the example, a resource is identified by its color.
  
  A system has one red, one blue, and one green resource.
  
  Agents \(x\), \(y\) and \(z\) are deadlocked in the following
  state.Agent \(x\) requires the red and blue resources to continue
    executing; \(x\) is holding the red resource and is waiting to
    acquire the blue resource.Agent \(y\) requires the blue and green resources to continue
    executing; \(y\) ; is holding the blue resource and is waiting to
    acquire the green resource.Agent \(z\) requires the green and red resources to continue
  executing; \(z\) holds the green resource and is waiting to acquire
    the red resource.Fig.2 - An Example of a DeadlockHow Should You Solve the Problem?StrategyA strategy to solve detection problems is to start with thegeneral detection algorithmand
  then explore optimizations by usingproperties of cuts.In the general detection algorithm, an observer gets global snapshots and
  determines if there is a cycle of waiting agents in the snapshot.
  Algorithms for determining cycles are foundhere.Next let's exploreoptimizations;
  see "Detection without Observers: Distributed Algorithms on Local
  Snapshots."
  The optimized algorithm has two phases: (1) First a global snapshot
  algorithm is executed. (2) After the global snapshot algorithm
  terminates a distributed detection algorithm is executed on
  the local snapshots recorded by the global snapshots.The two phases can be merged for some problems, including this one.A Distributed Algorithm to Detect a Cycle of
  Waiting AgentsMultiple detection algorithms and global snapshot algorithms may
  execute concurrently.
  
  These algorithms are disambiguated by tagging each algorithm with
  the initiator and a sequence id.
  Next we describe a single detection algorithm which is executed after 
  a single global snapshot algorithm terminates.Local ConstantsIn the detection algorithm, 
each agentvhas the following local constants:v.waitsis the set
  of resources thatvhas to acquire from other agents to start
  execution.v.holdsis the set of resources thatvholds and must continue to hold
  to start execution.ExampleIn the example of the figure:x.waits = {blue}, x.holds = {red}y.waits = {green}, y.holds = {blue}z.waits = {red}, y.holds = {green}v.waitsandv.holdsare constant for allvin the
detection algorithm  because these values are
specified in the global snapshot.MessagesThe algorithm to detect waiting cycles is similar to the global
  snapshot algorithm.
Instead of themarkermessage used in global snapshots, a
message in the detection identifies the set
of resources for which the sender of the message is waiting.Each messagemsent by an agentvhas a
fieldm.waitswhere:m.waits=v.waits.Initiating the AlgorithmA waiting agentuinitiates the algorithm by sending a messagemon each of its output channels wherem.waits=u.waits.Action by an Agent other than the
InitiatorWhen an agentvreceives a messagem:Ifvhas already sent messages thenvtakes no
  action.Ifvhas not sent messages, and if there is a resource
  common tom.waitsandv.holdsthenvsends a  messagem'on each of its
  output channels wherem'.waits=v.waits.Cycle DetectionIf the initiatorureceives a messagemwhere there is a resource common tom.waitsandu.holdsthenudetects a cycle of waiting
agents.Proof of CorrectnessThe proof outline is as follows.
An agentvsends messages only if there is a path of waiting
processes fromutov.
So,udetects a cycle of waiting processes only if there
exists a cycle of waiting processes.If there exists a cycle of waiting processes fromutouthen 
messages are sent along one such cycle, and soudetects a cycle.The algorithm terminates because it sends at most one message on each
channel.ExampleThis example shows steps in the case of the cycle of waiting processes shown in
figure 2.
The algorithm is initiated by agentxby broadcasting a
messagemwherem.waits = x.waits = {blue}.Whenyreceives a messagemwherem.waits = {blue}, there is a resource in bothm.waitsandy.holds, and soybroadcasts messagem'wherem'.waits = y.waits = {green}.Whenzreceives a messagemwherem.waits = {blue},ztakes no action because
there is no resource in bothm.waitsandz.holds.Whenzreceives a messagemwherem.waits = {green},zbroadcasts messagem'wherem'.waits = z.waits = {red}.When the initiatorxreceives a messagemwherem.waits = {red},xdetects a deadlock
because there is a resource common tom.waitsandx.holds.Combining Cycle Detection and Snapshot
AlgorithmsWe can use a a marker in the snapshot algorithm as a message in cycle
detection.
Modify a markermto have the fieldm.waitsused in cycle detection.
Then the snapshot algorithm is the same as the cycle detection
algorithm except that the snapshot algorithm records states of
channels which are not used in cycle detection.Write the algorithm for the combined algorithm as an exercise.K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology



Figure Link: https://kmchandy.github.io/ChannelSnapshots/DatabaseDeadlockDetection/DatabaseDeadlockDetection.002.jpeg

Caption: Fig.2 - An Example of a Deadlock


Agents in a system share a set of indivisible resources.
  An example of such a resource is exclusive access to a file.A deadlock arises when there is a cycle of agents,
  \([x_{0}, x_{1}, \ldots, x_{n-1}, x_{0}]\) where for all \(i\):agent \(x_{i}\) holds a resource \(r_{i}\), andagent \(x_{i}\) requires resources \(r_{i}\) and \(r_{(i+1)}\) to
    continue executing.(Operations on indices of agents are taken mod \(n\) and \(n >
    1\).)ExampleIn the example, a resource is identified by its color.
  
  A system has one red, one blue, and one green resource.
  
  Agents \(x\), \(y\) and \(z\) are deadlocked in the following
  state.Agent \(x\) requires the red and blue resources to continue
    executing; \(x\) is holding the red resource and is waiting to
    acquire the blue resource.Agent \(y\) requires the blue and green resources to continue
    executing; \(y\) ; is holding the blue resource and is waiting to
    acquire the green resource.Agent \(z\) requires the green and red resources to continue
  executing; \(z\) holds the green resource and is waiting to acquire
    the red resource.Fig.2 - An Example of a DeadlockHow Should You Solve the Problem?StrategyA strategy to solve detection problems is to start with thegeneral detection algorithmand
  then explore optimizations by usingproperties of cuts.In the general detection algorithm, an observer gets global snapshots and
  determines if there is a cycle of waiting agents in the snapshot.
  Algorithms for determining cycles are foundhere.Next let's exploreoptimizations;
  see "Detection without Observers: Distributed Algorithms on Local
  Snapshots."
  The optimized algorithm has two phases: (1) First a global snapshot
  algorithm is executed. (2) After the global snapshot algorithm
  terminates a distributed detection algorithm is executed on
  the local snapshots recorded by the global snapshots.The two phases can be merged for some problems, including this one.A Distributed Algorithm to Detect a Cycle of
  Waiting AgentsMultiple detection algorithms and global snapshot algorithms may
  execute concurrently.
  
  These algorithms are disambiguated by tagging each algorithm with
  the initiator and a sequence id.
  Next we describe a single detection algorithm which is executed after 
  a single global snapshot algorithm terminates.Local ConstantsIn the detection algorithm, 
each agentvhas the following local constants:v.waitsis the set
  of resources thatvhas to acquire from other agents to start
  execution.v.holdsis the set of resources thatvholds and must continue to hold
  to start execution.ExampleIn the example of the figure:x.waits = {blue}, x.holds = {red}y.waits = {green}, y.holds = {blue}z.waits = {red}, y.holds = {green}v.waitsandv.holdsare constant for allvin the
detection algorithm  because these values are
specified in the global snapshot.MessagesThe algorithm to detect waiting cycles is similar to the global
  snapshot algorithm.
Instead of themarkermessage used in global snapshots, a
message in the detection identifies the set
of resources for which the sender of the message is waiting.Each messagemsent by an agentvhas a
fieldm.waitswhere:m.waits=v.waits.Initiating the AlgorithmA waiting agentuinitiates the algorithm by sending a messagemon each of its output channels wherem.waits=u.waits.Action by an Agent other than the
InitiatorWhen an agentvreceives a messagem:Ifvhas already sent messages thenvtakes no
  action.Ifvhas not sent messages, and if there is a resource
  common tom.waitsandv.holdsthenvsends a  messagem'on each of its
  output channels wherem'.waits=v.waits.Cycle DetectionIf the initiatorureceives a messagemwhere there is a resource common tom.waitsandu.holdsthenudetects a cycle of waiting
agents.Proof of CorrectnessThe proof outline is as follows.
An agentvsends messages only if there is a path of waiting
processes fromutov.
So,udetects a cycle of waiting processes only if there
exists a cycle of waiting processes.If there exists a cycle of waiting processes fromutouthen 
messages are sent along one such cycle, and soudetects a cycle.The algorithm terminates because it sends at most one message on each
channel.ExampleThis example shows steps in the case of the cycle of waiting processes shown in
figure 2.
The algorithm is initiated by agentxby broadcasting a
messagemwherem.waits = x.waits = {blue}.Whenyreceives a messagemwherem.waits = {blue}, there is a resource in bothm.waitsandy.holds, and soybroadcasts messagem'wherem'.waits = y.waits = {green}.Whenzreceives a messagemwherem.waits = {blue},ztakes no action because
there is no resource in bothm.waitsandz.holds.Whenzreceives a messagemwherem.waits = {green},zbroadcasts messagem'wherem'.waits = z.waits = {red}.When the initiatorxreceives a messagemwherem.waits = {red},xdetects a deadlock
because there is a resource common tom.waitsandx.holds.Combining Cycle Detection and Snapshot
AlgorithmsWe can use a a marker in the snapshot algorithm as a message in cycle
detection.
Modify a markermto have the fieldm.waitsused in cycle detection.
Then the snapshot algorithm is the same as the cycle detection
algorithm except that the snapshot algorithm records states of
channels which are not used in cycle detection.Write the algorithm for the combined algorithm as an exercise.K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology



Figure Link: https://kmchandy.github.io/ChannelSnapshots/DatabaseDeadlockDetection/DatabaseDeadlockDetection.002.jpeg

Caption: Fig.2 - An Example of a Deadlock


A deadlock arises when there is a cycle of agents,
  \([x_{0}, x_{1}, \ldots, x_{n-1}, x_{0}]\) where for all \(i\):agent \(x_{i}\) holds a resource \(r_{i}\), andagent \(x_{i}\) requires resources \(r_{i}\) and \(r_{(i+1)}\) to
    continue executing.(Operations on indices of agents are taken mod \(n\) and \(n >
    1\).)ExampleIn the example, a resource is identified by its color.
  
  A system has one red, one blue, and one green resource.
  
  Agents \(x\), \(y\) and \(z\) are deadlocked in the following
  state.Agent \(x\) requires the red and blue resources to continue
    executing; \(x\) is holding the red resource and is waiting to
    acquire the blue resource.Agent \(y\) requires the blue and green resources to continue
    executing; \(y\) ; is holding the blue resource and is waiting to
    acquire the green resource.Agent \(z\) requires the green and red resources to continue
  executing; \(z\) holds the green resource and is waiting to acquire
    the red resource.Fig.2 - An Example of a DeadlockHow Should You Solve the Problem?StrategyA strategy to solve detection problems is to start with thegeneral detection algorithmand
  then explore optimizations by usingproperties of cuts.In the general detection algorithm, an observer gets global snapshots and
  determines if there is a cycle of waiting agents in the snapshot.
  Algorithms for determining cycles are foundhere.Next let's exploreoptimizations;
  see "Detection without Observers: Distributed Algorithms on Local
  Snapshots."
  The optimized algorithm has two phases: (1) First a global snapshot
  algorithm is executed. (2) After the global snapshot algorithm
  terminates a distributed detection algorithm is executed on
  the local snapshots recorded by the global snapshots.The two phases can be merged for some problems, including this one.A Distributed Algorithm to Detect a Cycle of
  Waiting AgentsMultiple detection algorithms and global snapshot algorithms may
  execute concurrently.
  
  These algorithms are disambiguated by tagging each algorithm with
  the initiator and a sequence id.
  Next we describe a single detection algorithm which is executed after 
  a single global snapshot algorithm terminates.Local ConstantsIn the detection algorithm, 
each agentvhas the following local constants:v.waitsis the set
  of resources thatvhas to acquire from other agents to start
  execution.v.holdsis the set of resources thatvholds and must continue to hold
  to start execution.ExampleIn the example of the figure:x.waits = {blue}, x.holds = {red}y.waits = {green}, y.holds = {blue}z.waits = {red}, y.holds = {green}v.waitsandv.holdsare constant for allvin the
detection algorithm  because these values are
specified in the global snapshot.MessagesThe algorithm to detect waiting cycles is similar to the global
  snapshot algorithm.
Instead of themarkermessage used in global snapshots, a
message in the detection identifies the set
of resources for which the sender of the message is waiting.Each messagemsent by an agentvhas a
fieldm.waitswhere:m.waits=v.waits.Initiating the AlgorithmA waiting agentuinitiates the algorithm by sending a messagemon each of its output channels wherem.waits=u.waits.Action by an Agent other than the
InitiatorWhen an agentvreceives a messagem:Ifvhas already sent messages thenvtakes no
  action.Ifvhas not sent messages, and if there is a resource
  common tom.waitsandv.holdsthenvsends a  messagem'on each of its
  output channels wherem'.waits=v.waits.Cycle DetectionIf the initiatorureceives a messagemwhere there is a resource common tom.waitsandu.holdsthenudetects a cycle of waiting
agents.Proof of CorrectnessThe proof outline is as follows.
An agentvsends messages only if there is a path of waiting
processes fromutov.
So,udetects a cycle of waiting processes only if there
exists a cycle of waiting processes.If there exists a cycle of waiting processes fromutouthen 
messages are sent along one such cycle, and soudetects a cycle.The algorithm terminates because it sends at most one message on each
channel.ExampleThis example shows steps in the case of the cycle of waiting processes shown in
figure 2.
The algorithm is initiated by agentxby broadcasting a
messagemwherem.waits = x.waits = {blue}.Whenyreceives a messagemwherem.waits = {blue}, there is a resource in bothm.waitsandy.holds, and soybroadcasts messagem'wherem'.waits = y.waits = {green}.Whenzreceives a messagemwherem.waits = {blue},ztakes no action because
there is no resource in bothm.waitsandz.holds.Whenzreceives a messagemwherem.waits = {green},zbroadcasts messagem'wherem'.waits = z.waits = {red}.When the initiatorxreceives a messagemwherem.waits = {red},xdetects a deadlock
because there is a resource common tom.waitsandx.holds.Combining Cycle Detection and Snapshot
AlgorithmsWe can use a a marker in the snapshot algorithm as a message in cycle
detection.
Modify a markermto have the fieldm.waitsused in cycle detection.
Then the snapshot algorithm is the same as the cycle detection
algorithm except that the snapshot algorithm records states of
channels which are not used in cycle detection.Write the algorithm for the combined algorithm as an exercise.K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology



Figure Link: https://kmchandy.github.io/ChannelSnapshots/DatabaseDeadlockDetection/DatabaseDeadlockDetection.002.jpeg

Caption: Fig.2 - An Example of a Deadlock


Strategy

In the general detection algorithm, an observer gets global snapshots and
  determines if there is a cycle of waiting agents in the snapshot.
  Algorithms for determining cycles are foundhere.Next let's exploreoptimizations;
  see "Detection without Observers: Distributed Algorithms on Local
  Snapshots."
  The optimized algorithm has two phases: (1) First a global snapshot
  algorithm is executed. (2) After the global snapshot algorithm
  terminates a distributed detection algorithm is executed on
  the local snapshots recorded by the global snapshots.The two phases can be merged for some problems, including this one.A Distributed Algorithm to Detect a Cycle of
  Waiting AgentsMultiple detection algorithms and global snapshot algorithms may
  execute concurrently.
  
  These algorithms are disambiguated by tagging each algorithm with
  the initiator and a sequence id.
  Next we describe a single detection algorithm which is executed after 
  a single global snapshot algorithm terminates.Local ConstantsIn the detection algorithm, 
each agentvhas the following local constants:v.waitsis the set
  of resources thatvhas to acquire from other agents to start
  execution.v.holdsis the set of resources thatvholds and must continue to hold
  to start execution.ExampleIn the example of the figure:x.waits = {blue}, x.holds = {red}y.waits = {green}, y.holds = {blue}z.waits = {red}, y.holds = {green}v.waitsandv.holdsare constant for allvin the
detection algorithm  because these values are
specified in the global snapshot.MessagesThe algorithm to detect waiting cycles is similar to the global
  snapshot algorithm.
Instead of themarkermessage used in global snapshots, a
message in the detection identifies the set
of resources for which the sender of the message is waiting.Each messagemsent by an agentvhas a
fieldm.waitswhere:m.waits=v.waits.Initiating the AlgorithmA waiting agentuinitiates the algorithm by sending a messagemon each of its output channels wherem.waits=u.waits.Action by an Agent other than the
InitiatorWhen an agentvreceives a messagem:Ifvhas already sent messages thenvtakes no
  action.Ifvhas not sent messages, and if there is a resource
  common tom.waitsandv.holdsthenvsends a  messagem'on each of its
  output channels wherem'.waits=v.waits.Cycle DetectionIf the initiatorureceives a messagemwhere there is a resource common tom.waitsandu.holdsthenudetects a cycle of waiting
agents.Proof of CorrectnessThe proof outline is as follows.
An agentvsends messages only if there is a path of waiting
processes fromutov.
So,udetects a cycle of waiting processes only if there
exists a cycle of waiting processes.If there exists a cycle of waiting processes fromutouthen 
messages are sent along one such cycle, and soudetects a cycle.The algorithm terminates because it sends at most one message on each
channel.ExampleThis example shows steps in the case of the cycle of waiting processes shown in
figure 2.
The algorithm is initiated by agentxby broadcasting a
messagemwherem.waits = x.waits = {blue}.Whenyreceives a messagemwherem.waits = {blue}, there is a resource in bothm.waitsandy.holds, and soybroadcasts messagem'wherem'.waits = y.waits = {green}.Whenzreceives a messagemwherem.waits = {blue},ztakes no action because
there is no resource in bothm.waitsandz.holds.Whenzreceives a messagemwherem.waits = {green},zbroadcasts messagem'wherem'.waits = z.waits = {red}.When the initiatorxreceives a messagemwherem.waits = {red},xdetects a deadlock
because there is a resource common tom.waitsandx.holds.Combining Cycle Detection and Snapshot
AlgorithmsWe can use a a marker in the snapshot algorithm as a message in cycle
detection.
Modify a markermto have the fieldm.waitsused in cycle detection.
Then the snapshot algorithm is the same as the cycle detection
algorithm except that the snapshot algorithm records states of
channels which are not used in cycle detection.Write the algorithm for the combined algorithm as an exercise.K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

Next let's exploreoptimizations;
  see "Detection without Observers: Distributed Algorithms on Local
  Snapshots."
  The optimized algorithm has two phases: (1) First a global snapshot
  algorithm is executed. (2) After the global snapshot algorithm
  terminates a distributed detection algorithm is executed on
  the local snapshots recorded by the global snapshots.The two phases can be merged for some problems, including this one.A Distributed Algorithm to Detect a Cycle of
  Waiting AgentsMultiple detection algorithms and global snapshot algorithms may
  execute concurrently.
  
  These algorithms are disambiguated by tagging each algorithm with
  the initiator and a sequence id.
  Next we describe a single detection algorithm which is executed after 
  a single global snapshot algorithm terminates.Local ConstantsIn the detection algorithm, 
each agentvhas the following local constants:v.waitsis the set
  of resources thatvhas to acquire from other agents to start
  execution.v.holdsis the set of resources thatvholds and must continue to hold
  to start execution.ExampleIn the example of the figure:x.waits = {blue}, x.holds = {red}y.waits = {green}, y.holds = {blue}z.waits = {red}, y.holds = {green}v.waitsandv.holdsare constant for allvin the
detection algorithm  because these values are
specified in the global snapshot.MessagesThe algorithm to detect waiting cycles is similar to the global
  snapshot algorithm.
Instead of themarkermessage used in global snapshots, a
message in the detection identifies the set
of resources for which the sender of the message is waiting.Each messagemsent by an agentvhas a
fieldm.waitswhere:m.waits=v.waits.Initiating the AlgorithmA waiting agentuinitiates the algorithm by sending a messagemon each of its output channels wherem.waits=u.waits.Action by an Agent other than the
InitiatorWhen an agentvreceives a messagem:Ifvhas already sent messages thenvtakes no
  action.Ifvhas not sent messages, and if there is a resource
  common tom.waitsandv.holdsthenvsends a  messagem'on each of its
  output channels wherem'.waits=v.waits.Cycle DetectionIf the initiatorureceives a messagemwhere there is a resource common tom.waitsandu.holdsthenudetects a cycle of waiting
agents.Proof of CorrectnessThe proof outline is as follows.
An agentvsends messages only if there is a path of waiting
processes fromutov.
So,udetects a cycle of waiting processes only if there
exists a cycle of waiting processes.If there exists a cycle of waiting processes fromutouthen 
messages are sent along one such cycle, and soudetects a cycle.The algorithm terminates because it sends at most one message on each
channel.ExampleThis example shows steps in the case of the cycle of waiting processes shown in
figure 2.
The algorithm is initiated by agentxby broadcasting a
messagemwherem.waits = x.waits = {blue}.Whenyreceives a messagemwherem.waits = {blue}, there is a resource in bothm.waitsandy.holds, and soybroadcasts messagem'wherem'.waits = y.waits = {green}.Whenzreceives a messagemwherem.waits = {blue},ztakes no action because
there is no resource in bothm.waitsandz.holds.Whenzreceives a messagemwherem.waits = {green},zbroadcasts messagem'wherem'.waits = z.waits = {red}.When the initiatorxreceives a messagemwherem.waits = {red},xdetects a deadlock
because there is a resource common tom.waitsandx.holds.Combining Cycle Detection and Snapshot
AlgorithmsWe can use a a marker in the snapshot algorithm as a message in cycle
detection.
Modify a markermto have the fieldm.waitsused in cycle detection.
Then the snapshot algorithm is the same as the cycle detection
algorithm except that the snapshot algorithm records states of
channels which are not used in cycle detection.Write the algorithm for the combined algorithm as an exercise.K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

The two phases can be merged for some problems, including this one.A Distributed Algorithm to Detect a Cycle of
  Waiting AgentsMultiple detection algorithms and global snapshot algorithms may
  execute concurrently.
  
  These algorithms are disambiguated by tagging each algorithm with
  the initiator and a sequence id.
  Next we describe a single detection algorithm which is executed after 
  a single global snapshot algorithm terminates.Local ConstantsIn the detection algorithm, 
each agentvhas the following local constants:v.waitsis the set
  of resources thatvhas to acquire from other agents to start
  execution.v.holdsis the set of resources thatvholds and must continue to hold
  to start execution.ExampleIn the example of the figure:x.waits = {blue}, x.holds = {red}y.waits = {green}, y.holds = {blue}z.waits = {red}, y.holds = {green}v.waitsandv.holdsare constant for allvin the
detection algorithm  because these values are
specified in the global snapshot.MessagesThe algorithm to detect waiting cycles is similar to the global
  snapshot algorithm.
Instead of themarkermessage used in global snapshots, a
message in the detection identifies the set
of resources for which the sender of the message is waiting.Each messagemsent by an agentvhas a
fieldm.waitswhere:m.waits=v.waits.Initiating the AlgorithmA waiting agentuinitiates the algorithm by sending a messagemon each of its output channels wherem.waits=u.waits.Action by an Agent other than the
InitiatorWhen an agentvreceives a messagem:Ifvhas already sent messages thenvtakes no
  action.Ifvhas not sent messages, and if there is a resource
  common tom.waitsandv.holdsthenvsends a  messagem'on each of its
  output channels wherem'.waits=v.waits.Cycle DetectionIf the initiatorureceives a messagemwhere there is a resource common tom.waitsandu.holdsthenudetects a cycle of waiting
agents.Proof of CorrectnessThe proof outline is as follows.
An agentvsends messages only if there is a path of waiting
processes fromutov.
So,udetects a cycle of waiting processes only if there
exists a cycle of waiting processes.If there exists a cycle of waiting processes fromutouthen 
messages are sent along one such cycle, and soudetects a cycle.The algorithm terminates because it sends at most one message on each
channel.ExampleThis example shows steps in the case of the cycle of waiting processes shown in
figure 2.
The algorithm is initiated by agentxby broadcasting a
messagemwherem.waits = x.waits = {blue}.Whenyreceives a messagemwherem.waits = {blue}, there is a resource in bothm.waitsandy.holds, and soybroadcasts messagem'wherem'.waits = y.waits = {green}.Whenzreceives a messagemwherem.waits = {blue},ztakes no action because
there is no resource in bothm.waitsandz.holds.Whenzreceives a messagemwherem.waits = {green},zbroadcasts messagem'wherem'.waits = z.waits = {red}.When the initiatorxreceives a messagemwherem.waits = {red},xdetects a deadlock
because there is a resource common tom.waitsandx.holds.Combining Cycle Detection and Snapshot
AlgorithmsWe can use a a marker in the snapshot algorithm as a message in cycle
detection.
Modify a markermto have the fieldm.waitsused in cycle detection.
Then the snapshot algorithm is the same as the cycle detection
algorithm except that the snapshot algorithm records states of
channels which are not used in cycle detection.Write the algorithm for the combined algorithm as an exercise.K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

Multiple detection algorithms and global snapshot algorithms may
  execute concurrently.
  
  These algorithms are disambiguated by tagging each algorithm with
  the initiator and a sequence id.
  Next we describe a single detection algorithm which is executed after 
  a single global snapshot algorithm terminates.Local ConstantsIn the detection algorithm, 
each agentvhas the following local constants:v.waitsis the set
  of resources thatvhas to acquire from other agents to start
  execution.v.holdsis the set of resources thatvholds and must continue to hold
  to start execution.ExampleIn the example of the figure:x.waits = {blue}, x.holds = {red}y.waits = {green}, y.holds = {blue}z.waits = {red}, y.holds = {green}v.waitsandv.holdsare constant for allvin the
detection algorithm  because these values are
specified in the global snapshot.MessagesThe algorithm to detect waiting cycles is similar to the global
  snapshot algorithm.
Instead of themarkermessage used in global snapshots, a
message in the detection identifies the set
of resources for which the sender of the message is waiting.Each messagemsent by an agentvhas a
fieldm.waitswhere:m.waits=v.waits.Initiating the AlgorithmA waiting agentuinitiates the algorithm by sending a messagemon each of its output channels wherem.waits=u.waits.Action by an Agent other than the
InitiatorWhen an agentvreceives a messagem:Ifvhas already sent messages thenvtakes no
  action.Ifvhas not sent messages, and if there is a resource
  common tom.waitsandv.holdsthenvsends a  messagem'on each of its
  output channels wherem'.waits=v.waits.Cycle DetectionIf the initiatorureceives a messagemwhere there is a resource common tom.waitsandu.holdsthenudetects a cycle of waiting
agents.Proof of CorrectnessThe proof outline is as follows.
An agentvsends messages only if there is a path of waiting
processes fromutov.
So,udetects a cycle of waiting processes only if there
exists a cycle of waiting processes.If there exists a cycle of waiting processes fromutouthen 
messages are sent along one such cycle, and soudetects a cycle.The algorithm terminates because it sends at most one message on each
channel.ExampleThis example shows steps in the case of the cycle of waiting processes shown in
figure 2.
The algorithm is initiated by agentxby broadcasting a
messagemwherem.waits = x.waits = {blue}.Whenyreceives a messagemwherem.waits = {blue}, there is a resource in bothm.waitsandy.holds, and soybroadcasts messagem'wherem'.waits = y.waits = {green}.Whenzreceives a messagemwherem.waits = {blue},ztakes no action because
there is no resource in bothm.waitsandz.holds.Whenzreceives a messagemwherem.waits = {green},zbroadcasts messagem'wherem'.waits = z.waits = {red}.When the initiatorxreceives a messagemwherem.waits = {red},xdetects a deadlock
because there is a resource common tom.waitsandx.holds.Combining Cycle Detection and Snapshot
AlgorithmsWe can use a a marker in the snapshot algorithm as a message in cycle
detection.
Modify a markermto have the fieldm.waitsused in cycle detection.
Then the snapshot algorithm is the same as the cycle detection
algorithm except that the snapshot algorithm records states of
channels which are not used in cycle detection.Write the algorithm for the combined algorithm as an exercise.K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

Local Constants

Example

v.waitsandv.holdsare constant for allvin the
detection algorithm  because these values are
specified in the global snapshot.MessagesThe algorithm to detect waiting cycles is similar to the global
  snapshot algorithm.
Instead of themarkermessage used in global snapshots, a
message in the detection identifies the set
of resources for which the sender of the message is waiting.Each messagemsent by an agentvhas a
fieldm.waitswhere:m.waits=v.waits.Initiating the AlgorithmA waiting agentuinitiates the algorithm by sending a messagemon each of its output channels wherem.waits=u.waits.Action by an Agent other than the
InitiatorWhen an agentvreceives a messagem:Ifvhas already sent messages thenvtakes no
  action.Ifvhas not sent messages, and if there is a resource
  common tom.waitsandv.holdsthenvsends a  messagem'on each of its
  output channels wherem'.waits=v.waits.Cycle DetectionIf the initiatorureceives a messagemwhere there is a resource common tom.waitsandu.holdsthenudetects a cycle of waiting
agents.Proof of CorrectnessThe proof outline is as follows.
An agentvsends messages only if there is a path of waiting
processes fromutov.
So,udetects a cycle of waiting processes only if there
exists a cycle of waiting processes.If there exists a cycle of waiting processes fromutouthen 
messages are sent along one such cycle, and soudetects a cycle.The algorithm terminates because it sends at most one message on each
channel.ExampleThis example shows steps in the case of the cycle of waiting processes shown in
figure 2.
The algorithm is initiated by agentxby broadcasting a
messagemwherem.waits = x.waits = {blue}.Whenyreceives a messagemwherem.waits = {blue}, there is a resource in bothm.waitsandy.holds, and soybroadcasts messagem'wherem'.waits = y.waits = {green}.Whenzreceives a messagemwherem.waits = {blue},ztakes no action because
there is no resource in bothm.waitsandz.holds.Whenzreceives a messagemwherem.waits = {green},zbroadcasts messagem'wherem'.waits = z.waits = {red}.When the initiatorxreceives a messagemwherem.waits = {red},xdetects a deadlock
because there is a resource common tom.waitsandx.holds.Combining Cycle Detection and Snapshot
AlgorithmsWe can use a a marker in the snapshot algorithm as a message in cycle
detection.
Modify a markermto have the fieldm.waitsused in cycle detection.
Then the snapshot algorithm is the same as the cycle detection
algorithm except that the snapshot algorithm records states of
channels which are not used in cycle detection.Write the algorithm for the combined algorithm as an exercise.K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

Messages

Each messagemsent by an agentvhas a
fieldm.waitswhere:m.waits=v.waits.Initiating the AlgorithmA waiting agentuinitiates the algorithm by sending a messagemon each of its output channels wherem.waits=u.waits.Action by an Agent other than the
InitiatorWhen an agentvreceives a messagem:Ifvhas already sent messages thenvtakes no
  action.Ifvhas not sent messages, and if there is a resource
  common tom.waitsandv.holdsthenvsends a  messagem'on each of its
  output channels wherem'.waits=v.waits.Cycle DetectionIf the initiatorureceives a messagemwhere there is a resource common tom.waitsandu.holdsthenudetects a cycle of waiting
agents.Proof of CorrectnessThe proof outline is as follows.
An agentvsends messages only if there is a path of waiting
processes fromutov.
So,udetects a cycle of waiting processes only if there
exists a cycle of waiting processes.If there exists a cycle of waiting processes fromutouthen 
messages are sent along one such cycle, and soudetects a cycle.The algorithm terminates because it sends at most one message on each
channel.ExampleThis example shows steps in the case of the cycle of waiting processes shown in
figure 2.
The algorithm is initiated by agentxby broadcasting a
messagemwherem.waits = x.waits = {blue}.Whenyreceives a messagemwherem.waits = {blue}, there is a resource in bothm.waitsandy.holds, and soybroadcasts messagem'wherem'.waits = y.waits = {green}.Whenzreceives a messagemwherem.waits = {blue},ztakes no action because
there is no resource in bothm.waitsandz.holds.Whenzreceives a messagemwherem.waits = {green},zbroadcasts messagem'wherem'.waits = z.waits = {red}.When the initiatorxreceives a messagemwherem.waits = {red},xdetects a deadlock
because there is a resource common tom.waitsandx.holds.Combining Cycle Detection and Snapshot
AlgorithmsWe can use a a marker in the snapshot algorithm as a message in cycle
detection.
Modify a markermto have the fieldm.waitsused in cycle detection.
Then the snapshot algorithm is the same as the cycle detection
algorithm except that the snapshot algorithm records states of
channels which are not used in cycle detection.Write the algorithm for the combined algorithm as an exercise.K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

Initiating the Algorithm

Action by an Agent other than the
Initiator

When an agentvreceives a messagem:Ifvhas already sent messages thenvtakes no
  action.Ifvhas not sent messages, and if there is a resource
  common tom.waitsandv.holdsthenvsends a  messagem'on each of its
  output channels wherem'.waits=v.waits.Cycle DetectionIf the initiatorureceives a messagemwhere there is a resource common tom.waitsandu.holdsthenudetects a cycle of waiting
agents.Proof of CorrectnessThe proof outline is as follows.
An agentvsends messages only if there is a path of waiting
processes fromutov.
So,udetects a cycle of waiting processes only if there
exists a cycle of waiting processes.If there exists a cycle of waiting processes fromutouthen 
messages are sent along one such cycle, and soudetects a cycle.The algorithm terminates because it sends at most one message on each
channel.ExampleThis example shows steps in the case of the cycle of waiting processes shown in
figure 2.
The algorithm is initiated by agentxby broadcasting a
messagemwherem.waits = x.waits = {blue}.Whenyreceives a messagemwherem.waits = {blue}, there is a resource in bothm.waitsandy.holds, and soybroadcasts messagem'wherem'.waits = y.waits = {green}.Whenzreceives a messagemwherem.waits = {blue},ztakes no action because
there is no resource in bothm.waitsandz.holds.Whenzreceives a messagemwherem.waits = {green},zbroadcasts messagem'wherem'.waits = z.waits = {red}.When the initiatorxreceives a messagemwherem.waits = {red},xdetects a deadlock
because there is a resource common tom.waitsandx.holds.Combining Cycle Detection and Snapshot
AlgorithmsWe can use a a marker in the snapshot algorithm as a message in cycle
detection.
Modify a markermto have the fieldm.waitsused in cycle detection.
Then the snapshot algorithm is the same as the cycle detection
algorithm except that the snapshot algorithm records states of
channels which are not used in cycle detection.Write the algorithm for the combined algorithm as an exercise.K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

Cycle Detection

If there exists a cycle of waiting processes fromutouthen 
messages are sent along one such cycle, and soudetects a cycle.The algorithm terminates because it sends at most one message on each
channel.ExampleThis example shows steps in the case of the cycle of waiting processes shown in
figure 2.
The algorithm is initiated by agentxby broadcasting a
messagemwherem.waits = x.waits = {blue}.Whenyreceives a messagemwherem.waits = {blue}, there is a resource in bothm.waitsandy.holds, and soybroadcasts messagem'wherem'.waits = y.waits = {green}.Whenzreceives a messagemwherem.waits = {blue},ztakes no action because
there is no resource in bothm.waitsandz.holds.Whenzreceives a messagemwherem.waits = {green},zbroadcasts messagem'wherem'.waits = z.waits = {red}.When the initiatorxreceives a messagemwherem.waits = {red},xdetects a deadlock
because there is a resource common tom.waitsandx.holds.Combining Cycle Detection and Snapshot
AlgorithmsWe can use a a marker in the snapshot algorithm as a message in cycle
detection.
Modify a markermto have the fieldm.waitsused in cycle detection.
Then the snapshot algorithm is the same as the cycle detection
algorithm except that the snapshot algorithm records states of
channels which are not used in cycle detection.Write the algorithm for the combined algorithm as an exercise.K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

The algorithm terminates because it sends at most one message on each
channel.ExampleThis example shows steps in the case of the cycle of waiting processes shown in
figure 2.
The algorithm is initiated by agentxby broadcasting a
messagemwherem.waits = x.waits = {blue}.Whenyreceives a messagemwherem.waits = {blue}, there is a resource in bothm.waitsandy.holds, and soybroadcasts messagem'wherem'.waits = y.waits = {green}.Whenzreceives a messagemwherem.waits = {blue},ztakes no action because
there is no resource in bothm.waitsandz.holds.Whenzreceives a messagemwherem.waits = {green},zbroadcasts messagem'wherem'.waits = z.waits = {red}.When the initiatorxreceives a messagemwherem.waits = {red},xdetects a deadlock
because there is a resource common tom.waitsandx.holds.Combining Cycle Detection and Snapshot
AlgorithmsWe can use a a marker in the snapshot algorithm as a message in cycle
detection.
Modify a markermto have the fieldm.waitsused in cycle detection.
Then the snapshot algorithm is the same as the cycle detection
algorithm except that the snapshot algorithm records states of
channels which are not used in cycle detection.Write the algorithm for the combined algorithm as an exercise.K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

Whenyreceives a messagemwherem.waits = {blue}, there is a resource in bothm.waitsandy.holds, and soybroadcasts messagem'wherem'.waits = y.waits = {green}.Whenzreceives a messagemwherem.waits = {blue},ztakes no action because
there is no resource in bothm.waitsandz.holds.Whenzreceives a messagemwherem.waits = {green},zbroadcasts messagem'wherem'.waits = z.waits = {red}.When the initiatorxreceives a messagemwherem.waits = {red},xdetects a deadlock
because there is a resource common tom.waitsandx.holds.Combining Cycle Detection and Snapshot
AlgorithmsWe can use a a marker in the snapshot algorithm as a message in cycle
detection.
Modify a markermto have the fieldm.waitsused in cycle detection.
Then the snapshot algorithm is the same as the cycle detection
algorithm except that the snapshot algorithm records states of
channels which are not used in cycle detection.Write the algorithm for the combined algorithm as an exercise.K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

Whenzreceives a messagemwherem.waits = {blue},ztakes no action because
there is no resource in bothm.waitsandz.holds.Whenzreceives a messagemwherem.waits = {green},zbroadcasts messagem'wherem'.waits = z.waits = {red}.When the initiatorxreceives a messagemwherem.waits = {red},xdetects a deadlock
because there is a resource common tom.waitsandx.holds.Combining Cycle Detection and Snapshot
AlgorithmsWe can use a a marker in the snapshot algorithm as a message in cycle
detection.
Modify a markermto have the fieldm.waitsused in cycle detection.
Then the snapshot algorithm is the same as the cycle detection
algorithm except that the snapshot algorithm records states of
channels which are not used in cycle detection.Write the algorithm for the combined algorithm as an exercise.K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

Whenzreceives a messagemwherem.waits = {green},zbroadcasts messagem'wherem'.waits = z.waits = {red}.When the initiatorxreceives a messagemwherem.waits = {red},xdetects a deadlock
because there is a resource common tom.waitsandx.holds.Combining Cycle Detection and Snapshot
AlgorithmsWe can use a a marker in the snapshot algorithm as a message in cycle
detection.
Modify a markermto have the fieldm.waitsused in cycle detection.
Then the snapshot algorithm is the same as the cycle detection
algorithm except that the snapshot algorithm records states of
channels which are not used in cycle detection.Write the algorithm for the combined algorithm as an exercise.K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

When the initiatorxreceives a messagemwherem.waits = {red},xdetects a deadlock
because there is a resource common tom.waitsandx.holds.Combining Cycle Detection and Snapshot
AlgorithmsWe can use a a marker in the snapshot algorithm as a message in cycle
detection.
Modify a markermto have the fieldm.waitsused in cycle detection.
Then the snapshot algorithm is the same as the cycle detection
algorithm except that the snapshot algorithm records states of
channels which are not used in cycle detection.Write the algorithm for the combined algorithm as an exercise.K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

Write the algorithm for the combined algorithm as an exercise.K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology
--- END Content from https://kmchandy.github.io/ChannelSnapshots/DatabaseDeadlockDetection.html ---


--- START Content from https://kmchandy.github.io/Paxos/ConsensusImpossible.html ---
TITLE: Paxos/ConsensusImpossible.html

(1) Importance of consensus (2) Impossibility of consensus in
  distributed systems with a faulty agent.Importance of ConsensusAlgorithms by which groups of agentscome to a consensusare among the most fundamental problems in
distributed computing.Why is consensus important? There are many
problems in which messages are sent to groups of agents who
collectively maintain a commonconsensusstate. A bank may use
a group of agents, rather than a single agent, to maintain bank
balances. Multiple agents reduce the possibility of system-wide
failure due to the failure of a single agent.  Managing replicated
databases requires the replications to come to a consensus on the
sequence of transactions that is applied to the
database. Cryptocurrency transactions also require collections of
agents to come to a consensus about sequences of the transactions.In a control system with multiple and actuators, the actuators have to
come to a consensus about the state of the environment so that they
can operate in concert. A vehicle would crash if some actuators caused
the vehicle to accelerate while other actuators applied brakes.  In
some applications, multiple agents have to elect a single leader.
There are many problems in which a collection of agents have to come
to aconsensusabout something.Consensus: Impossible with a
    faulty agentConsensus is impossible with even a single faulty agent. This was
    proved in a paper published by Fischer, Lynch and Patterson.You can get the idea of why consensus is not possible by
    considering the following problem in which
    when message delays are finite but arbitrarily long.
    A collection of 2N + 1 agents want to
    come to a consensus about a color. N of the agents pick blue and
    N+1 pick red. One of the red agents is arbitrarily slow. The
    2N non-slow agents exchange messages among each other, and each of
    these 2N agents gets N votes for red and N votes for blue. Agents
    decide to take a majority vote, and in the event of a tie pick
    blue.Fig.1: Problem with a slow agentHow long should they wait for the slow agent?Consider an algorithm in which agent waits until its local clock
    shows an elapsed time of T and then makes a decision based on the
    votes that it has.  An agent Y gets N red and N blue votes when
    its clock shows an elapsed time of T, and agent Y decides that the
    consensus is blue. Another agent Z has a slower clock and gets a
    red vote from the slow agent for a total of N+1 red votes, before
    Z's clock shows an elapsed time of T. So Z determines that the
    consensus is red. The algorithm fails because Y and Z have not
    come to a consensus.No algorithm is guaranteed to come to a consensus in finite time if
messages can be arbitrarily slow or if agents can be arbitrarily slow.
Systems with synchronized clocks don't have this particular
problem. We'll look at consensus in such systems later.Best Effort ConsensusThe theorem says that there is no algorithm that guarentees that
consensus can be reached in all scenarios; 
however, consensus can be reached in most practical situations. An idea
to overcome the counterexample given above is:
Agents keep trying repeatedly until they reach consensus. The theorem tells us
that the agents may have to keep trying for ever. We expect, however,
that in most practical situations their attempts will succeed at some point.What does keep trying mean? When does one trial
end and the next one begin? If agents use timeouts to end a trial,
then --- because clocks aren't synchronized --- the timeouts may complete
at different times.
We'll see that we can use the idea of time, even though clocks aren't
synchronized. We've done that before with logical clocks. The Paxos
algorithm shows how the idea of increasing values of timestamps (or
ids) are used for best-effort consensus.Central Ideas: ReviewMany applications of distributed systems require agents to come to a
consensus.
Agents cannot come to a consensus if an agent is faulty.Concepts:
Consensus -> impossibility with faulty agentsNextNext look atPaxosan important consensus
algorithm that may not terminate.
Later look atByzantine consensusandconsensus using block chain.K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology



Figure Link: https://kmchandy.github.io/Paxos/Slide01.jpg

Caption: Fig.1: Problem with a slow agent


Algorithms by which groups of agentscome to a consensusare among the most fundamental problems in
distributed computing.Why is consensus important? There are many
problems in which messages are sent to groups of agents who
collectively maintain a commonconsensusstate. A bank may use
a group of agents, rather than a single agent, to maintain bank
balances. Multiple agents reduce the possibility of system-wide
failure due to the failure of a single agent.  Managing replicated
databases requires the replications to come to a consensus on the
sequence of transactions that is applied to the
database. Cryptocurrency transactions also require collections of
agents to come to a consensus about sequences of the transactions.In a control system with multiple and actuators, the actuators have to
come to a consensus about the state of the environment so that they
can operate in concert. A vehicle would crash if some actuators caused
the vehicle to accelerate while other actuators applied brakes.  In
some applications, multiple agents have to elect a single leader.
There are many problems in which a collection of agents have to come
to aconsensusabout something.Consensus: Impossible with a
    faulty agentConsensus is impossible with even a single faulty agent. This was
    proved in a paper published by Fischer, Lynch and Patterson.You can get the idea of why consensus is not possible by
    considering the following problem in which
    when message delays are finite but arbitrarily long.
    A collection of 2N + 1 agents want to
    come to a consensus about a color. N of the agents pick blue and
    N+1 pick red. One of the red agents is arbitrarily slow. The
    2N non-slow agents exchange messages among each other, and each of
    these 2N agents gets N votes for red and N votes for blue. Agents
    decide to take a majority vote, and in the event of a tie pick
    blue.Fig.1: Problem with a slow agentHow long should they wait for the slow agent?Consider an algorithm in which agent waits until its local clock
    shows an elapsed time of T and then makes a decision based on the
    votes that it has.  An agent Y gets N red and N blue votes when
    its clock shows an elapsed time of T, and agent Y decides that the
    consensus is blue. Another agent Z has a slower clock and gets a
    red vote from the slow agent for a total of N+1 red votes, before
    Z's clock shows an elapsed time of T. So Z determines that the
    consensus is red. The algorithm fails because Y and Z have not
    come to a consensus.No algorithm is guaranteed to come to a consensus in finite time if
messages can be arbitrarily slow or if agents can be arbitrarily slow.
Systems with synchronized clocks don't have this particular
problem. We'll look at consensus in such systems later.Best Effort ConsensusThe theorem says that there is no algorithm that guarentees that
consensus can be reached in all scenarios; 
however, consensus can be reached in most practical situations. An idea
to overcome the counterexample given above is:
Agents keep trying repeatedly until they reach consensus. The theorem tells us
that the agents may have to keep trying for ever. We expect, however,
that in most practical situations their attempts will succeed at some point.What does keep trying mean? When does one trial
end and the next one begin? If agents use timeouts to end a trial,
then --- because clocks aren't synchronized --- the timeouts may complete
at different times.
We'll see that we can use the idea of time, even though clocks aren't
synchronized. We've done that before with logical clocks. The Paxos
algorithm shows how the idea of increasing values of timestamps (or
ids) are used for best-effort consensus.Central Ideas: ReviewMany applications of distributed systems require agents to come to a
consensus.
Agents cannot come to a consensus if an agent is faulty.Concepts:
Consensus -> impossibility with faulty agentsNextNext look atPaxosan important consensus
algorithm that may not terminate.
Later look atByzantine consensusandconsensus using block chain.K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology



Figure Link: https://kmchandy.github.io/Paxos/Slide01.jpg

Caption: Fig.1: Problem with a slow agent


Why is consensus important? There are many
problems in which messages are sent to groups of agents who
collectively maintain a commonconsensusstate. A bank may use
a group of agents, rather than a single agent, to maintain bank
balances. Multiple agents reduce the possibility of system-wide
failure due to the failure of a single agent.  Managing replicated
databases requires the replications to come to a consensus on the
sequence of transactions that is applied to the
database. Cryptocurrency transactions also require collections of
agents to come to a consensus about sequences of the transactions.In a control system with multiple and actuators, the actuators have to
come to a consensus about the state of the environment so that they
can operate in concert. A vehicle would crash if some actuators caused
the vehicle to accelerate while other actuators applied brakes.  In
some applications, multiple agents have to elect a single leader.
There are many problems in which a collection of agents have to come
to aconsensusabout something.Consensus: Impossible with a
    faulty agentConsensus is impossible with even a single faulty agent. This was
    proved in a paper published by Fischer, Lynch and Patterson.You can get the idea of why consensus is not possible by
    considering the following problem in which
    when message delays are finite but arbitrarily long.
    A collection of 2N + 1 agents want to
    come to a consensus about a color. N of the agents pick blue and
    N+1 pick red. One of the red agents is arbitrarily slow. The
    2N non-slow agents exchange messages among each other, and each of
    these 2N agents gets N votes for red and N votes for blue. Agents
    decide to take a majority vote, and in the event of a tie pick
    blue.Fig.1: Problem with a slow agentHow long should they wait for the slow agent?Consider an algorithm in which agent waits until its local clock
    shows an elapsed time of T and then makes a decision based on the
    votes that it has.  An agent Y gets N red and N blue votes when
    its clock shows an elapsed time of T, and agent Y decides that the
    consensus is blue. Another agent Z has a slower clock and gets a
    red vote from the slow agent for a total of N+1 red votes, before
    Z's clock shows an elapsed time of T. So Z determines that the
    consensus is red. The algorithm fails because Y and Z have not
    come to a consensus.No algorithm is guaranteed to come to a consensus in finite time if
messages can be arbitrarily slow or if agents can be arbitrarily slow.
Systems with synchronized clocks don't have this particular
problem. We'll look at consensus in such systems later.Best Effort ConsensusThe theorem says that there is no algorithm that guarentees that
consensus can be reached in all scenarios; 
however, consensus can be reached in most practical situations. An idea
to overcome the counterexample given above is:
Agents keep trying repeatedly until they reach consensus. The theorem tells us
that the agents may have to keep trying for ever. We expect, however,
that in most practical situations their attempts will succeed at some point.What does keep trying mean? When does one trial
end and the next one begin? If agents use timeouts to end a trial,
then --- because clocks aren't synchronized --- the timeouts may complete
at different times.
We'll see that we can use the idea of time, even though clocks aren't
synchronized. We've done that before with logical clocks. The Paxos
algorithm shows how the idea of increasing values of timestamps (or
ids) are used for best-effort consensus.Central Ideas: ReviewMany applications of distributed systems require agents to come to a
consensus.
Agents cannot come to a consensus if an agent is faulty.Concepts:
Consensus -> impossibility with faulty agentsNextNext look atPaxosan important consensus
algorithm that may not terminate.
Later look atByzantine consensusandconsensus using block chain.K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology



Figure Link: https://kmchandy.github.io/Paxos/Slide01.jpg

Caption: Fig.1: Problem with a slow agent


In a control system with multiple and actuators, the actuators have to
come to a consensus about the state of the environment so that they
can operate in concert. A vehicle would crash if some actuators caused
the vehicle to accelerate while other actuators applied brakes.  In
some applications, multiple agents have to elect a single leader.
There are many problems in which a collection of agents have to come
to aconsensusabout something.Consensus: Impossible with a
    faulty agentConsensus is impossible with even a single faulty agent. This was
    proved in a paper published by Fischer, Lynch and Patterson.You can get the idea of why consensus is not possible by
    considering the following problem in which
    when message delays are finite but arbitrarily long.
    A collection of 2N + 1 agents want to
    come to a consensus about a color. N of the agents pick blue and
    N+1 pick red. One of the red agents is arbitrarily slow. The
    2N non-slow agents exchange messages among each other, and each of
    these 2N agents gets N votes for red and N votes for blue. Agents
    decide to take a majority vote, and in the event of a tie pick
    blue.Fig.1: Problem with a slow agentHow long should they wait for the slow agent?Consider an algorithm in which agent waits until its local clock
    shows an elapsed time of T and then makes a decision based on the
    votes that it has.  An agent Y gets N red and N blue votes when
    its clock shows an elapsed time of T, and agent Y decides that the
    consensus is blue. Another agent Z has a slower clock and gets a
    red vote from the slow agent for a total of N+1 red votes, before
    Z's clock shows an elapsed time of T. So Z determines that the
    consensus is red. The algorithm fails because Y and Z have not
    come to a consensus.No algorithm is guaranteed to come to a consensus in finite time if
messages can be arbitrarily slow or if agents can be arbitrarily slow.
Systems with synchronized clocks don't have this particular
problem. We'll look at consensus in such systems later.Best Effort ConsensusThe theorem says that there is no algorithm that guarentees that
consensus can be reached in all scenarios; 
however, consensus can be reached in most practical situations. An idea
to overcome the counterexample given above is:
Agents keep trying repeatedly until they reach consensus. The theorem tells us
that the agents may have to keep trying for ever. We expect, however,
that in most practical situations their attempts will succeed at some point.What does keep trying mean? When does one trial
end and the next one begin? If agents use timeouts to end a trial,
then --- because clocks aren't synchronized --- the timeouts may complete
at different times.
We'll see that we can use the idea of time, even though clocks aren't
synchronized. We've done that before with logical clocks. The Paxos
algorithm shows how the idea of increasing values of timestamps (or
ids) are used for best-effort consensus.Central Ideas: ReviewMany applications of distributed systems require agents to come to a
consensus.
Agents cannot come to a consensus if an agent is faulty.Concepts:
Consensus -> impossibility with faulty agentsNextNext look atPaxosan important consensus
algorithm that may not terminate.
Later look atByzantine consensusandconsensus using block chain.K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology



Figure Link: https://kmchandy.github.io/Paxos/Slide01.jpg

Caption: Fig.1: Problem with a slow agent


You can get the idea of why consensus is not possible by
    considering the following problem in which
    when message delays are finite but arbitrarily long.
    A collection of 2N + 1 agents want to
    come to a consensus about a color. N of the agents pick blue and
    N+1 pick red. One of the red agents is arbitrarily slow. The
    2N non-slow agents exchange messages among each other, and each of
    these 2N agents gets N votes for red and N votes for blue. Agents
    decide to take a majority vote, and in the event of a tie pick
    blue.Fig.1: Problem with a slow agentHow long should they wait for the slow agent?Consider an algorithm in which agent waits until its local clock
    shows an elapsed time of T and then makes a decision based on the
    votes that it has.  An agent Y gets N red and N blue votes when
    its clock shows an elapsed time of T, and agent Y decides that the
    consensus is blue. Another agent Z has a slower clock and gets a
    red vote from the slow agent for a total of N+1 red votes, before
    Z's clock shows an elapsed time of T. So Z determines that the
    consensus is red. The algorithm fails because Y and Z have not
    come to a consensus.No algorithm is guaranteed to come to a consensus in finite time if
messages can be arbitrarily slow or if agents can be arbitrarily slow.
Systems with synchronized clocks don't have this particular
problem. We'll look at consensus in such systems later.Best Effort ConsensusThe theorem says that there is no algorithm that guarentees that
consensus can be reached in all scenarios; 
however, consensus can be reached in most practical situations. An idea
to overcome the counterexample given above is:
Agents keep trying repeatedly until they reach consensus. The theorem tells us
that the agents may have to keep trying for ever. We expect, however,
that in most practical situations their attempts will succeed at some point.What does keep trying mean? When does one trial
end and the next one begin? If agents use timeouts to end a trial,
then --- because clocks aren't synchronized --- the timeouts may complete
at different times.
We'll see that we can use the idea of time, even though clocks aren't
synchronized. We've done that before with logical clocks. The Paxos
algorithm shows how the idea of increasing values of timestamps (or
ids) are used for best-effort consensus.Central Ideas: ReviewMany applications of distributed systems require agents to come to a
consensus.
Agents cannot come to a consensus if an agent is faulty.Concepts:
Consensus -> impossibility with faulty agentsNextNext look atPaxosan important consensus
algorithm that may not terminate.
Later look atByzantine consensusandconsensus using block chain.K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology



Figure Link: https://kmchandy.github.io/Paxos/Slide01.jpg

Caption: Fig.1: Problem with a slow agent


Fig.1: Problem with a slow agentHow long should they wait for the slow agent?Consider an algorithm in which agent waits until its local clock
    shows an elapsed time of T and then makes a decision based on the
    votes that it has.  An agent Y gets N red and N blue votes when
    its clock shows an elapsed time of T, and agent Y decides that the
    consensus is blue. Another agent Z has a slower clock and gets a
    red vote from the slow agent for a total of N+1 red votes, before
    Z's clock shows an elapsed time of T. So Z determines that the
    consensus is red. The algorithm fails because Y and Z have not
    come to a consensus.No algorithm is guaranteed to come to a consensus in finite time if
messages can be arbitrarily slow or if agents can be arbitrarily slow.
Systems with synchronized clocks don't have this particular
problem. We'll look at consensus in such systems later.Best Effort ConsensusThe theorem says that there is no algorithm that guarentees that
consensus can be reached in all scenarios; 
however, consensus can be reached in most practical situations. An idea
to overcome the counterexample given above is:
Agents keep trying repeatedly until they reach consensus. The theorem tells us
that the agents may have to keep trying for ever. We expect, however,
that in most practical situations their attempts will succeed at some point.What does keep trying mean? When does one trial
end and the next one begin? If agents use timeouts to end a trial,
then --- because clocks aren't synchronized --- the timeouts may complete
at different times.
We'll see that we can use the idea of time, even though clocks aren't
synchronized. We've done that before with logical clocks. The Paxos
algorithm shows how the idea of increasing values of timestamps (or
ids) are used for best-effort consensus.Central Ideas: ReviewMany applications of distributed systems require agents to come to a
consensus.
Agents cannot come to a consensus if an agent is faulty.Concepts:
Consensus -> impossibility with faulty agentsNextNext look atPaxosan important consensus
algorithm that may not terminate.
Later look atByzantine consensusandconsensus using block chain.K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology



Figure Link: https://kmchandy.github.io/Paxos/Slide01.jpg

Caption: Fig.1: Problem with a slow agent


How long should they wait for the slow agent?Consider an algorithm in which agent waits until its local clock
    shows an elapsed time of T and then makes a decision based on the
    votes that it has.  An agent Y gets N red and N blue votes when
    its clock shows an elapsed time of T, and agent Y decides that the
    consensus is blue. Another agent Z has a slower clock and gets a
    red vote from the slow agent for a total of N+1 red votes, before
    Z's clock shows an elapsed time of T. So Z determines that the
    consensus is red. The algorithm fails because Y and Z have not
    come to a consensus.No algorithm is guaranteed to come to a consensus in finite time if
messages can be arbitrarily slow or if agents can be arbitrarily slow.
Systems with synchronized clocks don't have this particular
problem. We'll look at consensus in such systems later.Best Effort ConsensusThe theorem says that there is no algorithm that guarentees that
consensus can be reached in all scenarios; 
however, consensus can be reached in most practical situations. An idea
to overcome the counterexample given above is:
Agents keep trying repeatedly until they reach consensus. The theorem tells us
that the agents may have to keep trying for ever. We expect, however,
that in most practical situations their attempts will succeed at some point.What does keep trying mean? When does one trial
end and the next one begin? If agents use timeouts to end a trial,
then --- because clocks aren't synchronized --- the timeouts may complete
at different times.
We'll see that we can use the idea of time, even though clocks aren't
synchronized. We've done that before with logical clocks. The Paxos
algorithm shows how the idea of increasing values of timestamps (or
ids) are used for best-effort consensus.Central Ideas: ReviewMany applications of distributed systems require agents to come to a
consensus.
Agents cannot come to a consensus if an agent is faulty.Concepts:
Consensus -> impossibility with faulty agentsNextNext look atPaxosan important consensus
algorithm that may not terminate.
Later look atByzantine consensusandconsensus using block chain.K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

Consider an algorithm in which agent waits until its local clock
    shows an elapsed time of T and then makes a decision based on the
    votes that it has.  An agent Y gets N red and N blue votes when
    its clock shows an elapsed time of T, and agent Y decides that the
    consensus is blue. Another agent Z has a slower clock and gets a
    red vote from the slow agent for a total of N+1 red votes, before
    Z's clock shows an elapsed time of T. So Z determines that the
    consensus is red. The algorithm fails because Y and Z have not
    come to a consensus.No algorithm is guaranteed to come to a consensus in finite time if
messages can be arbitrarily slow or if agents can be arbitrarily slow.
Systems with synchronized clocks don't have this particular
problem. We'll look at consensus in such systems later.Best Effort ConsensusThe theorem says that there is no algorithm that guarentees that
consensus can be reached in all scenarios; 
however, consensus can be reached in most practical situations. An idea
to overcome the counterexample given above is:
Agents keep trying repeatedly until they reach consensus. The theorem tells us
that the agents may have to keep trying for ever. We expect, however,
that in most practical situations their attempts will succeed at some point.What does keep trying mean? When does one trial
end and the next one begin? If agents use timeouts to end a trial,
then --- because clocks aren't synchronized --- the timeouts may complete
at different times.
We'll see that we can use the idea of time, even though clocks aren't
synchronized. We've done that before with logical clocks. The Paxos
algorithm shows how the idea of increasing values of timestamps (or
ids) are used for best-effort consensus.Central Ideas: ReviewMany applications of distributed systems require agents to come to a
consensus.
Agents cannot come to a consensus if an agent is faulty.Concepts:
Consensus -> impossibility with faulty agentsNextNext look atPaxosan important consensus
algorithm that may not terminate.
Later look atByzantine consensusandconsensus using block chain.K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

No algorithm is guaranteed to come to a consensus in finite time if
messages can be arbitrarily slow or if agents can be arbitrarily slow.
Systems with synchronized clocks don't have this particular
problem. We'll look at consensus in such systems later.Best Effort ConsensusThe theorem says that there is no algorithm that guarentees that
consensus can be reached in all scenarios; 
however, consensus can be reached in most practical situations. An idea
to overcome the counterexample given above is:
Agents keep trying repeatedly until they reach consensus. The theorem tells us
that the agents may have to keep trying for ever. We expect, however,
that in most practical situations their attempts will succeed at some point.What does keep trying mean? When does one trial
end and the next one begin? If agents use timeouts to end a trial,
then --- because clocks aren't synchronized --- the timeouts may complete
at different times.
We'll see that we can use the idea of time, even though clocks aren't
synchronized. We've done that before with logical clocks. The Paxos
algorithm shows how the idea of increasing values of timestamps (or
ids) are used for best-effort consensus.Central Ideas: ReviewMany applications of distributed systems require agents to come to a
consensus.
Agents cannot come to a consensus if an agent is faulty.Concepts:
Consensus -> impossibility with faulty agentsNextNext look atPaxosan important consensus
algorithm that may not terminate.
Later look atByzantine consensusandconsensus using block chain.K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

What does keep trying mean? When does one trial
end and the next one begin? If agents use timeouts to end a trial,
then --- because clocks aren't synchronized --- the timeouts may complete
at different times.
We'll see that we can use the idea of time, even though clocks aren't
synchronized. We've done that before with logical clocks. The Paxos
algorithm shows how the idea of increasing values of timestamps (or
ids) are used for best-effort consensus.Central Ideas: ReviewMany applications of distributed systems require agents to come to a
consensus.
Agents cannot come to a consensus if an agent is faulty.Concepts:
Consensus -> impossibility with faulty agentsNextNext look atPaxosan important consensus
algorithm that may not terminate.
Later look atByzantine consensusandconsensus using block chain.K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

Many applications of distributed systems require agents to come to a
consensus.
Agents cannot come to a consensus if an agent is faulty.Concepts:
Consensus -> impossibility with faulty agentsNextNext look atPaxosan important consensus
algorithm that may not terminate.
Later look atByzantine consensusandconsensus using block chain.K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

Concepts:
Consensus -> impossibility with faulty agentsNextNext look atPaxosan important consensus
algorithm that may not terminate.
Later look atByzantine consensusandconsensus using block chain.K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology
--- END Content from https://kmchandy.github.io/Paxos/ConsensusImpossible.html ---


--- START Content from https://kmchandy.github.io/Paxos/ReadWriteLossyChannels.html ---
TITLE: Paxos/ReadWriteLossyChannels.html

In this webpage we develop algorithms for systems in which agents
  may halt or be arbitrarily slow, the same message may be
  delivered multiple times,
  messages may be delivered out of order, and messages
  may be lost.
  A transaction consists of two steps: a client reads and the writes
  shared variables.
  This page shows how a proxy for time can be used to develop
  algorithms in which computations can be serialized: each agent
  starts and completes a transaction before executing steps of another
  transaction.OverviewIn this webpage we develop algorithms for systems in which agents
  may halt or be arbitrarily slow, the same message may be
  delivered multiple times,
  messages may be delivered out of order, and messages
  may be lost.A system consists of a set of agents calledclientsand a
  set of agents calledservers.
  There is a channel from each server to each client, and from each
  client to each server.Clients execute transactions in which they send requests to servers
  to read and then write variables managed by servers.
  We describe an algorithm in which computations are serializable
  which allows us to treat the sequences of steps at each agent as
  though exactly one transaction is executed at a time.ServersEach serverqhas a variableq.v.
Clients send requests toqto read or writeq.v.
The only actions of a server are to respond to requests from clients.A serverqreplies to a read request from a clientpby sendingpa copy ofq.v.
A write request includes the valuevto be
written.
A serverqassignsvtoq.vwhenqreceives a write request containing valuev.ClientsEach client receives a sequence ofclock tickmessages.
The intervals between successive clock ticks are irrelevant for
the correctness of the algorithm.
The intervals do, however, impact performance.
There are many ways of generating clock tick messages and we
postpone discussion of them.A read request and a reply to the request may get lost or be delayed for an arbitrary
time. 
A client avoids waiting forever for a reply by only
accepting replies that the client receives before it receives its next
clock tick message.
Replies that arrive after the next clock tick are treated as lost.A transaction consists of a read step and a write step.Read Step of a TransactionA clientpsends read requests to all servers.
Some requests may be lost.
A server that receives a read request frompsends a
reply top.
Some replies may be lost.
LetRbe the set of servers from whichpreceives replies beforepreceives its next clock tick.IfRhasMor more elements, whereMis a given constant, thenpproceeds to the
write step.
IfRhas fewer thanMelements, then the
transaction terminates without executing the write step.Write Step of a TransactionThe list of replies thatpgets beforepreceives its next clock tick is:[q.v for q in R]Clientpsends a request to all servers to write:p.f(q.v for q in R)wherep.fis a function ofp.Write requests may get lost.
LetWbe the set of servers that receive write requests.A transaction consists of the step in which a clientpbroadcasts read requests to all servers, the steps in which servers
receive read requests and send replies, the steps in whichpreceives replies, the step in whichpsends write requests, and the steps in which servers
receive and execute write requests.Changes to server variables in a transaction are specified by the
following statement in whichp.fis a function executed
byp.Transactionif len(R) >= M:
   for q in W: q.v = p.f(q.v for q in R)Transactions are tailored to a specific application by specifyingMandp.f.
The selections ofRandWare
nondeterministic and they can be arbitrary sets of servers.Transactions executed concurrently by multiple clients can interfere
with each other as illustrated by the following example.ExampleLetxbe the amount of funds in an account and 
  assume thatxhas $110.
  
  Consider a computation in which clientspandp'both execute identical transactions concurrently.
In a transaction a client readsxand ifxhas at least $100 then the client transfers $100 out
ofxto an accounty.Consider a computation in which both clientspandp'readxand verify thatxhas at least $100,
and then both clients transfer $100 out ofxand set the amount
  inxto $10.
The computation transfers $200 out ofxbut 
debitsxby only $100.This situation does not occur if only one client executes a
transaction at a time.
Ifpexecutes its transaction first then whenpcompletes the transaction it sets the amount inxto $10. 
Ifp'executes its transaction next thenp'finds thatxhas only $10 and sop'does not
transfer $100 fromx.Serializable ComputationsA system executes multiple transactions concurrently.A computation \(x\) is serializableexactly when there exists a
sequence of transactions \(Z_{i}, i \geq 0\), such that in  \(x\):
For all agents \(v\), for all \(i\):all steps of \(v\) in \(Z_{i}\) occur before any step
of \(v\) in \(Z_{i+1}\).It is possible that in a serializable computation \(x\), an 
agent \(u\) takes a step in \(Z_{j}\) before adifferentagent
\(v\) takes a step in \(Z_{i}\), where \(j > i\).Transactions and Serializability in
DatabasesThere is an extensive literature ontransactions and serializability.Also seetransaction processing systemsandonline transaction processing.
In this webpage we provide a narrow definition of transactions and
serializability
that is adequate for describing algorithms such asPaxos.The Problem: Guarantee SerializabilityThe problem is to develop a distributed algorithm, with multiple
clients and servers, in which computations are serializable, and in
which the system may be faulty.How Should You Solve The Problem?What method comes to mind to partition the sequence of steps at an
agent into intervals where the agent completes all steps of exactly one
transaction in each interval?Logical time partitions computations into a past and a future.
Let's use a mechanism, similar to logical time.
As with logical time, each step \(e\) in a computation is assigned a
value \(t(e)\), called theepochof \(e\).
For any step \(e\), steps with epochs less than, equal to, or greater
than, \(t(e)\) are in the past,current, and future,
respectively.
We specify epochs such that thesteps in 
current consists of all the steps of exactly one
transaction.Epochs and Logical TimesRecall thatthe logical timeof a step \(e\) is a value \(t(e)\) assigned to
each step \(e\) in a computation such 
that for all edges \((e, e')\) of the dataflow graph:
\(t(e) < t(e')\).
Theepochof a step \(e\) in a computation is defined as a
value \(t(e)\) assigned to 
each step \(e\) in the computation such 
that for all edges \((e, e')\) of the dataflow graph:
\(t(e) \leq t(e')\).Rules for Assigning Epochs to StepsThe following rules ensure that the assignment \(t(e)\) to each step
\(e\) in a computation satisfies the specification for epochs. For all
steps \((e, e')\) of a computationfor all steps \(e\) and \(e'\) at the same agent: if \(e'\) occurs
  after \(e\) then \(t(e) \leq t(e')\), andif \(e\) is a step in which a message is sent and \(e'\) is a step
  in which that message is received then \(t(e) \leq t(e')\).ObservationIf \(e\) and \(e'\) are steps at the same agent and \(t(e) < t(e')\)
then \(e'\) occurs after \(e\).Theorem: A Sufficent Condition for SerializabilityA sufficient conditions for a computation to be serializable is:There exists epochs for all steps of a computation such thateach transaction has a unique epoch, andall steps in a transaction have the epoch of the transaction.If a computation \(x\) satisfies this condition then 
let the epochs of transactions in \(x\) be \([T_{0}, T_{1}, T_{2}, \ldots]\)
where \(T_{i} < T_{i+1}\), and let \(Z_{i}\) be the transaction with
epoch \(T_{i}\).
Then in \(x\), for all \(i\), for all agents \(v\):All steps of \(v\) in \(Z_{i}\) occur before any step
of \(v\) in \(Z_{i+1}\).ProofAssume that the sufficient condition is
satisfied for epochs of steps in a computation. 
Let \(e\) and \(e'\) be steps in transactions with epoch \(T\) and
\(T'\), respectively, and let \(T < T'\).
Let \(e\) and \(e'\) be steps at the same agent \(v\).
From the observation regarding epochs: \(e\) occurs before \(e'\).
Therefore all steps of a transaction with epoch \(T_{i}\) occur before
any step of a transaction with epoch \(T_{i+1}\).An Algorithm Based on the Sufficent ConditionNext, we describe an algorithm based on this idea.
Let's consider the two issues posed by the condition for serializability.
(1) How can the algorithm assign a unique epoch to each transaction?
(2) How can the algorithm assign epochs to steps so that
all steps in a transaction have the epoch of the transaction?Uniqueness of a Transaction's EpochA clientpinitiates a new transaction whenpgets a clock tick message.
To ensure that the epoch of the transaction is unique, an  epochtis a pair(n, p_id)wherenis a
number andp_idis the id of clientp.
Transactions initiated by different clients have different epochs
because their client ids are different.
A client sets the epoch of a new transaction that it initiates to be
greater than epochs of all previous transactions that it initiated.
So, different transactions initiated by the same client have different
epochs.
Therefore each epoch is unique.Client ids are totally ordered, and so epochs are also totally
ordered.
For brevity we refer to an epoch by a single valuetrather than a
pair(n, p_id).All Steps in a Transaction have the
Transaction's EpochWe associate a fieldtwith each agent -- client or
server -- wheretis the epoch of the step of the transaction that the agent is
executing.
Likewise, we associate a fieldm.twith each messagembetween clients and servers
wherem.tis the epoch of the step in which the message
is sent.A message sent in a step of a transaction is received in a step of the
same transaction.
So, for a messagembetween clients and servers,m.tis also the epoch of the step in which the message is received.We design the algorithm so that all steps of a transaction, all
messages sent in steps of the transaction, and all messages received
in steps of the transaction have the epoch of the transaction.Next we give the algorithm for clients and then for servers.Algorithm for a ClientWhen a clientpgets a clock tick message it executesp.t = p.t + pos()whereposreturns a positive value, and 
thenpinitiates a new transaction with epochp.t.
Clientpcontinues executing the transaction with epochp.tuntilpgets its next
clock tick message at which point it increasesp.tand
starts a new transaction.In the algorithm
all steps ofp, all requests sent bypto clients, and all
replies received bypfrom clients, have epochp.t.# Initialization
p.t = 0
start()
def receive(message, sender):
   if isinstance(message, ClockTick):
      # Received clock tick
      p.t = p.t + pos()
      # Start new transaction with epoch p.t
      # p.copy stores values in replies.
      p.copy = {}
      # Broadcast read request with epoch p.t
      for q in Q:  send(ReadRequest(p.t), q)
   else:
      # received a reply to a read request
      if message.t == p.t: 
         p.copy[sender] = message.v
         # send write requests if M replies received
         if len(p.copy) >= M:
            # Broadcast write request: value v, epoch t
            v, t = p.f(), p.t
            for q in Q: send(WriteRequest(v, t), q)Algorithm for a ServerA server waits to get requests from clients.When a serverqreceives a requestrfrom
the transaction that the server 
is currently processing i.e.r.t\(=\)q.t,
then the server responds to the request.When a serverqreceives a requestrfrom a
transaction with a smaller 
epoch than the transaction that the server is currently processing,
i.e.r.t\(<\)q.t, then the server does not
respond to the request. The request has the same effect as a request that
is lost.What should serverqdo when it receives a requestrfrom a transaction with a larger epoch than the
server's epoch, i.e.r.t\(>\)q.t?In this case, the server increases its epoch to the
epoch of the request and then responds to the request.
The request, the step in which the server responds to the request,
and the reply (to a read request), have the same epoch.# initialization
q.v, q.t = init, 0
start()
def receive(request, client):
   if request.t >= q.t
      q.t = request.t
      if isinstance(request, ReadRequest):
         send(Reply(q.v, q.t), client)
      else:
         // message is a WriteRequest
         q.v = request.vCorrectnessNext we prove that all computations are serializable in a system
specified by these algorithms for clients and servers.Epoch of a step in a computation of the algorithmThe epoch of a step taken by a clientpor serverqis the value ofp.torq.t(respectively) upon completion of
the step.TheoremThe assignment of epochs to steps satisfies the
condition for epochs, i.e.,
For all
steps \((e, e')\) of a computationif \(e\) and \(e'\) are steps at the same agent, and \(e'\) occurs
  after \(e\) then \(t(e) \leq t(e')\), andif \(e\) is a step in which a message is sent and \(e'\) is a step
  in which that message is received then \(t(e) \leq t(e')\).ProofFrom the algorithms for serverpand clientqit follows thatp.tandq.tdo not decrease in a
computation.
Therefore the first condition is satisfied.Also, from the algorithm, the epoch for step in which a request or reply is
received is equal to the epoch in which the message is sent.
So, the second condition is also satisfied.
Therefore the assignment of epochs to steps of a computation satisfy
the specification of epochs.TheoremThe sufficient condition for serializability is satisfied:
(1) each transaction has a unique epoch, and (2) all steps in a
transaction have the epoch of the transaction.ProofWe have shown that each transaction initiated by a clientphas a unique epochp.t.All steps atphave epochp.t.
Requests sent byphave epochp.t.
When a clientqgets a request with epochp.t, the client  sets its own epoch top.tand sends a reply with the same epoch.
So, all steps in a transaction have the epoch of the transaction.
Therefore the algorithm satisfies the sufficient condition for
serializability.CorollaryThe algorithm satisfies specifications for serializability.ProofFollows because the algorithm satisfies the sufficient condition for
serializability.Therefore,
if the epochs of transactions in a computation \(x\) are \([T_{0}, T_{1}, T_{2}, \ldots]\)
where \(T_{i} < T_{i+1}\), then in \(x\), for all \(i\), for all agents \(v\):
All steps of \(v\) in the transaction with epoch \(T_{i}\) occur before any step
of \(v\) in the transaction with epoch \(T_{i+1}\).Equivalence of Distributed
and Sequential AlgorithmsLet the sequence of increasing epochs in a computation \(x\) of the distributed
algorithm beT[1], T[2], T[3], ..., and letp[i], R[i], W[i]be the values ofp, R, Win
the transaction with epochT[i].The sequence of values of agent variables in 
computation \(x\) of the distributed algorithm is the same as in the
following nondeterministic sequential program by selectingp, R, W, on thei-th iteration to bep[i], R[i], W[i]respectively, and by selectingdeltaon thei-th iteration so thatT[i] = T[i-1] + deltaEquivalent nondeterministic sequential algorithmt = 0
while True:
   select positive delta, select client p
   t = t + delta
   # p executes transaction with epoch t
   p.t = t
   p.copy = {}
   # Read step
   select R
   for q in R: p.copy[q] = q.v
   # Write step
   if len(p.copy) >= M:
     select W
     for q in W: q.v = p.f()We will prove properties of the sequential algorithm and show that
these properties also hold for the distributed algorithm.NextNext we develop Paxos, a distributed consensus algorithm.
We prove
that Paxos satisfies the specifications for consensus by showing that
Paxos is serializable.
We prove properties of a nondeterministic sequential algorithm and
show that properties of the sequential algorithm also hold for Paxos.K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

A system consists of a set of agents calledclientsand a
  set of agents calledservers.
  There is a channel from each server to each client, and from each
  client to each server.Clients execute transactions in which they send requests to servers
  to read and then write variables managed by servers.
  We describe an algorithm in which computations are serializable
  which allows us to treat the sequences of steps at each agent as
  though exactly one transaction is executed at a time.ServersEach serverqhas a variableq.v.
Clients send requests toqto read or writeq.v.
The only actions of a server are to respond to requests from clients.A serverqreplies to a read request from a clientpby sendingpa copy ofq.v.
A write request includes the valuevto be
written.
A serverqassignsvtoq.vwhenqreceives a write request containing valuev.ClientsEach client receives a sequence ofclock tickmessages.
The intervals between successive clock ticks are irrelevant for
the correctness of the algorithm.
The intervals do, however, impact performance.
There are many ways of generating clock tick messages and we
postpone discussion of them.A read request and a reply to the request may get lost or be delayed for an arbitrary
time. 
A client avoids waiting forever for a reply by only
accepting replies that the client receives before it receives its next
clock tick message.
Replies that arrive after the next clock tick are treated as lost.A transaction consists of a read step and a write step.Read Step of a TransactionA clientpsends read requests to all servers.
Some requests may be lost.
A server that receives a read request frompsends a
reply top.
Some replies may be lost.
LetRbe the set of servers from whichpreceives replies beforepreceives its next clock tick.IfRhasMor more elements, whereMis a given constant, thenpproceeds to the
write step.
IfRhas fewer thanMelements, then the
transaction terminates without executing the write step.Write Step of a TransactionThe list of replies thatpgets beforepreceives its next clock tick is:[q.v for q in R]Clientpsends a request to all servers to write:p.f(q.v for q in R)wherep.fis a function ofp.Write requests may get lost.
LetWbe the set of servers that receive write requests.A transaction consists of the step in which a clientpbroadcasts read requests to all servers, the steps in which servers
receive read requests and send replies, the steps in whichpreceives replies, the step in whichpsends write requests, and the steps in which servers
receive and execute write requests.Changes to server variables in a transaction are specified by the
following statement in whichp.fis a function executed
byp.Transactionif len(R) >= M:
   for q in W: q.v = p.f(q.v for q in R)Transactions are tailored to a specific application by specifyingMandp.f.
The selections ofRandWare
nondeterministic and they can be arbitrary sets of servers.Transactions executed concurrently by multiple clients can interfere
with each other as illustrated by the following example.ExampleLetxbe the amount of funds in an account and 
  assume thatxhas $110.
  
  Consider a computation in which clientspandp'both execute identical transactions concurrently.
In a transaction a client readsxand ifxhas at least $100 then the client transfers $100 out
ofxto an accounty.Consider a computation in which both clientspandp'readxand verify thatxhas at least $100,
and then both clients transfer $100 out ofxand set the amount
  inxto $10.
The computation transfers $200 out ofxbut 
debitsxby only $100.This situation does not occur if only one client executes a
transaction at a time.
Ifpexecutes its transaction first then whenpcompletes the transaction it sets the amount inxto $10. 
Ifp'executes its transaction next thenp'finds thatxhas only $10 and sop'does not
transfer $100 fromx.Serializable ComputationsA system executes multiple transactions concurrently.A computation \(x\) is serializableexactly when there exists a
sequence of transactions \(Z_{i}, i \geq 0\), such that in  \(x\):
For all agents \(v\), for all \(i\):all steps of \(v\) in \(Z_{i}\) occur before any step
of \(v\) in \(Z_{i+1}\).It is possible that in a serializable computation \(x\), an 
agent \(u\) takes a step in \(Z_{j}\) before adifferentagent
\(v\) takes a step in \(Z_{i}\), where \(j > i\).Transactions and Serializability in
DatabasesThere is an extensive literature ontransactions and serializability.Also seetransaction processing systemsandonline transaction processing.
In this webpage we provide a narrow definition of transactions and
serializability
that is adequate for describing algorithms such asPaxos.The Problem: Guarantee SerializabilityThe problem is to develop a distributed algorithm, with multiple
clients and servers, in which computations are serializable, and in
which the system may be faulty.How Should You Solve The Problem?What method comes to mind to partition the sequence of steps at an
agent into intervals where the agent completes all steps of exactly one
transaction in each interval?Logical time partitions computations into a past and a future.
Let's use a mechanism, similar to logical time.
As with logical time, each step \(e\) in a computation is assigned a
value \(t(e)\), called theepochof \(e\).
For any step \(e\), steps with epochs less than, equal to, or greater
than, \(t(e)\) are in the past,current, and future,
respectively.
We specify epochs such that thesteps in 
current consists of all the steps of exactly one
transaction.Epochs and Logical TimesRecall thatthe logical timeof a step \(e\) is a value \(t(e)\) assigned to
each step \(e\) in a computation such 
that for all edges \((e, e')\) of the dataflow graph:
\(t(e) < t(e')\).
Theepochof a step \(e\) in a computation is defined as a
value \(t(e)\) assigned to 
each step \(e\) in the computation such 
that for all edges \((e, e')\) of the dataflow graph:
\(t(e) \leq t(e')\).Rules for Assigning Epochs to StepsThe following rules ensure that the assignment \(t(e)\) to each step
\(e\) in a computation satisfies the specification for epochs. For all
steps \((e, e')\) of a computationfor all steps \(e\) and \(e'\) at the same agent: if \(e'\) occurs
  after \(e\) then \(t(e) \leq t(e')\), andif \(e\) is a step in which a message is sent and \(e'\) is a step
  in which that message is received then \(t(e) \leq t(e')\).ObservationIf \(e\) and \(e'\) are steps at the same agent and \(t(e) < t(e')\)
then \(e'\) occurs after \(e\).Theorem: A Sufficent Condition for SerializabilityA sufficient conditions for a computation to be serializable is:There exists epochs for all steps of a computation such thateach transaction has a unique epoch, andall steps in a transaction have the epoch of the transaction.If a computation \(x\) satisfies this condition then 
let the epochs of transactions in \(x\) be \([T_{0}, T_{1}, T_{2}, \ldots]\)
where \(T_{i} < T_{i+1}\), and let \(Z_{i}\) be the transaction with
epoch \(T_{i}\).
Then in \(x\), for all \(i\), for all agents \(v\):All steps of \(v\) in \(Z_{i}\) occur before any step
of \(v\) in \(Z_{i+1}\).ProofAssume that the sufficient condition is
satisfied for epochs of steps in a computation. 
Let \(e\) and \(e'\) be steps in transactions with epoch \(T\) and
\(T'\), respectively, and let \(T < T'\).
Let \(e\) and \(e'\) be steps at the same agent \(v\).
From the observation regarding epochs: \(e\) occurs before \(e'\).
Therefore all steps of a transaction with epoch \(T_{i}\) occur before
any step of a transaction with epoch \(T_{i+1}\).An Algorithm Based on the Sufficent ConditionNext, we describe an algorithm based on this idea.
Let's consider the two issues posed by the condition for serializability.
(1) How can the algorithm assign a unique epoch to each transaction?
(2) How can the algorithm assign epochs to steps so that
all steps in a transaction have the epoch of the transaction?Uniqueness of a Transaction's EpochA clientpinitiates a new transaction whenpgets a clock tick message.
To ensure that the epoch of the transaction is unique, an  epochtis a pair(n, p_id)wherenis a
number andp_idis the id of clientp.
Transactions initiated by different clients have different epochs
because their client ids are different.
A client sets the epoch of a new transaction that it initiates to be
greater than epochs of all previous transactions that it initiated.
So, different transactions initiated by the same client have different
epochs.
Therefore each epoch is unique.Client ids are totally ordered, and so epochs are also totally
ordered.
For brevity we refer to an epoch by a single valuetrather than a
pair(n, p_id).All Steps in a Transaction have the
Transaction's EpochWe associate a fieldtwith each agent -- client or
server -- wheretis the epoch of the step of the transaction that the agent is
executing.
Likewise, we associate a fieldm.twith each messagembetween clients and servers
wherem.tis the epoch of the step in which the message
is sent.A message sent in a step of a transaction is received in a step of the
same transaction.
So, for a messagembetween clients and servers,m.tis also the epoch of the step in which the message is received.We design the algorithm so that all steps of a transaction, all
messages sent in steps of the transaction, and all messages received
in steps of the transaction have the epoch of the transaction.Next we give the algorithm for clients and then for servers.Algorithm for a ClientWhen a clientpgets a clock tick message it executesp.t = p.t + pos()whereposreturns a positive value, and 
thenpinitiates a new transaction with epochp.t.
Clientpcontinues executing the transaction with epochp.tuntilpgets its next
clock tick message at which point it increasesp.tand
starts a new transaction.In the algorithm
all steps ofp, all requests sent bypto clients, and all
replies received bypfrom clients, have epochp.t.# Initialization
p.t = 0
start()
def receive(message, sender):
   if isinstance(message, ClockTick):
      # Received clock tick
      p.t = p.t + pos()
      # Start new transaction with epoch p.t
      # p.copy stores values in replies.
      p.copy = {}
      # Broadcast read request with epoch p.t
      for q in Q:  send(ReadRequest(p.t), q)
   else:
      # received a reply to a read request
      if message.t == p.t: 
         p.copy[sender] = message.v
         # send write requests if M replies received
         if len(p.copy) >= M:
            # Broadcast write request: value v, epoch t
            v, t = p.f(), p.t
            for q in Q: send(WriteRequest(v, t), q)Algorithm for a ServerA server waits to get requests from clients.When a serverqreceives a requestrfrom
the transaction that the server 
is currently processing i.e.r.t\(=\)q.t,
then the server responds to the request.When a serverqreceives a requestrfrom a
transaction with a smaller 
epoch than the transaction that the server is currently processing,
i.e.r.t\(<\)q.t, then the server does not
respond to the request. The request has the same effect as a request that
is lost.What should serverqdo when it receives a requestrfrom a transaction with a larger epoch than the
server's epoch, i.e.r.t\(>\)q.t?In this case, the server increases its epoch to the
epoch of the request and then responds to the request.
The request, the step in which the server responds to the request,
and the reply (to a read request), have the same epoch.# initialization
q.v, q.t = init, 0
start()
def receive(request, client):
   if request.t >= q.t
      q.t = request.t
      if isinstance(request, ReadRequest):
         send(Reply(q.v, q.t), client)
      else:
         // message is a WriteRequest
         q.v = request.vCorrectnessNext we prove that all computations are serializable in a system
specified by these algorithms for clients and servers.Epoch of a step in a computation of the algorithmThe epoch of a step taken by a clientpor serverqis the value ofp.torq.t(respectively) upon completion of
the step.TheoremThe assignment of epochs to steps satisfies the
condition for epochs, i.e.,
For all
steps \((e, e')\) of a computationif \(e\) and \(e'\) are steps at the same agent, and \(e'\) occurs
  after \(e\) then \(t(e) \leq t(e')\), andif \(e\) is a step in which a message is sent and \(e'\) is a step
  in which that message is received then \(t(e) \leq t(e')\).ProofFrom the algorithms for serverpand clientqit follows thatp.tandq.tdo not decrease in a
computation.
Therefore the first condition is satisfied.Also, from the algorithm, the epoch for step in which a request or reply is
received is equal to the epoch in which the message is sent.
So, the second condition is also satisfied.
Therefore the assignment of epochs to steps of a computation satisfy
the specification of epochs.TheoremThe sufficient condition for serializability is satisfied:
(1) each transaction has a unique epoch, and (2) all steps in a
transaction have the epoch of the transaction.ProofWe have shown that each transaction initiated by a clientphas a unique epochp.t.All steps atphave epochp.t.
Requests sent byphave epochp.t.
When a clientqgets a request with epochp.t, the client  sets its own epoch top.tand sends a reply with the same epoch.
So, all steps in a transaction have the epoch of the transaction.
Therefore the algorithm satisfies the sufficient condition for
serializability.CorollaryThe algorithm satisfies specifications for serializability.ProofFollows because the algorithm satisfies the sufficient condition for
serializability.Therefore,
if the epochs of transactions in a computation \(x\) are \([T_{0}, T_{1}, T_{2}, \ldots]\)
where \(T_{i} < T_{i+1}\), then in \(x\), for all \(i\), for all agents \(v\):
All steps of \(v\) in the transaction with epoch \(T_{i}\) occur before any step
of \(v\) in the transaction with epoch \(T_{i+1}\).Equivalence of Distributed
and Sequential AlgorithmsLet the sequence of increasing epochs in a computation \(x\) of the distributed
algorithm beT[1], T[2], T[3], ..., and letp[i], R[i], W[i]be the values ofp, R, Win
the transaction with epochT[i].The sequence of values of agent variables in 
computation \(x\) of the distributed algorithm is the same as in the
following nondeterministic sequential program by selectingp, R, W, on thei-th iteration to bep[i], R[i], W[i]respectively, and by selectingdeltaon thei-th iteration so thatT[i] = T[i-1] + deltaEquivalent nondeterministic sequential algorithmt = 0
while True:
   select positive delta, select client p
   t = t + delta
   # p executes transaction with epoch t
   p.t = t
   p.copy = {}
   # Read step
   select R
   for q in R: p.copy[q] = q.v
   # Write step
   if len(p.copy) >= M:
     select W
     for q in W: q.v = p.f()We will prove properties of the sequential algorithm and show that
these properties also hold for the distributed algorithm.NextNext we develop Paxos, a distributed consensus algorithm.
We prove
that Paxos satisfies the specifications for consensus by showing that
Paxos is serializable.
We prove properties of a nondeterministic sequential algorithm and
show that properties of the sequential algorithm also hold for Paxos.K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

Clients execute transactions in which they send requests to servers
  to read and then write variables managed by servers.
  We describe an algorithm in which computations are serializable
  which allows us to treat the sequences of steps at each agent as
  though exactly one transaction is executed at a time.ServersEach serverqhas a variableq.v.
Clients send requests toqto read or writeq.v.
The only actions of a server are to respond to requests from clients.A serverqreplies to a read request from a clientpby sendingpa copy ofq.v.
A write request includes the valuevto be
written.
A serverqassignsvtoq.vwhenqreceives a write request containing valuev.ClientsEach client receives a sequence ofclock tickmessages.
The intervals between successive clock ticks are irrelevant for
the correctness of the algorithm.
The intervals do, however, impact performance.
There are many ways of generating clock tick messages and we
postpone discussion of them.A read request and a reply to the request may get lost or be delayed for an arbitrary
time. 
A client avoids waiting forever for a reply by only
accepting replies that the client receives before it receives its next
clock tick message.
Replies that arrive after the next clock tick are treated as lost.A transaction consists of a read step and a write step.Read Step of a TransactionA clientpsends read requests to all servers.
Some requests may be lost.
A server that receives a read request frompsends a
reply top.
Some replies may be lost.
LetRbe the set of servers from whichpreceives replies beforepreceives its next clock tick.IfRhasMor more elements, whereMis a given constant, thenpproceeds to the
write step.
IfRhas fewer thanMelements, then the
transaction terminates without executing the write step.Write Step of a TransactionThe list of replies thatpgets beforepreceives its next clock tick is:[q.v for q in R]Clientpsends a request to all servers to write:p.f(q.v for q in R)wherep.fis a function ofp.Write requests may get lost.
LetWbe the set of servers that receive write requests.A transaction consists of the step in which a clientpbroadcasts read requests to all servers, the steps in which servers
receive read requests and send replies, the steps in whichpreceives replies, the step in whichpsends write requests, and the steps in which servers
receive and execute write requests.Changes to server variables in a transaction are specified by the
following statement in whichp.fis a function executed
byp.Transactionif len(R) >= M:
   for q in W: q.v = p.f(q.v for q in R)Transactions are tailored to a specific application by specifyingMandp.f.
The selections ofRandWare
nondeterministic and they can be arbitrary sets of servers.Transactions executed concurrently by multiple clients can interfere
with each other as illustrated by the following example.ExampleLetxbe the amount of funds in an account and 
  assume thatxhas $110.
  
  Consider a computation in which clientspandp'both execute identical transactions concurrently.
In a transaction a client readsxand ifxhas at least $100 then the client transfers $100 out
ofxto an accounty.Consider a computation in which both clientspandp'readxand verify thatxhas at least $100,
and then both clients transfer $100 out ofxand set the amount
  inxto $10.
The computation transfers $200 out ofxbut 
debitsxby only $100.This situation does not occur if only one client executes a
transaction at a time.
Ifpexecutes its transaction first then whenpcompletes the transaction it sets the amount inxto $10. 
Ifp'executes its transaction next thenp'finds thatxhas only $10 and sop'does not
transfer $100 fromx.Serializable ComputationsA system executes multiple transactions concurrently.A computation \(x\) is serializableexactly when there exists a
sequence of transactions \(Z_{i}, i \geq 0\), such that in  \(x\):
For all agents \(v\), for all \(i\):all steps of \(v\) in \(Z_{i}\) occur before any step
of \(v\) in \(Z_{i+1}\).It is possible that in a serializable computation \(x\), an 
agent \(u\) takes a step in \(Z_{j}\) before adifferentagent
\(v\) takes a step in \(Z_{i}\), where \(j > i\).Transactions and Serializability in
DatabasesThere is an extensive literature ontransactions and serializability.Also seetransaction processing systemsandonline transaction processing.
In this webpage we provide a narrow definition of transactions and
serializability
that is adequate for describing algorithms such asPaxos.The Problem: Guarantee SerializabilityThe problem is to develop a distributed algorithm, with multiple
clients and servers, in which computations are serializable, and in
which the system may be faulty.How Should You Solve The Problem?What method comes to mind to partition the sequence of steps at an
agent into intervals where the agent completes all steps of exactly one
transaction in each interval?Logical time partitions computations into a past and a future.
Let's use a mechanism, similar to logical time.
As with logical time, each step \(e\) in a computation is assigned a
value \(t(e)\), called theepochof \(e\).
For any step \(e\), steps with epochs less than, equal to, or greater
than, \(t(e)\) are in the past,current, and future,
respectively.
We specify epochs such that thesteps in 
current consists of all the steps of exactly one
transaction.Epochs and Logical TimesRecall thatthe logical timeof a step \(e\) is a value \(t(e)\) assigned to
each step \(e\) in a computation such 
that for all edges \((e, e')\) of the dataflow graph:
\(t(e) < t(e')\).
Theepochof a step \(e\) in a computation is defined as a
value \(t(e)\) assigned to 
each step \(e\) in the computation such 
that for all edges \((e, e')\) of the dataflow graph:
\(t(e) \leq t(e')\).Rules for Assigning Epochs to StepsThe following rules ensure that the assignment \(t(e)\) to each step
\(e\) in a computation satisfies the specification for epochs. For all
steps \((e, e')\) of a computationfor all steps \(e\) and \(e'\) at the same agent: if \(e'\) occurs
  after \(e\) then \(t(e) \leq t(e')\), andif \(e\) is a step in which a message is sent and \(e'\) is a step
  in which that message is received then \(t(e) \leq t(e')\).ObservationIf \(e\) and \(e'\) are steps at the same agent and \(t(e) < t(e')\)
then \(e'\) occurs after \(e\).Theorem: A Sufficent Condition for SerializabilityA sufficient conditions for a computation to be serializable is:There exists epochs for all steps of a computation such thateach transaction has a unique epoch, andall steps in a transaction have the epoch of the transaction.If a computation \(x\) satisfies this condition then 
let the epochs of transactions in \(x\) be \([T_{0}, T_{1}, T_{2}, \ldots]\)
where \(T_{i} < T_{i+1}\), and let \(Z_{i}\) be the transaction with
epoch \(T_{i}\).
Then in \(x\), for all \(i\), for all agents \(v\):All steps of \(v\) in \(Z_{i}\) occur before any step
of \(v\) in \(Z_{i+1}\).ProofAssume that the sufficient condition is
satisfied for epochs of steps in a computation. 
Let \(e\) and \(e'\) be steps in transactions with epoch \(T\) and
\(T'\), respectively, and let \(T < T'\).
Let \(e\) and \(e'\) be steps at the same agent \(v\).
From the observation regarding epochs: \(e\) occurs before \(e'\).
Therefore all steps of a transaction with epoch \(T_{i}\) occur before
any step of a transaction with epoch \(T_{i+1}\).An Algorithm Based on the Sufficent ConditionNext, we describe an algorithm based on this idea.
Let's consider the two issues posed by the condition for serializability.
(1) How can the algorithm assign a unique epoch to each transaction?
(2) How can the algorithm assign epochs to steps so that
all steps in a transaction have the epoch of the transaction?Uniqueness of a Transaction's EpochA clientpinitiates a new transaction whenpgets a clock tick message.
To ensure that the epoch of the transaction is unique, an  epochtis a pair(n, p_id)wherenis a
number andp_idis the id of clientp.
Transactions initiated by different clients have different epochs
because their client ids are different.
A client sets the epoch of a new transaction that it initiates to be
greater than epochs of all previous transactions that it initiated.
So, different transactions initiated by the same client have different
epochs.
Therefore each epoch is unique.Client ids are totally ordered, and so epochs are also totally
ordered.
For brevity we refer to an epoch by a single valuetrather than a
pair(n, p_id).All Steps in a Transaction have the
Transaction's EpochWe associate a fieldtwith each agent -- client or
server -- wheretis the epoch of the step of the transaction that the agent is
executing.
Likewise, we associate a fieldm.twith each messagembetween clients and servers
wherem.tis the epoch of the step in which the message
is sent.A message sent in a step of a transaction is received in a step of the
same transaction.
So, for a messagembetween clients and servers,m.tis also the epoch of the step in which the message is received.We design the algorithm so that all steps of a transaction, all
messages sent in steps of the transaction, and all messages received
in steps of the transaction have the epoch of the transaction.Next we give the algorithm for clients and then for servers.Algorithm for a ClientWhen a clientpgets a clock tick message it executesp.t = p.t + pos()whereposreturns a positive value, and 
thenpinitiates a new transaction with epochp.t.
Clientpcontinues executing the transaction with epochp.tuntilpgets its next
clock tick message at which point it increasesp.tand
starts a new transaction.In the algorithm
all steps ofp, all requests sent bypto clients, and all
replies received bypfrom clients, have epochp.t.# Initialization
p.t = 0
start()
def receive(message, sender):
   if isinstance(message, ClockTick):
      # Received clock tick
      p.t = p.t + pos()
      # Start new transaction with epoch p.t
      # p.copy stores values in replies.
      p.copy = {}
      # Broadcast read request with epoch p.t
      for q in Q:  send(ReadRequest(p.t), q)
   else:
      # received a reply to a read request
      if message.t == p.t: 
         p.copy[sender] = message.v
         # send write requests if M replies received
         if len(p.copy) >= M:
            # Broadcast write request: value v, epoch t
            v, t = p.f(), p.t
            for q in Q: send(WriteRequest(v, t), q)Algorithm for a ServerA server waits to get requests from clients.When a serverqreceives a requestrfrom
the transaction that the server 
is currently processing i.e.r.t\(=\)q.t,
then the server responds to the request.When a serverqreceives a requestrfrom a
transaction with a smaller 
epoch than the transaction that the server is currently processing,
i.e.r.t\(<\)q.t, then the server does not
respond to the request. The request has the same effect as a request that
is lost.What should serverqdo when it receives a requestrfrom a transaction with a larger epoch than the
server's epoch, i.e.r.t\(>\)q.t?In this case, the server increases its epoch to the
epoch of the request and then responds to the request.
The request, the step in which the server responds to the request,
and the reply (to a read request), have the same epoch.# initialization
q.v, q.t = init, 0
start()
def receive(request, client):
   if request.t >= q.t
      q.t = request.t
      if isinstance(request, ReadRequest):
         send(Reply(q.v, q.t), client)
      else:
         // message is a WriteRequest
         q.v = request.vCorrectnessNext we prove that all computations are serializable in a system
specified by these algorithms for clients and servers.Epoch of a step in a computation of the algorithmThe epoch of a step taken by a clientpor serverqis the value ofp.torq.t(respectively) upon completion of
the step.TheoremThe assignment of epochs to steps satisfies the
condition for epochs, i.e.,
For all
steps \((e, e')\) of a computationif \(e\) and \(e'\) are steps at the same agent, and \(e'\) occurs
  after \(e\) then \(t(e) \leq t(e')\), andif \(e\) is a step in which a message is sent and \(e'\) is a step
  in which that message is received then \(t(e) \leq t(e')\).ProofFrom the algorithms for serverpand clientqit follows thatp.tandq.tdo not decrease in a
computation.
Therefore the first condition is satisfied.Also, from the algorithm, the epoch for step in which a request or reply is
received is equal to the epoch in which the message is sent.
So, the second condition is also satisfied.
Therefore the assignment of epochs to steps of a computation satisfy
the specification of epochs.TheoremThe sufficient condition for serializability is satisfied:
(1) each transaction has a unique epoch, and (2) all steps in a
transaction have the epoch of the transaction.ProofWe have shown that each transaction initiated by a clientphas a unique epochp.t.All steps atphave epochp.t.
Requests sent byphave epochp.t.
When a clientqgets a request with epochp.t, the client  sets its own epoch top.tand sends a reply with the same epoch.
So, all steps in a transaction have the epoch of the transaction.
Therefore the algorithm satisfies the sufficient condition for
serializability.CorollaryThe algorithm satisfies specifications for serializability.ProofFollows because the algorithm satisfies the sufficient condition for
serializability.Therefore,
if the epochs of transactions in a computation \(x\) are \([T_{0}, T_{1}, T_{2}, \ldots]\)
where \(T_{i} < T_{i+1}\), then in \(x\), for all \(i\), for all agents \(v\):
All steps of \(v\) in the transaction with epoch \(T_{i}\) occur before any step
of \(v\) in the transaction with epoch \(T_{i+1}\).Equivalence of Distributed
and Sequential AlgorithmsLet the sequence of increasing epochs in a computation \(x\) of the distributed
algorithm beT[1], T[2], T[3], ..., and letp[i], R[i], W[i]be the values ofp, R, Win
the transaction with epochT[i].The sequence of values of agent variables in 
computation \(x\) of the distributed algorithm is the same as in the
following nondeterministic sequential program by selectingp, R, W, on thei-th iteration to bep[i], R[i], W[i]respectively, and by selectingdeltaon thei-th iteration so thatT[i] = T[i-1] + deltaEquivalent nondeterministic sequential algorithmt = 0
while True:
   select positive delta, select client p
   t = t + delta
   # p executes transaction with epoch t
   p.t = t
   p.copy = {}
   # Read step
   select R
   for q in R: p.copy[q] = q.v
   # Write step
   if len(p.copy) >= M:
     select W
     for q in W: q.v = p.f()We will prove properties of the sequential algorithm and show that
these properties also hold for the distributed algorithm.NextNext we develop Paxos, a distributed consensus algorithm.
We prove
that Paxos satisfies the specifications for consensus by showing that
Paxos is serializable.
We prove properties of a nondeterministic sequential algorithm and
show that properties of the sequential algorithm also hold for Paxos.K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

A serverqreplies to a read request from a clientpby sendingpa copy ofq.v.
A write request includes the valuevto be
written.
A serverqassignsvtoq.vwhenqreceives a write request containing valuev.ClientsEach client receives a sequence ofclock tickmessages.
The intervals between successive clock ticks are irrelevant for
the correctness of the algorithm.
The intervals do, however, impact performance.
There are many ways of generating clock tick messages and we
postpone discussion of them.A read request and a reply to the request may get lost or be delayed for an arbitrary
time. 
A client avoids waiting forever for a reply by only
accepting replies that the client receives before it receives its next
clock tick message.
Replies that arrive after the next clock tick are treated as lost.A transaction consists of a read step and a write step.Read Step of a TransactionA clientpsends read requests to all servers.
Some requests may be lost.
A server that receives a read request frompsends a
reply top.
Some replies may be lost.
LetRbe the set of servers from whichpreceives replies beforepreceives its next clock tick.IfRhasMor more elements, whereMis a given constant, thenpproceeds to the
write step.
IfRhas fewer thanMelements, then the
transaction terminates without executing the write step.Write Step of a TransactionThe list of replies thatpgets beforepreceives its next clock tick is:[q.v for q in R]Clientpsends a request to all servers to write:p.f(q.v for q in R)wherep.fis a function ofp.Write requests may get lost.
LetWbe the set of servers that receive write requests.A transaction consists of the step in which a clientpbroadcasts read requests to all servers, the steps in which servers
receive read requests and send replies, the steps in whichpreceives replies, the step in whichpsends write requests, and the steps in which servers
receive and execute write requests.Changes to server variables in a transaction are specified by the
following statement in whichp.fis a function executed
byp.Transactionif len(R) >= M:
   for q in W: q.v = p.f(q.v for q in R)Transactions are tailored to a specific application by specifyingMandp.f.
The selections ofRandWare
nondeterministic and they can be arbitrary sets of servers.Transactions executed concurrently by multiple clients can interfere
with each other as illustrated by the following example.ExampleLetxbe the amount of funds in an account and 
  assume thatxhas $110.
  
  Consider a computation in which clientspandp'both execute identical transactions concurrently.
In a transaction a client readsxand ifxhas at least $100 then the client transfers $100 out
ofxto an accounty.Consider a computation in which both clientspandp'readxand verify thatxhas at least $100,
and then both clients transfer $100 out ofxand set the amount
  inxto $10.
The computation transfers $200 out ofxbut 
debitsxby only $100.This situation does not occur if only one client executes a
transaction at a time.
Ifpexecutes its transaction first then whenpcompletes the transaction it sets the amount inxto $10. 
Ifp'executes its transaction next thenp'finds thatxhas only $10 and sop'does not
transfer $100 fromx.Serializable ComputationsA system executes multiple transactions concurrently.A computation \(x\) is serializableexactly when there exists a
sequence of transactions \(Z_{i}, i \geq 0\), such that in  \(x\):
For all agents \(v\), for all \(i\):all steps of \(v\) in \(Z_{i}\) occur before any step
of \(v\) in \(Z_{i+1}\).It is possible that in a serializable computation \(x\), an 
agent \(u\) takes a step in \(Z_{j}\) before adifferentagent
\(v\) takes a step in \(Z_{i}\), where \(j > i\).Transactions and Serializability in
DatabasesThere is an extensive literature ontransactions and serializability.Also seetransaction processing systemsandonline transaction processing.
In this webpage we provide a narrow definition of transactions and
serializability
that is adequate for describing algorithms such asPaxos.The Problem: Guarantee SerializabilityThe problem is to develop a distributed algorithm, with multiple
clients and servers, in which computations are serializable, and in
which the system may be faulty.How Should You Solve The Problem?What method comes to mind to partition the sequence of steps at an
agent into intervals where the agent completes all steps of exactly one
transaction in each interval?Logical time partitions computations into a past and a future.
Let's use a mechanism, similar to logical time.
As with logical time, each step \(e\) in a computation is assigned a
value \(t(e)\), called theepochof \(e\).
For any step \(e\), steps with epochs less than, equal to, or greater
than, \(t(e)\) are in the past,current, and future,
respectively.
We specify epochs such that thesteps in 
current consists of all the steps of exactly one
transaction.Epochs and Logical TimesRecall thatthe logical timeof a step \(e\) is a value \(t(e)\) assigned to
each step \(e\) in a computation such 
that for all edges \((e, e')\) of the dataflow graph:
\(t(e) < t(e')\).
Theepochof a step \(e\) in a computation is defined as a
value \(t(e)\) assigned to 
each step \(e\) in the computation such 
that for all edges \((e, e')\) of the dataflow graph:
\(t(e) \leq t(e')\).Rules for Assigning Epochs to StepsThe following rules ensure that the assignment \(t(e)\) to each step
\(e\) in a computation satisfies the specification for epochs. For all
steps \((e, e')\) of a computationfor all steps \(e\) and \(e'\) at the same agent: if \(e'\) occurs
  after \(e\) then \(t(e) \leq t(e')\), andif \(e\) is a step in which a message is sent and \(e'\) is a step
  in which that message is received then \(t(e) \leq t(e')\).ObservationIf \(e\) and \(e'\) are steps at the same agent and \(t(e) < t(e')\)
then \(e'\) occurs after \(e\).Theorem: A Sufficent Condition for SerializabilityA sufficient conditions for a computation to be serializable is:There exists epochs for all steps of a computation such thateach transaction has a unique epoch, andall steps in a transaction have the epoch of the transaction.If a computation \(x\) satisfies this condition then 
let the epochs of transactions in \(x\) be \([T_{0}, T_{1}, T_{2}, \ldots]\)
where \(T_{i} < T_{i+1}\), and let \(Z_{i}\) be the transaction with
epoch \(T_{i}\).
Then in \(x\), for all \(i\), for all agents \(v\):All steps of \(v\) in \(Z_{i}\) occur before any step
of \(v\) in \(Z_{i+1}\).ProofAssume that the sufficient condition is
satisfied for epochs of steps in a computation. 
Let \(e\) and \(e'\) be steps in transactions with epoch \(T\) and
\(T'\), respectively, and let \(T < T'\).
Let \(e\) and \(e'\) be steps at the same agent \(v\).
From the observation regarding epochs: \(e\) occurs before \(e'\).
Therefore all steps of a transaction with epoch \(T_{i}\) occur before
any step of a transaction with epoch \(T_{i+1}\).An Algorithm Based on the Sufficent ConditionNext, we describe an algorithm based on this idea.
Let's consider the two issues posed by the condition for serializability.
(1) How can the algorithm assign a unique epoch to each transaction?
(2) How can the algorithm assign epochs to steps so that
all steps in a transaction have the epoch of the transaction?Uniqueness of a Transaction's EpochA clientpinitiates a new transaction whenpgets a clock tick message.
To ensure that the epoch of the transaction is unique, an  epochtis a pair(n, p_id)wherenis a
number andp_idis the id of clientp.
Transactions initiated by different clients have different epochs
because their client ids are different.
A client sets the epoch of a new transaction that it initiates to be
greater than epochs of all previous transactions that it initiated.
So, different transactions initiated by the same client have different
epochs.
Therefore each epoch is unique.Client ids are totally ordered, and so epochs are also totally
ordered.
For brevity we refer to an epoch by a single valuetrather than a
pair(n, p_id).All Steps in a Transaction have the
Transaction's EpochWe associate a fieldtwith each agent -- client or
server -- wheretis the epoch of the step of the transaction that the agent is
executing.
Likewise, we associate a fieldm.twith each messagembetween clients and servers
wherem.tis the epoch of the step in which the message
is sent.A message sent in a step of a transaction is received in a step of the
same transaction.
So, for a messagembetween clients and servers,m.tis also the epoch of the step in which the message is received.We design the algorithm so that all steps of a transaction, all
messages sent in steps of the transaction, and all messages received
in steps of the transaction have the epoch of the transaction.Next we give the algorithm for clients and then for servers.Algorithm for a ClientWhen a clientpgets a clock tick message it executesp.t = p.t + pos()whereposreturns a positive value, and 
thenpinitiates a new transaction with epochp.t.
Clientpcontinues executing the transaction with epochp.tuntilpgets its next
clock tick message at which point it increasesp.tand
starts a new transaction.In the algorithm
all steps ofp, all requests sent bypto clients, and all
replies received bypfrom clients, have epochp.t.# Initialization
p.t = 0
start()
def receive(message, sender):
   if isinstance(message, ClockTick):
      # Received clock tick
      p.t = p.t + pos()
      # Start new transaction with epoch p.t
      # p.copy stores values in replies.
      p.copy = {}
      # Broadcast read request with epoch p.t
      for q in Q:  send(ReadRequest(p.t), q)
   else:
      # received a reply to a read request
      if message.t == p.t: 
         p.copy[sender] = message.v
         # send write requests if M replies received
         if len(p.copy) >= M:
            # Broadcast write request: value v, epoch t
            v, t = p.f(), p.t
            for q in Q: send(WriteRequest(v, t), q)Algorithm for a ServerA server waits to get requests from clients.When a serverqreceives a requestrfrom
the transaction that the server 
is currently processing i.e.r.t\(=\)q.t,
then the server responds to the request.When a serverqreceives a requestrfrom a
transaction with a smaller 
epoch than the transaction that the server is currently processing,
i.e.r.t\(<\)q.t, then the server does not
respond to the request. The request has the same effect as a request that
is lost.What should serverqdo when it receives a requestrfrom a transaction with a larger epoch than the
server's epoch, i.e.r.t\(>\)q.t?In this case, the server increases its epoch to the
epoch of the request and then responds to the request.
The request, the step in which the server responds to the request,
and the reply (to a read request), have the same epoch.# initialization
q.v, q.t = init, 0
start()
def receive(request, client):
   if request.t >= q.t
      q.t = request.t
      if isinstance(request, ReadRequest):
         send(Reply(q.v, q.t), client)
      else:
         // message is a WriteRequest
         q.v = request.vCorrectnessNext we prove that all computations are serializable in a system
specified by these algorithms for clients and servers.Epoch of a step in a computation of the algorithmThe epoch of a step taken by a clientpor serverqis the value ofp.torq.t(respectively) upon completion of
the step.TheoremThe assignment of epochs to steps satisfies the
condition for epochs, i.e.,
For all
steps \((e, e')\) of a computationif \(e\) and \(e'\) are steps at the same agent, and \(e'\) occurs
  after \(e\) then \(t(e) \leq t(e')\), andif \(e\) is a step in which a message is sent and \(e'\) is a step
  in which that message is received then \(t(e) \leq t(e')\).ProofFrom the algorithms for serverpand clientqit follows thatp.tandq.tdo not decrease in a
computation.
Therefore the first condition is satisfied.Also, from the algorithm, the epoch for step in which a request or reply is
received is equal to the epoch in which the message is sent.
So, the second condition is also satisfied.
Therefore the assignment of epochs to steps of a computation satisfy
the specification of epochs.TheoremThe sufficient condition for serializability is satisfied:
(1) each transaction has a unique epoch, and (2) all steps in a
transaction have the epoch of the transaction.ProofWe have shown that each transaction initiated by a clientphas a unique epochp.t.All steps atphave epochp.t.
Requests sent byphave epochp.t.
When a clientqgets a request with epochp.t, the client  sets its own epoch top.tand sends a reply with the same epoch.
So, all steps in a transaction have the epoch of the transaction.
Therefore the algorithm satisfies the sufficient condition for
serializability.CorollaryThe algorithm satisfies specifications for serializability.ProofFollows because the algorithm satisfies the sufficient condition for
serializability.Therefore,
if the epochs of transactions in a computation \(x\) are \([T_{0}, T_{1}, T_{2}, \ldots]\)
where \(T_{i} < T_{i+1}\), then in \(x\), for all \(i\), for all agents \(v\):
All steps of \(v\) in the transaction with epoch \(T_{i}\) occur before any step
of \(v\) in the transaction with epoch \(T_{i+1}\).Equivalence of Distributed
and Sequential AlgorithmsLet the sequence of increasing epochs in a computation \(x\) of the distributed
algorithm beT[1], T[2], T[3], ..., and letp[i], R[i], W[i]be the values ofp, R, Win
the transaction with epochT[i].The sequence of values of agent variables in 
computation \(x\) of the distributed algorithm is the same as in the
following nondeterministic sequential program by selectingp, R, W, on thei-th iteration to bep[i], R[i], W[i]respectively, and by selectingdeltaon thei-th iteration so thatT[i] = T[i-1] + deltaEquivalent nondeterministic sequential algorithmt = 0
while True:
   select positive delta, select client p
   t = t + delta
   # p executes transaction with epoch t
   p.t = t
   p.copy = {}
   # Read step
   select R
   for q in R: p.copy[q] = q.v
   # Write step
   if len(p.copy) >= M:
     select W
     for q in W: q.v = p.f()We will prove properties of the sequential algorithm and show that
these properties also hold for the distributed algorithm.NextNext we develop Paxos, a distributed consensus algorithm.
We prove
that Paxos satisfies the specifications for consensus by showing that
Paxos is serializable.
We prove properties of a nondeterministic sequential algorithm and
show that properties of the sequential algorithm also hold for Paxos.K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

A read request and a reply to the request may get lost or be delayed for an arbitrary
time. 
A client avoids waiting forever for a reply by only
accepting replies that the client receives before it receives its next
clock tick message.
Replies that arrive after the next clock tick are treated as lost.A transaction consists of a read step and a write step.Read Step of a TransactionA clientpsends read requests to all servers.
Some requests may be lost.
A server that receives a read request frompsends a
reply top.
Some replies may be lost.
LetRbe the set of servers from whichpreceives replies beforepreceives its next clock tick.IfRhasMor more elements, whereMis a given constant, thenpproceeds to the
write step.
IfRhas fewer thanMelements, then the
transaction terminates without executing the write step.Write Step of a TransactionThe list of replies thatpgets beforepreceives its next clock tick is:[q.v for q in R]Clientpsends a request to all servers to write:p.f(q.v for q in R)wherep.fis a function ofp.Write requests may get lost.
LetWbe the set of servers that receive write requests.A transaction consists of the step in which a clientpbroadcasts read requests to all servers, the steps in which servers
receive read requests and send replies, the steps in whichpreceives replies, the step in whichpsends write requests, and the steps in which servers
receive and execute write requests.Changes to server variables in a transaction are specified by the
following statement in whichp.fis a function executed
byp.Transactionif len(R) >= M:
   for q in W: q.v = p.f(q.v for q in R)Transactions are tailored to a specific application by specifyingMandp.f.
The selections ofRandWare
nondeterministic and they can be arbitrary sets of servers.Transactions executed concurrently by multiple clients can interfere
with each other as illustrated by the following example.ExampleLetxbe the amount of funds in an account and 
  assume thatxhas $110.
  
  Consider a computation in which clientspandp'both execute identical transactions concurrently.
In a transaction a client readsxand ifxhas at least $100 then the client transfers $100 out
ofxto an accounty.Consider a computation in which both clientspandp'readxand verify thatxhas at least $100,
and then both clients transfer $100 out ofxand set the amount
  inxto $10.
The computation transfers $200 out ofxbut 
debitsxby only $100.This situation does not occur if only one client executes a
transaction at a time.
Ifpexecutes its transaction first then whenpcompletes the transaction it sets the amount inxto $10. 
Ifp'executes its transaction next thenp'finds thatxhas only $10 and sop'does not
transfer $100 fromx.Serializable ComputationsA system executes multiple transactions concurrently.A computation \(x\) is serializableexactly when there exists a
sequence of transactions \(Z_{i}, i \geq 0\), such that in  \(x\):
For all agents \(v\), for all \(i\):all steps of \(v\) in \(Z_{i}\) occur before any step
of \(v\) in \(Z_{i+1}\).It is possible that in a serializable computation \(x\), an 
agent \(u\) takes a step in \(Z_{j}\) before adifferentagent
\(v\) takes a step in \(Z_{i}\), where \(j > i\).Transactions and Serializability in
DatabasesThere is an extensive literature ontransactions and serializability.Also seetransaction processing systemsandonline transaction processing.
In this webpage we provide a narrow definition of transactions and
serializability
that is adequate for describing algorithms such asPaxos.The Problem: Guarantee SerializabilityThe problem is to develop a distributed algorithm, with multiple
clients and servers, in which computations are serializable, and in
which the system may be faulty.How Should You Solve The Problem?What method comes to mind to partition the sequence of steps at an
agent into intervals where the agent completes all steps of exactly one
transaction in each interval?Logical time partitions computations into a past and a future.
Let's use a mechanism, similar to logical time.
As with logical time, each step \(e\) in a computation is assigned a
value \(t(e)\), called theepochof \(e\).
For any step \(e\), steps with epochs less than, equal to, or greater
than, \(t(e)\) are in the past,current, and future,
respectively.
We specify epochs such that thesteps in 
current consists of all the steps of exactly one
transaction.Epochs and Logical TimesRecall thatthe logical timeof a step \(e\) is a value \(t(e)\) assigned to
each step \(e\) in a computation such 
that for all edges \((e, e')\) of the dataflow graph:
\(t(e) < t(e')\).
Theepochof a step \(e\) in a computation is defined as a
value \(t(e)\) assigned to 
each step \(e\) in the computation such 
that for all edges \((e, e')\) of the dataflow graph:
\(t(e) \leq t(e')\).Rules for Assigning Epochs to StepsThe following rules ensure that the assignment \(t(e)\) to each step
\(e\) in a computation satisfies the specification for epochs. For all
steps \((e, e')\) of a computationfor all steps \(e\) and \(e'\) at the same agent: if \(e'\) occurs
  after \(e\) then \(t(e) \leq t(e')\), andif \(e\) is a step in which a message is sent and \(e'\) is a step
  in which that message is received then \(t(e) \leq t(e')\).ObservationIf \(e\) and \(e'\) are steps at the same agent and \(t(e) < t(e')\)
then \(e'\) occurs after \(e\).Theorem: A Sufficent Condition for SerializabilityA sufficient conditions for a computation to be serializable is:There exists epochs for all steps of a computation such thateach transaction has a unique epoch, andall steps in a transaction have the epoch of the transaction.If a computation \(x\) satisfies this condition then 
let the epochs of transactions in \(x\) be \([T_{0}, T_{1}, T_{2}, \ldots]\)
where \(T_{i} < T_{i+1}\), and let \(Z_{i}\) be the transaction with
epoch \(T_{i}\).
Then in \(x\), for all \(i\), for all agents \(v\):All steps of \(v\) in \(Z_{i}\) occur before any step
of \(v\) in \(Z_{i+1}\).ProofAssume that the sufficient condition is
satisfied for epochs of steps in a computation. 
Let \(e\) and \(e'\) be steps in transactions with epoch \(T\) and
\(T'\), respectively, and let \(T < T'\).
Let \(e\) and \(e'\) be steps at the same agent \(v\).
From the observation regarding epochs: \(e\) occurs before \(e'\).
Therefore all steps of a transaction with epoch \(T_{i}\) occur before
any step of a transaction with epoch \(T_{i+1}\).An Algorithm Based on the Sufficent ConditionNext, we describe an algorithm based on this idea.
Let's consider the two issues posed by the condition for serializability.
(1) How can the algorithm assign a unique epoch to each transaction?
(2) How can the algorithm assign epochs to steps so that
all steps in a transaction have the epoch of the transaction?Uniqueness of a Transaction's EpochA clientpinitiates a new transaction whenpgets a clock tick message.
To ensure that the epoch of the transaction is unique, an  epochtis a pair(n, p_id)wherenis a
number andp_idis the id of clientp.
Transactions initiated by different clients have different epochs
because their client ids are different.
A client sets the epoch of a new transaction that it initiates to be
greater than epochs of all previous transactions that it initiated.
So, different transactions initiated by the same client have different
epochs.
Therefore each epoch is unique.Client ids are totally ordered, and so epochs are also totally
ordered.
For brevity we refer to an epoch by a single valuetrather than a
pair(n, p_id).All Steps in a Transaction have the
Transaction's EpochWe associate a fieldtwith each agent -- client or
server -- wheretis the epoch of the step of the transaction that the agent is
executing.
Likewise, we associate a fieldm.twith each messagembetween clients and servers
wherem.tis the epoch of the step in which the message
is sent.A message sent in a step of a transaction is received in a step of the
same transaction.
So, for a messagembetween clients and servers,m.tis also the epoch of the step in which the message is received.We design the algorithm so that all steps of a transaction, all
messages sent in steps of the transaction, and all messages received
in steps of the transaction have the epoch of the transaction.Next we give the algorithm for clients and then for servers.Algorithm for a ClientWhen a clientpgets a clock tick message it executesp.t = p.t + pos()whereposreturns a positive value, and 
thenpinitiates a new transaction with epochp.t.
Clientpcontinues executing the transaction with epochp.tuntilpgets its next
clock tick message at which point it increasesp.tand
starts a new transaction.In the algorithm
all steps ofp, all requests sent bypto clients, and all
replies received bypfrom clients, have epochp.t.# Initialization
p.t = 0
start()
def receive(message, sender):
   if isinstance(message, ClockTick):
      # Received clock tick
      p.t = p.t + pos()
      # Start new transaction with epoch p.t
      # p.copy stores values in replies.
      p.copy = {}
      # Broadcast read request with epoch p.t
      for q in Q:  send(ReadRequest(p.t), q)
   else:
      # received a reply to a read request
      if message.t == p.t: 
         p.copy[sender] = message.v
         # send write requests if M replies received
         if len(p.copy) >= M:
            # Broadcast write request: value v, epoch t
            v, t = p.f(), p.t
            for q in Q: send(WriteRequest(v, t), q)Algorithm for a ServerA server waits to get requests from clients.When a serverqreceives a requestrfrom
the transaction that the server 
is currently processing i.e.r.t\(=\)q.t,
then the server responds to the request.When a serverqreceives a requestrfrom a
transaction with a smaller 
epoch than the transaction that the server is currently processing,
i.e.r.t\(<\)q.t, then the server does not
respond to the request. The request has the same effect as a request that
is lost.What should serverqdo when it receives a requestrfrom a transaction with a larger epoch than the
server's epoch, i.e.r.t\(>\)q.t?In this case, the server increases its epoch to the
epoch of the request and then responds to the request.
The request, the step in which the server responds to the request,
and the reply (to a read request), have the same epoch.# initialization
q.v, q.t = init, 0
start()
def receive(request, client):
   if request.t >= q.t
      q.t = request.t
      if isinstance(request, ReadRequest):
         send(Reply(q.v, q.t), client)
      else:
         // message is a WriteRequest
         q.v = request.vCorrectnessNext we prove that all computations are serializable in a system
specified by these algorithms for clients and servers.Epoch of a step in a computation of the algorithmThe epoch of a step taken by a clientpor serverqis the value ofp.torq.t(respectively) upon completion of
the step.TheoremThe assignment of epochs to steps satisfies the
condition for epochs, i.e.,
For all
steps \((e, e')\) of a computationif \(e\) and \(e'\) are steps at the same agent, and \(e'\) occurs
  after \(e\) then \(t(e) \leq t(e')\), andif \(e\) is a step in which a message is sent and \(e'\) is a step
  in which that message is received then \(t(e) \leq t(e')\).ProofFrom the algorithms for serverpand clientqit follows thatp.tandq.tdo not decrease in a
computation.
Therefore the first condition is satisfied.Also, from the algorithm, the epoch for step in which a request or reply is
received is equal to the epoch in which the message is sent.
So, the second condition is also satisfied.
Therefore the assignment of epochs to steps of a computation satisfy
the specification of epochs.TheoremThe sufficient condition for serializability is satisfied:
(1) each transaction has a unique epoch, and (2) all steps in a
transaction have the epoch of the transaction.ProofWe have shown that each transaction initiated by a clientphas a unique epochp.t.All steps atphave epochp.t.
Requests sent byphave epochp.t.
When a clientqgets a request with epochp.t, the client  sets its own epoch top.tand sends a reply with the same epoch.
So, all steps in a transaction have the epoch of the transaction.
Therefore the algorithm satisfies the sufficient condition for
serializability.CorollaryThe algorithm satisfies specifications for serializability.ProofFollows because the algorithm satisfies the sufficient condition for
serializability.Therefore,
if the epochs of transactions in a computation \(x\) are \([T_{0}, T_{1}, T_{2}, \ldots]\)
where \(T_{i} < T_{i+1}\), then in \(x\), for all \(i\), for all agents \(v\):
All steps of \(v\) in the transaction with epoch \(T_{i}\) occur before any step
of \(v\) in the transaction with epoch \(T_{i+1}\).Equivalence of Distributed
and Sequential AlgorithmsLet the sequence of increasing epochs in a computation \(x\) of the distributed
algorithm beT[1], T[2], T[3], ..., and letp[i], R[i], W[i]be the values ofp, R, Win
the transaction with epochT[i].The sequence of values of agent variables in 
computation \(x\) of the distributed algorithm is the same as in the
following nondeterministic sequential program by selectingp, R, W, on thei-th iteration to bep[i], R[i], W[i]respectively, and by selectingdeltaon thei-th iteration so thatT[i] = T[i-1] + deltaEquivalent nondeterministic sequential algorithmt = 0
while True:
   select positive delta, select client p
   t = t + delta
   # p executes transaction with epoch t
   p.t = t
   p.copy = {}
   # Read step
   select R
   for q in R: p.copy[q] = q.v
   # Write step
   if len(p.copy) >= M:
     select W
     for q in W: q.v = p.f()We will prove properties of the sequential algorithm and show that
these properties also hold for the distributed algorithm.NextNext we develop Paxos, a distributed consensus algorithm.
We prove
that Paxos satisfies the specifications for consensus by showing that
Paxos is serializable.
We prove properties of a nondeterministic sequential algorithm and
show that properties of the sequential algorithm also hold for Paxos.K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

A transaction consists of a read step and a write step.Read Step of a TransactionA clientpsends read requests to all servers.
Some requests may be lost.
A server that receives a read request frompsends a
reply top.
Some replies may be lost.
LetRbe the set of servers from whichpreceives replies beforepreceives its next clock tick.IfRhasMor more elements, whereMis a given constant, thenpproceeds to the
write step.
IfRhas fewer thanMelements, then the
transaction terminates without executing the write step.Write Step of a TransactionThe list of replies thatpgets beforepreceives its next clock tick is:[q.v for q in R]Clientpsends a request to all servers to write:p.f(q.v for q in R)wherep.fis a function ofp.Write requests may get lost.
LetWbe the set of servers that receive write requests.A transaction consists of the step in which a clientpbroadcasts read requests to all servers, the steps in which servers
receive read requests and send replies, the steps in whichpreceives replies, the step in whichpsends write requests, and the steps in which servers
receive and execute write requests.Changes to server variables in a transaction are specified by the
following statement in whichp.fis a function executed
byp.Transactionif len(R) >= M:
   for q in W: q.v = p.f(q.v for q in R)Transactions are tailored to a specific application by specifyingMandp.f.
The selections ofRandWare
nondeterministic and they can be arbitrary sets of servers.Transactions executed concurrently by multiple clients can interfere
with each other as illustrated by the following example.ExampleLetxbe the amount of funds in an account and 
  assume thatxhas $110.
  
  Consider a computation in which clientspandp'both execute identical transactions concurrently.
In a transaction a client readsxand ifxhas at least $100 then the client transfers $100 out
ofxto an accounty.Consider a computation in which both clientspandp'readxand verify thatxhas at least $100,
and then both clients transfer $100 out ofxand set the amount
  inxto $10.
The computation transfers $200 out ofxbut 
debitsxby only $100.This situation does not occur if only one client executes a
transaction at a time.
Ifpexecutes its transaction first then whenpcompletes the transaction it sets the amount inxto $10. 
Ifp'executes its transaction next thenp'finds thatxhas only $10 and sop'does not
transfer $100 fromx.Serializable ComputationsA system executes multiple transactions concurrently.A computation \(x\) is serializableexactly when there exists a
sequence of transactions \(Z_{i}, i \geq 0\), such that in  \(x\):
For all agents \(v\), for all \(i\):all steps of \(v\) in \(Z_{i}\) occur before any step
of \(v\) in \(Z_{i+1}\).It is possible that in a serializable computation \(x\), an 
agent \(u\) takes a step in \(Z_{j}\) before adifferentagent
\(v\) takes a step in \(Z_{i}\), where \(j > i\).Transactions and Serializability in
DatabasesThere is an extensive literature ontransactions and serializability.Also seetransaction processing systemsandonline transaction processing.
In this webpage we provide a narrow definition of transactions and
serializability
that is adequate for describing algorithms such asPaxos.The Problem: Guarantee SerializabilityThe problem is to develop a distributed algorithm, with multiple
clients and servers, in which computations are serializable, and in
which the system may be faulty.How Should You Solve The Problem?What method comes to mind to partition the sequence of steps at an
agent into intervals where the agent completes all steps of exactly one
transaction in each interval?Logical time partitions computations into a past and a future.
Let's use a mechanism, similar to logical time.
As with logical time, each step \(e\) in a computation is assigned a
value \(t(e)\), called theepochof \(e\).
For any step \(e\), steps with epochs less than, equal to, or greater
than, \(t(e)\) are in the past,current, and future,
respectively.
We specify epochs such that thesteps in 
current consists of all the steps of exactly one
transaction.Epochs and Logical TimesRecall thatthe logical timeof a step \(e\) is a value \(t(e)\) assigned to
each step \(e\) in a computation such 
that for all edges \((e, e')\) of the dataflow graph:
\(t(e) < t(e')\).
Theepochof a step \(e\) in a computation is defined as a
value \(t(e)\) assigned to 
each step \(e\) in the computation such 
that for all edges \((e, e')\) of the dataflow graph:
\(t(e) \leq t(e')\).Rules for Assigning Epochs to StepsThe following rules ensure that the assignment \(t(e)\) to each step
\(e\) in a computation satisfies the specification for epochs. For all
steps \((e, e')\) of a computationfor all steps \(e\) and \(e'\) at the same agent: if \(e'\) occurs
  after \(e\) then \(t(e) \leq t(e')\), andif \(e\) is a step in which a message is sent and \(e'\) is a step
  in which that message is received then \(t(e) \leq t(e')\).ObservationIf \(e\) and \(e'\) are steps at the same agent and \(t(e) < t(e')\)
then \(e'\) occurs after \(e\).Theorem: A Sufficent Condition for SerializabilityA sufficient conditions for a computation to be serializable is:There exists epochs for all steps of a computation such thateach transaction has a unique epoch, andall steps in a transaction have the epoch of the transaction.If a computation \(x\) satisfies this condition then 
let the epochs of transactions in \(x\) be \([T_{0}, T_{1}, T_{2}, \ldots]\)
where \(T_{i} < T_{i+1}\), and let \(Z_{i}\) be the transaction with
epoch \(T_{i}\).
Then in \(x\), for all \(i\), for all agents \(v\):All steps of \(v\) in \(Z_{i}\) occur before any step
of \(v\) in \(Z_{i+1}\).ProofAssume that the sufficient condition is
satisfied for epochs of steps in a computation. 
Let \(e\) and \(e'\) be steps in transactions with epoch \(T\) and
\(T'\), respectively, and let \(T < T'\).
Let \(e\) and \(e'\) be steps at the same agent \(v\).
From the observation regarding epochs: \(e\) occurs before \(e'\).
Therefore all steps of a transaction with epoch \(T_{i}\) occur before
any step of a transaction with epoch \(T_{i+1}\).An Algorithm Based on the Sufficent ConditionNext, we describe an algorithm based on this idea.
Let's consider the two issues posed by the condition for serializability.
(1) How can the algorithm assign a unique epoch to each transaction?
(2) How can the algorithm assign epochs to steps so that
all steps in a transaction have the epoch of the transaction?Uniqueness of a Transaction's EpochA clientpinitiates a new transaction whenpgets a clock tick message.
To ensure that the epoch of the transaction is unique, an  epochtis a pair(n, p_id)wherenis a
number andp_idis the id of clientp.
Transactions initiated by different clients have different epochs
because their client ids are different.
A client sets the epoch of a new transaction that it initiates to be
greater than epochs of all previous transactions that it initiated.
So, different transactions initiated by the same client have different
epochs.
Therefore each epoch is unique.Client ids are totally ordered, and so epochs are also totally
ordered.
For brevity we refer to an epoch by a single valuetrather than a
pair(n, p_id).All Steps in a Transaction have the
Transaction's EpochWe associate a fieldtwith each agent -- client or
server -- wheretis the epoch of the step of the transaction that the agent is
executing.
Likewise, we associate a fieldm.twith each messagembetween clients and servers
wherem.tis the epoch of the step in which the message
is sent.A message sent in a step of a transaction is received in a step of the
same transaction.
So, for a messagembetween clients and servers,m.tis also the epoch of the step in which the message is received.We design the algorithm so that all steps of a transaction, all
messages sent in steps of the transaction, and all messages received
in steps of the transaction have the epoch of the transaction.Next we give the algorithm for clients and then for servers.Algorithm for a ClientWhen a clientpgets a clock tick message it executesp.t = p.t + pos()whereposreturns a positive value, and 
thenpinitiates a new transaction with epochp.t.
Clientpcontinues executing the transaction with epochp.tuntilpgets its next
clock tick message at which point it increasesp.tand
starts a new transaction.In the algorithm
all steps ofp, all requests sent bypto clients, and all
replies received bypfrom clients, have epochp.t.# Initialization
p.t = 0
start()
def receive(message, sender):
   if isinstance(message, ClockTick):
      # Received clock tick
      p.t = p.t + pos()
      # Start new transaction with epoch p.t
      # p.copy stores values in replies.
      p.copy = {}
      # Broadcast read request with epoch p.t
      for q in Q:  send(ReadRequest(p.t), q)
   else:
      # received a reply to a read request
      if message.t == p.t: 
         p.copy[sender] = message.v
         # send write requests if M replies received
         if len(p.copy) >= M:
            # Broadcast write request: value v, epoch t
            v, t = p.f(), p.t
            for q in Q: send(WriteRequest(v, t), q)Algorithm for a ServerA server waits to get requests from clients.When a serverqreceives a requestrfrom
the transaction that the server 
is currently processing i.e.r.t\(=\)q.t,
then the server responds to the request.When a serverqreceives a requestrfrom a
transaction with a smaller 
epoch than the transaction that the server is currently processing,
i.e.r.t\(<\)q.t, then the server does not
respond to the request. The request has the same effect as a request that
is lost.What should serverqdo when it receives a requestrfrom a transaction with a larger epoch than the
server's epoch, i.e.r.t\(>\)q.t?In this case, the server increases its epoch to the
epoch of the request and then responds to the request.
The request, the step in which the server responds to the request,
and the reply (to a read request), have the same epoch.# initialization
q.v, q.t = init, 0
start()
def receive(request, client):
   if request.t >= q.t
      q.t = request.t
      if isinstance(request, ReadRequest):
         send(Reply(q.v, q.t), client)
      else:
         // message is a WriteRequest
         q.v = request.vCorrectnessNext we prove that all computations are serializable in a system
specified by these algorithms for clients and servers.Epoch of a step in a computation of the algorithmThe epoch of a step taken by a clientpor serverqis the value ofp.torq.t(respectively) upon completion of
the step.TheoremThe assignment of epochs to steps satisfies the
condition for epochs, i.e.,
For all
steps \((e, e')\) of a computationif \(e\) and \(e'\) are steps at the same agent, and \(e'\) occurs
  after \(e\) then \(t(e) \leq t(e')\), andif \(e\) is a step in which a message is sent and \(e'\) is a step
  in which that message is received then \(t(e) \leq t(e')\).ProofFrom the algorithms for serverpand clientqit follows thatp.tandq.tdo not decrease in a
computation.
Therefore the first condition is satisfied.Also, from the algorithm, the epoch for step in which a request or reply is
received is equal to the epoch in which the message is sent.
So, the second condition is also satisfied.
Therefore the assignment of epochs to steps of a computation satisfy
the specification of epochs.TheoremThe sufficient condition for serializability is satisfied:
(1) each transaction has a unique epoch, and (2) all steps in a
transaction have the epoch of the transaction.ProofWe have shown that each transaction initiated by a clientphas a unique epochp.t.All steps atphave epochp.t.
Requests sent byphave epochp.t.
When a clientqgets a request with epochp.t, the client  sets its own epoch top.tand sends a reply with the same epoch.
So, all steps in a transaction have the epoch of the transaction.
Therefore the algorithm satisfies the sufficient condition for
serializability.CorollaryThe algorithm satisfies specifications for serializability.ProofFollows because the algorithm satisfies the sufficient condition for
serializability.Therefore,
if the epochs of transactions in a computation \(x\) are \([T_{0}, T_{1}, T_{2}, \ldots]\)
where \(T_{i} < T_{i+1}\), then in \(x\), for all \(i\), for all agents \(v\):
All steps of \(v\) in the transaction with epoch \(T_{i}\) occur before any step
of \(v\) in the transaction with epoch \(T_{i+1}\).Equivalence of Distributed
and Sequential AlgorithmsLet the sequence of increasing epochs in a computation \(x\) of the distributed
algorithm beT[1], T[2], T[3], ..., and letp[i], R[i], W[i]be the values ofp, R, Win
the transaction with epochT[i].The sequence of values of agent variables in 
computation \(x\) of the distributed algorithm is the same as in the
following nondeterministic sequential program by selectingp, R, W, on thei-th iteration to bep[i], R[i], W[i]respectively, and by selectingdeltaon thei-th iteration so thatT[i] = T[i-1] + deltaEquivalent nondeterministic sequential algorithmt = 0
while True:
   select positive delta, select client p
   t = t + delta
   # p executes transaction with epoch t
   p.t = t
   p.copy = {}
   # Read step
   select R
   for q in R: p.copy[q] = q.v
   # Write step
   if len(p.copy) >= M:
     select W
     for q in W: q.v = p.f()We will prove properties of the sequential algorithm and show that
these properties also hold for the distributed algorithm.NextNext we develop Paxos, a distributed consensus algorithm.
We prove
that Paxos satisfies the specifications for consensus by showing that
Paxos is serializable.
We prove properties of a nondeterministic sequential algorithm and
show that properties of the sequential algorithm also hold for Paxos.K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

IfRhasMor more elements, whereMis a given constant, thenpproceeds to the
write step.
IfRhas fewer thanMelements, then the
transaction terminates without executing the write step.Write Step of a TransactionThe list of replies thatpgets beforepreceives its next clock tick is:[q.v for q in R]Clientpsends a request to all servers to write:p.f(q.v for q in R)wherep.fis a function ofp.Write requests may get lost.
LetWbe the set of servers that receive write requests.A transaction consists of the step in which a clientpbroadcasts read requests to all servers, the steps in which servers
receive read requests and send replies, the steps in whichpreceives replies, the step in whichpsends write requests, and the steps in which servers
receive and execute write requests.Changes to server variables in a transaction are specified by the
following statement in whichp.fis a function executed
byp.Transactionif len(R) >= M:
   for q in W: q.v = p.f(q.v for q in R)Transactions are tailored to a specific application by specifyingMandp.f.
The selections ofRandWare
nondeterministic and they can be arbitrary sets of servers.Transactions executed concurrently by multiple clients can interfere
with each other as illustrated by the following example.ExampleLetxbe the amount of funds in an account and 
  assume thatxhas $110.
  
  Consider a computation in which clientspandp'both execute identical transactions concurrently.
In a transaction a client readsxand ifxhas at least $100 then the client transfers $100 out
ofxto an accounty.Consider a computation in which both clientspandp'readxand verify thatxhas at least $100,
and then both clients transfer $100 out ofxand set the amount
  inxto $10.
The computation transfers $200 out ofxbut 
debitsxby only $100.This situation does not occur if only one client executes a
transaction at a time.
Ifpexecutes its transaction first then whenpcompletes the transaction it sets the amount inxto $10. 
Ifp'executes its transaction next thenp'finds thatxhas only $10 and sop'does not
transfer $100 fromx.Serializable ComputationsA system executes multiple transactions concurrently.A computation \(x\) is serializableexactly when there exists a
sequence of transactions \(Z_{i}, i \geq 0\), such that in  \(x\):
For all agents \(v\), for all \(i\):all steps of \(v\) in \(Z_{i}\) occur before any step
of \(v\) in \(Z_{i+1}\).It is possible that in a serializable computation \(x\), an 
agent \(u\) takes a step in \(Z_{j}\) before adifferentagent
\(v\) takes a step in \(Z_{i}\), where \(j > i\).Transactions and Serializability in
DatabasesThere is an extensive literature ontransactions and serializability.Also seetransaction processing systemsandonline transaction processing.
In this webpage we provide a narrow definition of transactions and
serializability
that is adequate for describing algorithms such asPaxos.The Problem: Guarantee SerializabilityThe problem is to develop a distributed algorithm, with multiple
clients and servers, in which computations are serializable, and in
which the system may be faulty.How Should You Solve The Problem?What method comes to mind to partition the sequence of steps at an
agent into intervals where the agent completes all steps of exactly one
transaction in each interval?Logical time partitions computations into a past and a future.
Let's use a mechanism, similar to logical time.
As with logical time, each step \(e\) in a computation is assigned a
value \(t(e)\), called theepochof \(e\).
For any step \(e\), steps with epochs less than, equal to, or greater
than, \(t(e)\) are in the past,current, and future,
respectively.
We specify epochs such that thesteps in 
current consists of all the steps of exactly one
transaction.Epochs and Logical TimesRecall thatthe logical timeof a step \(e\) is a value \(t(e)\) assigned to
each step \(e\) in a computation such 
that for all edges \((e, e')\) of the dataflow graph:
\(t(e) < t(e')\).
Theepochof a step \(e\) in a computation is defined as a
value \(t(e)\) assigned to 
each step \(e\) in the computation such 
that for all edges \((e, e')\) of the dataflow graph:
\(t(e) \leq t(e')\).Rules for Assigning Epochs to StepsThe following rules ensure that the assignment \(t(e)\) to each step
\(e\) in a computation satisfies the specification for epochs. For all
steps \((e, e')\) of a computationfor all steps \(e\) and \(e'\) at the same agent: if \(e'\) occurs
  after \(e\) then \(t(e) \leq t(e')\), andif \(e\) is a step in which a message is sent and \(e'\) is a step
  in which that message is received then \(t(e) \leq t(e')\).ObservationIf \(e\) and \(e'\) are steps at the same agent and \(t(e) < t(e')\)
then \(e'\) occurs after \(e\).Theorem: A Sufficent Condition for SerializabilityA sufficient conditions for a computation to be serializable is:There exists epochs for all steps of a computation such thateach transaction has a unique epoch, andall steps in a transaction have the epoch of the transaction.If a computation \(x\) satisfies this condition then 
let the epochs of transactions in \(x\) be \([T_{0}, T_{1}, T_{2}, \ldots]\)
where \(T_{i} < T_{i+1}\), and let \(Z_{i}\) be the transaction with
epoch \(T_{i}\).
Then in \(x\), for all \(i\), for all agents \(v\):All steps of \(v\) in \(Z_{i}\) occur before any step
of \(v\) in \(Z_{i+1}\).ProofAssume that the sufficient condition is
satisfied for epochs of steps in a computation. 
Let \(e\) and \(e'\) be steps in transactions with epoch \(T\) and
\(T'\), respectively, and let \(T < T'\).
Let \(e\) and \(e'\) be steps at the same agent \(v\).
From the observation regarding epochs: \(e\) occurs before \(e'\).
Therefore all steps of a transaction with epoch \(T_{i}\) occur before
any step of a transaction with epoch \(T_{i+1}\).An Algorithm Based on the Sufficent ConditionNext, we describe an algorithm based on this idea.
Let's consider the two issues posed by the condition for serializability.
(1) How can the algorithm assign a unique epoch to each transaction?
(2) How can the algorithm assign epochs to steps so that
all steps in a transaction have the epoch of the transaction?Uniqueness of a Transaction's EpochA clientpinitiates a new transaction whenpgets a clock tick message.
To ensure that the epoch of the transaction is unique, an  epochtis a pair(n, p_id)wherenis a
number andp_idis the id of clientp.
Transactions initiated by different clients have different epochs
because their client ids are different.
A client sets the epoch of a new transaction that it initiates to be
greater than epochs of all previous transactions that it initiated.
So, different transactions initiated by the same client have different
epochs.
Therefore each epoch is unique.Client ids are totally ordered, and so epochs are also totally
ordered.
For brevity we refer to an epoch by a single valuetrather than a
pair(n, p_id).All Steps in a Transaction have the
Transaction's EpochWe associate a fieldtwith each agent -- client or
server -- wheretis the epoch of the step of the transaction that the agent is
executing.
Likewise, we associate a fieldm.twith each messagembetween clients and servers
wherem.tis the epoch of the step in which the message
is sent.A message sent in a step of a transaction is received in a step of the
same transaction.
So, for a messagembetween clients and servers,m.tis also the epoch of the step in which the message is received.We design the algorithm so that all steps of a transaction, all
messages sent in steps of the transaction, and all messages received
in steps of the transaction have the epoch of the transaction.Next we give the algorithm for clients and then for servers.Algorithm for a ClientWhen a clientpgets a clock tick message it executesp.t = p.t + pos()whereposreturns a positive value, and 
thenpinitiates a new transaction with epochp.t.
Clientpcontinues executing the transaction with epochp.tuntilpgets its next
clock tick message at which point it increasesp.tand
starts a new transaction.In the algorithm
all steps ofp, all requests sent bypto clients, and all
replies received bypfrom clients, have epochp.t.# Initialization
p.t = 0
start()
def receive(message, sender):
   if isinstance(message, ClockTick):
      # Received clock tick
      p.t = p.t + pos()
      # Start new transaction with epoch p.t
      # p.copy stores values in replies.
      p.copy = {}
      # Broadcast read request with epoch p.t
      for q in Q:  send(ReadRequest(p.t), q)
   else:
      # received a reply to a read request
      if message.t == p.t: 
         p.copy[sender] = message.v
         # send write requests if M replies received
         if len(p.copy) >= M:
            # Broadcast write request: value v, epoch t
            v, t = p.f(), p.t
            for q in Q: send(WriteRequest(v, t), q)Algorithm for a ServerA server waits to get requests from clients.When a serverqreceives a requestrfrom
the transaction that the server 
is currently processing i.e.r.t\(=\)q.t,
then the server responds to the request.When a serverqreceives a requestrfrom a
transaction with a smaller 
epoch than the transaction that the server is currently processing,
i.e.r.t\(<\)q.t, then the server does not
respond to the request. The request has the same effect as a request that
is lost.What should serverqdo when it receives a requestrfrom a transaction with a larger epoch than the
server's epoch, i.e.r.t\(>\)q.t?In this case, the server increases its epoch to the
epoch of the request and then responds to the request.
The request, the step in which the server responds to the request,
and the reply (to a read request), have the same epoch.# initialization
q.v, q.t = init, 0
start()
def receive(request, client):
   if request.t >= q.t
      q.t = request.t
      if isinstance(request, ReadRequest):
         send(Reply(q.v, q.t), client)
      else:
         // message is a WriteRequest
         q.v = request.vCorrectnessNext we prove that all computations are serializable in a system
specified by these algorithms for clients and servers.Epoch of a step in a computation of the algorithmThe epoch of a step taken by a clientpor serverqis the value ofp.torq.t(respectively) upon completion of
the step.TheoremThe assignment of epochs to steps satisfies the
condition for epochs, i.e.,
For all
steps \((e, e')\) of a computationif \(e\) and \(e'\) are steps at the same agent, and \(e'\) occurs
  after \(e\) then \(t(e) \leq t(e')\), andif \(e\) is a step in which a message is sent and \(e'\) is a step
  in which that message is received then \(t(e) \leq t(e')\).ProofFrom the algorithms for serverpand clientqit follows thatp.tandq.tdo not decrease in a
computation.
Therefore the first condition is satisfied.Also, from the algorithm, the epoch for step in which a request or reply is
received is equal to the epoch in which the message is sent.
So, the second condition is also satisfied.
Therefore the assignment of epochs to steps of a computation satisfy
the specification of epochs.TheoremThe sufficient condition for serializability is satisfied:
(1) each transaction has a unique epoch, and (2) all steps in a
transaction have the epoch of the transaction.ProofWe have shown that each transaction initiated by a clientphas a unique epochp.t.All steps atphave epochp.t.
Requests sent byphave epochp.t.
When a clientqgets a request with epochp.t, the client  sets its own epoch top.tand sends a reply with the same epoch.
So, all steps in a transaction have the epoch of the transaction.
Therefore the algorithm satisfies the sufficient condition for
serializability.CorollaryThe algorithm satisfies specifications for serializability.ProofFollows because the algorithm satisfies the sufficient condition for
serializability.Therefore,
if the epochs of transactions in a computation \(x\) are \([T_{0}, T_{1}, T_{2}, \ldots]\)
where \(T_{i} < T_{i+1}\), then in \(x\), for all \(i\), for all agents \(v\):
All steps of \(v\) in the transaction with epoch \(T_{i}\) occur before any step
of \(v\) in the transaction with epoch \(T_{i+1}\).Equivalence of Distributed
and Sequential AlgorithmsLet the sequence of increasing epochs in a computation \(x\) of the distributed
algorithm beT[1], T[2], T[3], ..., and letp[i], R[i], W[i]be the values ofp, R, Win
the transaction with epochT[i].The sequence of values of agent variables in 
computation \(x\) of the distributed algorithm is the same as in the
following nondeterministic sequential program by selectingp, R, W, on thei-th iteration to bep[i], R[i], W[i]respectively, and by selectingdeltaon thei-th iteration so thatT[i] = T[i-1] + deltaEquivalent nondeterministic sequential algorithmt = 0
while True:
   select positive delta, select client p
   t = t + delta
   # p executes transaction with epoch t
   p.t = t
   p.copy = {}
   # Read step
   select R
   for q in R: p.copy[q] = q.v
   # Write step
   if len(p.copy) >= M:
     select W
     for q in W: q.v = p.f()We will prove properties of the sequential algorithm and show that
these properties also hold for the distributed algorithm.NextNext we develop Paxos, a distributed consensus algorithm.
We prove
that Paxos satisfies the specifications for consensus by showing that
Paxos is serializable.
We prove properties of a nondeterministic sequential algorithm and
show that properties of the sequential algorithm also hold for Paxos.K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

The list of replies thatpgets beforepreceives its next clock tick is:[q.v for q in R]Clientpsends a request to all servers to write:p.f(q.v for q in R)wherep.fis a function ofp.Write requests may get lost.
LetWbe the set of servers that receive write requests.A transaction consists of the step in which a clientpbroadcasts read requests to all servers, the steps in which servers
receive read requests and send replies, the steps in whichpreceives replies, the step in whichpsends write requests, and the steps in which servers
receive and execute write requests.Changes to server variables in a transaction are specified by the
following statement in whichp.fis a function executed
byp.Transactionif len(R) >= M:
   for q in W: q.v = p.f(q.v for q in R)Transactions are tailored to a specific application by specifyingMandp.f.
The selections ofRandWare
nondeterministic and they can be arbitrary sets of servers.Transactions executed concurrently by multiple clients can interfere
with each other as illustrated by the following example.ExampleLetxbe the amount of funds in an account and 
  assume thatxhas $110.
  
  Consider a computation in which clientspandp'both execute identical transactions concurrently.
In a transaction a client readsxand ifxhas at least $100 then the client transfers $100 out
ofxto an accounty.Consider a computation in which both clientspandp'readxand verify thatxhas at least $100,
and then both clients transfer $100 out ofxand set the amount
  inxto $10.
The computation transfers $200 out ofxbut 
debitsxby only $100.This situation does not occur if only one client executes a
transaction at a time.
Ifpexecutes its transaction first then whenpcompletes the transaction it sets the amount inxto $10. 
Ifp'executes its transaction next thenp'finds thatxhas only $10 and sop'does not
transfer $100 fromx.Serializable ComputationsA system executes multiple transactions concurrently.A computation \(x\) is serializableexactly when there exists a
sequence of transactions \(Z_{i}, i \geq 0\), such that in  \(x\):
For all agents \(v\), for all \(i\):all steps of \(v\) in \(Z_{i}\) occur before any step
of \(v\) in \(Z_{i+1}\).It is possible that in a serializable computation \(x\), an 
agent \(u\) takes a step in \(Z_{j}\) before adifferentagent
\(v\) takes a step in \(Z_{i}\), where \(j > i\).Transactions and Serializability in
DatabasesThere is an extensive literature ontransactions and serializability.Also seetransaction processing systemsandonline transaction processing.
In this webpage we provide a narrow definition of transactions and
serializability
that is adequate for describing algorithms such asPaxos.The Problem: Guarantee SerializabilityThe problem is to develop a distributed algorithm, with multiple
clients and servers, in which computations are serializable, and in
which the system may be faulty.How Should You Solve The Problem?What method comes to mind to partition the sequence of steps at an
agent into intervals where the agent completes all steps of exactly one
transaction in each interval?Logical time partitions computations into a past and a future.
Let's use a mechanism, similar to logical time.
As with logical time, each step \(e\) in a computation is assigned a
value \(t(e)\), called theepochof \(e\).
For any step \(e\), steps with epochs less than, equal to, or greater
than, \(t(e)\) are in the past,current, and future,
respectively.
We specify epochs such that thesteps in 
current consists of all the steps of exactly one
transaction.Epochs and Logical TimesRecall thatthe logical timeof a step \(e\) is a value \(t(e)\) assigned to
each step \(e\) in a computation such 
that for all edges \((e, e')\) of the dataflow graph:
\(t(e) < t(e')\).
Theepochof a step \(e\) in a computation is defined as a
value \(t(e)\) assigned to 
each step \(e\) in the computation such 
that for all edges \((e, e')\) of the dataflow graph:
\(t(e) \leq t(e')\).Rules for Assigning Epochs to StepsThe following rules ensure that the assignment \(t(e)\) to each step
\(e\) in a computation satisfies the specification for epochs. For all
steps \((e, e')\) of a computationfor all steps \(e\) and \(e'\) at the same agent: if \(e'\) occurs
  after \(e\) then \(t(e) \leq t(e')\), andif \(e\) is a step in which a message is sent and \(e'\) is a step
  in which that message is received then \(t(e) \leq t(e')\).ObservationIf \(e\) and \(e'\) are steps at the same agent and \(t(e) < t(e')\)
then \(e'\) occurs after \(e\).Theorem: A Sufficent Condition for SerializabilityA sufficient conditions for a computation to be serializable is:There exists epochs for all steps of a computation such thateach transaction has a unique epoch, andall steps in a transaction have the epoch of the transaction.If a computation \(x\) satisfies this condition then 
let the epochs of transactions in \(x\) be \([T_{0}, T_{1}, T_{2}, \ldots]\)
where \(T_{i} < T_{i+1}\), and let \(Z_{i}\) be the transaction with
epoch \(T_{i}\).
Then in \(x\), for all \(i\), for all agents \(v\):All steps of \(v\) in \(Z_{i}\) occur before any step
of \(v\) in \(Z_{i+1}\).ProofAssume that the sufficient condition is
satisfied for epochs of steps in a computation. 
Let \(e\) and \(e'\) be steps in transactions with epoch \(T\) and
\(T'\), respectively, and let \(T < T'\).
Let \(e\) and \(e'\) be steps at the same agent \(v\).
From the observation regarding epochs: \(e\) occurs before \(e'\).
Therefore all steps of a transaction with epoch \(T_{i}\) occur before
any step of a transaction with epoch \(T_{i+1}\).An Algorithm Based on the Sufficent ConditionNext, we describe an algorithm based on this idea.
Let's consider the two issues posed by the condition for serializability.
(1) How can the algorithm assign a unique epoch to each transaction?
(2) How can the algorithm assign epochs to steps so that
all steps in a transaction have the epoch of the transaction?Uniqueness of a Transaction's EpochA clientpinitiates a new transaction whenpgets a clock tick message.
To ensure that the epoch of the transaction is unique, an  epochtis a pair(n, p_id)wherenis a
number andp_idis the id of clientp.
Transactions initiated by different clients have different epochs
because their client ids are different.
A client sets the epoch of a new transaction that it initiates to be
greater than epochs of all previous transactions that it initiated.
So, different transactions initiated by the same client have different
epochs.
Therefore each epoch is unique.Client ids are totally ordered, and so epochs are also totally
ordered.
For brevity we refer to an epoch by a single valuetrather than a
pair(n, p_id).All Steps in a Transaction have the
Transaction's EpochWe associate a fieldtwith each agent -- client or
server -- wheretis the epoch of the step of the transaction that the agent is
executing.
Likewise, we associate a fieldm.twith each messagembetween clients and servers
wherem.tis the epoch of the step in which the message
is sent.A message sent in a step of a transaction is received in a step of the
same transaction.
So, for a messagembetween clients and servers,m.tis also the epoch of the step in which the message is received.We design the algorithm so that all steps of a transaction, all
messages sent in steps of the transaction, and all messages received
in steps of the transaction have the epoch of the transaction.Next we give the algorithm for clients and then for servers.Algorithm for a ClientWhen a clientpgets a clock tick message it executesp.t = p.t + pos()whereposreturns a positive value, and 
thenpinitiates a new transaction with epochp.t.
Clientpcontinues executing the transaction with epochp.tuntilpgets its next
clock tick message at which point it increasesp.tand
starts a new transaction.In the algorithm
all steps ofp, all requests sent bypto clients, and all
replies received bypfrom clients, have epochp.t.# Initialization
p.t = 0
start()
def receive(message, sender):
   if isinstance(message, ClockTick):
      # Received clock tick
      p.t = p.t + pos()
      # Start new transaction with epoch p.t
      # p.copy stores values in replies.
      p.copy = {}
      # Broadcast read request with epoch p.t
      for q in Q:  send(ReadRequest(p.t), q)
   else:
      # received a reply to a read request
      if message.t == p.t: 
         p.copy[sender] = message.v
         # send write requests if M replies received
         if len(p.copy) >= M:
            # Broadcast write request: value v, epoch t
            v, t = p.f(), p.t
            for q in Q: send(WriteRequest(v, t), q)Algorithm for a ServerA server waits to get requests from clients.When a serverqreceives a requestrfrom
the transaction that the server 
is currently processing i.e.r.t\(=\)q.t,
then the server responds to the request.When a serverqreceives a requestrfrom a
transaction with a smaller 
epoch than the transaction that the server is currently processing,
i.e.r.t\(<\)q.t, then the server does not
respond to the request. The request has the same effect as a request that
is lost.What should serverqdo when it receives a requestrfrom a transaction with a larger epoch than the
server's epoch, i.e.r.t\(>\)q.t?In this case, the server increases its epoch to the
epoch of the request and then responds to the request.
The request, the step in which the server responds to the request,
and the reply (to a read request), have the same epoch.# initialization
q.v, q.t = init, 0
start()
def receive(request, client):
   if request.t >= q.t
      q.t = request.t
      if isinstance(request, ReadRequest):
         send(Reply(q.v, q.t), client)
      else:
         // message is a WriteRequest
         q.v = request.vCorrectnessNext we prove that all computations are serializable in a system
specified by these algorithms for clients and servers.Epoch of a step in a computation of the algorithmThe epoch of a step taken by a clientpor serverqis the value ofp.torq.t(respectively) upon completion of
the step.TheoremThe assignment of epochs to steps satisfies the
condition for epochs, i.e.,
For all
steps \((e, e')\) of a computationif \(e\) and \(e'\) are steps at the same agent, and \(e'\) occurs
  after \(e\) then \(t(e) \leq t(e')\), andif \(e\) is a step in which a message is sent and \(e'\) is a step
  in which that message is received then \(t(e) \leq t(e')\).ProofFrom the algorithms for serverpand clientqit follows thatp.tandq.tdo not decrease in a
computation.
Therefore the first condition is satisfied.Also, from the algorithm, the epoch for step in which a request or reply is
received is equal to the epoch in which the message is sent.
So, the second condition is also satisfied.
Therefore the assignment of epochs to steps of a computation satisfy
the specification of epochs.TheoremThe sufficient condition for serializability is satisfied:
(1) each transaction has a unique epoch, and (2) all steps in a
transaction have the epoch of the transaction.ProofWe have shown that each transaction initiated by a clientphas a unique epochp.t.All steps atphave epochp.t.
Requests sent byphave epochp.t.
When a clientqgets a request with epochp.t, the client  sets its own epoch top.tand sends a reply with the same epoch.
So, all steps in a transaction have the epoch of the transaction.
Therefore the algorithm satisfies the sufficient condition for
serializability.CorollaryThe algorithm satisfies specifications for serializability.ProofFollows because the algorithm satisfies the sufficient condition for
serializability.Therefore,
if the epochs of transactions in a computation \(x\) are \([T_{0}, T_{1}, T_{2}, \ldots]\)
where \(T_{i} < T_{i+1}\), then in \(x\), for all \(i\), for all agents \(v\):
All steps of \(v\) in the transaction with epoch \(T_{i}\) occur before any step
of \(v\) in the transaction with epoch \(T_{i+1}\).Equivalence of Distributed
and Sequential AlgorithmsLet the sequence of increasing epochs in a computation \(x\) of the distributed
algorithm beT[1], T[2], T[3], ..., and letp[i], R[i], W[i]be the values ofp, R, Win
the transaction with epochT[i].The sequence of values of agent variables in 
computation \(x\) of the distributed algorithm is the same as in the
following nondeterministic sequential program by selectingp, R, W, on thei-th iteration to bep[i], R[i], W[i]respectively, and by selectingdeltaon thei-th iteration so thatT[i] = T[i-1] + deltaEquivalent nondeterministic sequential algorithmt = 0
while True:
   select positive delta, select client p
   t = t + delta
   # p executes transaction with epoch t
   p.t = t
   p.copy = {}
   # Read step
   select R
   for q in R: p.copy[q] = q.v
   # Write step
   if len(p.copy) >= M:
     select W
     for q in W: q.v = p.f()We will prove properties of the sequential algorithm and show that
these properties also hold for the distributed algorithm.NextNext we develop Paxos, a distributed consensus algorithm.
We prove
that Paxos satisfies the specifications for consensus by showing that
Paxos is serializable.
We prove properties of a nondeterministic sequential algorithm and
show that properties of the sequential algorithm also hold for Paxos.K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

Write requests may get lost.
LetWbe the set of servers that receive write requests.A transaction consists of the step in which a clientpbroadcasts read requests to all servers, the steps in which servers
receive read requests and send replies, the steps in whichpreceives replies, the step in whichpsends write requests, and the steps in which servers
receive and execute write requests.Changes to server variables in a transaction are specified by the
following statement in whichp.fis a function executed
byp.Transactionif len(R) >= M:
   for q in W: q.v = p.f(q.v for q in R)Transactions are tailored to a specific application by specifyingMandp.f.
The selections ofRandWare
nondeterministic and they can be arbitrary sets of servers.Transactions executed concurrently by multiple clients can interfere
with each other as illustrated by the following example.ExampleLetxbe the amount of funds in an account and 
  assume thatxhas $110.
  
  Consider a computation in which clientspandp'both execute identical transactions concurrently.
In a transaction a client readsxand ifxhas at least $100 then the client transfers $100 out
ofxto an accounty.Consider a computation in which both clientspandp'readxand verify thatxhas at least $100,
and then both clients transfer $100 out ofxand set the amount
  inxto $10.
The computation transfers $200 out ofxbut 
debitsxby only $100.This situation does not occur if only one client executes a
transaction at a time.
Ifpexecutes its transaction first then whenpcompletes the transaction it sets the amount inxto $10. 
Ifp'executes its transaction next thenp'finds thatxhas only $10 and sop'does not
transfer $100 fromx.Serializable ComputationsA system executes multiple transactions concurrently.A computation \(x\) is serializableexactly when there exists a
sequence of transactions \(Z_{i}, i \geq 0\), such that in  \(x\):
For all agents \(v\), for all \(i\):all steps of \(v\) in \(Z_{i}\) occur before any step
of \(v\) in \(Z_{i+1}\).It is possible that in a serializable computation \(x\), an 
agent \(u\) takes a step in \(Z_{j}\) before adifferentagent
\(v\) takes a step in \(Z_{i}\), where \(j > i\).Transactions and Serializability in
DatabasesThere is an extensive literature ontransactions and serializability.Also seetransaction processing systemsandonline transaction processing.
In this webpage we provide a narrow definition of transactions and
serializability
that is adequate for describing algorithms such asPaxos.The Problem: Guarantee SerializabilityThe problem is to develop a distributed algorithm, with multiple
clients and servers, in which computations are serializable, and in
which the system may be faulty.How Should You Solve The Problem?What method comes to mind to partition the sequence of steps at an
agent into intervals where the agent completes all steps of exactly one
transaction in each interval?Logical time partitions computations into a past and a future.
Let's use a mechanism, similar to logical time.
As with logical time, each step \(e\) in a computation is assigned a
value \(t(e)\), called theepochof \(e\).
For any step \(e\), steps with epochs less than, equal to, or greater
than, \(t(e)\) are in the past,current, and future,
respectively.
We specify epochs such that thesteps in 
current consists of all the steps of exactly one
transaction.Epochs and Logical TimesRecall thatthe logical timeof a step \(e\) is a value \(t(e)\) assigned to
each step \(e\) in a computation such 
that for all edges \((e, e')\) of the dataflow graph:
\(t(e) < t(e')\).
Theepochof a step \(e\) in a computation is defined as a
value \(t(e)\) assigned to 
each step \(e\) in the computation such 
that for all edges \((e, e')\) of the dataflow graph:
\(t(e) \leq t(e')\).Rules for Assigning Epochs to StepsThe following rules ensure that the assignment \(t(e)\) to each step
\(e\) in a computation satisfies the specification for epochs. For all
steps \((e, e')\) of a computationfor all steps \(e\) and \(e'\) at the same agent: if \(e'\) occurs
  after \(e\) then \(t(e) \leq t(e')\), andif \(e\) is a step in which a message is sent and \(e'\) is a step
  in which that message is received then \(t(e) \leq t(e')\).ObservationIf \(e\) and \(e'\) are steps at the same agent and \(t(e) < t(e')\)
then \(e'\) occurs after \(e\).Theorem: A Sufficent Condition for SerializabilityA sufficient conditions for a computation to be serializable is:There exists epochs for all steps of a computation such thateach transaction has a unique epoch, andall steps in a transaction have the epoch of the transaction.If a computation \(x\) satisfies this condition then 
let the epochs of transactions in \(x\) be \([T_{0}, T_{1}, T_{2}, \ldots]\)
where \(T_{i} < T_{i+1}\), and let \(Z_{i}\) be the transaction with
epoch \(T_{i}\).
Then in \(x\), for all \(i\), for all agents \(v\):All steps of \(v\) in \(Z_{i}\) occur before any step
of \(v\) in \(Z_{i+1}\).ProofAssume that the sufficient condition is
satisfied for epochs of steps in a computation. 
Let \(e\) and \(e'\) be steps in transactions with epoch \(T\) and
\(T'\), respectively, and let \(T < T'\).
Let \(e\) and \(e'\) be steps at the same agent \(v\).
From the observation regarding epochs: \(e\) occurs before \(e'\).
Therefore all steps of a transaction with epoch \(T_{i}\) occur before
any step of a transaction with epoch \(T_{i+1}\).An Algorithm Based on the Sufficent ConditionNext, we describe an algorithm based on this idea.
Let's consider the two issues posed by the condition for serializability.
(1) How can the algorithm assign a unique epoch to each transaction?
(2) How can the algorithm assign epochs to steps so that
all steps in a transaction have the epoch of the transaction?Uniqueness of a Transaction's EpochA clientpinitiates a new transaction whenpgets a clock tick message.
To ensure that the epoch of the transaction is unique, an  epochtis a pair(n, p_id)wherenis a
number andp_idis the id of clientp.
Transactions initiated by different clients have different epochs
because their client ids are different.
A client sets the epoch of a new transaction that it initiates to be
greater than epochs of all previous transactions that it initiated.
So, different transactions initiated by the same client have different
epochs.
Therefore each epoch is unique.Client ids are totally ordered, and so epochs are also totally
ordered.
For brevity we refer to an epoch by a single valuetrather than a
pair(n, p_id).All Steps in a Transaction have the
Transaction's EpochWe associate a fieldtwith each agent -- client or
server -- wheretis the epoch of the step of the transaction that the agent is
executing.
Likewise, we associate a fieldm.twith each messagembetween clients and servers
wherem.tis the epoch of the step in which the message
is sent.A message sent in a step of a transaction is received in a step of the
same transaction.
So, for a messagembetween clients and servers,m.tis also the epoch of the step in which the message is received.We design the algorithm so that all steps of a transaction, all
messages sent in steps of the transaction, and all messages received
in steps of the transaction have the epoch of the transaction.Next we give the algorithm for clients and then for servers.Algorithm for a ClientWhen a clientpgets a clock tick message it executesp.t = p.t + pos()whereposreturns a positive value, and 
thenpinitiates a new transaction with epochp.t.
Clientpcontinues executing the transaction with epochp.tuntilpgets its next
clock tick message at which point it increasesp.tand
starts a new transaction.In the algorithm
all steps ofp, all requests sent bypto clients, and all
replies received bypfrom clients, have epochp.t.# Initialization
p.t = 0
start()
def receive(message, sender):
   if isinstance(message, ClockTick):
      # Received clock tick
      p.t = p.t + pos()
      # Start new transaction with epoch p.t
      # p.copy stores values in replies.
      p.copy = {}
      # Broadcast read request with epoch p.t
      for q in Q:  send(ReadRequest(p.t), q)
   else:
      # received a reply to a read request
      if message.t == p.t: 
         p.copy[sender] = message.v
         # send write requests if M replies received
         if len(p.copy) >= M:
            # Broadcast write request: value v, epoch t
            v, t = p.f(), p.t
            for q in Q: send(WriteRequest(v, t), q)Algorithm for a ServerA server waits to get requests from clients.When a serverqreceives a requestrfrom
the transaction that the server 
is currently processing i.e.r.t\(=\)q.t,
then the server responds to the request.When a serverqreceives a requestrfrom a
transaction with a smaller 
epoch than the transaction that the server is currently processing,
i.e.r.t\(<\)q.t, then the server does not
respond to the request. The request has the same effect as a request that
is lost.What should serverqdo when it receives a requestrfrom a transaction with a larger epoch than the
server's epoch, i.e.r.t\(>\)q.t?In this case, the server increases its epoch to the
epoch of the request and then responds to the request.
The request, the step in which the server responds to the request,
and the reply (to a read request), have the same epoch.# initialization
q.v, q.t = init, 0
start()
def receive(request, client):
   if request.t >= q.t
      q.t = request.t
      if isinstance(request, ReadRequest):
         send(Reply(q.v, q.t), client)
      else:
         // message is a WriteRequest
         q.v = request.vCorrectnessNext we prove that all computations are serializable in a system
specified by these algorithms for clients and servers.Epoch of a step in a computation of the algorithmThe epoch of a step taken by a clientpor serverqis the value ofp.torq.t(respectively) upon completion of
the step.TheoremThe assignment of epochs to steps satisfies the
condition for epochs, i.e.,
For all
steps \((e, e')\) of a computationif \(e\) and \(e'\) are steps at the same agent, and \(e'\) occurs
  after \(e\) then \(t(e) \leq t(e')\), andif \(e\) is a step in which a message is sent and \(e'\) is a step
  in which that message is received then \(t(e) \leq t(e')\).ProofFrom the algorithms for serverpand clientqit follows thatp.tandq.tdo not decrease in a
computation.
Therefore the first condition is satisfied.Also, from the algorithm, the epoch for step in which a request or reply is
received is equal to the epoch in which the message is sent.
So, the second condition is also satisfied.
Therefore the assignment of epochs to steps of a computation satisfy
the specification of epochs.TheoremThe sufficient condition for serializability is satisfied:
(1) each transaction has a unique epoch, and (2) all steps in a
transaction have the epoch of the transaction.ProofWe have shown that each transaction initiated by a clientphas a unique epochp.t.All steps atphave epochp.t.
Requests sent byphave epochp.t.
When a clientqgets a request with epochp.t, the client  sets its own epoch top.tand sends a reply with the same epoch.
So, all steps in a transaction have the epoch of the transaction.
Therefore the algorithm satisfies the sufficient condition for
serializability.CorollaryThe algorithm satisfies specifications for serializability.ProofFollows because the algorithm satisfies the sufficient condition for
serializability.Therefore,
if the epochs of transactions in a computation \(x\) are \([T_{0}, T_{1}, T_{2}, \ldots]\)
where \(T_{i} < T_{i+1}\), then in \(x\), for all \(i\), for all agents \(v\):
All steps of \(v\) in the transaction with epoch \(T_{i}\) occur before any step
of \(v\) in the transaction with epoch \(T_{i+1}\).Equivalence of Distributed
and Sequential AlgorithmsLet the sequence of increasing epochs in a computation \(x\) of the distributed
algorithm beT[1], T[2], T[3], ..., and letp[i], R[i], W[i]be the values ofp, R, Win
the transaction with epochT[i].The sequence of values of agent variables in 
computation \(x\) of the distributed algorithm is the same as in the
following nondeterministic sequential program by selectingp, R, W, on thei-th iteration to bep[i], R[i], W[i]respectively, and by selectingdeltaon thei-th iteration so thatT[i] = T[i-1] + deltaEquivalent nondeterministic sequential algorithmt = 0
while True:
   select positive delta, select client p
   t = t + delta
   # p executes transaction with epoch t
   p.t = t
   p.copy = {}
   # Read step
   select R
   for q in R: p.copy[q] = q.v
   # Write step
   if len(p.copy) >= M:
     select W
     for q in W: q.v = p.f()We will prove properties of the sequential algorithm and show that
these properties also hold for the distributed algorithm.NextNext we develop Paxos, a distributed consensus algorithm.
We prove
that Paxos satisfies the specifications for consensus by showing that
Paxos is serializable.
We prove properties of a nondeterministic sequential algorithm and
show that properties of the sequential algorithm also hold for Paxos.K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

A transaction consists of the step in which a clientpbroadcasts read requests to all servers, the steps in which servers
receive read requests and send replies, the steps in whichpreceives replies, the step in whichpsends write requests, and the steps in which servers
receive and execute write requests.Changes to server variables in a transaction are specified by the
following statement in whichp.fis a function executed
byp.Transactionif len(R) >= M:
   for q in W: q.v = p.f(q.v for q in R)Transactions are tailored to a specific application by specifyingMandp.f.
The selections ofRandWare
nondeterministic and they can be arbitrary sets of servers.Transactions executed concurrently by multiple clients can interfere
with each other as illustrated by the following example.ExampleLetxbe the amount of funds in an account and 
  assume thatxhas $110.
  
  Consider a computation in which clientspandp'both execute identical transactions concurrently.
In a transaction a client readsxand ifxhas at least $100 then the client transfers $100 out
ofxto an accounty.Consider a computation in which both clientspandp'readxand verify thatxhas at least $100,
and then both clients transfer $100 out ofxand set the amount
  inxto $10.
The computation transfers $200 out ofxbut 
debitsxby only $100.This situation does not occur if only one client executes a
transaction at a time.
Ifpexecutes its transaction first then whenpcompletes the transaction it sets the amount inxto $10. 
Ifp'executes its transaction next thenp'finds thatxhas only $10 and sop'does not
transfer $100 fromx.Serializable ComputationsA system executes multiple transactions concurrently.A computation \(x\) is serializableexactly when there exists a
sequence of transactions \(Z_{i}, i \geq 0\), such that in  \(x\):
For all agents \(v\), for all \(i\):all steps of \(v\) in \(Z_{i}\) occur before any step
of \(v\) in \(Z_{i+1}\).It is possible that in a serializable computation \(x\), an 
agent \(u\) takes a step in \(Z_{j}\) before adifferentagent
\(v\) takes a step in \(Z_{i}\), where \(j > i\).Transactions and Serializability in
DatabasesThere is an extensive literature ontransactions and serializability.Also seetransaction processing systemsandonline transaction processing.
In this webpage we provide a narrow definition of transactions and
serializability
that is adequate for describing algorithms such asPaxos.The Problem: Guarantee SerializabilityThe problem is to develop a distributed algorithm, with multiple
clients and servers, in which computations are serializable, and in
which the system may be faulty.How Should You Solve The Problem?What method comes to mind to partition the sequence of steps at an
agent into intervals where the agent completes all steps of exactly one
transaction in each interval?Logical time partitions computations into a past and a future.
Let's use a mechanism, similar to logical time.
As with logical time, each step \(e\) in a computation is assigned a
value \(t(e)\), called theepochof \(e\).
For any step \(e\), steps with epochs less than, equal to, or greater
than, \(t(e)\) are in the past,current, and future,
respectively.
We specify epochs such that thesteps in 
current consists of all the steps of exactly one
transaction.Epochs and Logical TimesRecall thatthe logical timeof a step \(e\) is a value \(t(e)\) assigned to
each step \(e\) in a computation such 
that for all edges \((e, e')\) of the dataflow graph:
\(t(e) < t(e')\).
Theepochof a step \(e\) in a computation is defined as a
value \(t(e)\) assigned to 
each step \(e\) in the computation such 
that for all edges \((e, e')\) of the dataflow graph:
\(t(e) \leq t(e')\).Rules for Assigning Epochs to StepsThe following rules ensure that the assignment \(t(e)\) to each step
\(e\) in a computation satisfies the specification for epochs. For all
steps \((e, e')\) of a computationfor all steps \(e\) and \(e'\) at the same agent: if \(e'\) occurs
  after \(e\) then \(t(e) \leq t(e')\), andif \(e\) is a step in which a message is sent and \(e'\) is a step
  in which that message is received then \(t(e) \leq t(e')\).ObservationIf \(e\) and \(e'\) are steps at the same agent and \(t(e) < t(e')\)
then \(e'\) occurs after \(e\).Theorem: A Sufficent Condition for SerializabilityA sufficient conditions for a computation to be serializable is:There exists epochs for all steps of a computation such thateach transaction has a unique epoch, andall steps in a transaction have the epoch of the transaction.If a computation \(x\) satisfies this condition then 
let the epochs of transactions in \(x\) be \([T_{0}, T_{1}, T_{2}, \ldots]\)
where \(T_{i} < T_{i+1}\), and let \(Z_{i}\) be the transaction with
epoch \(T_{i}\).
Then in \(x\), for all \(i\), for all agents \(v\):All steps of \(v\) in \(Z_{i}\) occur before any step
of \(v\) in \(Z_{i+1}\).ProofAssume that the sufficient condition is
satisfied for epochs of steps in a computation. 
Let \(e\) and \(e'\) be steps in transactions with epoch \(T\) and
\(T'\), respectively, and let \(T < T'\).
Let \(e\) and \(e'\) be steps at the same agent \(v\).
From the observation regarding epochs: \(e\) occurs before \(e'\).
Therefore all steps of a transaction with epoch \(T_{i}\) occur before
any step of a transaction with epoch \(T_{i+1}\).An Algorithm Based on the Sufficent ConditionNext, we describe an algorithm based on this idea.
Let's consider the two issues posed by the condition for serializability.
(1) How can the algorithm assign a unique epoch to each transaction?
(2) How can the algorithm assign epochs to steps so that
all steps in a transaction have the epoch of the transaction?Uniqueness of a Transaction's EpochA clientpinitiates a new transaction whenpgets a clock tick message.
To ensure that the epoch of the transaction is unique, an  epochtis a pair(n, p_id)wherenis a
number andp_idis the id of clientp.
Transactions initiated by different clients have different epochs
because their client ids are different.
A client sets the epoch of a new transaction that it initiates to be
greater than epochs of all previous transactions that it initiated.
So, different transactions initiated by the same client have different
epochs.
Therefore each epoch is unique.Client ids are totally ordered, and so epochs are also totally
ordered.
For brevity we refer to an epoch by a single valuetrather than a
pair(n, p_id).All Steps in a Transaction have the
Transaction's EpochWe associate a fieldtwith each agent -- client or
server -- wheretis the epoch of the step of the transaction that the agent is
executing.
Likewise, we associate a fieldm.twith each messagembetween clients and servers
wherem.tis the epoch of the step in which the message
is sent.A message sent in a step of a transaction is received in a step of the
same transaction.
So, for a messagembetween clients and servers,m.tis also the epoch of the step in which the message is received.We design the algorithm so that all steps of a transaction, all
messages sent in steps of the transaction, and all messages received
in steps of the transaction have the epoch of the transaction.Next we give the algorithm for clients and then for servers.Algorithm for a ClientWhen a clientpgets a clock tick message it executesp.t = p.t + pos()whereposreturns a positive value, and 
thenpinitiates a new transaction with epochp.t.
Clientpcontinues executing the transaction with epochp.tuntilpgets its next
clock tick message at which point it increasesp.tand
starts a new transaction.In the algorithm
all steps ofp, all requests sent bypto clients, and all
replies received bypfrom clients, have epochp.t.# Initialization
p.t = 0
start()
def receive(message, sender):
   if isinstance(message, ClockTick):
      # Received clock tick
      p.t = p.t + pos()
      # Start new transaction with epoch p.t
      # p.copy stores values in replies.
      p.copy = {}
      # Broadcast read request with epoch p.t
      for q in Q:  send(ReadRequest(p.t), q)
   else:
      # received a reply to a read request
      if message.t == p.t: 
         p.copy[sender] = message.v
         # send write requests if M replies received
         if len(p.copy) >= M:
            # Broadcast write request: value v, epoch t
            v, t = p.f(), p.t
            for q in Q: send(WriteRequest(v, t), q)Algorithm for a ServerA server waits to get requests from clients.When a serverqreceives a requestrfrom
the transaction that the server 
is currently processing i.e.r.t\(=\)q.t,
then the server responds to the request.When a serverqreceives a requestrfrom a
transaction with a smaller 
epoch than the transaction that the server is currently processing,
i.e.r.t\(<\)q.t, then the server does not
respond to the request. The request has the same effect as a request that
is lost.What should serverqdo when it receives a requestrfrom a transaction with a larger epoch than the
server's epoch, i.e.r.t\(>\)q.t?In this case, the server increases its epoch to the
epoch of the request and then responds to the request.
The request, the step in which the server responds to the request,
and the reply (to a read request), have the same epoch.# initialization
q.v, q.t = init, 0
start()
def receive(request, client):
   if request.t >= q.t
      q.t = request.t
      if isinstance(request, ReadRequest):
         send(Reply(q.v, q.t), client)
      else:
         // message is a WriteRequest
         q.v = request.vCorrectnessNext we prove that all computations are serializable in a system
specified by these algorithms for clients and servers.Epoch of a step in a computation of the algorithmThe epoch of a step taken by a clientpor serverqis the value ofp.torq.t(respectively) upon completion of
the step.TheoremThe assignment of epochs to steps satisfies the
condition for epochs, i.e.,
For all
steps \((e, e')\) of a computationif \(e\) and \(e'\) are steps at the same agent, and \(e'\) occurs
  after \(e\) then \(t(e) \leq t(e')\), andif \(e\) is a step in which a message is sent and \(e'\) is a step
  in which that message is received then \(t(e) \leq t(e')\).ProofFrom the algorithms for serverpand clientqit follows thatp.tandq.tdo not decrease in a
computation.
Therefore the first condition is satisfied.Also, from the algorithm, the epoch for step in which a request or reply is
received is equal to the epoch in which the message is sent.
So, the second condition is also satisfied.
Therefore the assignment of epochs to steps of a computation satisfy
the specification of epochs.TheoremThe sufficient condition for serializability is satisfied:
(1) each transaction has a unique epoch, and (2) all steps in a
transaction have the epoch of the transaction.ProofWe have shown that each transaction initiated by a clientphas a unique epochp.t.All steps atphave epochp.t.
Requests sent byphave epochp.t.
When a clientqgets a request with epochp.t, the client  sets its own epoch top.tand sends a reply with the same epoch.
So, all steps in a transaction have the epoch of the transaction.
Therefore the algorithm satisfies the sufficient condition for
serializability.CorollaryThe algorithm satisfies specifications for serializability.ProofFollows because the algorithm satisfies the sufficient condition for
serializability.Therefore,
if the epochs of transactions in a computation \(x\) are \([T_{0}, T_{1}, T_{2}, \ldots]\)
where \(T_{i} < T_{i+1}\), then in \(x\), for all \(i\), for all agents \(v\):
All steps of \(v\) in the transaction with epoch \(T_{i}\) occur before any step
of \(v\) in the transaction with epoch \(T_{i+1}\).Equivalence of Distributed
and Sequential AlgorithmsLet the sequence of increasing epochs in a computation \(x\) of the distributed
algorithm beT[1], T[2], T[3], ..., and letp[i], R[i], W[i]be the values ofp, R, Win
the transaction with epochT[i].The sequence of values of agent variables in 
computation \(x\) of the distributed algorithm is the same as in the
following nondeterministic sequential program by selectingp, R, W, on thei-th iteration to bep[i], R[i], W[i]respectively, and by selectingdeltaon thei-th iteration so thatT[i] = T[i-1] + deltaEquivalent nondeterministic sequential algorithmt = 0
while True:
   select positive delta, select client p
   t = t + delta
   # p executes transaction with epoch t
   p.t = t
   p.copy = {}
   # Read step
   select R
   for q in R: p.copy[q] = q.v
   # Write step
   if len(p.copy) >= M:
     select W
     for q in W: q.v = p.f()We will prove properties of the sequential algorithm and show that
these properties also hold for the distributed algorithm.NextNext we develop Paxos, a distributed consensus algorithm.
We prove
that Paxos satisfies the specifications for consensus by showing that
Paxos is serializable.
We prove properties of a nondeterministic sequential algorithm and
show that properties of the sequential algorithm also hold for Paxos.K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

Changes to server variables in a transaction are specified by the
following statement in whichp.fis a function executed
byp.Transactionif len(R) >= M:
   for q in W: q.v = p.f(q.v for q in R)Transactions are tailored to a specific application by specifyingMandp.f.
The selections ofRandWare
nondeterministic and they can be arbitrary sets of servers.Transactions executed concurrently by multiple clients can interfere
with each other as illustrated by the following example.ExampleLetxbe the amount of funds in an account and 
  assume thatxhas $110.
  
  Consider a computation in which clientspandp'both execute identical transactions concurrently.
In a transaction a client readsxand ifxhas at least $100 then the client transfers $100 out
ofxto an accounty.Consider a computation in which both clientspandp'readxand verify thatxhas at least $100,
and then both clients transfer $100 out ofxand set the amount
  inxto $10.
The computation transfers $200 out ofxbut 
debitsxby only $100.This situation does not occur if only one client executes a
transaction at a time.
Ifpexecutes its transaction first then whenpcompletes the transaction it sets the amount inxto $10. 
Ifp'executes its transaction next thenp'finds thatxhas only $10 and sop'does not
transfer $100 fromx.Serializable ComputationsA system executes multiple transactions concurrently.A computation \(x\) is serializableexactly when there exists a
sequence of transactions \(Z_{i}, i \geq 0\), such that in  \(x\):
For all agents \(v\), for all \(i\):all steps of \(v\) in \(Z_{i}\) occur before any step
of \(v\) in \(Z_{i+1}\).It is possible that in a serializable computation \(x\), an 
agent \(u\) takes a step in \(Z_{j}\) before adifferentagent
\(v\) takes a step in \(Z_{i}\), where \(j > i\).Transactions and Serializability in
DatabasesThere is an extensive literature ontransactions and serializability.Also seetransaction processing systemsandonline transaction processing.
In this webpage we provide a narrow definition of transactions and
serializability
that is adequate for describing algorithms such asPaxos.The Problem: Guarantee SerializabilityThe problem is to develop a distributed algorithm, with multiple
clients and servers, in which computations are serializable, and in
which the system may be faulty.How Should You Solve The Problem?What method comes to mind to partition the sequence of steps at an
agent into intervals where the agent completes all steps of exactly one
transaction in each interval?Logical time partitions computations into a past and a future.
Let's use a mechanism, similar to logical time.
As with logical time, each step \(e\) in a computation is assigned a
value \(t(e)\), called theepochof \(e\).
For any step \(e\), steps with epochs less than, equal to, or greater
than, \(t(e)\) are in the past,current, and future,
respectively.
We specify epochs such that thesteps in 
current consists of all the steps of exactly one
transaction.Epochs and Logical TimesRecall thatthe logical timeof a step \(e\) is a value \(t(e)\) assigned to
each step \(e\) in a computation such 
that for all edges \((e, e')\) of the dataflow graph:
\(t(e) < t(e')\).
Theepochof a step \(e\) in a computation is defined as a
value \(t(e)\) assigned to 
each step \(e\) in the computation such 
that for all edges \((e, e')\) of the dataflow graph:
\(t(e) \leq t(e')\).Rules for Assigning Epochs to StepsThe following rules ensure that the assignment \(t(e)\) to each step
\(e\) in a computation satisfies the specification for epochs. For all
steps \((e, e')\) of a computationfor all steps \(e\) and \(e'\) at the same agent: if \(e'\) occurs
  after \(e\) then \(t(e) \leq t(e')\), andif \(e\) is a step in which a message is sent and \(e'\) is a step
  in which that message is received then \(t(e) \leq t(e')\).ObservationIf \(e\) and \(e'\) are steps at the same agent and \(t(e) < t(e')\)
then \(e'\) occurs after \(e\).Theorem: A Sufficent Condition for SerializabilityA sufficient conditions for a computation to be serializable is:There exists epochs for all steps of a computation such thateach transaction has a unique epoch, andall steps in a transaction have the epoch of the transaction.If a computation \(x\) satisfies this condition then 
let the epochs of transactions in \(x\) be \([T_{0}, T_{1}, T_{2}, \ldots]\)
where \(T_{i} < T_{i+1}\), and let \(Z_{i}\) be the transaction with
epoch \(T_{i}\).
Then in \(x\), for all \(i\), for all agents \(v\):All steps of \(v\) in \(Z_{i}\) occur before any step
of \(v\) in \(Z_{i+1}\).ProofAssume that the sufficient condition is
satisfied for epochs of steps in a computation. 
Let \(e\) and \(e'\) be steps in transactions with epoch \(T\) and
\(T'\), respectively, and let \(T < T'\).
Let \(e\) and \(e'\) be steps at the same agent \(v\).
From the observation regarding epochs: \(e\) occurs before \(e'\).
Therefore all steps of a transaction with epoch \(T_{i}\) occur before
any step of a transaction with epoch \(T_{i+1}\).An Algorithm Based on the Sufficent ConditionNext, we describe an algorithm based on this idea.
Let's consider the two issues posed by the condition for serializability.
(1) How can the algorithm assign a unique epoch to each transaction?
(2) How can the algorithm assign epochs to steps so that
all steps in a transaction have the epoch of the transaction?Uniqueness of a Transaction's EpochA clientpinitiates a new transaction whenpgets a clock tick message.
To ensure that the epoch of the transaction is unique, an  epochtis a pair(n, p_id)wherenis a
number andp_idis the id of clientp.
Transactions initiated by different clients have different epochs
because their client ids are different.
A client sets the epoch of a new transaction that it initiates to be
greater than epochs of all previous transactions that it initiated.
So, different transactions initiated by the same client have different
epochs.
Therefore each epoch is unique.Client ids are totally ordered, and so epochs are also totally
ordered.
For brevity we refer to an epoch by a single valuetrather than a
pair(n, p_id).All Steps in a Transaction have the
Transaction's EpochWe associate a fieldtwith each agent -- client or
server -- wheretis the epoch of the step of the transaction that the agent is
executing.
Likewise, we associate a fieldm.twith each messagembetween clients and servers
wherem.tis the epoch of the step in which the message
is sent.A message sent in a step of a transaction is received in a step of the
same transaction.
So, for a messagembetween clients and servers,m.tis also the epoch of the step in which the message is received.We design the algorithm so that all steps of a transaction, all
messages sent in steps of the transaction, and all messages received
in steps of the transaction have the epoch of the transaction.Next we give the algorithm for clients and then for servers.Algorithm for a ClientWhen a clientpgets a clock tick message it executesp.t = p.t + pos()whereposreturns a positive value, and 
thenpinitiates a new transaction with epochp.t.
Clientpcontinues executing the transaction with epochp.tuntilpgets its next
clock tick message at which point it increasesp.tand
starts a new transaction.In the algorithm
all steps ofp, all requests sent bypto clients, and all
replies received bypfrom clients, have epochp.t.# Initialization
p.t = 0
start()
def receive(message, sender):
   if isinstance(message, ClockTick):
      # Received clock tick
      p.t = p.t + pos()
      # Start new transaction with epoch p.t
      # p.copy stores values in replies.
      p.copy = {}
      # Broadcast read request with epoch p.t
      for q in Q:  send(ReadRequest(p.t), q)
   else:
      # received a reply to a read request
      if message.t == p.t: 
         p.copy[sender] = message.v
         # send write requests if M replies received
         if len(p.copy) >= M:
            # Broadcast write request: value v, epoch t
            v, t = p.f(), p.t
            for q in Q: send(WriteRequest(v, t), q)Algorithm for a ServerA server waits to get requests from clients.When a serverqreceives a requestrfrom
the transaction that the server 
is currently processing i.e.r.t\(=\)q.t,
then the server responds to the request.When a serverqreceives a requestrfrom a
transaction with a smaller 
epoch than the transaction that the server is currently processing,
i.e.r.t\(<\)q.t, then the server does not
respond to the request. The request has the same effect as a request that
is lost.What should serverqdo when it receives a requestrfrom a transaction with a larger epoch than the
server's epoch, i.e.r.t\(>\)q.t?In this case, the server increases its epoch to the
epoch of the request and then responds to the request.
The request, the step in which the server responds to the request,
and the reply (to a read request), have the same epoch.# initialization
q.v, q.t = init, 0
start()
def receive(request, client):
   if request.t >= q.t
      q.t = request.t
      if isinstance(request, ReadRequest):
         send(Reply(q.v, q.t), client)
      else:
         // message is a WriteRequest
         q.v = request.vCorrectnessNext we prove that all computations are serializable in a system
specified by these algorithms for clients and servers.Epoch of a step in a computation of the algorithmThe epoch of a step taken by a clientpor serverqis the value ofp.torq.t(respectively) upon completion of
the step.TheoremThe assignment of epochs to steps satisfies the
condition for epochs, i.e.,
For all
steps \((e, e')\) of a computationif \(e\) and \(e'\) are steps at the same agent, and \(e'\) occurs
  after \(e\) then \(t(e) \leq t(e')\), andif \(e\) is a step in which a message is sent and \(e'\) is a step
  in which that message is received then \(t(e) \leq t(e')\).ProofFrom the algorithms for serverpand clientqit follows thatp.tandq.tdo not decrease in a
computation.
Therefore the first condition is satisfied.Also, from the algorithm, the epoch for step in which a request or reply is
received is equal to the epoch in which the message is sent.
So, the second condition is also satisfied.
Therefore the assignment of epochs to steps of a computation satisfy
the specification of epochs.TheoremThe sufficient condition for serializability is satisfied:
(1) each transaction has a unique epoch, and (2) all steps in a
transaction have the epoch of the transaction.ProofWe have shown that each transaction initiated by a clientphas a unique epochp.t.All steps atphave epochp.t.
Requests sent byphave epochp.t.
When a clientqgets a request with epochp.t, the client  sets its own epoch top.tand sends a reply with the same epoch.
So, all steps in a transaction have the epoch of the transaction.
Therefore the algorithm satisfies the sufficient condition for
serializability.CorollaryThe algorithm satisfies specifications for serializability.ProofFollows because the algorithm satisfies the sufficient condition for
serializability.Therefore,
if the epochs of transactions in a computation \(x\) are \([T_{0}, T_{1}, T_{2}, \ldots]\)
where \(T_{i} < T_{i+1}\), then in \(x\), for all \(i\), for all agents \(v\):
All steps of \(v\) in the transaction with epoch \(T_{i}\) occur before any step
of \(v\) in the transaction with epoch \(T_{i+1}\).Equivalence of Distributed
and Sequential AlgorithmsLet the sequence of increasing epochs in a computation \(x\) of the distributed
algorithm beT[1], T[2], T[3], ..., and letp[i], R[i], W[i]be the values ofp, R, Win
the transaction with epochT[i].The sequence of values of agent variables in 
computation \(x\) of the distributed algorithm is the same as in the
following nondeterministic sequential program by selectingp, R, W, on thei-th iteration to bep[i], R[i], W[i]respectively, and by selectingdeltaon thei-th iteration so thatT[i] = T[i-1] + deltaEquivalent nondeterministic sequential algorithmt = 0
while True:
   select positive delta, select client p
   t = t + delta
   # p executes transaction with epoch t
   p.t = t
   p.copy = {}
   # Read step
   select R
   for q in R: p.copy[q] = q.v
   # Write step
   if len(p.copy) >= M:
     select W
     for q in W: q.v = p.f()We will prove properties of the sequential algorithm and show that
these properties also hold for the distributed algorithm.NextNext we develop Paxos, a distributed consensus algorithm.
We prove
that Paxos satisfies the specifications for consensus by showing that
Paxos is serializable.
We prove properties of a nondeterministic sequential algorithm and
show that properties of the sequential algorithm also hold for Paxos.K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

Transactions are tailored to a specific application by specifyingMandp.f.
The selections ofRandWare
nondeterministic and they can be arbitrary sets of servers.Transactions executed concurrently by multiple clients can interfere
with each other as illustrated by the following example.ExampleLetxbe the amount of funds in an account and 
  assume thatxhas $110.
  
  Consider a computation in which clientspandp'both execute identical transactions concurrently.
In a transaction a client readsxand ifxhas at least $100 then the client transfers $100 out
ofxto an accounty.Consider a computation in which both clientspandp'readxand verify thatxhas at least $100,
and then both clients transfer $100 out ofxand set the amount
  inxto $10.
The computation transfers $200 out ofxbut 
debitsxby only $100.This situation does not occur if only one client executes a
transaction at a time.
Ifpexecutes its transaction first then whenpcompletes the transaction it sets the amount inxto $10. 
Ifp'executes its transaction next thenp'finds thatxhas only $10 and sop'does not
transfer $100 fromx.Serializable ComputationsA system executes multiple transactions concurrently.A computation \(x\) is serializableexactly when there exists a
sequence of transactions \(Z_{i}, i \geq 0\), such that in  \(x\):
For all agents \(v\), for all \(i\):all steps of \(v\) in \(Z_{i}\) occur before any step
of \(v\) in \(Z_{i+1}\).It is possible that in a serializable computation \(x\), an 
agent \(u\) takes a step in \(Z_{j}\) before adifferentagent
\(v\) takes a step in \(Z_{i}\), where \(j > i\).Transactions and Serializability in
DatabasesThere is an extensive literature ontransactions and serializability.Also seetransaction processing systemsandonline transaction processing.
In this webpage we provide a narrow definition of transactions and
serializability
that is adequate for describing algorithms such asPaxos.The Problem: Guarantee SerializabilityThe problem is to develop a distributed algorithm, with multiple
clients and servers, in which computations are serializable, and in
which the system may be faulty.How Should You Solve The Problem?What method comes to mind to partition the sequence of steps at an
agent into intervals where the agent completes all steps of exactly one
transaction in each interval?Logical time partitions computations into a past and a future.
Let's use a mechanism, similar to logical time.
As with logical time, each step \(e\) in a computation is assigned a
value \(t(e)\), called theepochof \(e\).
For any step \(e\), steps with epochs less than, equal to, or greater
than, \(t(e)\) are in the past,current, and future,
respectively.
We specify epochs such that thesteps in 
current consists of all the steps of exactly one
transaction.Epochs and Logical TimesRecall thatthe logical timeof a step \(e\) is a value \(t(e)\) assigned to
each step \(e\) in a computation such 
that for all edges \((e, e')\) of the dataflow graph:
\(t(e) < t(e')\).
Theepochof a step \(e\) in a computation is defined as a
value \(t(e)\) assigned to 
each step \(e\) in the computation such 
that for all edges \((e, e')\) of the dataflow graph:
\(t(e) \leq t(e')\).Rules for Assigning Epochs to StepsThe following rules ensure that the assignment \(t(e)\) to each step
\(e\) in a computation satisfies the specification for epochs. For all
steps \((e, e')\) of a computationfor all steps \(e\) and \(e'\) at the same agent: if \(e'\) occurs
  after \(e\) then \(t(e) \leq t(e')\), andif \(e\) is a step in which a message is sent and \(e'\) is a step
  in which that message is received then \(t(e) \leq t(e')\).ObservationIf \(e\) and \(e'\) are steps at the same agent and \(t(e) < t(e')\)
then \(e'\) occurs after \(e\).Theorem: A Sufficent Condition for SerializabilityA sufficient conditions for a computation to be serializable is:There exists epochs for all steps of a computation such thateach transaction has a unique epoch, andall steps in a transaction have the epoch of the transaction.If a computation \(x\) satisfies this condition then 
let the epochs of transactions in \(x\) be \([T_{0}, T_{1}, T_{2}, \ldots]\)
where \(T_{i} < T_{i+1}\), and let \(Z_{i}\) be the transaction with
epoch \(T_{i}\).
Then in \(x\), for all \(i\), for all agents \(v\):All steps of \(v\) in \(Z_{i}\) occur before any step
of \(v\) in \(Z_{i+1}\).ProofAssume that the sufficient condition is
satisfied for epochs of steps in a computation. 
Let \(e\) and \(e'\) be steps in transactions with epoch \(T\) and
\(T'\), respectively, and let \(T < T'\).
Let \(e\) and \(e'\) be steps at the same agent \(v\).
From the observation regarding epochs: \(e\) occurs before \(e'\).
Therefore all steps of a transaction with epoch \(T_{i}\) occur before
any step of a transaction with epoch \(T_{i+1}\).An Algorithm Based on the Sufficent ConditionNext, we describe an algorithm based on this idea.
Let's consider the two issues posed by the condition for serializability.
(1) How can the algorithm assign a unique epoch to each transaction?
(2) How can the algorithm assign epochs to steps so that
all steps in a transaction have the epoch of the transaction?Uniqueness of a Transaction's EpochA clientpinitiates a new transaction whenpgets a clock tick message.
To ensure that the epoch of the transaction is unique, an  epochtis a pair(n, p_id)wherenis a
number andp_idis the id of clientp.
Transactions initiated by different clients have different epochs
because their client ids are different.
A client sets the epoch of a new transaction that it initiates to be
greater than epochs of all previous transactions that it initiated.
So, different transactions initiated by the same client have different
epochs.
Therefore each epoch is unique.Client ids are totally ordered, and so epochs are also totally
ordered.
For brevity we refer to an epoch by a single valuetrather than a
pair(n, p_id).All Steps in a Transaction have the
Transaction's EpochWe associate a fieldtwith each agent -- client or
server -- wheretis the epoch of the step of the transaction that the agent is
executing.
Likewise, we associate a fieldm.twith each messagembetween clients and servers
wherem.tis the epoch of the step in which the message
is sent.A message sent in a step of a transaction is received in a step of the
same transaction.
So, for a messagembetween clients and servers,m.tis also the epoch of the step in which the message is received.We design the algorithm so that all steps of a transaction, all
messages sent in steps of the transaction, and all messages received
in steps of the transaction have the epoch of the transaction.Next we give the algorithm for clients and then for servers.Algorithm for a ClientWhen a clientpgets a clock tick message it executesp.t = p.t + pos()whereposreturns a positive value, and 
thenpinitiates a new transaction with epochp.t.
Clientpcontinues executing the transaction with epochp.tuntilpgets its next
clock tick message at which point it increasesp.tand
starts a new transaction.In the algorithm
all steps ofp, all requests sent bypto clients, and all
replies received bypfrom clients, have epochp.t.# Initialization
p.t = 0
start()
def receive(message, sender):
   if isinstance(message, ClockTick):
      # Received clock tick
      p.t = p.t + pos()
      # Start new transaction with epoch p.t
      # p.copy stores values in replies.
      p.copy = {}
      # Broadcast read request with epoch p.t
      for q in Q:  send(ReadRequest(p.t), q)
   else:
      # received a reply to a read request
      if message.t == p.t: 
         p.copy[sender] = message.v
         # send write requests if M replies received
         if len(p.copy) >= M:
            # Broadcast write request: value v, epoch t
            v, t = p.f(), p.t
            for q in Q: send(WriteRequest(v, t), q)Algorithm for a ServerA server waits to get requests from clients.When a serverqreceives a requestrfrom
the transaction that the server 
is currently processing i.e.r.t\(=\)q.t,
then the server responds to the request.When a serverqreceives a requestrfrom a
transaction with a smaller 
epoch than the transaction that the server is currently processing,
i.e.r.t\(<\)q.t, then the server does not
respond to the request. The request has the same effect as a request that
is lost.What should serverqdo when it receives a requestrfrom a transaction with a larger epoch than the
server's epoch, i.e.r.t\(>\)q.t?In this case, the server increases its epoch to the
epoch of the request and then responds to the request.
The request, the step in which the server responds to the request,
and the reply (to a read request), have the same epoch.# initialization
q.v, q.t = init, 0
start()
def receive(request, client):
   if request.t >= q.t
      q.t = request.t
      if isinstance(request, ReadRequest):
         send(Reply(q.v, q.t), client)
      else:
         // message is a WriteRequest
         q.v = request.vCorrectnessNext we prove that all computations are serializable in a system
specified by these algorithms for clients and servers.Epoch of a step in a computation of the algorithmThe epoch of a step taken by a clientpor serverqis the value ofp.torq.t(respectively) upon completion of
the step.TheoremThe assignment of epochs to steps satisfies the
condition for epochs, i.e.,
For all
steps \((e, e')\) of a computationif \(e\) and \(e'\) are steps at the same agent, and \(e'\) occurs
  after \(e\) then \(t(e) \leq t(e')\), andif \(e\) is a step in which a message is sent and \(e'\) is a step
  in which that message is received then \(t(e) \leq t(e')\).ProofFrom the algorithms for serverpand clientqit follows thatp.tandq.tdo not decrease in a
computation.
Therefore the first condition is satisfied.Also, from the algorithm, the epoch for step in which a request or reply is
received is equal to the epoch in which the message is sent.
So, the second condition is also satisfied.
Therefore the assignment of epochs to steps of a computation satisfy
the specification of epochs.TheoremThe sufficient condition for serializability is satisfied:
(1) each transaction has a unique epoch, and (2) all steps in a
transaction have the epoch of the transaction.ProofWe have shown that each transaction initiated by a clientphas a unique epochp.t.All steps atphave epochp.t.
Requests sent byphave epochp.t.
When a clientqgets a request with epochp.t, the client  sets its own epoch top.tand sends a reply with the same epoch.
So, all steps in a transaction have the epoch of the transaction.
Therefore the algorithm satisfies the sufficient condition for
serializability.CorollaryThe algorithm satisfies specifications for serializability.ProofFollows because the algorithm satisfies the sufficient condition for
serializability.Therefore,
if the epochs of transactions in a computation \(x\) are \([T_{0}, T_{1}, T_{2}, \ldots]\)
where \(T_{i} < T_{i+1}\), then in \(x\), for all \(i\), for all agents \(v\):
All steps of \(v\) in the transaction with epoch \(T_{i}\) occur before any step
of \(v\) in the transaction with epoch \(T_{i+1}\).Equivalence of Distributed
and Sequential AlgorithmsLet the sequence of increasing epochs in a computation \(x\) of the distributed
algorithm beT[1], T[2], T[3], ..., and letp[i], R[i], W[i]be the values ofp, R, Win
the transaction with epochT[i].The sequence of values of agent variables in 
computation \(x\) of the distributed algorithm is the same as in the
following nondeterministic sequential program by selectingp, R, W, on thei-th iteration to bep[i], R[i], W[i]respectively, and by selectingdeltaon thei-th iteration so thatT[i] = T[i-1] + deltaEquivalent nondeterministic sequential algorithmt = 0
while True:
   select positive delta, select client p
   t = t + delta
   # p executes transaction with epoch t
   p.t = t
   p.copy = {}
   # Read step
   select R
   for q in R: p.copy[q] = q.v
   # Write step
   if len(p.copy) >= M:
     select W
     for q in W: q.v = p.f()We will prove properties of the sequential algorithm and show that
these properties also hold for the distributed algorithm.NextNext we develop Paxos, a distributed consensus algorithm.
We prove
that Paxos satisfies the specifications for consensus by showing that
Paxos is serializable.
We prove properties of a nondeterministic sequential algorithm and
show that properties of the sequential algorithm also hold for Paxos.K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

Transactions executed concurrently by multiple clients can interfere
with each other as illustrated by the following example.ExampleLetxbe the amount of funds in an account and 
  assume thatxhas $110.
  
  Consider a computation in which clientspandp'both execute identical transactions concurrently.
In a transaction a client readsxand ifxhas at least $100 then the client transfers $100 out
ofxto an accounty.Consider a computation in which both clientspandp'readxand verify thatxhas at least $100,
and then both clients transfer $100 out ofxand set the amount
  inxto $10.
The computation transfers $200 out ofxbut 
debitsxby only $100.This situation does not occur if only one client executes a
transaction at a time.
Ifpexecutes its transaction first then whenpcompletes the transaction it sets the amount inxto $10. 
Ifp'executes its transaction next thenp'finds thatxhas only $10 and sop'does not
transfer $100 fromx.Serializable ComputationsA system executes multiple transactions concurrently.A computation \(x\) is serializableexactly when there exists a
sequence of transactions \(Z_{i}, i \geq 0\), such that in  \(x\):
For all agents \(v\), for all \(i\):all steps of \(v\) in \(Z_{i}\) occur before any step
of \(v\) in \(Z_{i+1}\).It is possible that in a serializable computation \(x\), an 
agent \(u\) takes a step in \(Z_{j}\) before adifferentagent
\(v\) takes a step in \(Z_{i}\), where \(j > i\).Transactions and Serializability in
DatabasesThere is an extensive literature ontransactions and serializability.Also seetransaction processing systemsandonline transaction processing.
In this webpage we provide a narrow definition of transactions and
serializability
that is adequate for describing algorithms such asPaxos.The Problem: Guarantee SerializabilityThe problem is to develop a distributed algorithm, with multiple
clients and servers, in which computations are serializable, and in
which the system may be faulty.How Should You Solve The Problem?What method comes to mind to partition the sequence of steps at an
agent into intervals where the agent completes all steps of exactly one
transaction in each interval?Logical time partitions computations into a past and a future.
Let's use a mechanism, similar to logical time.
As with logical time, each step \(e\) in a computation is assigned a
value \(t(e)\), called theepochof \(e\).
For any step \(e\), steps with epochs less than, equal to, or greater
than, \(t(e)\) are in the past,current, and future,
respectively.
We specify epochs such that thesteps in 
current consists of all the steps of exactly one
transaction.Epochs and Logical TimesRecall thatthe logical timeof a step \(e\) is a value \(t(e)\) assigned to
each step \(e\) in a computation such 
that for all edges \((e, e')\) of the dataflow graph:
\(t(e) < t(e')\).
Theepochof a step \(e\) in a computation is defined as a
value \(t(e)\) assigned to 
each step \(e\) in the computation such 
that for all edges \((e, e')\) of the dataflow graph:
\(t(e) \leq t(e')\).Rules for Assigning Epochs to StepsThe following rules ensure that the assignment \(t(e)\) to each step
\(e\) in a computation satisfies the specification for epochs. For all
steps \((e, e')\) of a computationfor all steps \(e\) and \(e'\) at the same agent: if \(e'\) occurs
  after \(e\) then \(t(e) \leq t(e')\), andif \(e\) is a step in which a message is sent and \(e'\) is a step
  in which that message is received then \(t(e) \leq t(e')\).ObservationIf \(e\) and \(e'\) are steps at the same agent and \(t(e) < t(e')\)
then \(e'\) occurs after \(e\).Theorem: A Sufficent Condition for SerializabilityA sufficient conditions for a computation to be serializable is:There exists epochs for all steps of a computation such thateach transaction has a unique epoch, andall steps in a transaction have the epoch of the transaction.If a computation \(x\) satisfies this condition then 
let the epochs of transactions in \(x\) be \([T_{0}, T_{1}, T_{2}, \ldots]\)
where \(T_{i} < T_{i+1}\), and let \(Z_{i}\) be the transaction with
epoch \(T_{i}\).
Then in \(x\), for all \(i\), for all agents \(v\):All steps of \(v\) in \(Z_{i}\) occur before any step
of \(v\) in \(Z_{i+1}\).ProofAssume that the sufficient condition is
satisfied for epochs of steps in a computation. 
Let \(e\) and \(e'\) be steps in transactions with epoch \(T\) and
\(T'\), respectively, and let \(T < T'\).
Let \(e\) and \(e'\) be steps at the same agent \(v\).
From the observation regarding epochs: \(e\) occurs before \(e'\).
Therefore all steps of a transaction with epoch \(T_{i}\) occur before
any step of a transaction with epoch \(T_{i+1}\).An Algorithm Based on the Sufficent ConditionNext, we describe an algorithm based on this idea.
Let's consider the two issues posed by the condition for serializability.
(1) How can the algorithm assign a unique epoch to each transaction?
(2) How can the algorithm assign epochs to steps so that
all steps in a transaction have the epoch of the transaction?Uniqueness of a Transaction's EpochA clientpinitiates a new transaction whenpgets a clock tick message.
To ensure that the epoch of the transaction is unique, an  epochtis a pair(n, p_id)wherenis a
number andp_idis the id of clientp.
Transactions initiated by different clients have different epochs
because their client ids are different.
A client sets the epoch of a new transaction that it initiates to be
greater than epochs of all previous transactions that it initiated.
So, different transactions initiated by the same client have different
epochs.
Therefore each epoch is unique.Client ids are totally ordered, and so epochs are also totally
ordered.
For brevity we refer to an epoch by a single valuetrather than a
pair(n, p_id).All Steps in a Transaction have the
Transaction's EpochWe associate a fieldtwith each agent -- client or
server -- wheretis the epoch of the step of the transaction that the agent is
executing.
Likewise, we associate a fieldm.twith each messagembetween clients and servers
wherem.tis the epoch of the step in which the message
is sent.A message sent in a step of a transaction is received in a step of the
same transaction.
So, for a messagembetween clients and servers,m.tis also the epoch of the step in which the message is received.We design the algorithm so that all steps of a transaction, all
messages sent in steps of the transaction, and all messages received
in steps of the transaction have the epoch of the transaction.Next we give the algorithm for clients and then for servers.Algorithm for a ClientWhen a clientpgets a clock tick message it executesp.t = p.t + pos()whereposreturns a positive value, and 
thenpinitiates a new transaction with epochp.t.
Clientpcontinues executing the transaction with epochp.tuntilpgets its next
clock tick message at which point it increasesp.tand
starts a new transaction.In the algorithm
all steps ofp, all requests sent bypto clients, and all
replies received bypfrom clients, have epochp.t.# Initialization
p.t = 0
start()
def receive(message, sender):
   if isinstance(message, ClockTick):
      # Received clock tick
      p.t = p.t + pos()
      # Start new transaction with epoch p.t
      # p.copy stores values in replies.
      p.copy = {}
      # Broadcast read request with epoch p.t
      for q in Q:  send(ReadRequest(p.t), q)
   else:
      # received a reply to a read request
      if message.t == p.t: 
         p.copy[sender] = message.v
         # send write requests if M replies received
         if len(p.copy) >= M:
            # Broadcast write request: value v, epoch t
            v, t = p.f(), p.t
            for q in Q: send(WriteRequest(v, t), q)Algorithm for a ServerA server waits to get requests from clients.When a serverqreceives a requestrfrom
the transaction that the server 
is currently processing i.e.r.t\(=\)q.t,
then the server responds to the request.When a serverqreceives a requestrfrom a
transaction with a smaller 
epoch than the transaction that the server is currently processing,
i.e.r.t\(<\)q.t, then the server does not
respond to the request. The request has the same effect as a request that
is lost.What should serverqdo when it receives a requestrfrom a transaction with a larger epoch than the
server's epoch, i.e.r.t\(>\)q.t?In this case, the server increases its epoch to the
epoch of the request and then responds to the request.
The request, the step in which the server responds to the request,
and the reply (to a read request), have the same epoch.# initialization
q.v, q.t = init, 0
start()
def receive(request, client):
   if request.t >= q.t
      q.t = request.t
      if isinstance(request, ReadRequest):
         send(Reply(q.v, q.t), client)
      else:
         // message is a WriteRequest
         q.v = request.vCorrectnessNext we prove that all computations are serializable in a system
specified by these algorithms for clients and servers.Epoch of a step in a computation of the algorithmThe epoch of a step taken by a clientpor serverqis the value ofp.torq.t(respectively) upon completion of
the step.TheoremThe assignment of epochs to steps satisfies the
condition for epochs, i.e.,
For all
steps \((e, e')\) of a computationif \(e\) and \(e'\) are steps at the same agent, and \(e'\) occurs
  after \(e\) then \(t(e) \leq t(e')\), andif \(e\) is a step in which a message is sent and \(e'\) is a step
  in which that message is received then \(t(e) \leq t(e')\).ProofFrom the algorithms for serverpand clientqit follows thatp.tandq.tdo not decrease in a
computation.
Therefore the first condition is satisfied.Also, from the algorithm, the epoch for step in which a request or reply is
received is equal to the epoch in which the message is sent.
So, the second condition is also satisfied.
Therefore the assignment of epochs to steps of a computation satisfy
the specification of epochs.TheoremThe sufficient condition for serializability is satisfied:
(1) each transaction has a unique epoch, and (2) all steps in a
transaction have the epoch of the transaction.ProofWe have shown that each transaction initiated by a clientphas a unique epochp.t.All steps atphave epochp.t.
Requests sent byphave epochp.t.
When a clientqgets a request with epochp.t, the client  sets its own epoch top.tand sends a reply with the same epoch.
So, all steps in a transaction have the epoch of the transaction.
Therefore the algorithm satisfies the sufficient condition for
serializability.CorollaryThe algorithm satisfies specifications for serializability.ProofFollows because the algorithm satisfies the sufficient condition for
serializability.Therefore,
if the epochs of transactions in a computation \(x\) are \([T_{0}, T_{1}, T_{2}, \ldots]\)
where \(T_{i} < T_{i+1}\), then in \(x\), for all \(i\), for all agents \(v\):
All steps of \(v\) in the transaction with epoch \(T_{i}\) occur before any step
of \(v\) in the transaction with epoch \(T_{i+1}\).Equivalence of Distributed
and Sequential AlgorithmsLet the sequence of increasing epochs in a computation \(x\) of the distributed
algorithm beT[1], T[2], T[3], ..., and letp[i], R[i], W[i]be the values ofp, R, Win
the transaction with epochT[i].The sequence of values of agent variables in 
computation \(x\) of the distributed algorithm is the same as in the
following nondeterministic sequential program by selectingp, R, W, on thei-th iteration to bep[i], R[i], W[i]respectively, and by selectingdeltaon thei-th iteration so thatT[i] = T[i-1] + deltaEquivalent nondeterministic sequential algorithmt = 0
while True:
   select positive delta, select client p
   t = t + delta
   # p executes transaction with epoch t
   p.t = t
   p.copy = {}
   # Read step
   select R
   for q in R: p.copy[q] = q.v
   # Write step
   if len(p.copy) >= M:
     select W
     for q in W: q.v = p.f()We will prove properties of the sequential algorithm and show that
these properties also hold for the distributed algorithm.NextNext we develop Paxos, a distributed consensus algorithm.
We prove
that Paxos satisfies the specifications for consensus by showing that
Paxos is serializable.
We prove properties of a nondeterministic sequential algorithm and
show that properties of the sequential algorithm also hold for Paxos.K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

Consider a computation in which both clientspandp'readxand verify thatxhas at least $100,
and then both clients transfer $100 out ofxand set the amount
  inxto $10.
The computation transfers $200 out ofxbut 
debitsxby only $100.This situation does not occur if only one client executes a
transaction at a time.
Ifpexecutes its transaction first then whenpcompletes the transaction it sets the amount inxto $10. 
Ifp'executes its transaction next thenp'finds thatxhas only $10 and sop'does not
transfer $100 fromx.Serializable ComputationsA system executes multiple transactions concurrently.A computation \(x\) is serializableexactly when there exists a
sequence of transactions \(Z_{i}, i \geq 0\), such that in  \(x\):
For all agents \(v\), for all \(i\):all steps of \(v\) in \(Z_{i}\) occur before any step
of \(v\) in \(Z_{i+1}\).It is possible that in a serializable computation \(x\), an 
agent \(u\) takes a step in \(Z_{j}\) before adifferentagent
\(v\) takes a step in \(Z_{i}\), where \(j > i\).Transactions and Serializability in
DatabasesThere is an extensive literature ontransactions and serializability.Also seetransaction processing systemsandonline transaction processing.
In this webpage we provide a narrow definition of transactions and
serializability
that is adequate for describing algorithms such asPaxos.The Problem: Guarantee SerializabilityThe problem is to develop a distributed algorithm, with multiple
clients and servers, in which computations are serializable, and in
which the system may be faulty.How Should You Solve The Problem?What method comes to mind to partition the sequence of steps at an
agent into intervals where the agent completes all steps of exactly one
transaction in each interval?Logical time partitions computations into a past and a future.
Let's use a mechanism, similar to logical time.
As with logical time, each step \(e\) in a computation is assigned a
value \(t(e)\), called theepochof \(e\).
For any step \(e\), steps with epochs less than, equal to, or greater
than, \(t(e)\) are in the past,current, and future,
respectively.
We specify epochs such that thesteps in 
current consists of all the steps of exactly one
transaction.Epochs and Logical TimesRecall thatthe logical timeof a step \(e\) is a value \(t(e)\) assigned to
each step \(e\) in a computation such 
that for all edges \((e, e')\) of the dataflow graph:
\(t(e) < t(e')\).
Theepochof a step \(e\) in a computation is defined as a
value \(t(e)\) assigned to 
each step \(e\) in the computation such 
that for all edges \((e, e')\) of the dataflow graph:
\(t(e) \leq t(e')\).Rules for Assigning Epochs to StepsThe following rules ensure that the assignment \(t(e)\) to each step
\(e\) in a computation satisfies the specification for epochs. For all
steps \((e, e')\) of a computationfor all steps \(e\) and \(e'\) at the same agent: if \(e'\) occurs
  after \(e\) then \(t(e) \leq t(e')\), andif \(e\) is a step in which a message is sent and \(e'\) is a step
  in which that message is received then \(t(e) \leq t(e')\).ObservationIf \(e\) and \(e'\) are steps at the same agent and \(t(e) < t(e')\)
then \(e'\) occurs after \(e\).Theorem: A Sufficent Condition for SerializabilityA sufficient conditions for a computation to be serializable is:There exists epochs for all steps of a computation such thateach transaction has a unique epoch, andall steps in a transaction have the epoch of the transaction.If a computation \(x\) satisfies this condition then 
let the epochs of transactions in \(x\) be \([T_{0}, T_{1}, T_{2}, \ldots]\)
where \(T_{i} < T_{i+1}\), and let \(Z_{i}\) be the transaction with
epoch \(T_{i}\).
Then in \(x\), for all \(i\), for all agents \(v\):All steps of \(v\) in \(Z_{i}\) occur before any step
of \(v\) in \(Z_{i+1}\).ProofAssume that the sufficient condition is
satisfied for epochs of steps in a computation. 
Let \(e\) and \(e'\) be steps in transactions with epoch \(T\) and
\(T'\), respectively, and let \(T < T'\).
Let \(e\) and \(e'\) be steps at the same agent \(v\).
From the observation regarding epochs: \(e\) occurs before \(e'\).
Therefore all steps of a transaction with epoch \(T_{i}\) occur before
any step of a transaction with epoch \(T_{i+1}\).An Algorithm Based on the Sufficent ConditionNext, we describe an algorithm based on this idea.
Let's consider the two issues posed by the condition for serializability.
(1) How can the algorithm assign a unique epoch to each transaction?
(2) How can the algorithm assign epochs to steps so that
all steps in a transaction have the epoch of the transaction?Uniqueness of a Transaction's EpochA clientpinitiates a new transaction whenpgets a clock tick message.
To ensure that the epoch of the transaction is unique, an  epochtis a pair(n, p_id)wherenis a
number andp_idis the id of clientp.
Transactions initiated by different clients have different epochs
because their client ids are different.
A client sets the epoch of a new transaction that it initiates to be
greater than epochs of all previous transactions that it initiated.
So, different transactions initiated by the same client have different
epochs.
Therefore each epoch is unique.Client ids are totally ordered, and so epochs are also totally
ordered.
For brevity we refer to an epoch by a single valuetrather than a
pair(n, p_id).All Steps in a Transaction have the
Transaction's EpochWe associate a fieldtwith each agent -- client or
server -- wheretis the epoch of the step of the transaction that the agent is
executing.
Likewise, we associate a fieldm.twith each messagembetween clients and servers
wherem.tis the epoch of the step in which the message
is sent.A message sent in a step of a transaction is received in a step of the
same transaction.
So, for a messagembetween clients and servers,m.tis also the epoch of the step in which the message is received.We design the algorithm so that all steps of a transaction, all
messages sent in steps of the transaction, and all messages received
in steps of the transaction have the epoch of the transaction.Next we give the algorithm for clients and then for servers.Algorithm for a ClientWhen a clientpgets a clock tick message it executesp.t = p.t + pos()whereposreturns a positive value, and 
thenpinitiates a new transaction with epochp.t.
Clientpcontinues executing the transaction with epochp.tuntilpgets its next
clock tick message at which point it increasesp.tand
starts a new transaction.In the algorithm
all steps ofp, all requests sent bypto clients, and all
replies received bypfrom clients, have epochp.t.# Initialization
p.t = 0
start()
def receive(message, sender):
   if isinstance(message, ClockTick):
      # Received clock tick
      p.t = p.t + pos()
      # Start new transaction with epoch p.t
      # p.copy stores values in replies.
      p.copy = {}
      # Broadcast read request with epoch p.t
      for q in Q:  send(ReadRequest(p.t), q)
   else:
      # received a reply to a read request
      if message.t == p.t: 
         p.copy[sender] = message.v
         # send write requests if M replies received
         if len(p.copy) >= M:
            # Broadcast write request: value v, epoch t
            v, t = p.f(), p.t
            for q in Q: send(WriteRequest(v, t), q)Algorithm for a ServerA server waits to get requests from clients.When a serverqreceives a requestrfrom
the transaction that the server 
is currently processing i.e.r.t\(=\)q.t,
then the server responds to the request.When a serverqreceives a requestrfrom a
transaction with a smaller 
epoch than the transaction that the server is currently processing,
i.e.r.t\(<\)q.t, then the server does not
respond to the request. The request has the same effect as a request that
is lost.What should serverqdo when it receives a requestrfrom a transaction with a larger epoch than the
server's epoch, i.e.r.t\(>\)q.t?In this case, the server increases its epoch to the
epoch of the request and then responds to the request.
The request, the step in which the server responds to the request,
and the reply (to a read request), have the same epoch.# initialization
q.v, q.t = init, 0
start()
def receive(request, client):
   if request.t >= q.t
      q.t = request.t
      if isinstance(request, ReadRequest):
         send(Reply(q.v, q.t), client)
      else:
         // message is a WriteRequest
         q.v = request.vCorrectnessNext we prove that all computations are serializable in a system
specified by these algorithms for clients and servers.Epoch of a step in a computation of the algorithmThe epoch of a step taken by a clientpor serverqis the value ofp.torq.t(respectively) upon completion of
the step.TheoremThe assignment of epochs to steps satisfies the
condition for epochs, i.e.,
For all
steps \((e, e')\) of a computationif \(e\) and \(e'\) are steps at the same agent, and \(e'\) occurs
  after \(e\) then \(t(e) \leq t(e')\), andif \(e\) is a step in which a message is sent and \(e'\) is a step
  in which that message is received then \(t(e) \leq t(e')\).ProofFrom the algorithms for serverpand clientqit follows thatp.tandq.tdo not decrease in a
computation.
Therefore the first condition is satisfied.Also, from the algorithm, the epoch for step in which a request or reply is
received is equal to the epoch in which the message is sent.
So, the second condition is also satisfied.
Therefore the assignment of epochs to steps of a computation satisfy
the specification of epochs.TheoremThe sufficient condition for serializability is satisfied:
(1) each transaction has a unique epoch, and (2) all steps in a
transaction have the epoch of the transaction.ProofWe have shown that each transaction initiated by a clientphas a unique epochp.t.All steps atphave epochp.t.
Requests sent byphave epochp.t.
When a clientqgets a request with epochp.t, the client  sets its own epoch top.tand sends a reply with the same epoch.
So, all steps in a transaction have the epoch of the transaction.
Therefore the algorithm satisfies the sufficient condition for
serializability.CorollaryThe algorithm satisfies specifications for serializability.ProofFollows because the algorithm satisfies the sufficient condition for
serializability.Therefore,
if the epochs of transactions in a computation \(x\) are \([T_{0}, T_{1}, T_{2}, \ldots]\)
where \(T_{i} < T_{i+1}\), then in \(x\), for all \(i\), for all agents \(v\):
All steps of \(v\) in the transaction with epoch \(T_{i}\) occur before any step
of \(v\) in the transaction with epoch \(T_{i+1}\).Equivalence of Distributed
and Sequential AlgorithmsLet the sequence of increasing epochs in a computation \(x\) of the distributed
algorithm beT[1], T[2], T[3], ..., and letp[i], R[i], W[i]be the values ofp, R, Win
the transaction with epochT[i].The sequence of values of agent variables in 
computation \(x\) of the distributed algorithm is the same as in the
following nondeterministic sequential program by selectingp, R, W, on thei-th iteration to bep[i], R[i], W[i]respectively, and by selectingdeltaon thei-th iteration so thatT[i] = T[i-1] + deltaEquivalent nondeterministic sequential algorithmt = 0
while True:
   select positive delta, select client p
   t = t + delta
   # p executes transaction with epoch t
   p.t = t
   p.copy = {}
   # Read step
   select R
   for q in R: p.copy[q] = q.v
   # Write step
   if len(p.copy) >= M:
     select W
     for q in W: q.v = p.f()We will prove properties of the sequential algorithm and show that
these properties also hold for the distributed algorithm.NextNext we develop Paxos, a distributed consensus algorithm.
We prove
that Paxos satisfies the specifications for consensus by showing that
Paxos is serializable.
We prove properties of a nondeterministic sequential algorithm and
show that properties of the sequential algorithm also hold for Paxos.K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

This situation does not occur if only one client executes a
transaction at a time.
Ifpexecutes its transaction first then whenpcompletes the transaction it sets the amount inxto $10. 
Ifp'executes its transaction next thenp'finds thatxhas only $10 and sop'does not
transfer $100 fromx.Serializable ComputationsA system executes multiple transactions concurrently.A computation \(x\) is serializableexactly when there exists a
sequence of transactions \(Z_{i}, i \geq 0\), such that in  \(x\):
For all agents \(v\), for all \(i\):all steps of \(v\) in \(Z_{i}\) occur before any step
of \(v\) in \(Z_{i+1}\).It is possible that in a serializable computation \(x\), an 
agent \(u\) takes a step in \(Z_{j}\) before adifferentagent
\(v\) takes a step in \(Z_{i}\), where \(j > i\).Transactions and Serializability in
DatabasesThere is an extensive literature ontransactions and serializability.Also seetransaction processing systemsandonline transaction processing.
In this webpage we provide a narrow definition of transactions and
serializability
that is adequate for describing algorithms such asPaxos.The Problem: Guarantee SerializabilityThe problem is to develop a distributed algorithm, with multiple
clients and servers, in which computations are serializable, and in
which the system may be faulty.How Should You Solve The Problem?What method comes to mind to partition the sequence of steps at an
agent into intervals where the agent completes all steps of exactly one
transaction in each interval?Logical time partitions computations into a past and a future.
Let's use a mechanism, similar to logical time.
As with logical time, each step \(e\) in a computation is assigned a
value \(t(e)\), called theepochof \(e\).
For any step \(e\), steps with epochs less than, equal to, or greater
than, \(t(e)\) are in the past,current, and future,
respectively.
We specify epochs such that thesteps in 
current consists of all the steps of exactly one
transaction.Epochs and Logical TimesRecall thatthe logical timeof a step \(e\) is a value \(t(e)\) assigned to
each step \(e\) in a computation such 
that for all edges \((e, e')\) of the dataflow graph:
\(t(e) < t(e')\).
Theepochof a step \(e\) in a computation is defined as a
value \(t(e)\) assigned to 
each step \(e\) in the computation such 
that for all edges \((e, e')\) of the dataflow graph:
\(t(e) \leq t(e')\).Rules for Assigning Epochs to StepsThe following rules ensure that the assignment \(t(e)\) to each step
\(e\) in a computation satisfies the specification for epochs. For all
steps \((e, e')\) of a computationfor all steps \(e\) and \(e'\) at the same agent: if \(e'\) occurs
  after \(e\) then \(t(e) \leq t(e')\), andif \(e\) is a step in which a message is sent and \(e'\) is a step
  in which that message is received then \(t(e) \leq t(e')\).ObservationIf \(e\) and \(e'\) are steps at the same agent and \(t(e) < t(e')\)
then \(e'\) occurs after \(e\).Theorem: A Sufficent Condition for SerializabilityA sufficient conditions for a computation to be serializable is:There exists epochs for all steps of a computation such thateach transaction has a unique epoch, andall steps in a transaction have the epoch of the transaction.If a computation \(x\) satisfies this condition then 
let the epochs of transactions in \(x\) be \([T_{0}, T_{1}, T_{2}, \ldots]\)
where \(T_{i} < T_{i+1}\), and let \(Z_{i}\) be the transaction with
epoch \(T_{i}\).
Then in \(x\), for all \(i\), for all agents \(v\):All steps of \(v\) in \(Z_{i}\) occur before any step
of \(v\) in \(Z_{i+1}\).ProofAssume that the sufficient condition is
satisfied for epochs of steps in a computation. 
Let \(e\) and \(e'\) be steps in transactions with epoch \(T\) and
\(T'\), respectively, and let \(T < T'\).
Let \(e\) and \(e'\) be steps at the same agent \(v\).
From the observation regarding epochs: \(e\) occurs before \(e'\).
Therefore all steps of a transaction with epoch \(T_{i}\) occur before
any step of a transaction with epoch \(T_{i+1}\).An Algorithm Based on the Sufficent ConditionNext, we describe an algorithm based on this idea.
Let's consider the two issues posed by the condition for serializability.
(1) How can the algorithm assign a unique epoch to each transaction?
(2) How can the algorithm assign epochs to steps so that
all steps in a transaction have the epoch of the transaction?Uniqueness of a Transaction's EpochA clientpinitiates a new transaction whenpgets a clock tick message.
To ensure that the epoch of the transaction is unique, an  epochtis a pair(n, p_id)wherenis a
number andp_idis the id of clientp.
Transactions initiated by different clients have different epochs
because their client ids are different.
A client sets the epoch of a new transaction that it initiates to be
greater than epochs of all previous transactions that it initiated.
So, different transactions initiated by the same client have different
epochs.
Therefore each epoch is unique.Client ids are totally ordered, and so epochs are also totally
ordered.
For brevity we refer to an epoch by a single valuetrather than a
pair(n, p_id).All Steps in a Transaction have the
Transaction's EpochWe associate a fieldtwith each agent -- client or
server -- wheretis the epoch of the step of the transaction that the agent is
executing.
Likewise, we associate a fieldm.twith each messagembetween clients and servers
wherem.tis the epoch of the step in which the message
is sent.A message sent in a step of a transaction is received in a step of the
same transaction.
So, for a messagembetween clients and servers,m.tis also the epoch of the step in which the message is received.We design the algorithm so that all steps of a transaction, all
messages sent in steps of the transaction, and all messages received
in steps of the transaction have the epoch of the transaction.Next we give the algorithm for clients and then for servers.Algorithm for a ClientWhen a clientpgets a clock tick message it executesp.t = p.t + pos()whereposreturns a positive value, and 
thenpinitiates a new transaction with epochp.t.
Clientpcontinues executing the transaction with epochp.tuntilpgets its next
clock tick message at which point it increasesp.tand
starts a new transaction.In the algorithm
all steps ofp, all requests sent bypto clients, and all
replies received bypfrom clients, have epochp.t.# Initialization
p.t = 0
start()
def receive(message, sender):
   if isinstance(message, ClockTick):
      # Received clock tick
      p.t = p.t + pos()
      # Start new transaction with epoch p.t
      # p.copy stores values in replies.
      p.copy = {}
      # Broadcast read request with epoch p.t
      for q in Q:  send(ReadRequest(p.t), q)
   else:
      # received a reply to a read request
      if message.t == p.t: 
         p.copy[sender] = message.v
         # send write requests if M replies received
         if len(p.copy) >= M:
            # Broadcast write request: value v, epoch t
            v, t = p.f(), p.t
            for q in Q: send(WriteRequest(v, t), q)Algorithm for a ServerA server waits to get requests from clients.When a serverqreceives a requestrfrom
the transaction that the server 
is currently processing i.e.r.t\(=\)q.t,
then the server responds to the request.When a serverqreceives a requestrfrom a
transaction with a smaller 
epoch than the transaction that the server is currently processing,
i.e.r.t\(<\)q.t, then the server does not
respond to the request. The request has the same effect as a request that
is lost.What should serverqdo when it receives a requestrfrom a transaction with a larger epoch than the
server's epoch, i.e.r.t\(>\)q.t?In this case, the server increases its epoch to the
epoch of the request and then responds to the request.
The request, the step in which the server responds to the request,
and the reply (to a read request), have the same epoch.# initialization
q.v, q.t = init, 0
start()
def receive(request, client):
   if request.t >= q.t
      q.t = request.t
      if isinstance(request, ReadRequest):
         send(Reply(q.v, q.t), client)
      else:
         // message is a WriteRequest
         q.v = request.vCorrectnessNext we prove that all computations are serializable in a system
specified by these algorithms for clients and servers.Epoch of a step in a computation of the algorithmThe epoch of a step taken by a clientpor serverqis the value ofp.torq.t(respectively) upon completion of
the step.TheoremThe assignment of epochs to steps satisfies the
condition for epochs, i.e.,
For all
steps \((e, e')\) of a computationif \(e\) and \(e'\) are steps at the same agent, and \(e'\) occurs
  after \(e\) then \(t(e) \leq t(e')\), andif \(e\) is a step in which a message is sent and \(e'\) is a step
  in which that message is received then \(t(e) \leq t(e')\).ProofFrom the algorithms for serverpand clientqit follows thatp.tandq.tdo not decrease in a
computation.
Therefore the first condition is satisfied.Also, from the algorithm, the epoch for step in which a request or reply is
received is equal to the epoch in which the message is sent.
So, the second condition is also satisfied.
Therefore the assignment of epochs to steps of a computation satisfy
the specification of epochs.TheoremThe sufficient condition for serializability is satisfied:
(1) each transaction has a unique epoch, and (2) all steps in a
transaction have the epoch of the transaction.ProofWe have shown that each transaction initiated by a clientphas a unique epochp.t.All steps atphave epochp.t.
Requests sent byphave epochp.t.
When a clientqgets a request with epochp.t, the client  sets its own epoch top.tand sends a reply with the same epoch.
So, all steps in a transaction have the epoch of the transaction.
Therefore the algorithm satisfies the sufficient condition for
serializability.CorollaryThe algorithm satisfies specifications for serializability.ProofFollows because the algorithm satisfies the sufficient condition for
serializability.Therefore,
if the epochs of transactions in a computation \(x\) are \([T_{0}, T_{1}, T_{2}, \ldots]\)
where \(T_{i} < T_{i+1}\), then in \(x\), for all \(i\), for all agents \(v\):
All steps of \(v\) in the transaction with epoch \(T_{i}\) occur before any step
of \(v\) in the transaction with epoch \(T_{i+1}\).Equivalence of Distributed
and Sequential AlgorithmsLet the sequence of increasing epochs in a computation \(x\) of the distributed
algorithm beT[1], T[2], T[3], ..., and letp[i], R[i], W[i]be the values ofp, R, Win
the transaction with epochT[i].The sequence of values of agent variables in 
computation \(x\) of the distributed algorithm is the same as in the
following nondeterministic sequential program by selectingp, R, W, on thei-th iteration to bep[i], R[i], W[i]respectively, and by selectingdeltaon thei-th iteration so thatT[i] = T[i-1] + deltaEquivalent nondeterministic sequential algorithmt = 0
while True:
   select positive delta, select client p
   t = t + delta
   # p executes transaction with epoch t
   p.t = t
   p.copy = {}
   # Read step
   select R
   for q in R: p.copy[q] = q.v
   # Write step
   if len(p.copy) >= M:
     select W
     for q in W: q.v = p.f()We will prove properties of the sequential algorithm and show that
these properties also hold for the distributed algorithm.NextNext we develop Paxos, a distributed consensus algorithm.
We prove
that Paxos satisfies the specifications for consensus by showing that
Paxos is serializable.
We prove properties of a nondeterministic sequential algorithm and
show that properties of the sequential algorithm also hold for Paxos.K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

It is possible that in a serializable computation \(x\), an 
agent \(u\) takes a step in \(Z_{j}\) before adifferentagent
\(v\) takes a step in \(Z_{i}\), where \(j > i\).Transactions and Serializability in
DatabasesThere is an extensive literature ontransactions and serializability.Also seetransaction processing systemsandonline transaction processing.
In this webpage we provide a narrow definition of transactions and
serializability
that is adequate for describing algorithms such asPaxos.The Problem: Guarantee SerializabilityThe problem is to develop a distributed algorithm, with multiple
clients and servers, in which computations are serializable, and in
which the system may be faulty.How Should You Solve The Problem?What method comes to mind to partition the sequence of steps at an
agent into intervals where the agent completes all steps of exactly one
transaction in each interval?Logical time partitions computations into a past and a future.
Let's use a mechanism, similar to logical time.
As with logical time, each step \(e\) in a computation is assigned a
value \(t(e)\), called theepochof \(e\).
For any step \(e\), steps with epochs less than, equal to, or greater
than, \(t(e)\) are in the past,current, and future,
respectively.
We specify epochs such that thesteps in 
current consists of all the steps of exactly one
transaction.Epochs and Logical TimesRecall thatthe logical timeof a step \(e\) is a value \(t(e)\) assigned to
each step \(e\) in a computation such 
that for all edges \((e, e')\) of the dataflow graph:
\(t(e) < t(e')\).
Theepochof a step \(e\) in a computation is defined as a
value \(t(e)\) assigned to 
each step \(e\) in the computation such 
that for all edges \((e, e')\) of the dataflow graph:
\(t(e) \leq t(e')\).Rules for Assigning Epochs to StepsThe following rules ensure that the assignment \(t(e)\) to each step
\(e\) in a computation satisfies the specification for epochs. For all
steps \((e, e')\) of a computationfor all steps \(e\) and \(e'\) at the same agent: if \(e'\) occurs
  after \(e\) then \(t(e) \leq t(e')\), andif \(e\) is a step in which a message is sent and \(e'\) is a step
  in which that message is received then \(t(e) \leq t(e')\).ObservationIf \(e\) and \(e'\) are steps at the same agent and \(t(e) < t(e')\)
then \(e'\) occurs after \(e\).Theorem: A Sufficent Condition for SerializabilityA sufficient conditions for a computation to be serializable is:There exists epochs for all steps of a computation such thateach transaction has a unique epoch, andall steps in a transaction have the epoch of the transaction.If a computation \(x\) satisfies this condition then 
let the epochs of transactions in \(x\) be \([T_{0}, T_{1}, T_{2}, \ldots]\)
where \(T_{i} < T_{i+1}\), and let \(Z_{i}\) be the transaction with
epoch \(T_{i}\).
Then in \(x\), for all \(i\), for all agents \(v\):All steps of \(v\) in \(Z_{i}\) occur before any step
of \(v\) in \(Z_{i+1}\).ProofAssume that the sufficient condition is
satisfied for epochs of steps in a computation. 
Let \(e\) and \(e'\) be steps in transactions with epoch \(T\) and
\(T'\), respectively, and let \(T < T'\).
Let \(e\) and \(e'\) be steps at the same agent \(v\).
From the observation regarding epochs: \(e\) occurs before \(e'\).
Therefore all steps of a transaction with epoch \(T_{i}\) occur before
any step of a transaction with epoch \(T_{i+1}\).An Algorithm Based on the Sufficent ConditionNext, we describe an algorithm based on this idea.
Let's consider the two issues posed by the condition for serializability.
(1) How can the algorithm assign a unique epoch to each transaction?
(2) How can the algorithm assign epochs to steps so that
all steps in a transaction have the epoch of the transaction?Uniqueness of a Transaction's EpochA clientpinitiates a new transaction whenpgets a clock tick message.
To ensure that the epoch of the transaction is unique, an  epochtis a pair(n, p_id)wherenis a
number andp_idis the id of clientp.
Transactions initiated by different clients have different epochs
because their client ids are different.
A client sets the epoch of a new transaction that it initiates to be
greater than epochs of all previous transactions that it initiated.
So, different transactions initiated by the same client have different
epochs.
Therefore each epoch is unique.Client ids are totally ordered, and so epochs are also totally
ordered.
For brevity we refer to an epoch by a single valuetrather than a
pair(n, p_id).All Steps in a Transaction have the
Transaction's EpochWe associate a fieldtwith each agent -- client or
server -- wheretis the epoch of the step of the transaction that the agent is
executing.
Likewise, we associate a fieldm.twith each messagembetween clients and servers
wherem.tis the epoch of the step in which the message
is sent.A message sent in a step of a transaction is received in a step of the
same transaction.
So, for a messagembetween clients and servers,m.tis also the epoch of the step in which the message is received.We design the algorithm so that all steps of a transaction, all
messages sent in steps of the transaction, and all messages received
in steps of the transaction have the epoch of the transaction.Next we give the algorithm for clients and then for servers.Algorithm for a ClientWhen a clientpgets a clock tick message it executesp.t = p.t + pos()whereposreturns a positive value, and 
thenpinitiates a new transaction with epochp.t.
Clientpcontinues executing the transaction with epochp.tuntilpgets its next
clock tick message at which point it increasesp.tand
starts a new transaction.In the algorithm
all steps ofp, all requests sent bypto clients, and all
replies received bypfrom clients, have epochp.t.# Initialization
p.t = 0
start()
def receive(message, sender):
   if isinstance(message, ClockTick):
      # Received clock tick
      p.t = p.t + pos()
      # Start new transaction with epoch p.t
      # p.copy stores values in replies.
      p.copy = {}
      # Broadcast read request with epoch p.t
      for q in Q:  send(ReadRequest(p.t), q)
   else:
      # received a reply to a read request
      if message.t == p.t: 
         p.copy[sender] = message.v
         # send write requests if M replies received
         if len(p.copy) >= M:
            # Broadcast write request: value v, epoch t
            v, t = p.f(), p.t
            for q in Q: send(WriteRequest(v, t), q)Algorithm for a ServerA server waits to get requests from clients.When a serverqreceives a requestrfrom
the transaction that the server 
is currently processing i.e.r.t\(=\)q.t,
then the server responds to the request.When a serverqreceives a requestrfrom a
transaction with a smaller 
epoch than the transaction that the server is currently processing,
i.e.r.t\(<\)q.t, then the server does not
respond to the request. The request has the same effect as a request that
is lost.What should serverqdo when it receives a requestrfrom a transaction with a larger epoch than the
server's epoch, i.e.r.t\(>\)q.t?In this case, the server increases its epoch to the
epoch of the request and then responds to the request.
The request, the step in which the server responds to the request,
and the reply (to a read request), have the same epoch.# initialization
q.v, q.t = init, 0
start()
def receive(request, client):
   if request.t >= q.t
      q.t = request.t
      if isinstance(request, ReadRequest):
         send(Reply(q.v, q.t), client)
      else:
         // message is a WriteRequest
         q.v = request.vCorrectnessNext we prove that all computations are serializable in a system
specified by these algorithms for clients and servers.Epoch of a step in a computation of the algorithmThe epoch of a step taken by a clientpor serverqis the value ofp.torq.t(respectively) upon completion of
the step.TheoremThe assignment of epochs to steps satisfies the
condition for epochs, i.e.,
For all
steps \((e, e')\) of a computationif \(e\) and \(e'\) are steps at the same agent, and \(e'\) occurs
  after \(e\) then \(t(e) \leq t(e')\), andif \(e\) is a step in which a message is sent and \(e'\) is a step
  in which that message is received then \(t(e) \leq t(e')\).ProofFrom the algorithms for serverpand clientqit follows thatp.tandq.tdo not decrease in a
computation.
Therefore the first condition is satisfied.Also, from the algorithm, the epoch for step in which a request or reply is
received is equal to the epoch in which the message is sent.
So, the second condition is also satisfied.
Therefore the assignment of epochs to steps of a computation satisfy
the specification of epochs.TheoremThe sufficient condition for serializability is satisfied:
(1) each transaction has a unique epoch, and (2) all steps in a
transaction have the epoch of the transaction.ProofWe have shown that each transaction initiated by a clientphas a unique epochp.t.All steps atphave epochp.t.
Requests sent byphave epochp.t.
When a clientqgets a request with epochp.t, the client  sets its own epoch top.tand sends a reply with the same epoch.
So, all steps in a transaction have the epoch of the transaction.
Therefore the algorithm satisfies the sufficient condition for
serializability.CorollaryThe algorithm satisfies specifications for serializability.ProofFollows because the algorithm satisfies the sufficient condition for
serializability.Therefore,
if the epochs of transactions in a computation \(x\) are \([T_{0}, T_{1}, T_{2}, \ldots]\)
where \(T_{i} < T_{i+1}\), then in \(x\), for all \(i\), for all agents \(v\):
All steps of \(v\) in the transaction with epoch \(T_{i}\) occur before any step
of \(v\) in the transaction with epoch \(T_{i+1}\).Equivalence of Distributed
and Sequential AlgorithmsLet the sequence of increasing epochs in a computation \(x\) of the distributed
algorithm beT[1], T[2], T[3], ..., and letp[i], R[i], W[i]be the values ofp, R, Win
the transaction with epochT[i].The sequence of values of agent variables in 
computation \(x\) of the distributed algorithm is the same as in the
following nondeterministic sequential program by selectingp, R, W, on thei-th iteration to bep[i], R[i], W[i]respectively, and by selectingdeltaon thei-th iteration so thatT[i] = T[i-1] + deltaEquivalent nondeterministic sequential algorithmt = 0
while True:
   select positive delta, select client p
   t = t + delta
   # p executes transaction with epoch t
   p.t = t
   p.copy = {}
   # Read step
   select R
   for q in R: p.copy[q] = q.v
   # Write step
   if len(p.copy) >= M:
     select W
     for q in W: q.v = p.f()We will prove properties of the sequential algorithm and show that
these properties also hold for the distributed algorithm.NextNext we develop Paxos, a distributed consensus algorithm.
We prove
that Paxos satisfies the specifications for consensus by showing that
Paxos is serializable.
We prove properties of a nondeterministic sequential algorithm and
show that properties of the sequential algorithm also hold for Paxos.K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

Transactions and Serializability in
Databases

Logical time partitions computations into a past and a future.
Let's use a mechanism, similar to logical time.
As with logical time, each step \(e\) in a computation is assigned a
value \(t(e)\), called theepochof \(e\).
For any step \(e\), steps with epochs less than, equal to, or greater
than, \(t(e)\) are in the past,current, and future,
respectively.
We specify epochs such that thesteps in 
current consists of all the steps of exactly one
transaction.Epochs and Logical TimesRecall thatthe logical timeof a step \(e\) is a value \(t(e)\) assigned to
each step \(e\) in a computation such 
that for all edges \((e, e')\) of the dataflow graph:
\(t(e) < t(e')\).
Theepochof a step \(e\) in a computation is defined as a
value \(t(e)\) assigned to 
each step \(e\) in the computation such 
that for all edges \((e, e')\) of the dataflow graph:
\(t(e) \leq t(e')\).Rules for Assigning Epochs to StepsThe following rules ensure that the assignment \(t(e)\) to each step
\(e\) in a computation satisfies the specification for epochs. For all
steps \((e, e')\) of a computationfor all steps \(e\) and \(e'\) at the same agent: if \(e'\) occurs
  after \(e\) then \(t(e) \leq t(e')\), andif \(e\) is a step in which a message is sent and \(e'\) is a step
  in which that message is received then \(t(e) \leq t(e')\).ObservationIf \(e\) and \(e'\) are steps at the same agent and \(t(e) < t(e')\)
then \(e'\) occurs after \(e\).Theorem: A Sufficent Condition for SerializabilityA sufficient conditions for a computation to be serializable is:There exists epochs for all steps of a computation such thateach transaction has a unique epoch, andall steps in a transaction have the epoch of the transaction.If a computation \(x\) satisfies this condition then 
let the epochs of transactions in \(x\) be \([T_{0}, T_{1}, T_{2}, \ldots]\)
where \(T_{i} < T_{i+1}\), and let \(Z_{i}\) be the transaction with
epoch \(T_{i}\).
Then in \(x\), for all \(i\), for all agents \(v\):All steps of \(v\) in \(Z_{i}\) occur before any step
of \(v\) in \(Z_{i+1}\).ProofAssume that the sufficient condition is
satisfied for epochs of steps in a computation. 
Let \(e\) and \(e'\) be steps in transactions with epoch \(T\) and
\(T'\), respectively, and let \(T < T'\).
Let \(e\) and \(e'\) be steps at the same agent \(v\).
From the observation regarding epochs: \(e\) occurs before \(e'\).
Therefore all steps of a transaction with epoch \(T_{i}\) occur before
any step of a transaction with epoch \(T_{i+1}\).An Algorithm Based on the Sufficent ConditionNext, we describe an algorithm based on this idea.
Let's consider the two issues posed by the condition for serializability.
(1) How can the algorithm assign a unique epoch to each transaction?
(2) How can the algorithm assign epochs to steps so that
all steps in a transaction have the epoch of the transaction?Uniqueness of a Transaction's EpochA clientpinitiates a new transaction whenpgets a clock tick message.
To ensure that the epoch of the transaction is unique, an  epochtis a pair(n, p_id)wherenis a
number andp_idis the id of clientp.
Transactions initiated by different clients have different epochs
because their client ids are different.
A client sets the epoch of a new transaction that it initiates to be
greater than epochs of all previous transactions that it initiated.
So, different transactions initiated by the same client have different
epochs.
Therefore each epoch is unique.Client ids are totally ordered, and so epochs are also totally
ordered.
For brevity we refer to an epoch by a single valuetrather than a
pair(n, p_id).All Steps in a Transaction have the
Transaction's EpochWe associate a fieldtwith each agent -- client or
server -- wheretis the epoch of the step of the transaction that the agent is
executing.
Likewise, we associate a fieldm.twith each messagembetween clients and servers
wherem.tis the epoch of the step in which the message
is sent.A message sent in a step of a transaction is received in a step of the
same transaction.
So, for a messagembetween clients and servers,m.tis also the epoch of the step in which the message is received.We design the algorithm so that all steps of a transaction, all
messages sent in steps of the transaction, and all messages received
in steps of the transaction have the epoch of the transaction.Next we give the algorithm for clients and then for servers.Algorithm for a ClientWhen a clientpgets a clock tick message it executesp.t = p.t + pos()whereposreturns a positive value, and 
thenpinitiates a new transaction with epochp.t.
Clientpcontinues executing the transaction with epochp.tuntilpgets its next
clock tick message at which point it increasesp.tand
starts a new transaction.In the algorithm
all steps ofp, all requests sent bypto clients, and all
replies received bypfrom clients, have epochp.t.# Initialization
p.t = 0
start()
def receive(message, sender):
   if isinstance(message, ClockTick):
      # Received clock tick
      p.t = p.t + pos()
      # Start new transaction with epoch p.t
      # p.copy stores values in replies.
      p.copy = {}
      # Broadcast read request with epoch p.t
      for q in Q:  send(ReadRequest(p.t), q)
   else:
      # received a reply to a read request
      if message.t == p.t: 
         p.copy[sender] = message.v
         # send write requests if M replies received
         if len(p.copy) >= M:
            # Broadcast write request: value v, epoch t
            v, t = p.f(), p.t
            for q in Q: send(WriteRequest(v, t), q)Algorithm for a ServerA server waits to get requests from clients.When a serverqreceives a requestrfrom
the transaction that the server 
is currently processing i.e.r.t\(=\)q.t,
then the server responds to the request.When a serverqreceives a requestrfrom a
transaction with a smaller 
epoch than the transaction that the server is currently processing,
i.e.r.t\(<\)q.t, then the server does not
respond to the request. The request has the same effect as a request that
is lost.What should serverqdo when it receives a requestrfrom a transaction with a larger epoch than the
server's epoch, i.e.r.t\(>\)q.t?In this case, the server increases its epoch to the
epoch of the request and then responds to the request.
The request, the step in which the server responds to the request,
and the reply (to a read request), have the same epoch.# initialization
q.v, q.t = init, 0
start()
def receive(request, client):
   if request.t >= q.t
      q.t = request.t
      if isinstance(request, ReadRequest):
         send(Reply(q.v, q.t), client)
      else:
         // message is a WriteRequest
         q.v = request.vCorrectnessNext we prove that all computations are serializable in a system
specified by these algorithms for clients and servers.Epoch of a step in a computation of the algorithmThe epoch of a step taken by a clientpor serverqis the value ofp.torq.t(respectively) upon completion of
the step.TheoremThe assignment of epochs to steps satisfies the
condition for epochs, i.e.,
For all
steps \((e, e')\) of a computationif \(e\) and \(e'\) are steps at the same agent, and \(e'\) occurs
  after \(e\) then \(t(e) \leq t(e')\), andif \(e\) is a step in which a message is sent and \(e'\) is a step
  in which that message is received then \(t(e) \leq t(e')\).ProofFrom the algorithms for serverpand clientqit follows thatp.tandq.tdo not decrease in a
computation.
Therefore the first condition is satisfied.Also, from the algorithm, the epoch for step in which a request or reply is
received is equal to the epoch in which the message is sent.
So, the second condition is also satisfied.
Therefore the assignment of epochs to steps of a computation satisfy
the specification of epochs.TheoremThe sufficient condition for serializability is satisfied:
(1) each transaction has a unique epoch, and (2) all steps in a
transaction have the epoch of the transaction.ProofWe have shown that each transaction initiated by a clientphas a unique epochp.t.All steps atphave epochp.t.
Requests sent byphave epochp.t.
When a clientqgets a request with epochp.t, the client  sets its own epoch top.tand sends a reply with the same epoch.
So, all steps in a transaction have the epoch of the transaction.
Therefore the algorithm satisfies the sufficient condition for
serializability.CorollaryThe algorithm satisfies specifications for serializability.ProofFollows because the algorithm satisfies the sufficient condition for
serializability.Therefore,
if the epochs of transactions in a computation \(x\) are \([T_{0}, T_{1}, T_{2}, \ldots]\)
where \(T_{i} < T_{i+1}\), then in \(x\), for all \(i\), for all agents \(v\):
All steps of \(v\) in the transaction with epoch \(T_{i}\) occur before any step
of \(v\) in the transaction with epoch \(T_{i+1}\).Equivalence of Distributed
and Sequential AlgorithmsLet the sequence of increasing epochs in a computation \(x\) of the distributed
algorithm beT[1], T[2], T[3], ..., and letp[i], R[i], W[i]be the values ofp, R, Win
the transaction with epochT[i].The sequence of values of agent variables in 
computation \(x\) of the distributed algorithm is the same as in the
following nondeterministic sequential program by selectingp, R, W, on thei-th iteration to bep[i], R[i], W[i]respectively, and by selectingdeltaon thei-th iteration so thatT[i] = T[i-1] + deltaEquivalent nondeterministic sequential algorithmt = 0
while True:
   select positive delta, select client p
   t = t + delta
   # p executes transaction with epoch t
   p.t = t
   p.copy = {}
   # Read step
   select R
   for q in R: p.copy[q] = q.v
   # Write step
   if len(p.copy) >= M:
     select W
     for q in W: q.v = p.f()We will prove properties of the sequential algorithm and show that
these properties also hold for the distributed algorithm.NextNext we develop Paxos, a distributed consensus algorithm.
We prove
that Paxos satisfies the specifications for consensus by showing that
Paxos is serializable.
We prove properties of a nondeterministic sequential algorithm and
show that properties of the sequential algorithm also hold for Paxos.K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

Recall thatthe logical timeof a step \(e\) is a value \(t(e)\) assigned to
each step \(e\) in a computation such 
that for all edges \((e, e')\) of the dataflow graph:
\(t(e) < t(e')\).
Theepochof a step \(e\) in a computation is defined as a
value \(t(e)\) assigned to 
each step \(e\) in the computation such 
that for all edges \((e, e')\) of the dataflow graph:
\(t(e) \leq t(e')\).Rules for Assigning Epochs to StepsThe following rules ensure that the assignment \(t(e)\) to each step
\(e\) in a computation satisfies the specification for epochs. For all
steps \((e, e')\) of a computationfor all steps \(e\) and \(e'\) at the same agent: if \(e'\) occurs
  after \(e\) then \(t(e) \leq t(e')\), andif \(e\) is a step in which a message is sent and \(e'\) is a step
  in which that message is received then \(t(e) \leq t(e')\).ObservationIf \(e\) and \(e'\) are steps at the same agent and \(t(e) < t(e')\)
then \(e'\) occurs after \(e\).Theorem: A Sufficent Condition for SerializabilityA sufficient conditions for a computation to be serializable is:There exists epochs for all steps of a computation such thateach transaction has a unique epoch, andall steps in a transaction have the epoch of the transaction.If a computation \(x\) satisfies this condition then 
let the epochs of transactions in \(x\) be \([T_{0}, T_{1}, T_{2}, \ldots]\)
where \(T_{i} < T_{i+1}\), and let \(Z_{i}\) be the transaction with
epoch \(T_{i}\).
Then in \(x\), for all \(i\), for all agents \(v\):All steps of \(v\) in \(Z_{i}\) occur before any step
of \(v\) in \(Z_{i+1}\).ProofAssume that the sufficient condition is
satisfied for epochs of steps in a computation. 
Let \(e\) and \(e'\) be steps in transactions with epoch \(T\) and
\(T'\), respectively, and let \(T < T'\).
Let \(e\) and \(e'\) be steps at the same agent \(v\).
From the observation regarding epochs: \(e\) occurs before \(e'\).
Therefore all steps of a transaction with epoch \(T_{i}\) occur before
any step of a transaction with epoch \(T_{i+1}\).An Algorithm Based on the Sufficent ConditionNext, we describe an algorithm based on this idea.
Let's consider the two issues posed by the condition for serializability.
(1) How can the algorithm assign a unique epoch to each transaction?
(2) How can the algorithm assign epochs to steps so that
all steps in a transaction have the epoch of the transaction?Uniqueness of a Transaction's EpochA clientpinitiates a new transaction whenpgets a clock tick message.
To ensure that the epoch of the transaction is unique, an  epochtis a pair(n, p_id)wherenis a
number andp_idis the id of clientp.
Transactions initiated by different clients have different epochs
because their client ids are different.
A client sets the epoch of a new transaction that it initiates to be
greater than epochs of all previous transactions that it initiated.
So, different transactions initiated by the same client have different
epochs.
Therefore each epoch is unique.Client ids are totally ordered, and so epochs are also totally
ordered.
For brevity we refer to an epoch by a single valuetrather than a
pair(n, p_id).All Steps in a Transaction have the
Transaction's EpochWe associate a fieldtwith each agent -- client or
server -- wheretis the epoch of the step of the transaction that the agent is
executing.
Likewise, we associate a fieldm.twith each messagembetween clients and servers
wherem.tis the epoch of the step in which the message
is sent.A message sent in a step of a transaction is received in a step of the
same transaction.
So, for a messagembetween clients and servers,m.tis also the epoch of the step in which the message is received.We design the algorithm so that all steps of a transaction, all
messages sent in steps of the transaction, and all messages received
in steps of the transaction have the epoch of the transaction.Next we give the algorithm for clients and then for servers.Algorithm for a ClientWhen a clientpgets a clock tick message it executesp.t = p.t + pos()whereposreturns a positive value, and 
thenpinitiates a new transaction with epochp.t.
Clientpcontinues executing the transaction with epochp.tuntilpgets its next
clock tick message at which point it increasesp.tand
starts a new transaction.In the algorithm
all steps ofp, all requests sent bypto clients, and all
replies received bypfrom clients, have epochp.t.# Initialization
p.t = 0
start()
def receive(message, sender):
   if isinstance(message, ClockTick):
      # Received clock tick
      p.t = p.t + pos()
      # Start new transaction with epoch p.t
      # p.copy stores values in replies.
      p.copy = {}
      # Broadcast read request with epoch p.t
      for q in Q:  send(ReadRequest(p.t), q)
   else:
      # received a reply to a read request
      if message.t == p.t: 
         p.copy[sender] = message.v
         # send write requests if M replies received
         if len(p.copy) >= M:
            # Broadcast write request: value v, epoch t
            v, t = p.f(), p.t
            for q in Q: send(WriteRequest(v, t), q)Algorithm for a ServerA server waits to get requests from clients.When a serverqreceives a requestrfrom
the transaction that the server 
is currently processing i.e.r.t\(=\)q.t,
then the server responds to the request.When a serverqreceives a requestrfrom a
transaction with a smaller 
epoch than the transaction that the server is currently processing,
i.e.r.t\(<\)q.t, then the server does not
respond to the request. The request has the same effect as a request that
is lost.What should serverqdo when it receives a requestrfrom a transaction with a larger epoch than the
server's epoch, i.e.r.t\(>\)q.t?In this case, the server increases its epoch to the
epoch of the request and then responds to the request.
The request, the step in which the server responds to the request,
and the reply (to a read request), have the same epoch.# initialization
q.v, q.t = init, 0
start()
def receive(request, client):
   if request.t >= q.t
      q.t = request.t
      if isinstance(request, ReadRequest):
         send(Reply(q.v, q.t), client)
      else:
         // message is a WriteRequest
         q.v = request.vCorrectnessNext we prove that all computations are serializable in a system
specified by these algorithms for clients and servers.Epoch of a step in a computation of the algorithmThe epoch of a step taken by a clientpor serverqis the value ofp.torq.t(respectively) upon completion of
the step.TheoremThe assignment of epochs to steps satisfies the
condition for epochs, i.e.,
For all
steps \((e, e')\) of a computationif \(e\) and \(e'\) are steps at the same agent, and \(e'\) occurs
  after \(e\) then \(t(e) \leq t(e')\), andif \(e\) is a step in which a message is sent and \(e'\) is a step
  in which that message is received then \(t(e) \leq t(e')\).ProofFrom the algorithms for serverpand clientqit follows thatp.tandq.tdo not decrease in a
computation.
Therefore the first condition is satisfied.Also, from the algorithm, the epoch for step in which a request or reply is
received is equal to the epoch in which the message is sent.
So, the second condition is also satisfied.
Therefore the assignment of epochs to steps of a computation satisfy
the specification of epochs.TheoremThe sufficient condition for serializability is satisfied:
(1) each transaction has a unique epoch, and (2) all steps in a
transaction have the epoch of the transaction.ProofWe have shown that each transaction initiated by a clientphas a unique epochp.t.All steps atphave epochp.t.
Requests sent byphave epochp.t.
When a clientqgets a request with epochp.t, the client  sets its own epoch top.tand sends a reply with the same epoch.
So, all steps in a transaction have the epoch of the transaction.
Therefore the algorithm satisfies the sufficient condition for
serializability.CorollaryThe algorithm satisfies specifications for serializability.ProofFollows because the algorithm satisfies the sufficient condition for
serializability.Therefore,
if the epochs of transactions in a computation \(x\) are \([T_{0}, T_{1}, T_{2}, \ldots]\)
where \(T_{i} < T_{i+1}\), then in \(x\), for all \(i\), for all agents \(v\):
All steps of \(v\) in the transaction with epoch \(T_{i}\) occur before any step
of \(v\) in the transaction with epoch \(T_{i+1}\).Equivalence of Distributed
and Sequential AlgorithmsLet the sequence of increasing epochs in a computation \(x\) of the distributed
algorithm beT[1], T[2], T[3], ..., and letp[i], R[i], W[i]be the values ofp, R, Win
the transaction with epochT[i].The sequence of values of agent variables in 
computation \(x\) of the distributed algorithm is the same as in the
following nondeterministic sequential program by selectingp, R, W, on thei-th iteration to bep[i], R[i], W[i]respectively, and by selectingdeltaon thei-th iteration so thatT[i] = T[i-1] + deltaEquivalent nondeterministic sequential algorithmt = 0
while True:
   select positive delta, select client p
   t = t + delta
   # p executes transaction with epoch t
   p.t = t
   p.copy = {}
   # Read step
   select R
   for q in R: p.copy[q] = q.v
   # Write step
   if len(p.copy) >= M:
     select W
     for q in W: q.v = p.f()We will prove properties of the sequential algorithm and show that
these properties also hold for the distributed algorithm.NextNext we develop Paxos, a distributed consensus algorithm.
We prove
that Paxos satisfies the specifications for consensus by showing that
Paxos is serializable.
We prove properties of a nondeterministic sequential algorithm and
show that properties of the sequential algorithm also hold for Paxos.K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

A sufficient conditions for a computation to be serializable is:There exists epochs for all steps of a computation such thateach transaction has a unique epoch, andall steps in a transaction have the epoch of the transaction.If a computation \(x\) satisfies this condition then 
let the epochs of transactions in \(x\) be \([T_{0}, T_{1}, T_{2}, \ldots]\)
where \(T_{i} < T_{i+1}\), and let \(Z_{i}\) be the transaction with
epoch \(T_{i}\).
Then in \(x\), for all \(i\), for all agents \(v\):All steps of \(v\) in \(Z_{i}\) occur before any step
of \(v\) in \(Z_{i+1}\).ProofAssume that the sufficient condition is
satisfied for epochs of steps in a computation. 
Let \(e\) and \(e'\) be steps in transactions with epoch \(T\) and
\(T'\), respectively, and let \(T < T'\).
Let \(e\) and \(e'\) be steps at the same agent \(v\).
From the observation regarding epochs: \(e\) occurs before \(e'\).
Therefore all steps of a transaction with epoch \(T_{i}\) occur before
any step of a transaction with epoch \(T_{i+1}\).An Algorithm Based on the Sufficent ConditionNext, we describe an algorithm based on this idea.
Let's consider the two issues posed by the condition for serializability.
(1) How can the algorithm assign a unique epoch to each transaction?
(2) How can the algorithm assign epochs to steps so that
all steps in a transaction have the epoch of the transaction?Uniqueness of a Transaction's EpochA clientpinitiates a new transaction whenpgets a clock tick message.
To ensure that the epoch of the transaction is unique, an  epochtis a pair(n, p_id)wherenis a
number andp_idis the id of clientp.
Transactions initiated by different clients have different epochs
because their client ids are different.
A client sets the epoch of a new transaction that it initiates to be
greater than epochs of all previous transactions that it initiated.
So, different transactions initiated by the same client have different
epochs.
Therefore each epoch is unique.Client ids are totally ordered, and so epochs are also totally
ordered.
For brevity we refer to an epoch by a single valuetrather than a
pair(n, p_id).All Steps in a Transaction have the
Transaction's EpochWe associate a fieldtwith each agent -- client or
server -- wheretis the epoch of the step of the transaction that the agent is
executing.
Likewise, we associate a fieldm.twith each messagembetween clients and servers
wherem.tis the epoch of the step in which the message
is sent.A message sent in a step of a transaction is received in a step of the
same transaction.
So, for a messagembetween clients and servers,m.tis also the epoch of the step in which the message is received.We design the algorithm so that all steps of a transaction, all
messages sent in steps of the transaction, and all messages received
in steps of the transaction have the epoch of the transaction.Next we give the algorithm for clients and then for servers.Algorithm for a ClientWhen a clientpgets a clock tick message it executesp.t = p.t + pos()whereposreturns a positive value, and 
thenpinitiates a new transaction with epochp.t.
Clientpcontinues executing the transaction with epochp.tuntilpgets its next
clock tick message at which point it increasesp.tand
starts a new transaction.In the algorithm
all steps ofp, all requests sent bypto clients, and all
replies received bypfrom clients, have epochp.t.# Initialization
p.t = 0
start()
def receive(message, sender):
   if isinstance(message, ClockTick):
      # Received clock tick
      p.t = p.t + pos()
      # Start new transaction with epoch p.t
      # p.copy stores values in replies.
      p.copy = {}
      # Broadcast read request with epoch p.t
      for q in Q:  send(ReadRequest(p.t), q)
   else:
      # received a reply to a read request
      if message.t == p.t: 
         p.copy[sender] = message.v
         # send write requests if M replies received
         if len(p.copy) >= M:
            # Broadcast write request: value v, epoch t
            v, t = p.f(), p.t
            for q in Q: send(WriteRequest(v, t), q)Algorithm for a ServerA server waits to get requests from clients.When a serverqreceives a requestrfrom
the transaction that the server 
is currently processing i.e.r.t\(=\)q.t,
then the server responds to the request.When a serverqreceives a requestrfrom a
transaction with a smaller 
epoch than the transaction that the server is currently processing,
i.e.r.t\(<\)q.t, then the server does not
respond to the request. The request has the same effect as a request that
is lost.What should serverqdo when it receives a requestrfrom a transaction with a larger epoch than the
server's epoch, i.e.r.t\(>\)q.t?In this case, the server increases its epoch to the
epoch of the request and then responds to the request.
The request, the step in which the server responds to the request,
and the reply (to a read request), have the same epoch.# initialization
q.v, q.t = init, 0
start()
def receive(request, client):
   if request.t >= q.t
      q.t = request.t
      if isinstance(request, ReadRequest):
         send(Reply(q.v, q.t), client)
      else:
         // message is a WriteRequest
         q.v = request.vCorrectnessNext we prove that all computations are serializable in a system
specified by these algorithms for clients and servers.Epoch of a step in a computation of the algorithmThe epoch of a step taken by a clientpor serverqis the value ofp.torq.t(respectively) upon completion of
the step.TheoremThe assignment of epochs to steps satisfies the
condition for epochs, i.e.,
For all
steps \((e, e')\) of a computationif \(e\) and \(e'\) are steps at the same agent, and \(e'\) occurs
  after \(e\) then \(t(e) \leq t(e')\), andif \(e\) is a step in which a message is sent and \(e'\) is a step
  in which that message is received then \(t(e) \leq t(e')\).ProofFrom the algorithms for serverpand clientqit follows thatp.tandq.tdo not decrease in a
computation.
Therefore the first condition is satisfied.Also, from the algorithm, the epoch for step in which a request or reply is
received is equal to the epoch in which the message is sent.
So, the second condition is also satisfied.
Therefore the assignment of epochs to steps of a computation satisfy
the specification of epochs.TheoremThe sufficient condition for serializability is satisfied:
(1) each transaction has a unique epoch, and (2) all steps in a
transaction have the epoch of the transaction.ProofWe have shown that each transaction initiated by a clientphas a unique epochp.t.All steps atphave epochp.t.
Requests sent byphave epochp.t.
When a clientqgets a request with epochp.t, the client  sets its own epoch top.tand sends a reply with the same epoch.
So, all steps in a transaction have the epoch of the transaction.
Therefore the algorithm satisfies the sufficient condition for
serializability.CorollaryThe algorithm satisfies specifications for serializability.ProofFollows because the algorithm satisfies the sufficient condition for
serializability.Therefore,
if the epochs of transactions in a computation \(x\) are \([T_{0}, T_{1}, T_{2}, \ldots]\)
where \(T_{i} < T_{i+1}\), then in \(x\), for all \(i\), for all agents \(v\):
All steps of \(v\) in the transaction with epoch \(T_{i}\) occur before any step
of \(v\) in the transaction with epoch \(T_{i+1}\).Equivalence of Distributed
and Sequential AlgorithmsLet the sequence of increasing epochs in a computation \(x\) of the distributed
algorithm beT[1], T[2], T[3], ..., and letp[i], R[i], W[i]be the values ofp, R, Win
the transaction with epochT[i].The sequence of values of agent variables in 
computation \(x\) of the distributed algorithm is the same as in the
following nondeterministic sequential program by selectingp, R, W, on thei-th iteration to bep[i], R[i], W[i]respectively, and by selectingdeltaon thei-th iteration so thatT[i] = T[i-1] + deltaEquivalent nondeterministic sequential algorithmt = 0
while True:
   select positive delta, select client p
   t = t + delta
   # p executes transaction with epoch t
   p.t = t
   p.copy = {}
   # Read step
   select R
   for q in R: p.copy[q] = q.v
   # Write step
   if len(p.copy) >= M:
     select W
     for q in W: q.v = p.f()We will prove properties of the sequential algorithm and show that
these properties also hold for the distributed algorithm.NextNext we develop Paxos, a distributed consensus algorithm.
We prove
that Paxos satisfies the specifications for consensus by showing that
Paxos is serializable.
We prove properties of a nondeterministic sequential algorithm and
show that properties of the sequential algorithm also hold for Paxos.K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

There exists epochs for all steps of a computation such thateach transaction has a unique epoch, andall steps in a transaction have the epoch of the transaction.

each transaction has a unique epoch, and

all steps in a transaction have the epoch of the transaction.

If a computation \(x\) satisfies this condition then 
let the epochs of transactions in \(x\) be \([T_{0}, T_{1}, T_{2}, \ldots]\)
where \(T_{i} < T_{i+1}\), and let \(Z_{i}\) be the transaction with
epoch \(T_{i}\).
Then in \(x\), for all \(i\), for all agents \(v\):All steps of \(v\) in \(Z_{i}\) occur before any step
of \(v\) in \(Z_{i+1}\).ProofAssume that the sufficient condition is
satisfied for epochs of steps in a computation. 
Let \(e\) and \(e'\) be steps in transactions with epoch \(T\) and
\(T'\), respectively, and let \(T < T'\).
Let \(e\) and \(e'\) be steps at the same agent \(v\).
From the observation regarding epochs: \(e\) occurs before \(e'\).
Therefore all steps of a transaction with epoch \(T_{i}\) occur before
any step of a transaction with epoch \(T_{i+1}\).An Algorithm Based on the Sufficent ConditionNext, we describe an algorithm based on this idea.
Let's consider the two issues posed by the condition for serializability.
(1) How can the algorithm assign a unique epoch to each transaction?
(2) How can the algorithm assign epochs to steps so that
all steps in a transaction have the epoch of the transaction?Uniqueness of a Transaction's EpochA clientpinitiates a new transaction whenpgets a clock tick message.
To ensure that the epoch of the transaction is unique, an  epochtis a pair(n, p_id)wherenis a
number andp_idis the id of clientp.
Transactions initiated by different clients have different epochs
because their client ids are different.
A client sets the epoch of a new transaction that it initiates to be
greater than epochs of all previous transactions that it initiated.
So, different transactions initiated by the same client have different
epochs.
Therefore each epoch is unique.Client ids are totally ordered, and so epochs are also totally
ordered.
For brevity we refer to an epoch by a single valuetrather than a
pair(n, p_id).All Steps in a Transaction have the
Transaction's EpochWe associate a fieldtwith each agent -- client or
server -- wheretis the epoch of the step of the transaction that the agent is
executing.
Likewise, we associate a fieldm.twith each messagembetween clients and servers
wherem.tis the epoch of the step in which the message
is sent.A message sent in a step of a transaction is received in a step of the
same transaction.
So, for a messagembetween clients and servers,m.tis also the epoch of the step in which the message is received.We design the algorithm so that all steps of a transaction, all
messages sent in steps of the transaction, and all messages received
in steps of the transaction have the epoch of the transaction.Next we give the algorithm for clients and then for servers.Algorithm for a ClientWhen a clientpgets a clock tick message it executesp.t = p.t + pos()whereposreturns a positive value, and 
thenpinitiates a new transaction with epochp.t.
Clientpcontinues executing the transaction with epochp.tuntilpgets its next
clock tick message at which point it increasesp.tand
starts a new transaction.In the algorithm
all steps ofp, all requests sent bypto clients, and all
replies received bypfrom clients, have epochp.t.# Initialization
p.t = 0
start()
def receive(message, sender):
   if isinstance(message, ClockTick):
      # Received clock tick
      p.t = p.t + pos()
      # Start new transaction with epoch p.t
      # p.copy stores values in replies.
      p.copy = {}
      # Broadcast read request with epoch p.t
      for q in Q:  send(ReadRequest(p.t), q)
   else:
      # received a reply to a read request
      if message.t == p.t: 
         p.copy[sender] = message.v
         # send write requests if M replies received
         if len(p.copy) >= M:
            # Broadcast write request: value v, epoch t
            v, t = p.f(), p.t
            for q in Q: send(WriteRequest(v, t), q)Algorithm for a ServerA server waits to get requests from clients.When a serverqreceives a requestrfrom
the transaction that the server 
is currently processing i.e.r.t\(=\)q.t,
then the server responds to the request.When a serverqreceives a requestrfrom a
transaction with a smaller 
epoch than the transaction that the server is currently processing,
i.e.r.t\(<\)q.t, then the server does not
respond to the request. The request has the same effect as a request that
is lost.What should serverqdo when it receives a requestrfrom a transaction with a larger epoch than the
server's epoch, i.e.r.t\(>\)q.t?In this case, the server increases its epoch to the
epoch of the request and then responds to the request.
The request, the step in which the server responds to the request,
and the reply (to a read request), have the same epoch.# initialization
q.v, q.t = init, 0
start()
def receive(request, client):
   if request.t >= q.t
      q.t = request.t
      if isinstance(request, ReadRequest):
         send(Reply(q.v, q.t), client)
      else:
         // message is a WriteRequest
         q.v = request.vCorrectnessNext we prove that all computations are serializable in a system
specified by these algorithms for clients and servers.Epoch of a step in a computation of the algorithmThe epoch of a step taken by a clientpor serverqis the value ofp.torq.t(respectively) upon completion of
the step.TheoremThe assignment of epochs to steps satisfies the
condition for epochs, i.e.,
For all
steps \((e, e')\) of a computationif \(e\) and \(e'\) are steps at the same agent, and \(e'\) occurs
  after \(e\) then \(t(e) \leq t(e')\), andif \(e\) is a step in which a message is sent and \(e'\) is a step
  in which that message is received then \(t(e) \leq t(e')\).ProofFrom the algorithms for serverpand clientqit follows thatp.tandq.tdo not decrease in a
computation.
Therefore the first condition is satisfied.Also, from the algorithm, the epoch for step in which a request or reply is
received is equal to the epoch in which the message is sent.
So, the second condition is also satisfied.
Therefore the assignment of epochs to steps of a computation satisfy
the specification of epochs.TheoremThe sufficient condition for serializability is satisfied:
(1) each transaction has a unique epoch, and (2) all steps in a
transaction have the epoch of the transaction.ProofWe have shown that each transaction initiated by a clientphas a unique epochp.t.All steps atphave epochp.t.
Requests sent byphave epochp.t.
When a clientqgets a request with epochp.t, the client  sets its own epoch top.tand sends a reply with the same epoch.
So, all steps in a transaction have the epoch of the transaction.
Therefore the algorithm satisfies the sufficient condition for
serializability.CorollaryThe algorithm satisfies specifications for serializability.ProofFollows because the algorithm satisfies the sufficient condition for
serializability.Therefore,
if the epochs of transactions in a computation \(x\) are \([T_{0}, T_{1}, T_{2}, \ldots]\)
where \(T_{i} < T_{i+1}\), then in \(x\), for all \(i\), for all agents \(v\):
All steps of \(v\) in the transaction with epoch \(T_{i}\) occur before any step
of \(v\) in the transaction with epoch \(T_{i+1}\).Equivalence of Distributed
and Sequential AlgorithmsLet the sequence of increasing epochs in a computation \(x\) of the distributed
algorithm beT[1], T[2], T[3], ..., and letp[i], R[i], W[i]be the values ofp, R, Win
the transaction with epochT[i].The sequence of values of agent variables in 
computation \(x\) of the distributed algorithm is the same as in the
following nondeterministic sequential program by selectingp, R, W, on thei-th iteration to bep[i], R[i], W[i]respectively, and by selectingdeltaon thei-th iteration so thatT[i] = T[i-1] + deltaEquivalent nondeterministic sequential algorithmt = 0
while True:
   select positive delta, select client p
   t = t + delta
   # p executes transaction with epoch t
   p.t = t
   p.copy = {}
   # Read step
   select R
   for q in R: p.copy[q] = q.v
   # Write step
   if len(p.copy) >= M:
     select W
     for q in W: q.v = p.f()We will prove properties of the sequential algorithm and show that
these properties also hold for the distributed algorithm.NextNext we develop Paxos, a distributed consensus algorithm.
We prove
that Paxos satisfies the specifications for consensus by showing that
Paxos is serializable.
We prove properties of a nondeterministic sequential algorithm and
show that properties of the sequential algorithm also hold for Paxos.K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

Client ids are totally ordered, and so epochs are also totally
ordered.
For brevity we refer to an epoch by a single valuetrather than a
pair(n, p_id).All Steps in a Transaction have the
Transaction's EpochWe associate a fieldtwith each agent -- client or
server -- wheretis the epoch of the step of the transaction that the agent is
executing.
Likewise, we associate a fieldm.twith each messagembetween clients and servers
wherem.tis the epoch of the step in which the message
is sent.A message sent in a step of a transaction is received in a step of the
same transaction.
So, for a messagembetween clients and servers,m.tis also the epoch of the step in which the message is received.We design the algorithm so that all steps of a transaction, all
messages sent in steps of the transaction, and all messages received
in steps of the transaction have the epoch of the transaction.Next we give the algorithm for clients and then for servers.Algorithm for a ClientWhen a clientpgets a clock tick message it executesp.t = p.t + pos()whereposreturns a positive value, and 
thenpinitiates a new transaction with epochp.t.
Clientpcontinues executing the transaction with epochp.tuntilpgets its next
clock tick message at which point it increasesp.tand
starts a new transaction.In the algorithm
all steps ofp, all requests sent bypto clients, and all
replies received bypfrom clients, have epochp.t.# Initialization
p.t = 0
start()
def receive(message, sender):
   if isinstance(message, ClockTick):
      # Received clock tick
      p.t = p.t + pos()
      # Start new transaction with epoch p.t
      # p.copy stores values in replies.
      p.copy = {}
      # Broadcast read request with epoch p.t
      for q in Q:  send(ReadRequest(p.t), q)
   else:
      # received a reply to a read request
      if message.t == p.t: 
         p.copy[sender] = message.v
         # send write requests if M replies received
         if len(p.copy) >= M:
            # Broadcast write request: value v, epoch t
            v, t = p.f(), p.t
            for q in Q: send(WriteRequest(v, t), q)Algorithm for a ServerA server waits to get requests from clients.When a serverqreceives a requestrfrom
the transaction that the server 
is currently processing i.e.r.t\(=\)q.t,
then the server responds to the request.When a serverqreceives a requestrfrom a
transaction with a smaller 
epoch than the transaction that the server is currently processing,
i.e.r.t\(<\)q.t, then the server does not
respond to the request. The request has the same effect as a request that
is lost.What should serverqdo when it receives a requestrfrom a transaction with a larger epoch than the
server's epoch, i.e.r.t\(>\)q.t?In this case, the server increases its epoch to the
epoch of the request and then responds to the request.
The request, the step in which the server responds to the request,
and the reply (to a read request), have the same epoch.# initialization
q.v, q.t = init, 0
start()
def receive(request, client):
   if request.t >= q.t
      q.t = request.t
      if isinstance(request, ReadRequest):
         send(Reply(q.v, q.t), client)
      else:
         // message is a WriteRequest
         q.v = request.vCorrectnessNext we prove that all computations are serializable in a system
specified by these algorithms for clients and servers.Epoch of a step in a computation of the algorithmThe epoch of a step taken by a clientpor serverqis the value ofp.torq.t(respectively) upon completion of
the step.TheoremThe assignment of epochs to steps satisfies the
condition for epochs, i.e.,
For all
steps \((e, e')\) of a computationif \(e\) and \(e'\) are steps at the same agent, and \(e'\) occurs
  after \(e\) then \(t(e) \leq t(e')\), andif \(e\) is a step in which a message is sent and \(e'\) is a step
  in which that message is received then \(t(e) \leq t(e')\).ProofFrom the algorithms for serverpand clientqit follows thatp.tandq.tdo not decrease in a
computation.
Therefore the first condition is satisfied.Also, from the algorithm, the epoch for step in which a request or reply is
received is equal to the epoch in which the message is sent.
So, the second condition is also satisfied.
Therefore the assignment of epochs to steps of a computation satisfy
the specification of epochs.TheoremThe sufficient condition for serializability is satisfied:
(1) each transaction has a unique epoch, and (2) all steps in a
transaction have the epoch of the transaction.ProofWe have shown that each transaction initiated by a clientphas a unique epochp.t.All steps atphave epochp.t.
Requests sent byphave epochp.t.
When a clientqgets a request with epochp.t, the client  sets its own epoch top.tand sends a reply with the same epoch.
So, all steps in a transaction have the epoch of the transaction.
Therefore the algorithm satisfies the sufficient condition for
serializability.CorollaryThe algorithm satisfies specifications for serializability.ProofFollows because the algorithm satisfies the sufficient condition for
serializability.Therefore,
if the epochs of transactions in a computation \(x\) are \([T_{0}, T_{1}, T_{2}, \ldots]\)
where \(T_{i} < T_{i+1}\), then in \(x\), for all \(i\), for all agents \(v\):
All steps of \(v\) in the transaction with epoch \(T_{i}\) occur before any step
of \(v\) in the transaction with epoch \(T_{i+1}\).Equivalence of Distributed
and Sequential AlgorithmsLet the sequence of increasing epochs in a computation \(x\) of the distributed
algorithm beT[1], T[2], T[3], ..., and letp[i], R[i], W[i]be the values ofp, R, Win
the transaction with epochT[i].The sequence of values of agent variables in 
computation \(x\) of the distributed algorithm is the same as in the
following nondeterministic sequential program by selectingp, R, W, on thei-th iteration to bep[i], R[i], W[i]respectively, and by selectingdeltaon thei-th iteration so thatT[i] = T[i-1] + deltaEquivalent nondeterministic sequential algorithmt = 0
while True:
   select positive delta, select client p
   t = t + delta
   # p executes transaction with epoch t
   p.t = t
   p.copy = {}
   # Read step
   select R
   for q in R: p.copy[q] = q.v
   # Write step
   if len(p.copy) >= M:
     select W
     for q in W: q.v = p.f()We will prove properties of the sequential algorithm and show that
these properties also hold for the distributed algorithm.NextNext we develop Paxos, a distributed consensus algorithm.
We prove
that Paxos satisfies the specifications for consensus by showing that
Paxos is serializable.
We prove properties of a nondeterministic sequential algorithm and
show that properties of the sequential algorithm also hold for Paxos.K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

A message sent in a step of a transaction is received in a step of the
same transaction.
So, for a messagembetween clients and servers,m.tis also the epoch of the step in which the message is received.We design the algorithm so that all steps of a transaction, all
messages sent in steps of the transaction, and all messages received
in steps of the transaction have the epoch of the transaction.Next we give the algorithm for clients and then for servers.Algorithm for a ClientWhen a clientpgets a clock tick message it executesp.t = p.t + pos()whereposreturns a positive value, and 
thenpinitiates a new transaction with epochp.t.
Clientpcontinues executing the transaction with epochp.tuntilpgets its next
clock tick message at which point it increasesp.tand
starts a new transaction.In the algorithm
all steps ofp, all requests sent bypto clients, and all
replies received bypfrom clients, have epochp.t.# Initialization
p.t = 0
start()
def receive(message, sender):
   if isinstance(message, ClockTick):
      # Received clock tick
      p.t = p.t + pos()
      # Start new transaction with epoch p.t
      # p.copy stores values in replies.
      p.copy = {}
      # Broadcast read request with epoch p.t
      for q in Q:  send(ReadRequest(p.t), q)
   else:
      # received a reply to a read request
      if message.t == p.t: 
         p.copy[sender] = message.v
         # send write requests if M replies received
         if len(p.copy) >= M:
            # Broadcast write request: value v, epoch t
            v, t = p.f(), p.t
            for q in Q: send(WriteRequest(v, t), q)Algorithm for a ServerA server waits to get requests from clients.When a serverqreceives a requestrfrom
the transaction that the server 
is currently processing i.e.r.t\(=\)q.t,
then the server responds to the request.When a serverqreceives a requestrfrom a
transaction with a smaller 
epoch than the transaction that the server is currently processing,
i.e.r.t\(<\)q.t, then the server does not
respond to the request. The request has the same effect as a request that
is lost.What should serverqdo when it receives a requestrfrom a transaction with a larger epoch than the
server's epoch, i.e.r.t\(>\)q.t?In this case, the server increases its epoch to the
epoch of the request and then responds to the request.
The request, the step in which the server responds to the request,
and the reply (to a read request), have the same epoch.# initialization
q.v, q.t = init, 0
start()
def receive(request, client):
   if request.t >= q.t
      q.t = request.t
      if isinstance(request, ReadRequest):
         send(Reply(q.v, q.t), client)
      else:
         // message is a WriteRequest
         q.v = request.vCorrectnessNext we prove that all computations are serializable in a system
specified by these algorithms for clients and servers.Epoch of a step in a computation of the algorithmThe epoch of a step taken by a clientpor serverqis the value ofp.torq.t(respectively) upon completion of
the step.TheoremThe assignment of epochs to steps satisfies the
condition for epochs, i.e.,
For all
steps \((e, e')\) of a computationif \(e\) and \(e'\) are steps at the same agent, and \(e'\) occurs
  after \(e\) then \(t(e) \leq t(e')\), andif \(e\) is a step in which a message is sent and \(e'\) is a step
  in which that message is received then \(t(e) \leq t(e')\).ProofFrom the algorithms for serverpand clientqit follows thatp.tandq.tdo not decrease in a
computation.
Therefore the first condition is satisfied.Also, from the algorithm, the epoch for step in which a request or reply is
received is equal to the epoch in which the message is sent.
So, the second condition is also satisfied.
Therefore the assignment of epochs to steps of a computation satisfy
the specification of epochs.TheoremThe sufficient condition for serializability is satisfied:
(1) each transaction has a unique epoch, and (2) all steps in a
transaction have the epoch of the transaction.ProofWe have shown that each transaction initiated by a clientphas a unique epochp.t.All steps atphave epochp.t.
Requests sent byphave epochp.t.
When a clientqgets a request with epochp.t, the client  sets its own epoch top.tand sends a reply with the same epoch.
So, all steps in a transaction have the epoch of the transaction.
Therefore the algorithm satisfies the sufficient condition for
serializability.CorollaryThe algorithm satisfies specifications for serializability.ProofFollows because the algorithm satisfies the sufficient condition for
serializability.Therefore,
if the epochs of transactions in a computation \(x\) are \([T_{0}, T_{1}, T_{2}, \ldots]\)
where \(T_{i} < T_{i+1}\), then in \(x\), for all \(i\), for all agents \(v\):
All steps of \(v\) in the transaction with epoch \(T_{i}\) occur before any step
of \(v\) in the transaction with epoch \(T_{i+1}\).Equivalence of Distributed
and Sequential AlgorithmsLet the sequence of increasing epochs in a computation \(x\) of the distributed
algorithm beT[1], T[2], T[3], ..., and letp[i], R[i], W[i]be the values ofp, R, Win
the transaction with epochT[i].The sequence of values of agent variables in 
computation \(x\) of the distributed algorithm is the same as in the
following nondeterministic sequential program by selectingp, R, W, on thei-th iteration to bep[i], R[i], W[i]respectively, and by selectingdeltaon thei-th iteration so thatT[i] = T[i-1] + deltaEquivalent nondeterministic sequential algorithmt = 0
while True:
   select positive delta, select client p
   t = t + delta
   # p executes transaction with epoch t
   p.t = t
   p.copy = {}
   # Read step
   select R
   for q in R: p.copy[q] = q.v
   # Write step
   if len(p.copy) >= M:
     select W
     for q in W: q.v = p.f()We will prove properties of the sequential algorithm and show that
these properties also hold for the distributed algorithm.NextNext we develop Paxos, a distributed consensus algorithm.
We prove
that Paxos satisfies the specifications for consensus by showing that
Paxos is serializable.
We prove properties of a nondeterministic sequential algorithm and
show that properties of the sequential algorithm also hold for Paxos.K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

We design the algorithm so that all steps of a transaction, all
messages sent in steps of the transaction, and all messages received
in steps of the transaction have the epoch of the transaction.Next we give the algorithm for clients and then for servers.Algorithm for a ClientWhen a clientpgets a clock tick message it executesp.t = p.t + pos()whereposreturns a positive value, and 
thenpinitiates a new transaction with epochp.t.
Clientpcontinues executing the transaction with epochp.tuntilpgets its next
clock tick message at which point it increasesp.tand
starts a new transaction.In the algorithm
all steps ofp, all requests sent bypto clients, and all
replies received bypfrom clients, have epochp.t.# Initialization
p.t = 0
start()
def receive(message, sender):
   if isinstance(message, ClockTick):
      # Received clock tick
      p.t = p.t + pos()
      # Start new transaction with epoch p.t
      # p.copy stores values in replies.
      p.copy = {}
      # Broadcast read request with epoch p.t
      for q in Q:  send(ReadRequest(p.t), q)
   else:
      # received a reply to a read request
      if message.t == p.t: 
         p.copy[sender] = message.v
         # send write requests if M replies received
         if len(p.copy) >= M:
            # Broadcast write request: value v, epoch t
            v, t = p.f(), p.t
            for q in Q: send(WriteRequest(v, t), q)Algorithm for a ServerA server waits to get requests from clients.When a serverqreceives a requestrfrom
the transaction that the server 
is currently processing i.e.r.t\(=\)q.t,
then the server responds to the request.When a serverqreceives a requestrfrom a
transaction with a smaller 
epoch than the transaction that the server is currently processing,
i.e.r.t\(<\)q.t, then the server does not
respond to the request. The request has the same effect as a request that
is lost.What should serverqdo when it receives a requestrfrom a transaction with a larger epoch than the
server's epoch, i.e.r.t\(>\)q.t?In this case, the server increases its epoch to the
epoch of the request and then responds to the request.
The request, the step in which the server responds to the request,
and the reply (to a read request), have the same epoch.# initialization
q.v, q.t = init, 0
start()
def receive(request, client):
   if request.t >= q.t
      q.t = request.t
      if isinstance(request, ReadRequest):
         send(Reply(q.v, q.t), client)
      else:
         // message is a WriteRequest
         q.v = request.vCorrectnessNext we prove that all computations are serializable in a system
specified by these algorithms for clients and servers.Epoch of a step in a computation of the algorithmThe epoch of a step taken by a clientpor serverqis the value ofp.torq.t(respectively) upon completion of
the step.TheoremThe assignment of epochs to steps satisfies the
condition for epochs, i.e.,
For all
steps \((e, e')\) of a computationif \(e\) and \(e'\) are steps at the same agent, and \(e'\) occurs
  after \(e\) then \(t(e) \leq t(e')\), andif \(e\) is a step in which a message is sent and \(e'\) is a step
  in which that message is received then \(t(e) \leq t(e')\).ProofFrom the algorithms for serverpand clientqit follows thatp.tandq.tdo not decrease in a
computation.
Therefore the first condition is satisfied.Also, from the algorithm, the epoch for step in which a request or reply is
received is equal to the epoch in which the message is sent.
So, the second condition is also satisfied.
Therefore the assignment of epochs to steps of a computation satisfy
the specification of epochs.TheoremThe sufficient condition for serializability is satisfied:
(1) each transaction has a unique epoch, and (2) all steps in a
transaction have the epoch of the transaction.ProofWe have shown that each transaction initiated by a clientphas a unique epochp.t.All steps atphave epochp.t.
Requests sent byphave epochp.t.
When a clientqgets a request with epochp.t, the client  sets its own epoch top.tand sends a reply with the same epoch.
So, all steps in a transaction have the epoch of the transaction.
Therefore the algorithm satisfies the sufficient condition for
serializability.CorollaryThe algorithm satisfies specifications for serializability.ProofFollows because the algorithm satisfies the sufficient condition for
serializability.Therefore,
if the epochs of transactions in a computation \(x\) are \([T_{0}, T_{1}, T_{2}, \ldots]\)
where \(T_{i} < T_{i+1}\), then in \(x\), for all \(i\), for all agents \(v\):
All steps of \(v\) in the transaction with epoch \(T_{i}\) occur before any step
of \(v\) in the transaction with epoch \(T_{i+1}\).Equivalence of Distributed
and Sequential AlgorithmsLet the sequence of increasing epochs in a computation \(x\) of the distributed
algorithm beT[1], T[2], T[3], ..., and letp[i], R[i], W[i]be the values ofp, R, Win
the transaction with epochT[i].The sequence of values of agent variables in 
computation \(x\) of the distributed algorithm is the same as in the
following nondeterministic sequential program by selectingp, R, W, on thei-th iteration to bep[i], R[i], W[i]respectively, and by selectingdeltaon thei-th iteration so thatT[i] = T[i-1] + deltaEquivalent nondeterministic sequential algorithmt = 0
while True:
   select positive delta, select client p
   t = t + delta
   # p executes transaction with epoch t
   p.t = t
   p.copy = {}
   # Read step
   select R
   for q in R: p.copy[q] = q.v
   # Write step
   if len(p.copy) >= M:
     select W
     for q in W: q.v = p.f()We will prove properties of the sequential algorithm and show that
these properties also hold for the distributed algorithm.NextNext we develop Paxos, a distributed consensus algorithm.
We prove
that Paxos satisfies the specifications for consensus by showing that
Paxos is serializable.
We prove properties of a nondeterministic sequential algorithm and
show that properties of the sequential algorithm also hold for Paxos.K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

Next we give the algorithm for clients and then for servers.Algorithm for a ClientWhen a clientpgets a clock tick message it executesp.t = p.t + pos()whereposreturns a positive value, and 
thenpinitiates a new transaction with epochp.t.
Clientpcontinues executing the transaction with epochp.tuntilpgets its next
clock tick message at which point it increasesp.tand
starts a new transaction.In the algorithm
all steps ofp, all requests sent bypto clients, and all
replies received bypfrom clients, have epochp.t.# Initialization
p.t = 0
start()
def receive(message, sender):
   if isinstance(message, ClockTick):
      # Received clock tick
      p.t = p.t + pos()
      # Start new transaction with epoch p.t
      # p.copy stores values in replies.
      p.copy = {}
      # Broadcast read request with epoch p.t
      for q in Q:  send(ReadRequest(p.t), q)
   else:
      # received a reply to a read request
      if message.t == p.t: 
         p.copy[sender] = message.v
         # send write requests if M replies received
         if len(p.copy) >= M:
            # Broadcast write request: value v, epoch t
            v, t = p.f(), p.t
            for q in Q: send(WriteRequest(v, t), q)Algorithm for a ServerA server waits to get requests from clients.When a serverqreceives a requestrfrom
the transaction that the server 
is currently processing i.e.r.t\(=\)q.t,
then the server responds to the request.When a serverqreceives a requestrfrom a
transaction with a smaller 
epoch than the transaction that the server is currently processing,
i.e.r.t\(<\)q.t, then the server does not
respond to the request. The request has the same effect as a request that
is lost.What should serverqdo when it receives a requestrfrom a transaction with a larger epoch than the
server's epoch, i.e.r.t\(>\)q.t?In this case, the server increases its epoch to the
epoch of the request and then responds to the request.
The request, the step in which the server responds to the request,
and the reply (to a read request), have the same epoch.# initialization
q.v, q.t = init, 0
start()
def receive(request, client):
   if request.t >= q.t
      q.t = request.t
      if isinstance(request, ReadRequest):
         send(Reply(q.v, q.t), client)
      else:
         // message is a WriteRequest
         q.v = request.vCorrectnessNext we prove that all computations are serializable in a system
specified by these algorithms for clients and servers.Epoch of a step in a computation of the algorithmThe epoch of a step taken by a clientpor serverqis the value ofp.torq.t(respectively) upon completion of
the step.TheoremThe assignment of epochs to steps satisfies the
condition for epochs, i.e.,
For all
steps \((e, e')\) of a computationif \(e\) and \(e'\) are steps at the same agent, and \(e'\) occurs
  after \(e\) then \(t(e) \leq t(e')\), andif \(e\) is a step in which a message is sent and \(e'\) is a step
  in which that message is received then \(t(e) \leq t(e')\).ProofFrom the algorithms for serverpand clientqit follows thatp.tandq.tdo not decrease in a
computation.
Therefore the first condition is satisfied.Also, from the algorithm, the epoch for step in which a request or reply is
received is equal to the epoch in which the message is sent.
So, the second condition is also satisfied.
Therefore the assignment of epochs to steps of a computation satisfy
the specification of epochs.TheoremThe sufficient condition for serializability is satisfied:
(1) each transaction has a unique epoch, and (2) all steps in a
transaction have the epoch of the transaction.ProofWe have shown that each transaction initiated by a clientphas a unique epochp.t.All steps atphave epochp.t.
Requests sent byphave epochp.t.
When a clientqgets a request with epochp.t, the client  sets its own epoch top.tand sends a reply with the same epoch.
So, all steps in a transaction have the epoch of the transaction.
Therefore the algorithm satisfies the sufficient condition for
serializability.CorollaryThe algorithm satisfies specifications for serializability.ProofFollows because the algorithm satisfies the sufficient condition for
serializability.Therefore,
if the epochs of transactions in a computation \(x\) are \([T_{0}, T_{1}, T_{2}, \ldots]\)
where \(T_{i} < T_{i+1}\), then in \(x\), for all \(i\), for all agents \(v\):
All steps of \(v\) in the transaction with epoch \(T_{i}\) occur before any step
of \(v\) in the transaction with epoch \(T_{i+1}\).Equivalence of Distributed
and Sequential AlgorithmsLet the sequence of increasing epochs in a computation \(x\) of the distributed
algorithm beT[1], T[2], T[3], ..., and letp[i], R[i], W[i]be the values ofp, R, Win
the transaction with epochT[i].The sequence of values of agent variables in 
computation \(x\) of the distributed algorithm is the same as in the
following nondeterministic sequential program by selectingp, R, W, on thei-th iteration to bep[i], R[i], W[i]respectively, and by selectingdeltaon thei-th iteration so thatT[i] = T[i-1] + deltaEquivalent nondeterministic sequential algorithmt = 0
while True:
   select positive delta, select client p
   t = t + delta
   # p executes transaction with epoch t
   p.t = t
   p.copy = {}
   # Read step
   select R
   for q in R: p.copy[q] = q.v
   # Write step
   if len(p.copy) >= M:
     select W
     for q in W: q.v = p.f()We will prove properties of the sequential algorithm and show that
these properties also hold for the distributed algorithm.NextNext we develop Paxos, a distributed consensus algorithm.
We prove
that Paxos satisfies the specifications for consensus by showing that
Paxos is serializable.
We prove properties of a nondeterministic sequential algorithm and
show that properties of the sequential algorithm also hold for Paxos.K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

In the algorithm
all steps ofp, all requests sent bypto clients, and all
replies received bypfrom clients, have epochp.t.# Initialization
p.t = 0
start()
def receive(message, sender):
   if isinstance(message, ClockTick):
      # Received clock tick
      p.t = p.t + pos()
      # Start new transaction with epoch p.t
      # p.copy stores values in replies.
      p.copy = {}
      # Broadcast read request with epoch p.t
      for q in Q:  send(ReadRequest(p.t), q)
   else:
      # received a reply to a read request
      if message.t == p.t: 
         p.copy[sender] = message.v
         # send write requests if M replies received
         if len(p.copy) >= M:
            # Broadcast write request: value v, epoch t
            v, t = p.f(), p.t
            for q in Q: send(WriteRequest(v, t), q)Algorithm for a ServerA server waits to get requests from clients.When a serverqreceives a requestrfrom
the transaction that the server 
is currently processing i.e.r.t\(=\)q.t,
then the server responds to the request.When a serverqreceives a requestrfrom a
transaction with a smaller 
epoch than the transaction that the server is currently processing,
i.e.r.t\(<\)q.t, then the server does not
respond to the request. The request has the same effect as a request that
is lost.What should serverqdo when it receives a requestrfrom a transaction with a larger epoch than the
server's epoch, i.e.r.t\(>\)q.t?In this case, the server increases its epoch to the
epoch of the request and then responds to the request.
The request, the step in which the server responds to the request,
and the reply (to a read request), have the same epoch.# initialization
q.v, q.t = init, 0
start()
def receive(request, client):
   if request.t >= q.t
      q.t = request.t
      if isinstance(request, ReadRequest):
         send(Reply(q.v, q.t), client)
      else:
         // message is a WriteRequest
         q.v = request.vCorrectnessNext we prove that all computations are serializable in a system
specified by these algorithms for clients and servers.Epoch of a step in a computation of the algorithmThe epoch of a step taken by a clientpor serverqis the value ofp.torq.t(respectively) upon completion of
the step.TheoremThe assignment of epochs to steps satisfies the
condition for epochs, i.e.,
For all
steps \((e, e')\) of a computationif \(e\) and \(e'\) are steps at the same agent, and \(e'\) occurs
  after \(e\) then \(t(e) \leq t(e')\), andif \(e\) is a step in which a message is sent and \(e'\) is a step
  in which that message is received then \(t(e) \leq t(e')\).ProofFrom the algorithms for serverpand clientqit follows thatp.tandq.tdo not decrease in a
computation.
Therefore the first condition is satisfied.Also, from the algorithm, the epoch for step in which a request or reply is
received is equal to the epoch in which the message is sent.
So, the second condition is also satisfied.
Therefore the assignment of epochs to steps of a computation satisfy
the specification of epochs.TheoremThe sufficient condition for serializability is satisfied:
(1) each transaction has a unique epoch, and (2) all steps in a
transaction have the epoch of the transaction.ProofWe have shown that each transaction initiated by a clientphas a unique epochp.t.All steps atphave epochp.t.
Requests sent byphave epochp.t.
When a clientqgets a request with epochp.t, the client  sets its own epoch top.tand sends a reply with the same epoch.
So, all steps in a transaction have the epoch of the transaction.
Therefore the algorithm satisfies the sufficient condition for
serializability.CorollaryThe algorithm satisfies specifications for serializability.ProofFollows because the algorithm satisfies the sufficient condition for
serializability.Therefore,
if the epochs of transactions in a computation \(x\) are \([T_{0}, T_{1}, T_{2}, \ldots]\)
where \(T_{i} < T_{i+1}\), then in \(x\), for all \(i\), for all agents \(v\):
All steps of \(v\) in the transaction with epoch \(T_{i}\) occur before any step
of \(v\) in the transaction with epoch \(T_{i+1}\).Equivalence of Distributed
and Sequential AlgorithmsLet the sequence of increasing epochs in a computation \(x\) of the distributed
algorithm beT[1], T[2], T[3], ..., and letp[i], R[i], W[i]be the values ofp, R, Win
the transaction with epochT[i].The sequence of values of agent variables in 
computation \(x\) of the distributed algorithm is the same as in the
following nondeterministic sequential program by selectingp, R, W, on thei-th iteration to bep[i], R[i], W[i]respectively, and by selectingdeltaon thei-th iteration so thatT[i] = T[i-1] + deltaEquivalent nondeterministic sequential algorithmt = 0
while True:
   select positive delta, select client p
   t = t + delta
   # p executes transaction with epoch t
   p.t = t
   p.copy = {}
   # Read step
   select R
   for q in R: p.copy[q] = q.v
   # Write step
   if len(p.copy) >= M:
     select W
     for q in W: q.v = p.f()We will prove properties of the sequential algorithm and show that
these properties also hold for the distributed algorithm.NextNext we develop Paxos, a distributed consensus algorithm.
We prove
that Paxos satisfies the specifications for consensus by showing that
Paxos is serializable.
We prove properties of a nondeterministic sequential algorithm and
show that properties of the sequential algorithm also hold for Paxos.K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

When a serverqreceives a requestrfrom
the transaction that the server 
is currently processing i.e.r.t\(=\)q.t,
then the server responds to the request.When a serverqreceives a requestrfrom a
transaction with a smaller 
epoch than the transaction that the server is currently processing,
i.e.r.t\(<\)q.t, then the server does not
respond to the request. The request has the same effect as a request that
is lost.What should serverqdo when it receives a requestrfrom a transaction with a larger epoch than the
server's epoch, i.e.r.t\(>\)q.t?In this case, the server increases its epoch to the
epoch of the request and then responds to the request.
The request, the step in which the server responds to the request,
and the reply (to a read request), have the same epoch.# initialization
q.v, q.t = init, 0
start()
def receive(request, client):
   if request.t >= q.t
      q.t = request.t
      if isinstance(request, ReadRequest):
         send(Reply(q.v, q.t), client)
      else:
         // message is a WriteRequest
         q.v = request.vCorrectnessNext we prove that all computations are serializable in a system
specified by these algorithms for clients and servers.Epoch of a step in a computation of the algorithmThe epoch of a step taken by a clientpor serverqis the value ofp.torq.t(respectively) upon completion of
the step.TheoremThe assignment of epochs to steps satisfies the
condition for epochs, i.e.,
For all
steps \((e, e')\) of a computationif \(e\) and \(e'\) are steps at the same agent, and \(e'\) occurs
  after \(e\) then \(t(e) \leq t(e')\), andif \(e\) is a step in which a message is sent and \(e'\) is a step
  in which that message is received then \(t(e) \leq t(e')\).ProofFrom the algorithms for serverpand clientqit follows thatp.tandq.tdo not decrease in a
computation.
Therefore the first condition is satisfied.Also, from the algorithm, the epoch for step in which a request or reply is
received is equal to the epoch in which the message is sent.
So, the second condition is also satisfied.
Therefore the assignment of epochs to steps of a computation satisfy
the specification of epochs.TheoremThe sufficient condition for serializability is satisfied:
(1) each transaction has a unique epoch, and (2) all steps in a
transaction have the epoch of the transaction.ProofWe have shown that each transaction initiated by a clientphas a unique epochp.t.All steps atphave epochp.t.
Requests sent byphave epochp.t.
When a clientqgets a request with epochp.t, the client  sets its own epoch top.tand sends a reply with the same epoch.
So, all steps in a transaction have the epoch of the transaction.
Therefore the algorithm satisfies the sufficient condition for
serializability.CorollaryThe algorithm satisfies specifications for serializability.ProofFollows because the algorithm satisfies the sufficient condition for
serializability.Therefore,
if the epochs of transactions in a computation \(x\) are \([T_{0}, T_{1}, T_{2}, \ldots]\)
where \(T_{i} < T_{i+1}\), then in \(x\), for all \(i\), for all agents \(v\):
All steps of \(v\) in the transaction with epoch \(T_{i}\) occur before any step
of \(v\) in the transaction with epoch \(T_{i+1}\).Equivalence of Distributed
and Sequential AlgorithmsLet the sequence of increasing epochs in a computation \(x\) of the distributed
algorithm beT[1], T[2], T[3], ..., and letp[i], R[i], W[i]be the values ofp, R, Win
the transaction with epochT[i].The sequence of values of agent variables in 
computation \(x\) of the distributed algorithm is the same as in the
following nondeterministic sequential program by selectingp, R, W, on thei-th iteration to bep[i], R[i], W[i]respectively, and by selectingdeltaon thei-th iteration so thatT[i] = T[i-1] + deltaEquivalent nondeterministic sequential algorithmt = 0
while True:
   select positive delta, select client p
   t = t + delta
   # p executes transaction with epoch t
   p.t = t
   p.copy = {}
   # Read step
   select R
   for q in R: p.copy[q] = q.v
   # Write step
   if len(p.copy) >= M:
     select W
     for q in W: q.v = p.f()We will prove properties of the sequential algorithm and show that
these properties also hold for the distributed algorithm.NextNext we develop Paxos, a distributed consensus algorithm.
We prove
that Paxos satisfies the specifications for consensus by showing that
Paxos is serializable.
We prove properties of a nondeterministic sequential algorithm and
show that properties of the sequential algorithm also hold for Paxos.K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

When a serverqreceives a requestrfrom a
transaction with a smaller 
epoch than the transaction that the server is currently processing,
i.e.r.t\(<\)q.t, then the server does not
respond to the request. The request has the same effect as a request that
is lost.What should serverqdo when it receives a requestrfrom a transaction with a larger epoch than the
server's epoch, i.e.r.t\(>\)q.t?In this case, the server increases its epoch to the
epoch of the request and then responds to the request.
The request, the step in which the server responds to the request,
and the reply (to a read request), have the same epoch.# initialization
q.v, q.t = init, 0
start()
def receive(request, client):
   if request.t >= q.t
      q.t = request.t
      if isinstance(request, ReadRequest):
         send(Reply(q.v, q.t), client)
      else:
         // message is a WriteRequest
         q.v = request.vCorrectnessNext we prove that all computations are serializable in a system
specified by these algorithms for clients and servers.Epoch of a step in a computation of the algorithmThe epoch of a step taken by a clientpor serverqis the value ofp.torq.t(respectively) upon completion of
the step.TheoremThe assignment of epochs to steps satisfies the
condition for epochs, i.e.,
For all
steps \((e, e')\) of a computationif \(e\) and \(e'\) are steps at the same agent, and \(e'\) occurs
  after \(e\) then \(t(e) \leq t(e')\), andif \(e\) is a step in which a message is sent and \(e'\) is a step
  in which that message is received then \(t(e) \leq t(e')\).ProofFrom the algorithms for serverpand clientqit follows thatp.tandq.tdo not decrease in a
computation.
Therefore the first condition is satisfied.Also, from the algorithm, the epoch for step in which a request or reply is
received is equal to the epoch in which the message is sent.
So, the second condition is also satisfied.
Therefore the assignment of epochs to steps of a computation satisfy
the specification of epochs.TheoremThe sufficient condition for serializability is satisfied:
(1) each transaction has a unique epoch, and (2) all steps in a
transaction have the epoch of the transaction.ProofWe have shown that each transaction initiated by a clientphas a unique epochp.t.All steps atphave epochp.t.
Requests sent byphave epochp.t.
When a clientqgets a request with epochp.t, the client  sets its own epoch top.tand sends a reply with the same epoch.
So, all steps in a transaction have the epoch of the transaction.
Therefore the algorithm satisfies the sufficient condition for
serializability.CorollaryThe algorithm satisfies specifications for serializability.ProofFollows because the algorithm satisfies the sufficient condition for
serializability.Therefore,
if the epochs of transactions in a computation \(x\) are \([T_{0}, T_{1}, T_{2}, \ldots]\)
where \(T_{i} < T_{i+1}\), then in \(x\), for all \(i\), for all agents \(v\):
All steps of \(v\) in the transaction with epoch \(T_{i}\) occur before any step
of \(v\) in the transaction with epoch \(T_{i+1}\).Equivalence of Distributed
and Sequential AlgorithmsLet the sequence of increasing epochs in a computation \(x\) of the distributed
algorithm beT[1], T[2], T[3], ..., and letp[i], R[i], W[i]be the values ofp, R, Win
the transaction with epochT[i].The sequence of values of agent variables in 
computation \(x\) of the distributed algorithm is the same as in the
following nondeterministic sequential program by selectingp, R, W, on thei-th iteration to bep[i], R[i], W[i]respectively, and by selectingdeltaon thei-th iteration so thatT[i] = T[i-1] + deltaEquivalent nondeterministic sequential algorithmt = 0
while True:
   select positive delta, select client p
   t = t + delta
   # p executes transaction with epoch t
   p.t = t
   p.copy = {}
   # Read step
   select R
   for q in R: p.copy[q] = q.v
   # Write step
   if len(p.copy) >= M:
     select W
     for q in W: q.v = p.f()We will prove properties of the sequential algorithm and show that
these properties also hold for the distributed algorithm.NextNext we develop Paxos, a distributed consensus algorithm.
We prove
that Paxos satisfies the specifications for consensus by showing that
Paxos is serializable.
We prove properties of a nondeterministic sequential algorithm and
show that properties of the sequential algorithm also hold for Paxos.K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

What should serverqdo when it receives a requestrfrom a transaction with a larger epoch than the
server's epoch, i.e.r.t\(>\)q.t?In this case, the server increases its epoch to the
epoch of the request and then responds to the request.
The request, the step in which the server responds to the request,
and the reply (to a read request), have the same epoch.# initialization
q.v, q.t = init, 0
start()
def receive(request, client):
   if request.t >= q.t
      q.t = request.t
      if isinstance(request, ReadRequest):
         send(Reply(q.v, q.t), client)
      else:
         // message is a WriteRequest
         q.v = request.vCorrectnessNext we prove that all computations are serializable in a system
specified by these algorithms for clients and servers.Epoch of a step in a computation of the algorithmThe epoch of a step taken by a clientpor serverqis the value ofp.torq.t(respectively) upon completion of
the step.TheoremThe assignment of epochs to steps satisfies the
condition for epochs, i.e.,
For all
steps \((e, e')\) of a computationif \(e\) and \(e'\) are steps at the same agent, and \(e'\) occurs
  after \(e\) then \(t(e) \leq t(e')\), andif \(e\) is a step in which a message is sent and \(e'\) is a step
  in which that message is received then \(t(e) \leq t(e')\).ProofFrom the algorithms for serverpand clientqit follows thatp.tandq.tdo not decrease in a
computation.
Therefore the first condition is satisfied.Also, from the algorithm, the epoch for step in which a request or reply is
received is equal to the epoch in which the message is sent.
So, the second condition is also satisfied.
Therefore the assignment of epochs to steps of a computation satisfy
the specification of epochs.TheoremThe sufficient condition for serializability is satisfied:
(1) each transaction has a unique epoch, and (2) all steps in a
transaction have the epoch of the transaction.ProofWe have shown that each transaction initiated by a clientphas a unique epochp.t.All steps atphave epochp.t.
Requests sent byphave epochp.t.
When a clientqgets a request with epochp.t, the client  sets its own epoch top.tand sends a reply with the same epoch.
So, all steps in a transaction have the epoch of the transaction.
Therefore the algorithm satisfies the sufficient condition for
serializability.CorollaryThe algorithm satisfies specifications for serializability.ProofFollows because the algorithm satisfies the sufficient condition for
serializability.Therefore,
if the epochs of transactions in a computation \(x\) are \([T_{0}, T_{1}, T_{2}, \ldots]\)
where \(T_{i} < T_{i+1}\), then in \(x\), for all \(i\), for all agents \(v\):
All steps of \(v\) in the transaction with epoch \(T_{i}\) occur before any step
of \(v\) in the transaction with epoch \(T_{i+1}\).Equivalence of Distributed
and Sequential AlgorithmsLet the sequence of increasing epochs in a computation \(x\) of the distributed
algorithm beT[1], T[2], T[3], ..., and letp[i], R[i], W[i]be the values ofp, R, Win
the transaction with epochT[i].The sequence of values of agent variables in 
computation \(x\) of the distributed algorithm is the same as in the
following nondeterministic sequential program by selectingp, R, W, on thei-th iteration to bep[i], R[i], W[i]respectively, and by selectingdeltaon thei-th iteration so thatT[i] = T[i-1] + deltaEquivalent nondeterministic sequential algorithmt = 0
while True:
   select positive delta, select client p
   t = t + delta
   # p executes transaction with epoch t
   p.t = t
   p.copy = {}
   # Read step
   select R
   for q in R: p.copy[q] = q.v
   # Write step
   if len(p.copy) >= M:
     select W
     for q in W: q.v = p.f()We will prove properties of the sequential algorithm and show that
these properties also hold for the distributed algorithm.NextNext we develop Paxos, a distributed consensus algorithm.
We prove
that Paxos satisfies the specifications for consensus by showing that
Paxos is serializable.
We prove properties of a nondeterministic sequential algorithm and
show that properties of the sequential algorithm also hold for Paxos.K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

In this case, the server increases its epoch to the
epoch of the request and then responds to the request.
The request, the step in which the server responds to the request,
and the reply (to a read request), have the same epoch.# initialization
q.v, q.t = init, 0
start()
def receive(request, client):
   if request.t >= q.t
      q.t = request.t
      if isinstance(request, ReadRequest):
         send(Reply(q.v, q.t), client)
      else:
         // message is a WriteRequest
         q.v = request.vCorrectnessNext we prove that all computations are serializable in a system
specified by these algorithms for clients and servers.Epoch of a step in a computation of the algorithmThe epoch of a step taken by a clientpor serverqis the value ofp.torq.t(respectively) upon completion of
the step.TheoremThe assignment of epochs to steps satisfies the
condition for epochs, i.e.,
For all
steps \((e, e')\) of a computationif \(e\) and \(e'\) are steps at the same agent, and \(e'\) occurs
  after \(e\) then \(t(e) \leq t(e')\), andif \(e\) is a step in which a message is sent and \(e'\) is a step
  in which that message is received then \(t(e) \leq t(e')\).ProofFrom the algorithms for serverpand clientqit follows thatp.tandq.tdo not decrease in a
computation.
Therefore the first condition is satisfied.Also, from the algorithm, the epoch for step in which a request or reply is
received is equal to the epoch in which the message is sent.
So, the second condition is also satisfied.
Therefore the assignment of epochs to steps of a computation satisfy
the specification of epochs.TheoremThe sufficient condition for serializability is satisfied:
(1) each transaction has a unique epoch, and (2) all steps in a
transaction have the epoch of the transaction.ProofWe have shown that each transaction initiated by a clientphas a unique epochp.t.All steps atphave epochp.t.
Requests sent byphave epochp.t.
When a clientqgets a request with epochp.t, the client  sets its own epoch top.tand sends a reply with the same epoch.
So, all steps in a transaction have the epoch of the transaction.
Therefore the algorithm satisfies the sufficient condition for
serializability.CorollaryThe algorithm satisfies specifications for serializability.ProofFollows because the algorithm satisfies the sufficient condition for
serializability.Therefore,
if the epochs of transactions in a computation \(x\) are \([T_{0}, T_{1}, T_{2}, \ldots]\)
where \(T_{i} < T_{i+1}\), then in \(x\), for all \(i\), for all agents \(v\):
All steps of \(v\) in the transaction with epoch \(T_{i}\) occur before any step
of \(v\) in the transaction with epoch \(T_{i+1}\).Equivalence of Distributed
and Sequential AlgorithmsLet the sequence of increasing epochs in a computation \(x\) of the distributed
algorithm beT[1], T[2], T[3], ..., and letp[i], R[i], W[i]be the values ofp, R, Win
the transaction with epochT[i].The sequence of values of agent variables in 
computation \(x\) of the distributed algorithm is the same as in the
following nondeterministic sequential program by selectingp, R, W, on thei-th iteration to bep[i], R[i], W[i]respectively, and by selectingdeltaon thei-th iteration so thatT[i] = T[i-1] + deltaEquivalent nondeterministic sequential algorithmt = 0
while True:
   select positive delta, select client p
   t = t + delta
   # p executes transaction with epoch t
   p.t = t
   p.copy = {}
   # Read step
   select R
   for q in R: p.copy[q] = q.v
   # Write step
   if len(p.copy) >= M:
     select W
     for q in W: q.v = p.f()We will prove properties of the sequential algorithm and show that
these properties also hold for the distributed algorithm.NextNext we develop Paxos, a distributed consensus algorithm.
We prove
that Paxos satisfies the specifications for consensus by showing that
Paxos is serializable.
We prove properties of a nondeterministic sequential algorithm and
show that properties of the sequential algorithm also hold for Paxos.K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

Epoch of a step in a computation of the algorithm

Proof

Also, from the algorithm, the epoch for step in which a request or reply is
received is equal to the epoch in which the message is sent.
So, the second condition is also satisfied.
Therefore the assignment of epochs to steps of a computation satisfy
the specification of epochs.TheoremThe sufficient condition for serializability is satisfied:
(1) each transaction has a unique epoch, and (2) all steps in a
transaction have the epoch of the transaction.ProofWe have shown that each transaction initiated by a clientphas a unique epochp.t.All steps atphave epochp.t.
Requests sent byphave epochp.t.
When a clientqgets a request with epochp.t, the client  sets its own epoch top.tand sends a reply with the same epoch.
So, all steps in a transaction have the epoch of the transaction.
Therefore the algorithm satisfies the sufficient condition for
serializability.CorollaryThe algorithm satisfies specifications for serializability.ProofFollows because the algorithm satisfies the sufficient condition for
serializability.Therefore,
if the epochs of transactions in a computation \(x\) are \([T_{0}, T_{1}, T_{2}, \ldots]\)
where \(T_{i} < T_{i+1}\), then in \(x\), for all \(i\), for all agents \(v\):
All steps of \(v\) in the transaction with epoch \(T_{i}\) occur before any step
of \(v\) in the transaction with epoch \(T_{i+1}\).Equivalence of Distributed
and Sequential AlgorithmsLet the sequence of increasing epochs in a computation \(x\) of the distributed
algorithm beT[1], T[2], T[3], ..., and letp[i], R[i], W[i]be the values ofp, R, Win
the transaction with epochT[i].The sequence of values of agent variables in 
computation \(x\) of the distributed algorithm is the same as in the
following nondeterministic sequential program by selectingp, R, W, on thei-th iteration to bep[i], R[i], W[i]respectively, and by selectingdeltaon thei-th iteration so thatT[i] = T[i-1] + deltaEquivalent nondeterministic sequential algorithmt = 0
while True:
   select positive delta, select client p
   t = t + delta
   # p executes transaction with epoch t
   p.t = t
   p.copy = {}
   # Read step
   select R
   for q in R: p.copy[q] = q.v
   # Write step
   if len(p.copy) >= M:
     select W
     for q in W: q.v = p.f()We will prove properties of the sequential algorithm and show that
these properties also hold for the distributed algorithm.NextNext we develop Paxos, a distributed consensus algorithm.
We prove
that Paxos satisfies the specifications for consensus by showing that
Paxos is serializable.
We prove properties of a nondeterministic sequential algorithm and
show that properties of the sequential algorithm also hold for Paxos.K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

Proof

All steps atphave epochp.t.
Requests sent byphave epochp.t.
When a clientqgets a request with epochp.t, the client  sets its own epoch top.tand sends a reply with the same epoch.
So, all steps in a transaction have the epoch of the transaction.
Therefore the algorithm satisfies the sufficient condition for
serializability.CorollaryThe algorithm satisfies specifications for serializability.ProofFollows because the algorithm satisfies the sufficient condition for
serializability.Therefore,
if the epochs of transactions in a computation \(x\) are \([T_{0}, T_{1}, T_{2}, \ldots]\)
where \(T_{i} < T_{i+1}\), then in \(x\), for all \(i\), for all agents \(v\):
All steps of \(v\) in the transaction with epoch \(T_{i}\) occur before any step
of \(v\) in the transaction with epoch \(T_{i+1}\).Equivalence of Distributed
and Sequential AlgorithmsLet the sequence of increasing epochs in a computation \(x\) of the distributed
algorithm beT[1], T[2], T[3], ..., and letp[i], R[i], W[i]be the values ofp, R, Win
the transaction with epochT[i].The sequence of values of agent variables in 
computation \(x\) of the distributed algorithm is the same as in the
following nondeterministic sequential program by selectingp, R, W, on thei-th iteration to bep[i], R[i], W[i]respectively, and by selectingdeltaon thei-th iteration so thatT[i] = T[i-1] + deltaEquivalent nondeterministic sequential algorithmt = 0
while True:
   select positive delta, select client p
   t = t + delta
   # p executes transaction with epoch t
   p.t = t
   p.copy = {}
   # Read step
   select R
   for q in R: p.copy[q] = q.v
   # Write step
   if len(p.copy) >= M:
     select W
     for q in W: q.v = p.f()We will prove properties of the sequential algorithm and show that
these properties also hold for the distributed algorithm.NextNext we develop Paxos, a distributed consensus algorithm.
We prove
that Paxos satisfies the specifications for consensus by showing that
Paxos is serializable.
We prove properties of a nondeterministic sequential algorithm and
show that properties of the sequential algorithm also hold for Paxos.K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

Proof

Therefore,
if the epochs of transactions in a computation \(x\) are \([T_{0}, T_{1}, T_{2}, \ldots]\)
where \(T_{i} < T_{i+1}\), then in \(x\), for all \(i\), for all agents \(v\):
All steps of \(v\) in the transaction with epoch \(T_{i}\) occur before any step
of \(v\) in the transaction with epoch \(T_{i+1}\).

The sequence of values of agent variables in 
computation \(x\) of the distributed algorithm is the same as in the
following nondeterministic sequential program by selectingp, R, W, on thei-th iteration to bep[i], R[i], W[i]respectively, and by selectingdeltaon thei-th iteration so thatT[i] = T[i-1] + deltaEquivalent nondeterministic sequential algorithmt = 0
while True:
   select positive delta, select client p
   t = t + delta
   # p executes transaction with epoch t
   p.t = t
   p.copy = {}
   # Read step
   select R
   for q in R: p.copy[q] = q.v
   # Write step
   if len(p.copy) >= M:
     select W
     for q in W: q.v = p.f()We will prove properties of the sequential algorithm and show that
these properties also hold for the distributed algorithm.NextNext we develop Paxos, a distributed consensus algorithm.
We prove
that Paxos satisfies the specifications for consensus by showing that
Paxos is serializable.
We prove properties of a nondeterministic sequential algorithm and
show that properties of the sequential algorithm also hold for Paxos.K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

Equivalent nondeterministic sequential algorithm

K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology
--- END Content from https://kmchandy.github.io/Paxos/ReadWriteLossyChannels.html ---


--- START Content from https://kmchandy.github.io/Paxos/StableMajority.html ---
TITLE: Paxos/StableMajority.html

Algorithms by which agents reach a consensus on a value are central
  in many applications.
  Paxos is an algorithm by which agents attempt to reach a consensus
  in distributed systems in which agents may halt, be arbitrarily
  slow, and messages may be duplicated, lost, and delivered out of
  order.
  In this page we describe and prove a nondeterministic sequential
  representation of Paxos, and describe the distributed algorithm in
  the next page.IntroductionPaxos is a consensus algorithm for systems in which messages may be lost; multiple
  copies of a message may be delivered; messages may not be
  delivered in the order sent; agents may be arbitrarily slow; and
  agents may stop.
  
  Fromthe
  FLP theorem,
  
  there is no algorithm that guarantees that consensus among agents
  will be reached in such systems. 
  Paxos may not terminate; however, if it does terminate then a
  consensus will have been reached at termination.
  
  We will discuss ways to improve the likelihood that the algorithm
  does terminate.Consider the problem of maintaining a ledger consisting
  of a sequence of transfers of funds into and out of accounts.
  
  To prevent a single point of failure
  the ledger may be implemented using copies at multiple agents.
  The copies may not be synchronized; however, for a ledger to be
  useful there must exist a 
  consensus.
  We call the sequence of operations on the ledger achainof
  operations and we call the the consensus value aconsensus chain.Suppose two clients
  simultaneously request extensions to a consensus chain by transferring funds
  from an account x to two different accounts.
  Account x may not have funds to allow both transfers, and so the
  consensus chain can be extended by at most one of the transfers.
  Clearly, the system must maintain a consensus about the sequence of
  operations in the chain.Agents that propose extensions to consensus chains are calledclients.
  Copies of the chain are stored at agents calledservers.
Clients determine consensus chains and propose extensions to the
chains.
The system determines a consensus among proposals for
extended chains.Before giving the specification of consensus we review the concept of
prefix of a sequence.Notation: prefix of a sequenceAprefixof a sequence S is an initial subsequence
  of S.
  For example, [A, B] is a prefix of [A, B, C] but [A, C] is not a
prefix of [A, B, C].The empty sequence is a prefix of all sequences.
A sequence is a prefix of itself.
We use the notation \(\leq\) for prefix, as in: 
[A, B] \(\leq \) [A, B, C]Specification of Consensus1. A consensus isn't changed.For all \(c\) and \(d\), if \(c\) is a consensus chain at any point in
a computation and \(d\) is a consensus chain at a later point in
the computation then 
\(c\) is a prefix of \(d\).ExampleFor example, a consensus chain [A, B] can be extended to form a
consensus chain [A, B, C], but
[A, B] cannot be extended to form [A, C, B].
Chains [A, B] and [A, B, C] can both be consensus chains in the same
computation; however,
[A, B] and [A, C, B] cannot both be consensus chains in the same
computation.2. Every consensus chain is proposed by a
clientFor all consensus chains \(c\) in a computation:
There exists a client that proposed \(c\) in the computation.This part of the specification merely says that consensus chains cannot be
arbitrary.
A client uses the algorithm to determine chain \(c\) that is a
consensus of the servers and then proposes an extension to \(c\). The
algorithm then reaches a consensus about extensions to \(c\) proposed
by clients; the extensions must be proposed by clients.
Later, we define this part of the specification in terms of algorithm
variables.A Nondeterministic Sequential Consensus AlgorithmWe will develop a consensus algorithm for the system described in the
previous pageSerializable Computations in Faulty Systems.A system has a sets of agents called servers and clients.
Each serverqhas a local variableq.v.
A client executes a transaction in which the client sends requests to
each serverqto readq.v;
waits to receive replies; then sends write requests to servers.
Read the previous page for details.A transaction is executed by exactly one client.
Clientpsends read requests and gets replies from a setRof servers.
IfRhas at leastMelements thenpsends write requests to all servers asking each serverqto assignp.f()toq.v.Some write requests may be lost. Let W be the set of servers that
receive the write requests.The design of the distributed consensus algorithm is based on this keyresult.For each agent, the sequence of values of the agent in the distributed
algorithmdescribed in the
previous pageis the same as in an equivalent nondeterministic
sequential algorithm.We now prove properties of the values of servers in the equivalent sequential
algorithm; these properties also hold for the distributed algorithm.
The equivalent sequential program from theprevious pageis given below.
The selections ofdelta, p, R, Win each iteration of the
algorithm are nondeterministic.Equivalent Nondeterministic Sequential Algorithmt = 0
while True:
   select positive delta, select client p
   t = t + delta
   # p executes transaction with epoch t
   p.t = t
   p.copy = {}
   # Read step
   select R
   for q in R: p.copy[q] = q.v
   # Write step
   if len(p.copy) >= M:
     select W
     for q in W: q.v = p.f()The ProblemThe problem is to define consensus in terms of variables of the sequential
algorithm and specify parametersMandfof the algorithm so that the
specification for consensus is satisfied.How Should You Solve The Problem?When is Consensus Reached?Let's assume that a consensus is reached when a sufficiently large
number of servers write the same value in a transaction.
Equivalently, consensus is reached when the setWof
servers that write the same 
value in an iteration of the sequential algorithm is large enough, and
a consensus is the value written by all servers inW.How large is enough?We could require thatWbe the set ofallservers, but
then consensus is reached in an iteration only if write requests reach
all servers.
The smaller the size ofWrequired for consensus, the
greater the likelihood of consensus being reached.A reasonable design choice is thatWis a majority of
servers. 
LetNbe the number of servers.A consensus is reached at the end of an iteration in whichWis a majority of servers.Tagging assignments with the epochs in which
they were assignedWe need some mechanism to determine which servers were written in the
same iteration, and which were written earlier.
So, we make variableq.vhave two fields,(q.v.s, q.v.t)whereq.v.sis the chain atq.vandq.v.tis the epoch in whichq.v.swas assigned its value.
In the sequential algorithm we replace the assignment stepq.v = p.f()byq.v.s, q.v.t = p.f(), p.tWe make the same change the distributed algorithm, replacing the
following assignment in the algorithm for clientpv, t = p.f(), p.tbyv.s, v.t, t = p.f(), p.t, p.tA consensus is reached when the setWof servers in an
iteration is a majority.
For allqin this majority,q.v.tis the
epoch of the iteration.
Therefore, consensus is determined as follows:Consensus: Majority of Servers Agree on a Consensus Values*is a consensus chain in a computation of seq if
during the computation
 there exists a majorityW*of servers and an
epocht*where:
For allqinW*:(q.v.s = s*)and(q.v.t = t*)SpecifyingM: The Number of
Replies Required to ProceedA client needs to receive at leastMcopies of chains
to proceed to the write step.
What is a reasonable value ofM?The smaller the value ofMthe greater the likelihood of
proceeding to the write step.
Again, a reasonable design choice is to require that the client
receive replies from a majority of servers.
If that choice doesn't work we will try a larger value ofM.A client must receive
replies from a majority of servers to proceed to the write step.Specifyingp.f: The Values to be
WrittenConsider a single iteration ofseq.
IfRis not a majority then the iteration terminates
without modifying server values.
Now we consider the case in whichRis a majority.
We design the function so that it returns an extension to a consensus
chain.
Consider two cases.Case 1: All replies are identical.Let each elementq.vinRbes*,
t*.
BecauseRis a majority of servers, from the definition
of consensus it follows thats*is a consensus chain.A client proposes an extension of a consensus chain,
Sop.freturns the consensus,s*, appended
withp.h()wherep.h()is the extension proposed byp.Case 2: Not all replies are identical.p.freturnsq*.v.swhereq*.vis the reply with the largest epoch.The algorithm is given below.def p.f():
  values_read = list(p.copy.values())
  if all(values_read[0] == v for v in values_read):
     # All elements of values_read are identical
     # consensus is any element of values_read.
     consensus = values_read[0].s
     return consensus.append(h())
  else:
     # Not all elements of values_read are identical
     # Compute max_v, the element with largest v.t
     max_v = values_read[0]
     for v in values_read:
        if v.t > max_v.t:  max_v = v
     return max_v.sCorrectness of the Nondeterministic Sequential AlgorithmObservationPart 2 of the specification --
every consensus chain is proposed by a client
--
follows directly from the algorithm
  because the only assignment toq.v.sisp.f().Next,  we prove part 1. We will show that ifcis a
consensus chain after an iteration anddis a
consensus chain after a later iteration thencis a
prefix ofd.Theorem: The Sequential Algorithm is CorrectLets*be a consensus at some point in a computation of
the sequential program.
Then, there exists a majority W* of servers and an epoch t* where: For all q in W*:(q.v.s = s*) and (q.v.t = t*)Let the iteration that executes the transaction with epocht*be then*-th iteration.
Because each assignment toq.vincreasesq.v.twe get the following.Equation 1At the end of then-th iteration,
for allqinW*:For alln\(\geq\)n*: \(\quad\)q.v.t\(\geq\)t*For alln\(<\)n*: \(\quad\)q.v.t\(<\)t*Next we will prove thats*is a prefix of any assignment
to any server variable in any 
iteration after iterationn*.
This can be written as: at all points in the computation, :At all points in the computation, for all serversq:(q.v.t\(<\)t*) \(\; \vee \;\)
(s*\(\leq\)q.v.s)Proof of the InvariantBase Case:n-th Iteration wheren\(\leq\)n*The condition trivially holds for iterationnwheren\(<\)n*because, from equation 1:q.v.t\(<\)t*.Next, consider the case wheren\(=\)n*.
Ifq.vis modified in the iteration thenq.v.s\(=\)s*.
Ifq.vis not modified in the iteration thenq.v.t\(<\)t*.Induction StepAssume that the condition holds before iterationnforn\(>\)n*and 
show that it holds after the iteration.From equation 1 and the induction hypothesis, the following holds
before iterationn.For allqinW*:(q.v.t\(\geq\)t*)
\(\; \wedge \; \)
(s*\(\leq\)q.v.s)In any iteration, server variables are modified only ifRis a majority of servers.
Any two majorities have at least one element in common.
So there is an
element that is in bothRandW*.
Let this element beq*.
Becauseq*is an element ofW*:(q*.v.t\(\geq\)t*)
\(\: \wedge \;\)
(s*\(\leq\)q*.v.s)This gives us the following:Equation 2There exists aq*inRsuch that:(q*.v.t\(\geq\)t*)
\(\: \wedge \;\)
(s*\(\leq\)q*.v.s)Letdbe the value returned by functionp.f.
Consider the statements inp.fand two cases.Case 1: All valuesq.vforqinRare identical.In this case, from equation 2:For allqinR:s*\(\leq\)q.v.s.In functionp.f,consensusis assignedq.v.sfor someqinR.Therefore,s*\(\leq\)consensus.The function returnsd = consensus.append(h()).Therefores*\(\leq\)d.Case 2: Not all valuesq.vforqinRare identical.The function returnsq'.v.twhereq'is inRand:For allqinR: \(\quad\)q'.v.t\(\geq\)q.v.tFrom equation 2:q'.v.t\(\geq\)q*.v.t\(\geq\)t*From the induction hypothesis, andq'.v.t\(\geq\)t*, it follows thats*\(\leq\)q'.v.t.Therefores*\(\leq\)d.
This completes the proof of the invariant.We have shown thats*is a prefix ofallassignments made to server variables after iterationn*.
Therefore,s*is a prefix of all consensus values
reached in iterations after iterationn*.The Distributed Algorithm is CorrectWe showedthe equivalence of the
distributed and nondeterministic sequential algorithm: For every
computation \(c\) of the distributed algorithm there exists a
computation \(c'\) of
the nondeterministic sequential algorithm -- with appropriate choices of
nondeterministic selections -- such that for all serversqthe sequence of values ofq.vis the same
in \(c\) and \(c'\).Therefore properties of the sequence of values ofq.vfor
each serverqin the sequential algorithm are also
properties in the distributed algorithm.K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

Consider the problem of maintaining a ledger consisting
  of a sequence of transfers of funds into and out of accounts.
  
  To prevent a single point of failure
  the ledger may be implemented using copies at multiple agents.
  The copies may not be synchronized; however, for a ledger to be
  useful there must exist a 
  consensus.
  We call the sequence of operations on the ledger achainof
  operations and we call the the consensus value aconsensus chain.Suppose two clients
  simultaneously request extensions to a consensus chain by transferring funds
  from an account x to two different accounts.
  Account x may not have funds to allow both transfers, and so the
  consensus chain can be extended by at most one of the transfers.
  Clearly, the system must maintain a consensus about the sequence of
  operations in the chain.Agents that propose extensions to consensus chains are calledclients.
  Copies of the chain are stored at agents calledservers.
Clients determine consensus chains and propose extensions to the
chains.
The system determines a consensus among proposals for
extended chains.Before giving the specification of consensus we review the concept of
prefix of a sequence.Notation: prefix of a sequenceAprefixof a sequence S is an initial subsequence
  of S.
  For example, [A, B] is a prefix of [A, B, C] but [A, C] is not a
prefix of [A, B, C].The empty sequence is a prefix of all sequences.
A sequence is a prefix of itself.
We use the notation \(\leq\) for prefix, as in: 
[A, B] \(\leq \) [A, B, C]Specification of Consensus1. A consensus isn't changed.For all \(c\) and \(d\), if \(c\) is a consensus chain at any point in
a computation and \(d\) is a consensus chain at a later point in
the computation then 
\(c\) is a prefix of \(d\).ExampleFor example, a consensus chain [A, B] can be extended to form a
consensus chain [A, B, C], but
[A, B] cannot be extended to form [A, C, B].
Chains [A, B] and [A, B, C] can both be consensus chains in the same
computation; however,
[A, B] and [A, C, B] cannot both be consensus chains in the same
computation.2. Every consensus chain is proposed by a
clientFor all consensus chains \(c\) in a computation:
There exists a client that proposed \(c\) in the computation.This part of the specification merely says that consensus chains cannot be
arbitrary.
A client uses the algorithm to determine chain \(c\) that is a
consensus of the servers and then proposes an extension to \(c\). The
algorithm then reaches a consensus about extensions to \(c\) proposed
by clients; the extensions must be proposed by clients.
Later, we define this part of the specification in terms of algorithm
variables.A Nondeterministic Sequential Consensus AlgorithmWe will develop a consensus algorithm for the system described in the
previous pageSerializable Computations in Faulty Systems.A system has a sets of agents called servers and clients.
Each serverqhas a local variableq.v.
A client executes a transaction in which the client sends requests to
each serverqto readq.v;
waits to receive replies; then sends write requests to servers.
Read the previous page for details.A transaction is executed by exactly one client.
Clientpsends read requests and gets replies from a setRof servers.
IfRhas at leastMelements thenpsends write requests to all servers asking each serverqto assignp.f()toq.v.Some write requests may be lost. Let W be the set of servers that
receive the write requests.The design of the distributed consensus algorithm is based on this keyresult.For each agent, the sequence of values of the agent in the distributed
algorithmdescribed in the
previous pageis the same as in an equivalent nondeterministic
sequential algorithm.We now prove properties of the values of servers in the equivalent sequential
algorithm; these properties also hold for the distributed algorithm.
The equivalent sequential program from theprevious pageis given below.
The selections ofdelta, p, R, Win each iteration of the
algorithm are nondeterministic.Equivalent Nondeterministic Sequential Algorithmt = 0
while True:
   select positive delta, select client p
   t = t + delta
   # p executes transaction with epoch t
   p.t = t
   p.copy = {}
   # Read step
   select R
   for q in R: p.copy[q] = q.v
   # Write step
   if len(p.copy) >= M:
     select W
     for q in W: q.v = p.f()The ProblemThe problem is to define consensus in terms of variables of the sequential
algorithm and specify parametersMandfof the algorithm so that the
specification for consensus is satisfied.How Should You Solve The Problem?When is Consensus Reached?Let's assume that a consensus is reached when a sufficiently large
number of servers write the same value in a transaction.
Equivalently, consensus is reached when the setWof
servers that write the same 
value in an iteration of the sequential algorithm is large enough, and
a consensus is the value written by all servers inW.How large is enough?We could require thatWbe the set ofallservers, but
then consensus is reached in an iteration only if write requests reach
all servers.
The smaller the size ofWrequired for consensus, the
greater the likelihood of consensus being reached.A reasonable design choice is thatWis a majority of
servers. 
LetNbe the number of servers.A consensus is reached at the end of an iteration in whichWis a majority of servers.Tagging assignments with the epochs in which
they were assignedWe need some mechanism to determine which servers were written in the
same iteration, and which were written earlier.
So, we make variableq.vhave two fields,(q.v.s, q.v.t)whereq.v.sis the chain atq.vandq.v.tis the epoch in whichq.v.swas assigned its value.
In the sequential algorithm we replace the assignment stepq.v = p.f()byq.v.s, q.v.t = p.f(), p.tWe make the same change the distributed algorithm, replacing the
following assignment in the algorithm for clientpv, t = p.f(), p.tbyv.s, v.t, t = p.f(), p.t, p.tA consensus is reached when the setWof servers in an
iteration is a majority.
For allqin this majority,q.v.tis the
epoch of the iteration.
Therefore, consensus is determined as follows:Consensus: Majority of Servers Agree on a Consensus Values*is a consensus chain in a computation of seq if
during the computation
 there exists a majorityW*of servers and an
epocht*where:
For allqinW*:(q.v.s = s*)and(q.v.t = t*)SpecifyingM: The Number of
Replies Required to ProceedA client needs to receive at leastMcopies of chains
to proceed to the write step.
What is a reasonable value ofM?The smaller the value ofMthe greater the likelihood of
proceeding to the write step.
Again, a reasonable design choice is to require that the client
receive replies from a majority of servers.
If that choice doesn't work we will try a larger value ofM.A client must receive
replies from a majority of servers to proceed to the write step.Specifyingp.f: The Values to be
WrittenConsider a single iteration ofseq.
IfRis not a majority then the iteration terminates
without modifying server values.
Now we consider the case in whichRis a majority.
We design the function so that it returns an extension to a consensus
chain.
Consider two cases.Case 1: All replies are identical.Let each elementq.vinRbes*,
t*.
BecauseRis a majority of servers, from the definition
of consensus it follows thats*is a consensus chain.A client proposes an extension of a consensus chain,
Sop.freturns the consensus,s*, appended
withp.h()wherep.h()is the extension proposed byp.Case 2: Not all replies are identical.p.freturnsq*.v.swhereq*.vis the reply with the largest epoch.The algorithm is given below.def p.f():
  values_read = list(p.copy.values())
  if all(values_read[0] == v for v in values_read):
     # All elements of values_read are identical
     # consensus is any element of values_read.
     consensus = values_read[0].s
     return consensus.append(h())
  else:
     # Not all elements of values_read are identical
     # Compute max_v, the element with largest v.t
     max_v = values_read[0]
     for v in values_read:
        if v.t > max_v.t:  max_v = v
     return max_v.sCorrectness of the Nondeterministic Sequential AlgorithmObservationPart 2 of the specification --
every consensus chain is proposed by a client
--
follows directly from the algorithm
  because the only assignment toq.v.sisp.f().Next,  we prove part 1. We will show that ifcis a
consensus chain after an iteration anddis a
consensus chain after a later iteration thencis a
prefix ofd.Theorem: The Sequential Algorithm is CorrectLets*be a consensus at some point in a computation of
the sequential program.
Then, there exists a majority W* of servers and an epoch t* where: For all q in W*:(q.v.s = s*) and (q.v.t = t*)Let the iteration that executes the transaction with epocht*be then*-th iteration.
Because each assignment toq.vincreasesq.v.twe get the following.Equation 1At the end of then-th iteration,
for allqinW*:For alln\(\geq\)n*: \(\quad\)q.v.t\(\geq\)t*For alln\(<\)n*: \(\quad\)q.v.t\(<\)t*Next we will prove thats*is a prefix of any assignment
to any server variable in any 
iteration after iterationn*.
This can be written as: at all points in the computation, :At all points in the computation, for all serversq:(q.v.t\(<\)t*) \(\; \vee \;\)
(s*\(\leq\)q.v.s)Proof of the InvariantBase Case:n-th Iteration wheren\(\leq\)n*The condition trivially holds for iterationnwheren\(<\)n*because, from equation 1:q.v.t\(<\)t*.Next, consider the case wheren\(=\)n*.
Ifq.vis modified in the iteration thenq.v.s\(=\)s*.
Ifq.vis not modified in the iteration thenq.v.t\(<\)t*.Induction StepAssume that the condition holds before iterationnforn\(>\)n*and 
show that it holds after the iteration.From equation 1 and the induction hypothesis, the following holds
before iterationn.For allqinW*:(q.v.t\(\geq\)t*)
\(\; \wedge \; \)
(s*\(\leq\)q.v.s)In any iteration, server variables are modified only ifRis a majority of servers.
Any two majorities have at least one element in common.
So there is an
element that is in bothRandW*.
Let this element beq*.
Becauseq*is an element ofW*:(q*.v.t\(\geq\)t*)
\(\: \wedge \;\)
(s*\(\leq\)q*.v.s)This gives us the following:Equation 2There exists aq*inRsuch that:(q*.v.t\(\geq\)t*)
\(\: \wedge \;\)
(s*\(\leq\)q*.v.s)Letdbe the value returned by functionp.f.
Consider the statements inp.fand two cases.Case 1: All valuesq.vforqinRare identical.In this case, from equation 2:For allqinR:s*\(\leq\)q.v.s.In functionp.f,consensusis assignedq.v.sfor someqinR.Therefore,s*\(\leq\)consensus.The function returnsd = consensus.append(h()).Therefores*\(\leq\)d.Case 2: Not all valuesq.vforqinRare identical.The function returnsq'.v.twhereq'is inRand:For allqinR: \(\quad\)q'.v.t\(\geq\)q.v.tFrom equation 2:q'.v.t\(\geq\)q*.v.t\(\geq\)t*From the induction hypothesis, andq'.v.t\(\geq\)t*, it follows thats*\(\leq\)q'.v.t.Therefores*\(\leq\)d.
This completes the proof of the invariant.We have shown thats*is a prefix ofallassignments made to server variables after iterationn*.
Therefore,s*is a prefix of all consensus values
reached in iterations after iterationn*.The Distributed Algorithm is CorrectWe showedthe equivalence of the
distributed and nondeterministic sequential algorithm: For every
computation \(c\) of the distributed algorithm there exists a
computation \(c'\) of
the nondeterministic sequential algorithm -- with appropriate choices of
nondeterministic selections -- such that for all serversqthe sequence of values ofq.vis the same
in \(c\) and \(c'\).Therefore properties of the sequence of values ofq.vfor
each serverqin the sequential algorithm are also
properties in the distributed algorithm.K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

Suppose two clients
  simultaneously request extensions to a consensus chain by transferring funds
  from an account x to two different accounts.
  Account x may not have funds to allow both transfers, and so the
  consensus chain can be extended by at most one of the transfers.
  Clearly, the system must maintain a consensus about the sequence of
  operations in the chain.Agents that propose extensions to consensus chains are calledclients.
  Copies of the chain are stored at agents calledservers.
Clients determine consensus chains and propose extensions to the
chains.
The system determines a consensus among proposals for
extended chains.Before giving the specification of consensus we review the concept of
prefix of a sequence.Notation: prefix of a sequenceAprefixof a sequence S is an initial subsequence
  of S.
  For example, [A, B] is a prefix of [A, B, C] but [A, C] is not a
prefix of [A, B, C].The empty sequence is a prefix of all sequences.
A sequence is a prefix of itself.
We use the notation \(\leq\) for prefix, as in: 
[A, B] \(\leq \) [A, B, C]Specification of Consensus1. A consensus isn't changed.For all \(c\) and \(d\), if \(c\) is a consensus chain at any point in
a computation and \(d\) is a consensus chain at a later point in
the computation then 
\(c\) is a prefix of \(d\).ExampleFor example, a consensus chain [A, B] can be extended to form a
consensus chain [A, B, C], but
[A, B] cannot be extended to form [A, C, B].
Chains [A, B] and [A, B, C] can both be consensus chains in the same
computation; however,
[A, B] and [A, C, B] cannot both be consensus chains in the same
computation.2. Every consensus chain is proposed by a
clientFor all consensus chains \(c\) in a computation:
There exists a client that proposed \(c\) in the computation.This part of the specification merely says that consensus chains cannot be
arbitrary.
A client uses the algorithm to determine chain \(c\) that is a
consensus of the servers and then proposes an extension to \(c\). The
algorithm then reaches a consensus about extensions to \(c\) proposed
by clients; the extensions must be proposed by clients.
Later, we define this part of the specification in terms of algorithm
variables.A Nondeterministic Sequential Consensus AlgorithmWe will develop a consensus algorithm for the system described in the
previous pageSerializable Computations in Faulty Systems.A system has a sets of agents called servers and clients.
Each serverqhas a local variableq.v.
A client executes a transaction in which the client sends requests to
each serverqto readq.v;
waits to receive replies; then sends write requests to servers.
Read the previous page for details.A transaction is executed by exactly one client.
Clientpsends read requests and gets replies from a setRof servers.
IfRhas at leastMelements thenpsends write requests to all servers asking each serverqto assignp.f()toq.v.Some write requests may be lost. Let W be the set of servers that
receive the write requests.The design of the distributed consensus algorithm is based on this keyresult.For each agent, the sequence of values of the agent in the distributed
algorithmdescribed in the
previous pageis the same as in an equivalent nondeterministic
sequential algorithm.We now prove properties of the values of servers in the equivalent sequential
algorithm; these properties also hold for the distributed algorithm.
The equivalent sequential program from theprevious pageis given below.
The selections ofdelta, p, R, Win each iteration of the
algorithm are nondeterministic.Equivalent Nondeterministic Sequential Algorithmt = 0
while True:
   select positive delta, select client p
   t = t + delta
   # p executes transaction with epoch t
   p.t = t
   p.copy = {}
   # Read step
   select R
   for q in R: p.copy[q] = q.v
   # Write step
   if len(p.copy) >= M:
     select W
     for q in W: q.v = p.f()The ProblemThe problem is to define consensus in terms of variables of the sequential
algorithm and specify parametersMandfof the algorithm so that the
specification for consensus is satisfied.How Should You Solve The Problem?When is Consensus Reached?Let's assume that a consensus is reached when a sufficiently large
number of servers write the same value in a transaction.
Equivalently, consensus is reached when the setWof
servers that write the same 
value in an iteration of the sequential algorithm is large enough, and
a consensus is the value written by all servers inW.How large is enough?We could require thatWbe the set ofallservers, but
then consensus is reached in an iteration only if write requests reach
all servers.
The smaller the size ofWrequired for consensus, the
greater the likelihood of consensus being reached.A reasonable design choice is thatWis a majority of
servers. 
LetNbe the number of servers.A consensus is reached at the end of an iteration in whichWis a majority of servers.Tagging assignments with the epochs in which
they were assignedWe need some mechanism to determine which servers were written in the
same iteration, and which were written earlier.
So, we make variableq.vhave two fields,(q.v.s, q.v.t)whereq.v.sis the chain atq.vandq.v.tis the epoch in whichq.v.swas assigned its value.
In the sequential algorithm we replace the assignment stepq.v = p.f()byq.v.s, q.v.t = p.f(), p.tWe make the same change the distributed algorithm, replacing the
following assignment in the algorithm for clientpv, t = p.f(), p.tbyv.s, v.t, t = p.f(), p.t, p.tA consensus is reached when the setWof servers in an
iteration is a majority.
For allqin this majority,q.v.tis the
epoch of the iteration.
Therefore, consensus is determined as follows:Consensus: Majority of Servers Agree on a Consensus Values*is a consensus chain in a computation of seq if
during the computation
 there exists a majorityW*of servers and an
epocht*where:
For allqinW*:(q.v.s = s*)and(q.v.t = t*)SpecifyingM: The Number of
Replies Required to ProceedA client needs to receive at leastMcopies of chains
to proceed to the write step.
What is a reasonable value ofM?The smaller the value ofMthe greater the likelihood of
proceeding to the write step.
Again, a reasonable design choice is to require that the client
receive replies from a majority of servers.
If that choice doesn't work we will try a larger value ofM.A client must receive
replies from a majority of servers to proceed to the write step.Specifyingp.f: The Values to be
WrittenConsider a single iteration ofseq.
IfRis not a majority then the iteration terminates
without modifying server values.
Now we consider the case in whichRis a majority.
We design the function so that it returns an extension to a consensus
chain.
Consider two cases.Case 1: All replies are identical.Let each elementq.vinRbes*,
t*.
BecauseRis a majority of servers, from the definition
of consensus it follows thats*is a consensus chain.A client proposes an extension of a consensus chain,
Sop.freturns the consensus,s*, appended
withp.h()wherep.h()is the extension proposed byp.Case 2: Not all replies are identical.p.freturnsq*.v.swhereq*.vis the reply with the largest epoch.The algorithm is given below.def p.f():
  values_read = list(p.copy.values())
  if all(values_read[0] == v for v in values_read):
     # All elements of values_read are identical
     # consensus is any element of values_read.
     consensus = values_read[0].s
     return consensus.append(h())
  else:
     # Not all elements of values_read are identical
     # Compute max_v, the element with largest v.t
     max_v = values_read[0]
     for v in values_read:
        if v.t > max_v.t:  max_v = v
     return max_v.sCorrectness of the Nondeterministic Sequential AlgorithmObservationPart 2 of the specification --
every consensus chain is proposed by a client
--
follows directly from the algorithm
  because the only assignment toq.v.sisp.f().Next,  we prove part 1. We will show that ifcis a
consensus chain after an iteration anddis a
consensus chain after a later iteration thencis a
prefix ofd.Theorem: The Sequential Algorithm is CorrectLets*be a consensus at some point in a computation of
the sequential program.
Then, there exists a majority W* of servers and an epoch t* where: For all q in W*:(q.v.s = s*) and (q.v.t = t*)Let the iteration that executes the transaction with epocht*be then*-th iteration.
Because each assignment toq.vincreasesq.v.twe get the following.Equation 1At the end of then-th iteration,
for allqinW*:For alln\(\geq\)n*: \(\quad\)q.v.t\(\geq\)t*For alln\(<\)n*: \(\quad\)q.v.t\(<\)t*Next we will prove thats*is a prefix of any assignment
to any server variable in any 
iteration after iterationn*.
This can be written as: at all points in the computation, :At all points in the computation, for all serversq:(q.v.t\(<\)t*) \(\; \vee \;\)
(s*\(\leq\)q.v.s)Proof of the InvariantBase Case:n-th Iteration wheren\(\leq\)n*The condition trivially holds for iterationnwheren\(<\)n*because, from equation 1:q.v.t\(<\)t*.Next, consider the case wheren\(=\)n*.
Ifq.vis modified in the iteration thenq.v.s\(=\)s*.
Ifq.vis not modified in the iteration thenq.v.t\(<\)t*.Induction StepAssume that the condition holds before iterationnforn\(>\)n*and 
show that it holds after the iteration.From equation 1 and the induction hypothesis, the following holds
before iterationn.For allqinW*:(q.v.t\(\geq\)t*)
\(\; \wedge \; \)
(s*\(\leq\)q.v.s)In any iteration, server variables are modified only ifRis a majority of servers.
Any two majorities have at least one element in common.
So there is an
element that is in bothRandW*.
Let this element beq*.
Becauseq*is an element ofW*:(q*.v.t\(\geq\)t*)
\(\: \wedge \;\)
(s*\(\leq\)q*.v.s)This gives us the following:Equation 2There exists aq*inRsuch that:(q*.v.t\(\geq\)t*)
\(\: \wedge \;\)
(s*\(\leq\)q*.v.s)Letdbe the value returned by functionp.f.
Consider the statements inp.fand two cases.Case 1: All valuesq.vforqinRare identical.In this case, from equation 2:For allqinR:s*\(\leq\)q.v.s.In functionp.f,consensusis assignedq.v.sfor someqinR.Therefore,s*\(\leq\)consensus.The function returnsd = consensus.append(h()).Therefores*\(\leq\)d.Case 2: Not all valuesq.vforqinRare identical.The function returnsq'.v.twhereq'is inRand:For allqinR: \(\quad\)q'.v.t\(\geq\)q.v.tFrom equation 2:q'.v.t\(\geq\)q*.v.t\(\geq\)t*From the induction hypothesis, andq'.v.t\(\geq\)t*, it follows thats*\(\leq\)q'.v.t.Therefores*\(\leq\)d.
This completes the proof of the invariant.We have shown thats*is a prefix ofallassignments made to server variables after iterationn*.
Therefore,s*is a prefix of all consensus values
reached in iterations after iterationn*.The Distributed Algorithm is CorrectWe showedthe equivalence of the
distributed and nondeterministic sequential algorithm: For every
computation \(c\) of the distributed algorithm there exists a
computation \(c'\) of
the nondeterministic sequential algorithm -- with appropriate choices of
nondeterministic selections -- such that for all serversqthe sequence of values ofq.vis the same
in \(c\) and \(c'\).Therefore properties of the sequence of values ofq.vfor
each serverqin the sequential algorithm are also
properties in the distributed algorithm.K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

Agents that propose extensions to consensus chains are calledclients.
  Copies of the chain are stored at agents calledservers.
Clients determine consensus chains and propose extensions to the
chains.
The system determines a consensus among proposals for
extended chains.Before giving the specification of consensus we review the concept of
prefix of a sequence.Notation: prefix of a sequenceAprefixof a sequence S is an initial subsequence
  of S.
  For example, [A, B] is a prefix of [A, B, C] but [A, C] is not a
prefix of [A, B, C].The empty sequence is a prefix of all sequences.
A sequence is a prefix of itself.
We use the notation \(\leq\) for prefix, as in: 
[A, B] \(\leq \) [A, B, C]Specification of Consensus1. A consensus isn't changed.For all \(c\) and \(d\), if \(c\) is a consensus chain at any point in
a computation and \(d\) is a consensus chain at a later point in
the computation then 
\(c\) is a prefix of \(d\).ExampleFor example, a consensus chain [A, B] can be extended to form a
consensus chain [A, B, C], but
[A, B] cannot be extended to form [A, C, B].
Chains [A, B] and [A, B, C] can both be consensus chains in the same
computation; however,
[A, B] and [A, C, B] cannot both be consensus chains in the same
computation.2. Every consensus chain is proposed by a
clientFor all consensus chains \(c\) in a computation:
There exists a client that proposed \(c\) in the computation.This part of the specification merely says that consensus chains cannot be
arbitrary.
A client uses the algorithm to determine chain \(c\) that is a
consensus of the servers and then proposes an extension to \(c\). The
algorithm then reaches a consensus about extensions to \(c\) proposed
by clients; the extensions must be proposed by clients.
Later, we define this part of the specification in terms of algorithm
variables.A Nondeterministic Sequential Consensus AlgorithmWe will develop a consensus algorithm for the system described in the
previous pageSerializable Computations in Faulty Systems.A system has a sets of agents called servers and clients.
Each serverqhas a local variableq.v.
A client executes a transaction in which the client sends requests to
each serverqto readq.v;
waits to receive replies; then sends write requests to servers.
Read the previous page for details.A transaction is executed by exactly one client.
Clientpsends read requests and gets replies from a setRof servers.
IfRhas at leastMelements thenpsends write requests to all servers asking each serverqto assignp.f()toq.v.Some write requests may be lost. Let W be the set of servers that
receive the write requests.The design of the distributed consensus algorithm is based on this keyresult.For each agent, the sequence of values of the agent in the distributed
algorithmdescribed in the
previous pageis the same as in an equivalent nondeterministic
sequential algorithm.We now prove properties of the values of servers in the equivalent sequential
algorithm; these properties also hold for the distributed algorithm.
The equivalent sequential program from theprevious pageis given below.
The selections ofdelta, p, R, Win each iteration of the
algorithm are nondeterministic.Equivalent Nondeterministic Sequential Algorithmt = 0
while True:
   select positive delta, select client p
   t = t + delta
   # p executes transaction with epoch t
   p.t = t
   p.copy = {}
   # Read step
   select R
   for q in R: p.copy[q] = q.v
   # Write step
   if len(p.copy) >= M:
     select W
     for q in W: q.v = p.f()The ProblemThe problem is to define consensus in terms of variables of the sequential
algorithm and specify parametersMandfof the algorithm so that the
specification for consensus is satisfied.How Should You Solve The Problem?When is Consensus Reached?Let's assume that a consensus is reached when a sufficiently large
number of servers write the same value in a transaction.
Equivalently, consensus is reached when the setWof
servers that write the same 
value in an iteration of the sequential algorithm is large enough, and
a consensus is the value written by all servers inW.How large is enough?We could require thatWbe the set ofallservers, but
then consensus is reached in an iteration only if write requests reach
all servers.
The smaller the size ofWrequired for consensus, the
greater the likelihood of consensus being reached.A reasonable design choice is thatWis a majority of
servers. 
LetNbe the number of servers.A consensus is reached at the end of an iteration in whichWis a majority of servers.Tagging assignments with the epochs in which
they were assignedWe need some mechanism to determine which servers were written in the
same iteration, and which were written earlier.
So, we make variableq.vhave two fields,(q.v.s, q.v.t)whereq.v.sis the chain atq.vandq.v.tis the epoch in whichq.v.swas assigned its value.
In the sequential algorithm we replace the assignment stepq.v = p.f()byq.v.s, q.v.t = p.f(), p.tWe make the same change the distributed algorithm, replacing the
following assignment in the algorithm for clientpv, t = p.f(), p.tbyv.s, v.t, t = p.f(), p.t, p.tA consensus is reached when the setWof servers in an
iteration is a majority.
For allqin this majority,q.v.tis the
epoch of the iteration.
Therefore, consensus is determined as follows:Consensus: Majority of Servers Agree on a Consensus Values*is a consensus chain in a computation of seq if
during the computation
 there exists a majorityW*of servers and an
epocht*where:
For allqinW*:(q.v.s = s*)and(q.v.t = t*)SpecifyingM: The Number of
Replies Required to ProceedA client needs to receive at leastMcopies of chains
to proceed to the write step.
What is a reasonable value ofM?The smaller the value ofMthe greater the likelihood of
proceeding to the write step.
Again, a reasonable design choice is to require that the client
receive replies from a majority of servers.
If that choice doesn't work we will try a larger value ofM.A client must receive
replies from a majority of servers to proceed to the write step.Specifyingp.f: The Values to be
WrittenConsider a single iteration ofseq.
IfRis not a majority then the iteration terminates
without modifying server values.
Now we consider the case in whichRis a majority.
We design the function so that it returns an extension to a consensus
chain.
Consider two cases.Case 1: All replies are identical.Let each elementq.vinRbes*,
t*.
BecauseRis a majority of servers, from the definition
of consensus it follows thats*is a consensus chain.A client proposes an extension of a consensus chain,
Sop.freturns the consensus,s*, appended
withp.h()wherep.h()is the extension proposed byp.Case 2: Not all replies are identical.p.freturnsq*.v.swhereq*.vis the reply with the largest epoch.The algorithm is given below.def p.f():
  values_read = list(p.copy.values())
  if all(values_read[0] == v for v in values_read):
     # All elements of values_read are identical
     # consensus is any element of values_read.
     consensus = values_read[0].s
     return consensus.append(h())
  else:
     # Not all elements of values_read are identical
     # Compute max_v, the element with largest v.t
     max_v = values_read[0]
     for v in values_read:
        if v.t > max_v.t:  max_v = v
     return max_v.sCorrectness of the Nondeterministic Sequential AlgorithmObservationPart 2 of the specification --
every consensus chain is proposed by a client
--
follows directly from the algorithm
  because the only assignment toq.v.sisp.f().Next,  we prove part 1. We will show that ifcis a
consensus chain after an iteration anddis a
consensus chain after a later iteration thencis a
prefix ofd.Theorem: The Sequential Algorithm is CorrectLets*be a consensus at some point in a computation of
the sequential program.
Then, there exists a majority W* of servers and an epoch t* where: For all q in W*:(q.v.s = s*) and (q.v.t = t*)Let the iteration that executes the transaction with epocht*be then*-th iteration.
Because each assignment toq.vincreasesq.v.twe get the following.Equation 1At the end of then-th iteration,
for allqinW*:For alln\(\geq\)n*: \(\quad\)q.v.t\(\geq\)t*For alln\(<\)n*: \(\quad\)q.v.t\(<\)t*Next we will prove thats*is a prefix of any assignment
to any server variable in any 
iteration after iterationn*.
This can be written as: at all points in the computation, :At all points in the computation, for all serversq:(q.v.t\(<\)t*) \(\; \vee \;\)
(s*\(\leq\)q.v.s)Proof of the InvariantBase Case:n-th Iteration wheren\(\leq\)n*The condition trivially holds for iterationnwheren\(<\)n*because, from equation 1:q.v.t\(<\)t*.Next, consider the case wheren\(=\)n*.
Ifq.vis modified in the iteration thenq.v.s\(=\)s*.
Ifq.vis not modified in the iteration thenq.v.t\(<\)t*.Induction StepAssume that the condition holds before iterationnforn\(>\)n*and 
show that it holds after the iteration.From equation 1 and the induction hypothesis, the following holds
before iterationn.For allqinW*:(q.v.t\(\geq\)t*)
\(\; \wedge \; \)
(s*\(\leq\)q.v.s)In any iteration, server variables are modified only ifRis a majority of servers.
Any two majorities have at least one element in common.
So there is an
element that is in bothRandW*.
Let this element beq*.
Becauseq*is an element ofW*:(q*.v.t\(\geq\)t*)
\(\: \wedge \;\)
(s*\(\leq\)q*.v.s)This gives us the following:Equation 2There exists aq*inRsuch that:(q*.v.t\(\geq\)t*)
\(\: \wedge \;\)
(s*\(\leq\)q*.v.s)Letdbe the value returned by functionp.f.
Consider the statements inp.fand two cases.Case 1: All valuesq.vforqinRare identical.In this case, from equation 2:For allqinR:s*\(\leq\)q.v.s.In functionp.f,consensusis assignedq.v.sfor someqinR.Therefore,s*\(\leq\)consensus.The function returnsd = consensus.append(h()).Therefores*\(\leq\)d.Case 2: Not all valuesq.vforqinRare identical.The function returnsq'.v.twhereq'is inRand:For allqinR: \(\quad\)q'.v.t\(\geq\)q.v.tFrom equation 2:q'.v.t\(\geq\)q*.v.t\(\geq\)t*From the induction hypothesis, andq'.v.t\(\geq\)t*, it follows thats*\(\leq\)q'.v.t.Therefores*\(\leq\)d.
This completes the proof of the invariant.We have shown thats*is a prefix ofallassignments made to server variables after iterationn*.
Therefore,s*is a prefix of all consensus values
reached in iterations after iterationn*.The Distributed Algorithm is CorrectWe showedthe equivalence of the
distributed and nondeterministic sequential algorithm: For every
computation \(c\) of the distributed algorithm there exists a
computation \(c'\) of
the nondeterministic sequential algorithm -- with appropriate choices of
nondeterministic selections -- such that for all serversqthe sequence of values ofq.vis the same
in \(c\) and \(c'\).Therefore properties of the sequence of values ofq.vfor
each serverqin the sequential algorithm are also
properties in the distributed algorithm.K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

Before giving the specification of consensus we review the concept of
prefix of a sequence.Notation: prefix of a sequenceAprefixof a sequence S is an initial subsequence
  of S.
  For example, [A, B] is a prefix of [A, B, C] but [A, C] is not a
prefix of [A, B, C].The empty sequence is a prefix of all sequences.
A sequence is a prefix of itself.
We use the notation \(\leq\) for prefix, as in: 
[A, B] \(\leq \) [A, B, C]Specification of Consensus1. A consensus isn't changed.For all \(c\) and \(d\), if \(c\) is a consensus chain at any point in
a computation and \(d\) is a consensus chain at a later point in
the computation then 
\(c\) is a prefix of \(d\).ExampleFor example, a consensus chain [A, B] can be extended to form a
consensus chain [A, B, C], but
[A, B] cannot be extended to form [A, C, B].
Chains [A, B] and [A, B, C] can both be consensus chains in the same
computation; however,
[A, B] and [A, C, B] cannot both be consensus chains in the same
computation.2. Every consensus chain is proposed by a
clientFor all consensus chains \(c\) in a computation:
There exists a client that proposed \(c\) in the computation.This part of the specification merely says that consensus chains cannot be
arbitrary.
A client uses the algorithm to determine chain \(c\) that is a
consensus of the servers and then proposes an extension to \(c\). The
algorithm then reaches a consensus about extensions to \(c\) proposed
by clients; the extensions must be proposed by clients.
Later, we define this part of the specification in terms of algorithm
variables.A Nondeterministic Sequential Consensus AlgorithmWe will develop a consensus algorithm for the system described in the
previous pageSerializable Computations in Faulty Systems.A system has a sets of agents called servers and clients.
Each serverqhas a local variableq.v.
A client executes a transaction in which the client sends requests to
each serverqto readq.v;
waits to receive replies; then sends write requests to servers.
Read the previous page for details.A transaction is executed by exactly one client.
Clientpsends read requests and gets replies from a setRof servers.
IfRhas at leastMelements thenpsends write requests to all servers asking each serverqto assignp.f()toq.v.Some write requests may be lost. Let W be the set of servers that
receive the write requests.The design of the distributed consensus algorithm is based on this keyresult.For each agent, the sequence of values of the agent in the distributed
algorithmdescribed in the
previous pageis the same as in an equivalent nondeterministic
sequential algorithm.We now prove properties of the values of servers in the equivalent sequential
algorithm; these properties also hold for the distributed algorithm.
The equivalent sequential program from theprevious pageis given below.
The selections ofdelta, p, R, Win each iteration of the
algorithm are nondeterministic.Equivalent Nondeterministic Sequential Algorithmt = 0
while True:
   select positive delta, select client p
   t = t + delta
   # p executes transaction with epoch t
   p.t = t
   p.copy = {}
   # Read step
   select R
   for q in R: p.copy[q] = q.v
   # Write step
   if len(p.copy) >= M:
     select W
     for q in W: q.v = p.f()The ProblemThe problem is to define consensus in terms of variables of the sequential
algorithm and specify parametersMandfof the algorithm so that the
specification for consensus is satisfied.How Should You Solve The Problem?When is Consensus Reached?Let's assume that a consensus is reached when a sufficiently large
number of servers write the same value in a transaction.
Equivalently, consensus is reached when the setWof
servers that write the same 
value in an iteration of the sequential algorithm is large enough, and
a consensus is the value written by all servers inW.How large is enough?We could require thatWbe the set ofallservers, but
then consensus is reached in an iteration only if write requests reach
all servers.
The smaller the size ofWrequired for consensus, the
greater the likelihood of consensus being reached.A reasonable design choice is thatWis a majority of
servers. 
LetNbe the number of servers.A consensus is reached at the end of an iteration in whichWis a majority of servers.Tagging assignments with the epochs in which
they were assignedWe need some mechanism to determine which servers were written in the
same iteration, and which were written earlier.
So, we make variableq.vhave two fields,(q.v.s, q.v.t)whereq.v.sis the chain atq.vandq.v.tis the epoch in whichq.v.swas assigned its value.
In the sequential algorithm we replace the assignment stepq.v = p.f()byq.v.s, q.v.t = p.f(), p.tWe make the same change the distributed algorithm, replacing the
following assignment in the algorithm for clientpv, t = p.f(), p.tbyv.s, v.t, t = p.f(), p.t, p.tA consensus is reached when the setWof servers in an
iteration is a majority.
For allqin this majority,q.v.tis the
epoch of the iteration.
Therefore, consensus is determined as follows:Consensus: Majority of Servers Agree on a Consensus Values*is a consensus chain in a computation of seq if
during the computation
 there exists a majorityW*of servers and an
epocht*where:
For allqinW*:(q.v.s = s*)and(q.v.t = t*)SpecifyingM: The Number of
Replies Required to ProceedA client needs to receive at leastMcopies of chains
to proceed to the write step.
What is a reasonable value ofM?The smaller the value ofMthe greater the likelihood of
proceeding to the write step.
Again, a reasonable design choice is to require that the client
receive replies from a majority of servers.
If that choice doesn't work we will try a larger value ofM.A client must receive
replies from a majority of servers to proceed to the write step.Specifyingp.f: The Values to be
WrittenConsider a single iteration ofseq.
IfRis not a majority then the iteration terminates
without modifying server values.
Now we consider the case in whichRis a majority.
We design the function so that it returns an extension to a consensus
chain.
Consider two cases.Case 1: All replies are identical.Let each elementq.vinRbes*,
t*.
BecauseRis a majority of servers, from the definition
of consensus it follows thats*is a consensus chain.A client proposes an extension of a consensus chain,
Sop.freturns the consensus,s*, appended
withp.h()wherep.h()is the extension proposed byp.Case 2: Not all replies are identical.p.freturnsq*.v.swhereq*.vis the reply with the largest epoch.The algorithm is given below.def p.f():
  values_read = list(p.copy.values())
  if all(values_read[0] == v for v in values_read):
     # All elements of values_read are identical
     # consensus is any element of values_read.
     consensus = values_read[0].s
     return consensus.append(h())
  else:
     # Not all elements of values_read are identical
     # Compute max_v, the element with largest v.t
     max_v = values_read[0]
     for v in values_read:
        if v.t > max_v.t:  max_v = v
     return max_v.sCorrectness of the Nondeterministic Sequential AlgorithmObservationPart 2 of the specification --
every consensus chain is proposed by a client
--
follows directly from the algorithm
  because the only assignment toq.v.sisp.f().Next,  we prove part 1. We will show that ifcis a
consensus chain after an iteration anddis a
consensus chain after a later iteration thencis a
prefix ofd.Theorem: The Sequential Algorithm is CorrectLets*be a consensus at some point in a computation of
the sequential program.
Then, there exists a majority W* of servers and an epoch t* where: For all q in W*:(q.v.s = s*) and (q.v.t = t*)Let the iteration that executes the transaction with epocht*be then*-th iteration.
Because each assignment toq.vincreasesq.v.twe get the following.Equation 1At the end of then-th iteration,
for allqinW*:For alln\(\geq\)n*: \(\quad\)q.v.t\(\geq\)t*For alln\(<\)n*: \(\quad\)q.v.t\(<\)t*Next we will prove thats*is a prefix of any assignment
to any server variable in any 
iteration after iterationn*.
This can be written as: at all points in the computation, :At all points in the computation, for all serversq:(q.v.t\(<\)t*) \(\; \vee \;\)
(s*\(\leq\)q.v.s)Proof of the InvariantBase Case:n-th Iteration wheren\(\leq\)n*The condition trivially holds for iterationnwheren\(<\)n*because, from equation 1:q.v.t\(<\)t*.Next, consider the case wheren\(=\)n*.
Ifq.vis modified in the iteration thenq.v.s\(=\)s*.
Ifq.vis not modified in the iteration thenq.v.t\(<\)t*.Induction StepAssume that the condition holds before iterationnforn\(>\)n*and 
show that it holds after the iteration.From equation 1 and the induction hypothesis, the following holds
before iterationn.For allqinW*:(q.v.t\(\geq\)t*)
\(\; \wedge \; \)
(s*\(\leq\)q.v.s)In any iteration, server variables are modified only ifRis a majority of servers.
Any two majorities have at least one element in common.
So there is an
element that is in bothRandW*.
Let this element beq*.
Becauseq*is an element ofW*:(q*.v.t\(\geq\)t*)
\(\: \wedge \;\)
(s*\(\leq\)q*.v.s)This gives us the following:Equation 2There exists aq*inRsuch that:(q*.v.t\(\geq\)t*)
\(\: \wedge \;\)
(s*\(\leq\)q*.v.s)Letdbe the value returned by functionp.f.
Consider the statements inp.fand two cases.Case 1: All valuesq.vforqinRare identical.In this case, from equation 2:For allqinR:s*\(\leq\)q.v.s.In functionp.f,consensusis assignedq.v.sfor someqinR.Therefore,s*\(\leq\)consensus.The function returnsd = consensus.append(h()).Therefores*\(\leq\)d.Case 2: Not all valuesq.vforqinRare identical.The function returnsq'.v.twhereq'is inRand:For allqinR: \(\quad\)q'.v.t\(\geq\)q.v.tFrom equation 2:q'.v.t\(\geq\)q*.v.t\(\geq\)t*From the induction hypothesis, andq'.v.t\(\geq\)t*, it follows thats*\(\leq\)q'.v.t.Therefores*\(\leq\)d.
This completes the proof of the invariant.We have shown thats*is a prefix ofallassignments made to server variables after iterationn*.
Therefore,s*is a prefix of all consensus values
reached in iterations after iterationn*.The Distributed Algorithm is CorrectWe showedthe equivalence of the
distributed and nondeterministic sequential algorithm: For every
computation \(c\) of the distributed algorithm there exists a
computation \(c'\) of
the nondeterministic sequential algorithm -- with appropriate choices of
nondeterministic selections -- such that for all serversqthe sequence of values ofq.vis the same
in \(c\) and \(c'\).Therefore properties of the sequence of values ofq.vfor
each serverqin the sequential algorithm are also
properties in the distributed algorithm.K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

Notation: prefix of a sequence

The empty sequence is a prefix of all sequences.
A sequence is a prefix of itself.
We use the notation \(\leq\) for prefix, as in: 
[A, B] \(\leq \) [A, B, C]Specification of Consensus1. A consensus isn't changed.For all \(c\) and \(d\), if \(c\) is a consensus chain at any point in
a computation and \(d\) is a consensus chain at a later point in
the computation then 
\(c\) is a prefix of \(d\).ExampleFor example, a consensus chain [A, B] can be extended to form a
consensus chain [A, B, C], but
[A, B] cannot be extended to form [A, C, B].
Chains [A, B] and [A, B, C] can both be consensus chains in the same
computation; however,
[A, B] and [A, C, B] cannot both be consensus chains in the same
computation.2. Every consensus chain is proposed by a
clientFor all consensus chains \(c\) in a computation:
There exists a client that proposed \(c\) in the computation.This part of the specification merely says that consensus chains cannot be
arbitrary.
A client uses the algorithm to determine chain \(c\) that is a
consensus of the servers and then proposes an extension to \(c\). The
algorithm then reaches a consensus about extensions to \(c\) proposed
by clients; the extensions must be proposed by clients.
Later, we define this part of the specification in terms of algorithm
variables.A Nondeterministic Sequential Consensus AlgorithmWe will develop a consensus algorithm for the system described in the
previous pageSerializable Computations in Faulty Systems.A system has a sets of agents called servers and clients.
Each serverqhas a local variableq.v.
A client executes a transaction in which the client sends requests to
each serverqto readq.v;
waits to receive replies; then sends write requests to servers.
Read the previous page for details.A transaction is executed by exactly one client.
Clientpsends read requests and gets replies from a setRof servers.
IfRhas at leastMelements thenpsends write requests to all servers asking each serverqto assignp.f()toq.v.Some write requests may be lost. Let W be the set of servers that
receive the write requests.The design of the distributed consensus algorithm is based on this keyresult.For each agent, the sequence of values of the agent in the distributed
algorithmdescribed in the
previous pageis the same as in an equivalent nondeterministic
sequential algorithm.We now prove properties of the values of servers in the equivalent sequential
algorithm; these properties also hold for the distributed algorithm.
The equivalent sequential program from theprevious pageis given below.
The selections ofdelta, p, R, Win each iteration of the
algorithm are nondeterministic.Equivalent Nondeterministic Sequential Algorithmt = 0
while True:
   select positive delta, select client p
   t = t + delta
   # p executes transaction with epoch t
   p.t = t
   p.copy = {}
   # Read step
   select R
   for q in R: p.copy[q] = q.v
   # Write step
   if len(p.copy) >= M:
     select W
     for q in W: q.v = p.f()The ProblemThe problem is to define consensus in terms of variables of the sequential
algorithm and specify parametersMandfof the algorithm so that the
specification for consensus is satisfied.How Should You Solve The Problem?When is Consensus Reached?Let's assume that a consensus is reached when a sufficiently large
number of servers write the same value in a transaction.
Equivalently, consensus is reached when the setWof
servers that write the same 
value in an iteration of the sequential algorithm is large enough, and
a consensus is the value written by all servers inW.How large is enough?We could require thatWbe the set ofallservers, but
then consensus is reached in an iteration only if write requests reach
all servers.
The smaller the size ofWrequired for consensus, the
greater the likelihood of consensus being reached.A reasonable design choice is thatWis a majority of
servers. 
LetNbe the number of servers.A consensus is reached at the end of an iteration in whichWis a majority of servers.Tagging assignments with the epochs in which
they were assignedWe need some mechanism to determine which servers were written in the
same iteration, and which were written earlier.
So, we make variableq.vhave two fields,(q.v.s, q.v.t)whereq.v.sis the chain atq.vandq.v.tis the epoch in whichq.v.swas assigned its value.
In the sequential algorithm we replace the assignment stepq.v = p.f()byq.v.s, q.v.t = p.f(), p.tWe make the same change the distributed algorithm, replacing the
following assignment in the algorithm for clientpv, t = p.f(), p.tbyv.s, v.t, t = p.f(), p.t, p.tA consensus is reached when the setWof servers in an
iteration is a majority.
For allqin this majority,q.v.tis the
epoch of the iteration.
Therefore, consensus is determined as follows:Consensus: Majority of Servers Agree on a Consensus Values*is a consensus chain in a computation of seq if
during the computation
 there exists a majorityW*of servers and an
epocht*where:
For allqinW*:(q.v.s = s*)and(q.v.t = t*)SpecifyingM: The Number of
Replies Required to ProceedA client needs to receive at leastMcopies of chains
to proceed to the write step.
What is a reasonable value ofM?The smaller the value ofMthe greater the likelihood of
proceeding to the write step.
Again, a reasonable design choice is to require that the client
receive replies from a majority of servers.
If that choice doesn't work we will try a larger value ofM.A client must receive
replies from a majority of servers to proceed to the write step.Specifyingp.f: The Values to be
WrittenConsider a single iteration ofseq.
IfRis not a majority then the iteration terminates
without modifying server values.
Now we consider the case in whichRis a majority.
We design the function so that it returns an extension to a consensus
chain.
Consider two cases.Case 1: All replies are identical.Let each elementq.vinRbes*,
t*.
BecauseRis a majority of servers, from the definition
of consensus it follows thats*is a consensus chain.A client proposes an extension of a consensus chain,
Sop.freturns the consensus,s*, appended
withp.h()wherep.h()is the extension proposed byp.Case 2: Not all replies are identical.p.freturnsq*.v.swhereq*.vis the reply with the largest epoch.The algorithm is given below.def p.f():
  values_read = list(p.copy.values())
  if all(values_read[0] == v for v in values_read):
     # All elements of values_read are identical
     # consensus is any element of values_read.
     consensus = values_read[0].s
     return consensus.append(h())
  else:
     # Not all elements of values_read are identical
     # Compute max_v, the element with largest v.t
     max_v = values_read[0]
     for v in values_read:
        if v.t > max_v.t:  max_v = v
     return max_v.sCorrectness of the Nondeterministic Sequential AlgorithmObservationPart 2 of the specification --
every consensus chain is proposed by a client
--
follows directly from the algorithm
  because the only assignment toq.v.sisp.f().Next,  we prove part 1. We will show that ifcis a
consensus chain after an iteration anddis a
consensus chain after a later iteration thencis a
prefix ofd.Theorem: The Sequential Algorithm is CorrectLets*be a consensus at some point in a computation of
the sequential program.
Then, there exists a majority W* of servers and an epoch t* where: For all q in W*:(q.v.s = s*) and (q.v.t = t*)Let the iteration that executes the transaction with epocht*be then*-th iteration.
Because each assignment toq.vincreasesq.v.twe get the following.Equation 1At the end of then-th iteration,
for allqinW*:For alln\(\geq\)n*: \(\quad\)q.v.t\(\geq\)t*For alln\(<\)n*: \(\quad\)q.v.t\(<\)t*Next we will prove thats*is a prefix of any assignment
to any server variable in any 
iteration after iterationn*.
This can be written as: at all points in the computation, :At all points in the computation, for all serversq:(q.v.t\(<\)t*) \(\; \vee \;\)
(s*\(\leq\)q.v.s)Proof of the InvariantBase Case:n-th Iteration wheren\(\leq\)n*The condition trivially holds for iterationnwheren\(<\)n*because, from equation 1:q.v.t\(<\)t*.Next, consider the case wheren\(=\)n*.
Ifq.vis modified in the iteration thenq.v.s\(=\)s*.
Ifq.vis not modified in the iteration thenq.v.t\(<\)t*.Induction StepAssume that the condition holds before iterationnforn\(>\)n*and 
show that it holds after the iteration.From equation 1 and the induction hypothesis, the following holds
before iterationn.For allqinW*:(q.v.t\(\geq\)t*)
\(\; \wedge \; \)
(s*\(\leq\)q.v.s)In any iteration, server variables are modified only ifRis a majority of servers.
Any two majorities have at least one element in common.
So there is an
element that is in bothRandW*.
Let this element beq*.
Becauseq*is an element ofW*:(q*.v.t\(\geq\)t*)
\(\: \wedge \;\)
(s*\(\leq\)q*.v.s)This gives us the following:Equation 2There exists aq*inRsuch that:(q*.v.t\(\geq\)t*)
\(\: \wedge \;\)
(s*\(\leq\)q*.v.s)Letdbe the value returned by functionp.f.
Consider the statements inp.fand two cases.Case 1: All valuesq.vforqinRare identical.In this case, from equation 2:For allqinR:s*\(\leq\)q.v.s.In functionp.f,consensusis assignedq.v.sfor someqinR.Therefore,s*\(\leq\)consensus.The function returnsd = consensus.append(h()).Therefores*\(\leq\)d.Case 2: Not all valuesq.vforqinRare identical.The function returnsq'.v.twhereq'is inRand:For allqinR: \(\quad\)q'.v.t\(\geq\)q.v.tFrom equation 2:q'.v.t\(\geq\)q*.v.t\(\geq\)t*From the induction hypothesis, andq'.v.t\(\geq\)t*, it follows thats*\(\leq\)q'.v.t.Therefores*\(\leq\)d.
This completes the proof of the invariant.We have shown thats*is a prefix ofallassignments made to server variables after iterationn*.
Therefore,s*is a prefix of all consensus values
reached in iterations after iterationn*.The Distributed Algorithm is CorrectWe showedthe equivalence of the
distributed and nondeterministic sequential algorithm: For every
computation \(c\) of the distributed algorithm there exists a
computation \(c'\) of
the nondeterministic sequential algorithm -- with appropriate choices of
nondeterministic selections -- such that for all serversqthe sequence of values ofq.vis the same
in \(c\) and \(c'\).Therefore properties of the sequence of values ofq.vfor
each serverqin the sequential algorithm are also
properties in the distributed algorithm.K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

Example

This part of the specification merely says that consensus chains cannot be
arbitrary.
A client uses the algorithm to determine chain \(c\) that is a
consensus of the servers and then proposes an extension to \(c\). The
algorithm then reaches a consensus about extensions to \(c\) proposed
by clients; the extensions must be proposed by clients.
Later, we define this part of the specification in terms of algorithm
variables.A Nondeterministic Sequential Consensus AlgorithmWe will develop a consensus algorithm for the system described in the
previous pageSerializable Computations in Faulty Systems.A system has a sets of agents called servers and clients.
Each serverqhas a local variableq.v.
A client executes a transaction in which the client sends requests to
each serverqto readq.v;
waits to receive replies; then sends write requests to servers.
Read the previous page for details.A transaction is executed by exactly one client.
Clientpsends read requests and gets replies from a setRof servers.
IfRhas at leastMelements thenpsends write requests to all servers asking each serverqto assignp.f()toq.v.Some write requests may be lost. Let W be the set of servers that
receive the write requests.The design of the distributed consensus algorithm is based on this keyresult.For each agent, the sequence of values of the agent in the distributed
algorithmdescribed in the
previous pageis the same as in an equivalent nondeterministic
sequential algorithm.We now prove properties of the values of servers in the equivalent sequential
algorithm; these properties also hold for the distributed algorithm.
The equivalent sequential program from theprevious pageis given below.
The selections ofdelta, p, R, Win each iteration of the
algorithm are nondeterministic.Equivalent Nondeterministic Sequential Algorithmt = 0
while True:
   select positive delta, select client p
   t = t + delta
   # p executes transaction with epoch t
   p.t = t
   p.copy = {}
   # Read step
   select R
   for q in R: p.copy[q] = q.v
   # Write step
   if len(p.copy) >= M:
     select W
     for q in W: q.v = p.f()The ProblemThe problem is to define consensus in terms of variables of the sequential
algorithm and specify parametersMandfof the algorithm so that the
specification for consensus is satisfied.How Should You Solve The Problem?When is Consensus Reached?Let's assume that a consensus is reached when a sufficiently large
number of servers write the same value in a transaction.
Equivalently, consensus is reached when the setWof
servers that write the same 
value in an iteration of the sequential algorithm is large enough, and
a consensus is the value written by all servers inW.How large is enough?We could require thatWbe the set ofallservers, but
then consensus is reached in an iteration only if write requests reach
all servers.
The smaller the size ofWrequired for consensus, the
greater the likelihood of consensus being reached.A reasonable design choice is thatWis a majority of
servers. 
LetNbe the number of servers.A consensus is reached at the end of an iteration in whichWis a majority of servers.Tagging assignments with the epochs in which
they were assignedWe need some mechanism to determine which servers were written in the
same iteration, and which were written earlier.
So, we make variableq.vhave two fields,(q.v.s, q.v.t)whereq.v.sis the chain atq.vandq.v.tis the epoch in whichq.v.swas assigned its value.
In the sequential algorithm we replace the assignment stepq.v = p.f()byq.v.s, q.v.t = p.f(), p.tWe make the same change the distributed algorithm, replacing the
following assignment in the algorithm for clientpv, t = p.f(), p.tbyv.s, v.t, t = p.f(), p.t, p.tA consensus is reached when the setWof servers in an
iteration is a majority.
For allqin this majority,q.v.tis the
epoch of the iteration.
Therefore, consensus is determined as follows:Consensus: Majority of Servers Agree on a Consensus Values*is a consensus chain in a computation of seq if
during the computation
 there exists a majorityW*of servers and an
epocht*where:
For allqinW*:(q.v.s = s*)and(q.v.t = t*)SpecifyingM: The Number of
Replies Required to ProceedA client needs to receive at leastMcopies of chains
to proceed to the write step.
What is a reasonable value ofM?The smaller the value ofMthe greater the likelihood of
proceeding to the write step.
Again, a reasonable design choice is to require that the client
receive replies from a majority of servers.
If that choice doesn't work we will try a larger value ofM.A client must receive
replies from a majority of servers to proceed to the write step.Specifyingp.f: The Values to be
WrittenConsider a single iteration ofseq.
IfRis not a majority then the iteration terminates
without modifying server values.
Now we consider the case in whichRis a majority.
We design the function so that it returns an extension to a consensus
chain.
Consider two cases.Case 1: All replies are identical.Let each elementq.vinRbes*,
t*.
BecauseRis a majority of servers, from the definition
of consensus it follows thats*is a consensus chain.A client proposes an extension of a consensus chain,
Sop.freturns the consensus,s*, appended
withp.h()wherep.h()is the extension proposed byp.Case 2: Not all replies are identical.p.freturnsq*.v.swhereq*.vis the reply with the largest epoch.The algorithm is given below.def p.f():
  values_read = list(p.copy.values())
  if all(values_read[0] == v for v in values_read):
     # All elements of values_read are identical
     # consensus is any element of values_read.
     consensus = values_read[0].s
     return consensus.append(h())
  else:
     # Not all elements of values_read are identical
     # Compute max_v, the element with largest v.t
     max_v = values_read[0]
     for v in values_read:
        if v.t > max_v.t:  max_v = v
     return max_v.sCorrectness of the Nondeterministic Sequential AlgorithmObservationPart 2 of the specification --
every consensus chain is proposed by a client
--
follows directly from the algorithm
  because the only assignment toq.v.sisp.f().Next,  we prove part 1. We will show that ifcis a
consensus chain after an iteration anddis a
consensus chain after a later iteration thencis a
prefix ofd.Theorem: The Sequential Algorithm is CorrectLets*be a consensus at some point in a computation of
the sequential program.
Then, there exists a majority W* of servers and an epoch t* where: For all q in W*:(q.v.s = s*) and (q.v.t = t*)Let the iteration that executes the transaction with epocht*be then*-th iteration.
Because each assignment toq.vincreasesq.v.twe get the following.Equation 1At the end of then-th iteration,
for allqinW*:For alln\(\geq\)n*: \(\quad\)q.v.t\(\geq\)t*For alln\(<\)n*: \(\quad\)q.v.t\(<\)t*Next we will prove thats*is a prefix of any assignment
to any server variable in any 
iteration after iterationn*.
This can be written as: at all points in the computation, :At all points in the computation, for all serversq:(q.v.t\(<\)t*) \(\; \vee \;\)
(s*\(\leq\)q.v.s)Proof of the InvariantBase Case:n-th Iteration wheren\(\leq\)n*The condition trivially holds for iterationnwheren\(<\)n*because, from equation 1:q.v.t\(<\)t*.Next, consider the case wheren\(=\)n*.
Ifq.vis modified in the iteration thenq.v.s\(=\)s*.
Ifq.vis not modified in the iteration thenq.v.t\(<\)t*.Induction StepAssume that the condition holds before iterationnforn\(>\)n*and 
show that it holds after the iteration.From equation 1 and the induction hypothesis, the following holds
before iterationn.For allqinW*:(q.v.t\(\geq\)t*)
\(\; \wedge \; \)
(s*\(\leq\)q.v.s)In any iteration, server variables are modified only ifRis a majority of servers.
Any two majorities have at least one element in common.
So there is an
element that is in bothRandW*.
Let this element beq*.
Becauseq*is an element ofW*:(q*.v.t\(\geq\)t*)
\(\: \wedge \;\)
(s*\(\leq\)q*.v.s)This gives us the following:Equation 2There exists aq*inRsuch that:(q*.v.t\(\geq\)t*)
\(\: \wedge \;\)
(s*\(\leq\)q*.v.s)Letdbe the value returned by functionp.f.
Consider the statements inp.fand two cases.Case 1: All valuesq.vforqinRare identical.In this case, from equation 2:For allqinR:s*\(\leq\)q.v.s.In functionp.f,consensusis assignedq.v.sfor someqinR.Therefore,s*\(\leq\)consensus.The function returnsd = consensus.append(h()).Therefores*\(\leq\)d.Case 2: Not all valuesq.vforqinRare identical.The function returnsq'.v.twhereq'is inRand:For allqinR: \(\quad\)q'.v.t\(\geq\)q.v.tFrom equation 2:q'.v.t\(\geq\)q*.v.t\(\geq\)t*From the induction hypothesis, andq'.v.t\(\geq\)t*, it follows thats*\(\leq\)q'.v.t.Therefores*\(\leq\)d.
This completes the proof of the invariant.We have shown thats*is a prefix ofallassignments made to server variables after iterationn*.
Therefore,s*is a prefix of all consensus values
reached in iterations after iterationn*.The Distributed Algorithm is CorrectWe showedthe equivalence of the
distributed and nondeterministic sequential algorithm: For every
computation \(c\) of the distributed algorithm there exists a
computation \(c'\) of
the nondeterministic sequential algorithm -- with appropriate choices of
nondeterministic selections -- such that for all serversqthe sequence of values ofq.vis the same
in \(c\) and \(c'\).Therefore properties of the sequence of values ofq.vfor
each serverqin the sequential algorithm are also
properties in the distributed algorithm.K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

A transaction is executed by exactly one client.
Clientpsends read requests and gets replies from a setRof servers.
IfRhas at leastMelements thenpsends write requests to all servers asking each serverqto assignp.f()toq.v.Some write requests may be lost. Let W be the set of servers that
receive the write requests.The design of the distributed consensus algorithm is based on this keyresult.For each agent, the sequence of values of the agent in the distributed
algorithmdescribed in the
previous pageis the same as in an equivalent nondeterministic
sequential algorithm.We now prove properties of the values of servers in the equivalent sequential
algorithm; these properties also hold for the distributed algorithm.
The equivalent sequential program from theprevious pageis given below.
The selections ofdelta, p, R, Win each iteration of the
algorithm are nondeterministic.Equivalent Nondeterministic Sequential Algorithmt = 0
while True:
   select positive delta, select client p
   t = t + delta
   # p executes transaction with epoch t
   p.t = t
   p.copy = {}
   # Read step
   select R
   for q in R: p.copy[q] = q.v
   # Write step
   if len(p.copy) >= M:
     select W
     for q in W: q.v = p.f()The ProblemThe problem is to define consensus in terms of variables of the sequential
algorithm and specify parametersMandfof the algorithm so that the
specification for consensus is satisfied.How Should You Solve The Problem?When is Consensus Reached?Let's assume that a consensus is reached when a sufficiently large
number of servers write the same value in a transaction.
Equivalently, consensus is reached when the setWof
servers that write the same 
value in an iteration of the sequential algorithm is large enough, and
a consensus is the value written by all servers inW.How large is enough?We could require thatWbe the set ofallservers, but
then consensus is reached in an iteration only if write requests reach
all servers.
The smaller the size ofWrequired for consensus, the
greater the likelihood of consensus being reached.A reasonable design choice is thatWis a majority of
servers. 
LetNbe the number of servers.A consensus is reached at the end of an iteration in whichWis a majority of servers.Tagging assignments with the epochs in which
they were assignedWe need some mechanism to determine which servers were written in the
same iteration, and which were written earlier.
So, we make variableq.vhave two fields,(q.v.s, q.v.t)whereq.v.sis the chain atq.vandq.v.tis the epoch in whichq.v.swas assigned its value.
In the sequential algorithm we replace the assignment stepq.v = p.f()byq.v.s, q.v.t = p.f(), p.tWe make the same change the distributed algorithm, replacing the
following assignment in the algorithm for clientpv, t = p.f(), p.tbyv.s, v.t, t = p.f(), p.t, p.tA consensus is reached when the setWof servers in an
iteration is a majority.
For allqin this majority,q.v.tis the
epoch of the iteration.
Therefore, consensus is determined as follows:Consensus: Majority of Servers Agree on a Consensus Values*is a consensus chain in a computation of seq if
during the computation
 there exists a majorityW*of servers and an
epocht*where:
For allqinW*:(q.v.s = s*)and(q.v.t = t*)SpecifyingM: The Number of
Replies Required to ProceedA client needs to receive at leastMcopies of chains
to proceed to the write step.
What is a reasonable value ofM?The smaller the value ofMthe greater the likelihood of
proceeding to the write step.
Again, a reasonable design choice is to require that the client
receive replies from a majority of servers.
If that choice doesn't work we will try a larger value ofM.A client must receive
replies from a majority of servers to proceed to the write step.Specifyingp.f: The Values to be
WrittenConsider a single iteration ofseq.
IfRis not a majority then the iteration terminates
without modifying server values.
Now we consider the case in whichRis a majority.
We design the function so that it returns an extension to a consensus
chain.
Consider two cases.Case 1: All replies are identical.Let each elementq.vinRbes*,
t*.
BecauseRis a majority of servers, from the definition
of consensus it follows thats*is a consensus chain.A client proposes an extension of a consensus chain,
Sop.freturns the consensus,s*, appended
withp.h()wherep.h()is the extension proposed byp.Case 2: Not all replies are identical.p.freturnsq*.v.swhereq*.vis the reply with the largest epoch.The algorithm is given below.def p.f():
  values_read = list(p.copy.values())
  if all(values_read[0] == v for v in values_read):
     # All elements of values_read are identical
     # consensus is any element of values_read.
     consensus = values_read[0].s
     return consensus.append(h())
  else:
     # Not all elements of values_read are identical
     # Compute max_v, the element with largest v.t
     max_v = values_read[0]
     for v in values_read:
        if v.t > max_v.t:  max_v = v
     return max_v.sCorrectness of the Nondeterministic Sequential AlgorithmObservationPart 2 of the specification --
every consensus chain is proposed by a client
--
follows directly from the algorithm
  because the only assignment toq.v.sisp.f().Next,  we prove part 1. We will show that ifcis a
consensus chain after an iteration anddis a
consensus chain after a later iteration thencis a
prefix ofd.Theorem: The Sequential Algorithm is CorrectLets*be a consensus at some point in a computation of
the sequential program.
Then, there exists a majority W* of servers and an epoch t* where: For all q in W*:(q.v.s = s*) and (q.v.t = t*)Let the iteration that executes the transaction with epocht*be then*-th iteration.
Because each assignment toq.vincreasesq.v.twe get the following.Equation 1At the end of then-th iteration,
for allqinW*:For alln\(\geq\)n*: \(\quad\)q.v.t\(\geq\)t*For alln\(<\)n*: \(\quad\)q.v.t\(<\)t*Next we will prove thats*is a prefix of any assignment
to any server variable in any 
iteration after iterationn*.
This can be written as: at all points in the computation, :At all points in the computation, for all serversq:(q.v.t\(<\)t*) \(\; \vee \;\)
(s*\(\leq\)q.v.s)Proof of the InvariantBase Case:n-th Iteration wheren\(\leq\)n*The condition trivially holds for iterationnwheren\(<\)n*because, from equation 1:q.v.t\(<\)t*.Next, consider the case wheren\(=\)n*.
Ifq.vis modified in the iteration thenq.v.s\(=\)s*.
Ifq.vis not modified in the iteration thenq.v.t\(<\)t*.Induction StepAssume that the condition holds before iterationnforn\(>\)n*and 
show that it holds after the iteration.From equation 1 and the induction hypothesis, the following holds
before iterationn.For allqinW*:(q.v.t\(\geq\)t*)
\(\; \wedge \; \)
(s*\(\leq\)q.v.s)In any iteration, server variables are modified only ifRis a majority of servers.
Any two majorities have at least one element in common.
So there is an
element that is in bothRandW*.
Let this element beq*.
Becauseq*is an element ofW*:(q*.v.t\(\geq\)t*)
\(\: \wedge \;\)
(s*\(\leq\)q*.v.s)This gives us the following:Equation 2There exists aq*inRsuch that:(q*.v.t\(\geq\)t*)
\(\: \wedge \;\)
(s*\(\leq\)q*.v.s)Letdbe the value returned by functionp.f.
Consider the statements inp.fand two cases.Case 1: All valuesq.vforqinRare identical.In this case, from equation 2:For allqinR:s*\(\leq\)q.v.s.In functionp.f,consensusis assignedq.v.sfor someqinR.Therefore,s*\(\leq\)consensus.The function returnsd = consensus.append(h()).Therefores*\(\leq\)d.Case 2: Not all valuesq.vforqinRare identical.The function returnsq'.v.twhereq'is inRand:For allqinR: \(\quad\)q'.v.t\(\geq\)q.v.tFrom equation 2:q'.v.t\(\geq\)q*.v.t\(\geq\)t*From the induction hypothesis, andq'.v.t\(\geq\)t*, it follows thats*\(\leq\)q'.v.t.Therefores*\(\leq\)d.
This completes the proof of the invariant.We have shown thats*is a prefix ofallassignments made to server variables after iterationn*.
Therefore,s*is a prefix of all consensus values
reached in iterations after iterationn*.The Distributed Algorithm is CorrectWe showedthe equivalence of the
distributed and nondeterministic sequential algorithm: For every
computation \(c\) of the distributed algorithm there exists a
computation \(c'\) of
the nondeterministic sequential algorithm -- with appropriate choices of
nondeterministic selections -- such that for all serversqthe sequence of values ofq.vis the same
in \(c\) and \(c'\).Therefore properties of the sequence of values ofq.vfor
each serverqin the sequential algorithm are also
properties in the distributed algorithm.K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

Some write requests may be lost. Let W be the set of servers that
receive the write requests.The design of the distributed consensus algorithm is based on this keyresult.For each agent, the sequence of values of the agent in the distributed
algorithmdescribed in the
previous pageis the same as in an equivalent nondeterministic
sequential algorithm.We now prove properties of the values of servers in the equivalent sequential
algorithm; these properties also hold for the distributed algorithm.
The equivalent sequential program from theprevious pageis given below.
The selections ofdelta, p, R, Win each iteration of the
algorithm are nondeterministic.Equivalent Nondeterministic Sequential Algorithmt = 0
while True:
   select positive delta, select client p
   t = t + delta
   # p executes transaction with epoch t
   p.t = t
   p.copy = {}
   # Read step
   select R
   for q in R: p.copy[q] = q.v
   # Write step
   if len(p.copy) >= M:
     select W
     for q in W: q.v = p.f()The ProblemThe problem is to define consensus in terms of variables of the sequential
algorithm and specify parametersMandfof the algorithm so that the
specification for consensus is satisfied.How Should You Solve The Problem?When is Consensus Reached?Let's assume that a consensus is reached when a sufficiently large
number of servers write the same value in a transaction.
Equivalently, consensus is reached when the setWof
servers that write the same 
value in an iteration of the sequential algorithm is large enough, and
a consensus is the value written by all servers inW.How large is enough?We could require thatWbe the set ofallservers, but
then consensus is reached in an iteration only if write requests reach
all servers.
The smaller the size ofWrequired for consensus, the
greater the likelihood of consensus being reached.A reasonable design choice is thatWis a majority of
servers. 
LetNbe the number of servers.A consensus is reached at the end of an iteration in whichWis a majority of servers.Tagging assignments with the epochs in which
they were assignedWe need some mechanism to determine which servers were written in the
same iteration, and which were written earlier.
So, we make variableq.vhave two fields,(q.v.s, q.v.t)whereq.v.sis the chain atq.vandq.v.tis the epoch in whichq.v.swas assigned its value.
In the sequential algorithm we replace the assignment stepq.v = p.f()byq.v.s, q.v.t = p.f(), p.tWe make the same change the distributed algorithm, replacing the
following assignment in the algorithm for clientpv, t = p.f(), p.tbyv.s, v.t, t = p.f(), p.t, p.tA consensus is reached when the setWof servers in an
iteration is a majority.
For allqin this majority,q.v.tis the
epoch of the iteration.
Therefore, consensus is determined as follows:Consensus: Majority of Servers Agree on a Consensus Values*is a consensus chain in a computation of seq if
during the computation
 there exists a majorityW*of servers and an
epocht*where:
For allqinW*:(q.v.s = s*)and(q.v.t = t*)SpecifyingM: The Number of
Replies Required to ProceedA client needs to receive at leastMcopies of chains
to proceed to the write step.
What is a reasonable value ofM?The smaller the value ofMthe greater the likelihood of
proceeding to the write step.
Again, a reasonable design choice is to require that the client
receive replies from a majority of servers.
If that choice doesn't work we will try a larger value ofM.A client must receive
replies from a majority of servers to proceed to the write step.Specifyingp.f: The Values to be
WrittenConsider a single iteration ofseq.
IfRis not a majority then the iteration terminates
without modifying server values.
Now we consider the case in whichRis a majority.
We design the function so that it returns an extension to a consensus
chain.
Consider two cases.Case 1: All replies are identical.Let each elementq.vinRbes*,
t*.
BecauseRis a majority of servers, from the definition
of consensus it follows thats*is a consensus chain.A client proposes an extension of a consensus chain,
Sop.freturns the consensus,s*, appended
withp.h()wherep.h()is the extension proposed byp.Case 2: Not all replies are identical.p.freturnsq*.v.swhereq*.vis the reply with the largest epoch.The algorithm is given below.def p.f():
  values_read = list(p.copy.values())
  if all(values_read[0] == v for v in values_read):
     # All elements of values_read are identical
     # consensus is any element of values_read.
     consensus = values_read[0].s
     return consensus.append(h())
  else:
     # Not all elements of values_read are identical
     # Compute max_v, the element with largest v.t
     max_v = values_read[0]
     for v in values_read:
        if v.t > max_v.t:  max_v = v
     return max_v.sCorrectness of the Nondeterministic Sequential AlgorithmObservationPart 2 of the specification --
every consensus chain is proposed by a client
--
follows directly from the algorithm
  because the only assignment toq.v.sisp.f().Next,  we prove part 1. We will show that ifcis a
consensus chain after an iteration anddis a
consensus chain after a later iteration thencis a
prefix ofd.Theorem: The Sequential Algorithm is CorrectLets*be a consensus at some point in a computation of
the sequential program.
Then, there exists a majority W* of servers and an epoch t* where: For all q in W*:(q.v.s = s*) and (q.v.t = t*)Let the iteration that executes the transaction with epocht*be then*-th iteration.
Because each assignment toq.vincreasesq.v.twe get the following.Equation 1At the end of then-th iteration,
for allqinW*:For alln\(\geq\)n*: \(\quad\)q.v.t\(\geq\)t*For alln\(<\)n*: \(\quad\)q.v.t\(<\)t*Next we will prove thats*is a prefix of any assignment
to any server variable in any 
iteration after iterationn*.
This can be written as: at all points in the computation, :At all points in the computation, for all serversq:(q.v.t\(<\)t*) \(\; \vee \;\)
(s*\(\leq\)q.v.s)Proof of the InvariantBase Case:n-th Iteration wheren\(\leq\)n*The condition trivially holds for iterationnwheren\(<\)n*because, from equation 1:q.v.t\(<\)t*.Next, consider the case wheren\(=\)n*.
Ifq.vis modified in the iteration thenq.v.s\(=\)s*.
Ifq.vis not modified in the iteration thenq.v.t\(<\)t*.Induction StepAssume that the condition holds before iterationnforn\(>\)n*and 
show that it holds after the iteration.From equation 1 and the induction hypothesis, the following holds
before iterationn.For allqinW*:(q.v.t\(\geq\)t*)
\(\; \wedge \; \)
(s*\(\leq\)q.v.s)In any iteration, server variables are modified only ifRis a majority of servers.
Any two majorities have at least one element in common.
So there is an
element that is in bothRandW*.
Let this element beq*.
Becauseq*is an element ofW*:(q*.v.t\(\geq\)t*)
\(\: \wedge \;\)
(s*\(\leq\)q*.v.s)This gives us the following:Equation 2There exists aq*inRsuch that:(q*.v.t\(\geq\)t*)
\(\: \wedge \;\)
(s*\(\leq\)q*.v.s)Letdbe the value returned by functionp.f.
Consider the statements inp.fand two cases.Case 1: All valuesq.vforqinRare identical.In this case, from equation 2:For allqinR:s*\(\leq\)q.v.s.In functionp.f,consensusis assignedq.v.sfor someqinR.Therefore,s*\(\leq\)consensus.The function returnsd = consensus.append(h()).Therefores*\(\leq\)d.Case 2: Not all valuesq.vforqinRare identical.The function returnsq'.v.twhereq'is inRand:For allqinR: \(\quad\)q'.v.t\(\geq\)q.v.tFrom equation 2:q'.v.t\(\geq\)q*.v.t\(\geq\)t*From the induction hypothesis, andq'.v.t\(\geq\)t*, it follows thats*\(\leq\)q'.v.t.Therefores*\(\leq\)d.
This completes the proof of the invariant.We have shown thats*is a prefix ofallassignments made to server variables after iterationn*.
Therefore,s*is a prefix of all consensus values
reached in iterations after iterationn*.The Distributed Algorithm is CorrectWe showedthe equivalence of the
distributed and nondeterministic sequential algorithm: For every
computation \(c\) of the distributed algorithm there exists a
computation \(c'\) of
the nondeterministic sequential algorithm -- with appropriate choices of
nondeterministic selections -- such that for all serversqthe sequence of values ofq.vis the same
in \(c\) and \(c'\).Therefore properties of the sequence of values ofq.vfor
each serverqin the sequential algorithm are also
properties in the distributed algorithm.K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

The design of the distributed consensus algorithm is based on this keyresult.For each agent, the sequence of values of the agent in the distributed
algorithmdescribed in the
previous pageis the same as in an equivalent nondeterministic
sequential algorithm.We now prove properties of the values of servers in the equivalent sequential
algorithm; these properties also hold for the distributed algorithm.
The equivalent sequential program from theprevious pageis given below.
The selections ofdelta, p, R, Win each iteration of the
algorithm are nondeterministic.Equivalent Nondeterministic Sequential Algorithmt = 0
while True:
   select positive delta, select client p
   t = t + delta
   # p executes transaction with epoch t
   p.t = t
   p.copy = {}
   # Read step
   select R
   for q in R: p.copy[q] = q.v
   # Write step
   if len(p.copy) >= M:
     select W
     for q in W: q.v = p.f()The ProblemThe problem is to define consensus in terms of variables of the sequential
algorithm and specify parametersMandfof the algorithm so that the
specification for consensus is satisfied.How Should You Solve The Problem?When is Consensus Reached?Let's assume that a consensus is reached when a sufficiently large
number of servers write the same value in a transaction.
Equivalently, consensus is reached when the setWof
servers that write the same 
value in an iteration of the sequential algorithm is large enough, and
a consensus is the value written by all servers inW.How large is enough?We could require thatWbe the set ofallservers, but
then consensus is reached in an iteration only if write requests reach
all servers.
The smaller the size ofWrequired for consensus, the
greater the likelihood of consensus being reached.A reasonable design choice is thatWis a majority of
servers. 
LetNbe the number of servers.A consensus is reached at the end of an iteration in whichWis a majority of servers.Tagging assignments with the epochs in which
they were assignedWe need some mechanism to determine which servers were written in the
same iteration, and which were written earlier.
So, we make variableq.vhave two fields,(q.v.s, q.v.t)whereq.v.sis the chain atq.vandq.v.tis the epoch in whichq.v.swas assigned its value.
In the sequential algorithm we replace the assignment stepq.v = p.f()byq.v.s, q.v.t = p.f(), p.tWe make the same change the distributed algorithm, replacing the
following assignment in the algorithm for clientpv, t = p.f(), p.tbyv.s, v.t, t = p.f(), p.t, p.tA consensus is reached when the setWof servers in an
iteration is a majority.
For allqin this majority,q.v.tis the
epoch of the iteration.
Therefore, consensus is determined as follows:Consensus: Majority of Servers Agree on a Consensus Values*is a consensus chain in a computation of seq if
during the computation
 there exists a majorityW*of servers and an
epocht*where:
For allqinW*:(q.v.s = s*)and(q.v.t = t*)SpecifyingM: The Number of
Replies Required to ProceedA client needs to receive at leastMcopies of chains
to proceed to the write step.
What is a reasonable value ofM?The smaller the value ofMthe greater the likelihood of
proceeding to the write step.
Again, a reasonable design choice is to require that the client
receive replies from a majority of servers.
If that choice doesn't work we will try a larger value ofM.A client must receive
replies from a majority of servers to proceed to the write step.Specifyingp.f: The Values to be
WrittenConsider a single iteration ofseq.
IfRis not a majority then the iteration terminates
without modifying server values.
Now we consider the case in whichRis a majority.
We design the function so that it returns an extension to a consensus
chain.
Consider two cases.Case 1: All replies are identical.Let each elementq.vinRbes*,
t*.
BecauseRis a majority of servers, from the definition
of consensus it follows thats*is a consensus chain.A client proposes an extension of a consensus chain,
Sop.freturns the consensus,s*, appended
withp.h()wherep.h()is the extension proposed byp.Case 2: Not all replies are identical.p.freturnsq*.v.swhereq*.vis the reply with the largest epoch.The algorithm is given below.def p.f():
  values_read = list(p.copy.values())
  if all(values_read[0] == v for v in values_read):
     # All elements of values_read are identical
     # consensus is any element of values_read.
     consensus = values_read[0].s
     return consensus.append(h())
  else:
     # Not all elements of values_read are identical
     # Compute max_v, the element with largest v.t
     max_v = values_read[0]
     for v in values_read:
        if v.t > max_v.t:  max_v = v
     return max_v.sCorrectness of the Nondeterministic Sequential AlgorithmObservationPart 2 of the specification --
every consensus chain is proposed by a client
--
follows directly from the algorithm
  because the only assignment toq.v.sisp.f().Next,  we prove part 1. We will show that ifcis a
consensus chain after an iteration anddis a
consensus chain after a later iteration thencis a
prefix ofd.Theorem: The Sequential Algorithm is CorrectLets*be a consensus at some point in a computation of
the sequential program.
Then, there exists a majority W* of servers and an epoch t* where: For all q in W*:(q.v.s = s*) and (q.v.t = t*)Let the iteration that executes the transaction with epocht*be then*-th iteration.
Because each assignment toq.vincreasesq.v.twe get the following.Equation 1At the end of then-th iteration,
for allqinW*:For alln\(\geq\)n*: \(\quad\)q.v.t\(\geq\)t*For alln\(<\)n*: \(\quad\)q.v.t\(<\)t*Next we will prove thats*is a prefix of any assignment
to any server variable in any 
iteration after iterationn*.
This can be written as: at all points in the computation, :At all points in the computation, for all serversq:(q.v.t\(<\)t*) \(\; \vee \;\)
(s*\(\leq\)q.v.s)Proof of the InvariantBase Case:n-th Iteration wheren\(\leq\)n*The condition trivially holds for iterationnwheren\(<\)n*because, from equation 1:q.v.t\(<\)t*.Next, consider the case wheren\(=\)n*.
Ifq.vis modified in the iteration thenq.v.s\(=\)s*.
Ifq.vis not modified in the iteration thenq.v.t\(<\)t*.Induction StepAssume that the condition holds before iterationnforn\(>\)n*and 
show that it holds after the iteration.From equation 1 and the induction hypothesis, the following holds
before iterationn.For allqinW*:(q.v.t\(\geq\)t*)
\(\; \wedge \; \)
(s*\(\leq\)q.v.s)In any iteration, server variables are modified only ifRis a majority of servers.
Any two majorities have at least one element in common.
So there is an
element that is in bothRandW*.
Let this element beq*.
Becauseq*is an element ofW*:(q*.v.t\(\geq\)t*)
\(\: \wedge \;\)
(s*\(\leq\)q*.v.s)This gives us the following:Equation 2There exists aq*inRsuch that:(q*.v.t\(\geq\)t*)
\(\: \wedge \;\)
(s*\(\leq\)q*.v.s)Letdbe the value returned by functionp.f.
Consider the statements inp.fand two cases.Case 1: All valuesq.vforqinRare identical.In this case, from equation 2:For allqinR:s*\(\leq\)q.v.s.In functionp.f,consensusis assignedq.v.sfor someqinR.Therefore,s*\(\leq\)consensus.The function returnsd = consensus.append(h()).Therefores*\(\leq\)d.Case 2: Not all valuesq.vforqinRare identical.The function returnsq'.v.twhereq'is inRand:For allqinR: \(\quad\)q'.v.t\(\geq\)q.v.tFrom equation 2:q'.v.t\(\geq\)q*.v.t\(\geq\)t*From the induction hypothesis, andq'.v.t\(\geq\)t*, it follows thats*\(\leq\)q'.v.t.Therefores*\(\leq\)d.
This completes the proof of the invariant.We have shown thats*is a prefix ofallassignments made to server variables after iterationn*.
Therefore,s*is a prefix of all consensus values
reached in iterations after iterationn*.The Distributed Algorithm is CorrectWe showedthe equivalence of the
distributed and nondeterministic sequential algorithm: For every
computation \(c\) of the distributed algorithm there exists a
computation \(c'\) of
the nondeterministic sequential algorithm -- with appropriate choices of
nondeterministic selections -- such that for all serversqthe sequence of values ofq.vis the same
in \(c\) and \(c'\).Therefore properties of the sequence of values ofq.vfor
each serverqin the sequential algorithm are also
properties in the distributed algorithm.K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

For each agent, the sequence of values of the agent in the distributed
algorithmdescribed in the
previous pageis the same as in an equivalent nondeterministic
sequential algorithm.

How large is enough?We could require thatWbe the set ofallservers, but
then consensus is reached in an iteration only if write requests reach
all servers.
The smaller the size ofWrequired for consensus, the
greater the likelihood of consensus being reached.A reasonable design choice is thatWis a majority of
servers. 
LetNbe the number of servers.A consensus is reached at the end of an iteration in whichWis a majority of servers.Tagging assignments with the epochs in which
they were assignedWe need some mechanism to determine which servers were written in the
same iteration, and which were written earlier.
So, we make variableq.vhave two fields,(q.v.s, q.v.t)whereq.v.sis the chain atq.vandq.v.tis the epoch in whichq.v.swas assigned its value.
In the sequential algorithm we replace the assignment stepq.v = p.f()byq.v.s, q.v.t = p.f(), p.tWe make the same change the distributed algorithm, replacing the
following assignment in the algorithm for clientpv, t = p.f(), p.tbyv.s, v.t, t = p.f(), p.t, p.tA consensus is reached when the setWof servers in an
iteration is a majority.
For allqin this majority,q.v.tis the
epoch of the iteration.
Therefore, consensus is determined as follows:Consensus: Majority of Servers Agree on a Consensus Values*is a consensus chain in a computation of seq if
during the computation
 there exists a majorityW*of servers and an
epocht*where:
For allqinW*:(q.v.s = s*)and(q.v.t = t*)SpecifyingM: The Number of
Replies Required to ProceedA client needs to receive at leastMcopies of chains
to proceed to the write step.
What is a reasonable value ofM?The smaller the value ofMthe greater the likelihood of
proceeding to the write step.
Again, a reasonable design choice is to require that the client
receive replies from a majority of servers.
If that choice doesn't work we will try a larger value ofM.A client must receive
replies from a majority of servers to proceed to the write step.Specifyingp.f: The Values to be
WrittenConsider a single iteration ofseq.
IfRis not a majority then the iteration terminates
without modifying server values.
Now we consider the case in whichRis a majority.
We design the function so that it returns an extension to a consensus
chain.
Consider two cases.Case 1: All replies are identical.Let each elementq.vinRbes*,
t*.
BecauseRis a majority of servers, from the definition
of consensus it follows thats*is a consensus chain.A client proposes an extension of a consensus chain,
Sop.freturns the consensus,s*, appended
withp.h()wherep.h()is the extension proposed byp.Case 2: Not all replies are identical.p.freturnsq*.v.swhereq*.vis the reply with the largest epoch.The algorithm is given below.def p.f():
  values_read = list(p.copy.values())
  if all(values_read[0] == v for v in values_read):
     # All elements of values_read are identical
     # consensus is any element of values_read.
     consensus = values_read[0].s
     return consensus.append(h())
  else:
     # Not all elements of values_read are identical
     # Compute max_v, the element with largest v.t
     max_v = values_read[0]
     for v in values_read:
        if v.t > max_v.t:  max_v = v
     return max_v.sCorrectness of the Nondeterministic Sequential AlgorithmObservationPart 2 of the specification --
every consensus chain is proposed by a client
--
follows directly from the algorithm
  because the only assignment toq.v.sisp.f().Next,  we prove part 1. We will show that ifcis a
consensus chain after an iteration anddis a
consensus chain after a later iteration thencis a
prefix ofd.Theorem: The Sequential Algorithm is CorrectLets*be a consensus at some point in a computation of
the sequential program.
Then, there exists a majority W* of servers and an epoch t* where: For all q in W*:(q.v.s = s*) and (q.v.t = t*)Let the iteration that executes the transaction with epocht*be then*-th iteration.
Because each assignment toq.vincreasesq.v.twe get the following.Equation 1At the end of then-th iteration,
for allqinW*:For alln\(\geq\)n*: \(\quad\)q.v.t\(\geq\)t*For alln\(<\)n*: \(\quad\)q.v.t\(<\)t*Next we will prove thats*is a prefix of any assignment
to any server variable in any 
iteration after iterationn*.
This can be written as: at all points in the computation, :At all points in the computation, for all serversq:(q.v.t\(<\)t*) \(\; \vee \;\)
(s*\(\leq\)q.v.s)Proof of the InvariantBase Case:n-th Iteration wheren\(\leq\)n*The condition trivially holds for iterationnwheren\(<\)n*because, from equation 1:q.v.t\(<\)t*.Next, consider the case wheren\(=\)n*.
Ifq.vis modified in the iteration thenq.v.s\(=\)s*.
Ifq.vis not modified in the iteration thenq.v.t\(<\)t*.Induction StepAssume that the condition holds before iterationnforn\(>\)n*and 
show that it holds after the iteration.From equation 1 and the induction hypothesis, the following holds
before iterationn.For allqinW*:(q.v.t\(\geq\)t*)
\(\; \wedge \; \)
(s*\(\leq\)q.v.s)In any iteration, server variables are modified only ifRis a majority of servers.
Any two majorities have at least one element in common.
So there is an
element that is in bothRandW*.
Let this element beq*.
Becauseq*is an element ofW*:(q*.v.t\(\geq\)t*)
\(\: \wedge \;\)
(s*\(\leq\)q*.v.s)This gives us the following:Equation 2There exists aq*inRsuch that:(q*.v.t\(\geq\)t*)
\(\: \wedge \;\)
(s*\(\leq\)q*.v.s)Letdbe the value returned by functionp.f.
Consider the statements inp.fand two cases.Case 1: All valuesq.vforqinRare identical.In this case, from equation 2:For allqinR:s*\(\leq\)q.v.s.In functionp.f,consensusis assignedq.v.sfor someqinR.Therefore,s*\(\leq\)consensus.The function returnsd = consensus.append(h()).Therefores*\(\leq\)d.Case 2: Not all valuesq.vforqinRare identical.The function returnsq'.v.twhereq'is inRand:For allqinR: \(\quad\)q'.v.t\(\geq\)q.v.tFrom equation 2:q'.v.t\(\geq\)q*.v.t\(\geq\)t*From the induction hypothesis, andq'.v.t\(\geq\)t*, it follows thats*\(\leq\)q'.v.t.Therefores*\(\leq\)d.
This completes the proof of the invariant.We have shown thats*is a prefix ofallassignments made to server variables after iterationn*.
Therefore,s*is a prefix of all consensus values
reached in iterations after iterationn*.The Distributed Algorithm is CorrectWe showedthe equivalence of the
distributed and nondeterministic sequential algorithm: For every
computation \(c\) of the distributed algorithm there exists a
computation \(c'\) of
the nondeterministic sequential algorithm -- with appropriate choices of
nondeterministic selections -- such that for all serversqthe sequence of values ofq.vis the same
in \(c\) and \(c'\).Therefore properties of the sequence of values ofq.vfor
each serverqin the sequential algorithm are also
properties in the distributed algorithm.K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

We could require thatWbe the set ofallservers, but
then consensus is reached in an iteration only if write requests reach
all servers.
The smaller the size ofWrequired for consensus, the
greater the likelihood of consensus being reached.A reasonable design choice is thatWis a majority of
servers. 
LetNbe the number of servers.A consensus is reached at the end of an iteration in whichWis a majority of servers.Tagging assignments with the epochs in which
they were assignedWe need some mechanism to determine which servers were written in the
same iteration, and which were written earlier.
So, we make variableq.vhave two fields,(q.v.s, q.v.t)whereq.v.sis the chain atq.vandq.v.tis the epoch in whichq.v.swas assigned its value.
In the sequential algorithm we replace the assignment stepq.v = p.f()byq.v.s, q.v.t = p.f(), p.tWe make the same change the distributed algorithm, replacing the
following assignment in the algorithm for clientpv, t = p.f(), p.tbyv.s, v.t, t = p.f(), p.t, p.tA consensus is reached when the setWof servers in an
iteration is a majority.
For allqin this majority,q.v.tis the
epoch of the iteration.
Therefore, consensus is determined as follows:Consensus: Majority of Servers Agree on a Consensus Values*is a consensus chain in a computation of seq if
during the computation
 there exists a majorityW*of servers and an
epocht*where:
For allqinW*:(q.v.s = s*)and(q.v.t = t*)SpecifyingM: The Number of
Replies Required to ProceedA client needs to receive at leastMcopies of chains
to proceed to the write step.
What is a reasonable value ofM?The smaller the value ofMthe greater the likelihood of
proceeding to the write step.
Again, a reasonable design choice is to require that the client
receive replies from a majority of servers.
If that choice doesn't work we will try a larger value ofM.A client must receive
replies from a majority of servers to proceed to the write step.Specifyingp.f: The Values to be
WrittenConsider a single iteration ofseq.
IfRis not a majority then the iteration terminates
without modifying server values.
Now we consider the case in whichRis a majority.
We design the function so that it returns an extension to a consensus
chain.
Consider two cases.Case 1: All replies are identical.Let each elementq.vinRbes*,
t*.
BecauseRis a majority of servers, from the definition
of consensus it follows thats*is a consensus chain.A client proposes an extension of a consensus chain,
Sop.freturns the consensus,s*, appended
withp.h()wherep.h()is the extension proposed byp.Case 2: Not all replies are identical.p.freturnsq*.v.swhereq*.vis the reply with the largest epoch.The algorithm is given below.def p.f():
  values_read = list(p.copy.values())
  if all(values_read[0] == v for v in values_read):
     # All elements of values_read are identical
     # consensus is any element of values_read.
     consensus = values_read[0].s
     return consensus.append(h())
  else:
     # Not all elements of values_read are identical
     # Compute max_v, the element with largest v.t
     max_v = values_read[0]
     for v in values_read:
        if v.t > max_v.t:  max_v = v
     return max_v.sCorrectness of the Nondeterministic Sequential AlgorithmObservationPart 2 of the specification --
every consensus chain is proposed by a client
--
follows directly from the algorithm
  because the only assignment toq.v.sisp.f().Next,  we prove part 1. We will show that ifcis a
consensus chain after an iteration anddis a
consensus chain after a later iteration thencis a
prefix ofd.Theorem: The Sequential Algorithm is CorrectLets*be a consensus at some point in a computation of
the sequential program.
Then, there exists a majority W* of servers and an epoch t* where: For all q in W*:(q.v.s = s*) and (q.v.t = t*)Let the iteration that executes the transaction with epocht*be then*-th iteration.
Because each assignment toq.vincreasesq.v.twe get the following.Equation 1At the end of then-th iteration,
for allqinW*:For alln\(\geq\)n*: \(\quad\)q.v.t\(\geq\)t*For alln\(<\)n*: \(\quad\)q.v.t\(<\)t*Next we will prove thats*is a prefix of any assignment
to any server variable in any 
iteration after iterationn*.
This can be written as: at all points in the computation, :At all points in the computation, for all serversq:(q.v.t\(<\)t*) \(\; \vee \;\)
(s*\(\leq\)q.v.s)Proof of the InvariantBase Case:n-th Iteration wheren\(\leq\)n*The condition trivially holds for iterationnwheren\(<\)n*because, from equation 1:q.v.t\(<\)t*.Next, consider the case wheren\(=\)n*.
Ifq.vis modified in the iteration thenq.v.s\(=\)s*.
Ifq.vis not modified in the iteration thenq.v.t\(<\)t*.Induction StepAssume that the condition holds before iterationnforn\(>\)n*and 
show that it holds after the iteration.From equation 1 and the induction hypothesis, the following holds
before iterationn.For allqinW*:(q.v.t\(\geq\)t*)
\(\; \wedge \; \)
(s*\(\leq\)q.v.s)In any iteration, server variables are modified only ifRis a majority of servers.
Any two majorities have at least one element in common.
So there is an
element that is in bothRandW*.
Let this element beq*.
Becauseq*is an element ofW*:(q*.v.t\(\geq\)t*)
\(\: \wedge \;\)
(s*\(\leq\)q*.v.s)This gives us the following:Equation 2There exists aq*inRsuch that:(q*.v.t\(\geq\)t*)
\(\: \wedge \;\)
(s*\(\leq\)q*.v.s)Letdbe the value returned by functionp.f.
Consider the statements inp.fand two cases.Case 1: All valuesq.vforqinRare identical.In this case, from equation 2:For allqinR:s*\(\leq\)q.v.s.In functionp.f,consensusis assignedq.v.sfor someqinR.Therefore,s*\(\leq\)consensus.The function returnsd = consensus.append(h()).Therefores*\(\leq\)d.Case 2: Not all valuesq.vforqinRare identical.The function returnsq'.v.twhereq'is inRand:For allqinR: \(\quad\)q'.v.t\(\geq\)q.v.tFrom equation 2:q'.v.t\(\geq\)q*.v.t\(\geq\)t*From the induction hypothesis, andq'.v.t\(\geq\)t*, it follows thats*\(\leq\)q'.v.t.Therefores*\(\leq\)d.
This completes the proof of the invariant.We have shown thats*is a prefix ofallassignments made to server variables after iterationn*.
Therefore,s*is a prefix of all consensus values
reached in iterations after iterationn*.The Distributed Algorithm is CorrectWe showedthe equivalence of the
distributed and nondeterministic sequential algorithm: For every
computation \(c\) of the distributed algorithm there exists a
computation \(c'\) of
the nondeterministic sequential algorithm -- with appropriate choices of
nondeterministic selections -- such that for all serversqthe sequence of values ofq.vis the same
in \(c\) and \(c'\).Therefore properties of the sequence of values ofq.vfor
each serverqin the sequential algorithm are also
properties in the distributed algorithm.K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

A reasonable design choice is thatWis a majority of
servers. 
LetNbe the number of servers.A consensus is reached at the end of an iteration in whichWis a majority of servers.Tagging assignments with the epochs in which
they were assignedWe need some mechanism to determine which servers were written in the
same iteration, and which were written earlier.
So, we make variableq.vhave two fields,(q.v.s, q.v.t)whereq.v.sis the chain atq.vandq.v.tis the epoch in whichq.v.swas assigned its value.
In the sequential algorithm we replace the assignment stepq.v = p.f()byq.v.s, q.v.t = p.f(), p.tWe make the same change the distributed algorithm, replacing the
following assignment in the algorithm for clientpv, t = p.f(), p.tbyv.s, v.t, t = p.f(), p.t, p.tA consensus is reached when the setWof servers in an
iteration is a majority.
For allqin this majority,q.v.tis the
epoch of the iteration.
Therefore, consensus is determined as follows:Consensus: Majority of Servers Agree on a Consensus Values*is a consensus chain in a computation of seq if
during the computation
 there exists a majorityW*of servers and an
epocht*where:
For allqinW*:(q.v.s = s*)and(q.v.t = t*)SpecifyingM: The Number of
Replies Required to ProceedA client needs to receive at leastMcopies of chains
to proceed to the write step.
What is a reasonable value ofM?The smaller the value ofMthe greater the likelihood of
proceeding to the write step.
Again, a reasonable design choice is to require that the client
receive replies from a majority of servers.
If that choice doesn't work we will try a larger value ofM.A client must receive
replies from a majority of servers to proceed to the write step.Specifyingp.f: The Values to be
WrittenConsider a single iteration ofseq.
IfRis not a majority then the iteration terminates
without modifying server values.
Now we consider the case in whichRis a majority.
We design the function so that it returns an extension to a consensus
chain.
Consider two cases.Case 1: All replies are identical.Let each elementq.vinRbes*,
t*.
BecauseRis a majority of servers, from the definition
of consensus it follows thats*is a consensus chain.A client proposes an extension of a consensus chain,
Sop.freturns the consensus,s*, appended
withp.h()wherep.h()is the extension proposed byp.Case 2: Not all replies are identical.p.freturnsq*.v.swhereq*.vis the reply with the largest epoch.The algorithm is given below.def p.f():
  values_read = list(p.copy.values())
  if all(values_read[0] == v for v in values_read):
     # All elements of values_read are identical
     # consensus is any element of values_read.
     consensus = values_read[0].s
     return consensus.append(h())
  else:
     # Not all elements of values_read are identical
     # Compute max_v, the element with largest v.t
     max_v = values_read[0]
     for v in values_read:
        if v.t > max_v.t:  max_v = v
     return max_v.sCorrectness of the Nondeterministic Sequential AlgorithmObservationPart 2 of the specification --
every consensus chain is proposed by a client
--
follows directly from the algorithm
  because the only assignment toq.v.sisp.f().Next,  we prove part 1. We will show that ifcis a
consensus chain after an iteration anddis a
consensus chain after a later iteration thencis a
prefix ofd.Theorem: The Sequential Algorithm is CorrectLets*be a consensus at some point in a computation of
the sequential program.
Then, there exists a majority W* of servers and an epoch t* where: For all q in W*:(q.v.s = s*) and (q.v.t = t*)Let the iteration that executes the transaction with epocht*be then*-th iteration.
Because each assignment toq.vincreasesq.v.twe get the following.Equation 1At the end of then-th iteration,
for allqinW*:For alln\(\geq\)n*: \(\quad\)q.v.t\(\geq\)t*For alln\(<\)n*: \(\quad\)q.v.t\(<\)t*Next we will prove thats*is a prefix of any assignment
to any server variable in any 
iteration after iterationn*.
This can be written as: at all points in the computation, :At all points in the computation, for all serversq:(q.v.t\(<\)t*) \(\; \vee \;\)
(s*\(\leq\)q.v.s)Proof of the InvariantBase Case:n-th Iteration wheren\(\leq\)n*The condition trivially holds for iterationnwheren\(<\)n*because, from equation 1:q.v.t\(<\)t*.Next, consider the case wheren\(=\)n*.
Ifq.vis modified in the iteration thenq.v.s\(=\)s*.
Ifq.vis not modified in the iteration thenq.v.t\(<\)t*.Induction StepAssume that the condition holds before iterationnforn\(>\)n*and 
show that it holds after the iteration.From equation 1 and the induction hypothesis, the following holds
before iterationn.For allqinW*:(q.v.t\(\geq\)t*)
\(\; \wedge \; \)
(s*\(\leq\)q.v.s)In any iteration, server variables are modified only ifRis a majority of servers.
Any two majorities have at least one element in common.
So there is an
element that is in bothRandW*.
Let this element beq*.
Becauseq*is an element ofW*:(q*.v.t\(\geq\)t*)
\(\: \wedge \;\)
(s*\(\leq\)q*.v.s)This gives us the following:Equation 2There exists aq*inRsuch that:(q*.v.t\(\geq\)t*)
\(\: \wedge \;\)
(s*\(\leq\)q*.v.s)Letdbe the value returned by functionp.f.
Consider the statements inp.fand two cases.Case 1: All valuesq.vforqinRare identical.In this case, from equation 2:For allqinR:s*\(\leq\)q.v.s.In functionp.f,consensusis assignedq.v.sfor someqinR.Therefore,s*\(\leq\)consensus.The function returnsd = consensus.append(h()).Therefores*\(\leq\)d.Case 2: Not all valuesq.vforqinRare identical.The function returnsq'.v.twhereq'is inRand:For allqinR: \(\quad\)q'.v.t\(\geq\)q.v.tFrom equation 2:q'.v.t\(\geq\)q*.v.t\(\geq\)t*From the induction hypothesis, andq'.v.t\(\geq\)t*, it follows thats*\(\leq\)q'.v.t.Therefores*\(\leq\)d.
This completes the proof of the invariant.We have shown thats*is a prefix ofallassignments made to server variables after iterationn*.
Therefore,s*is a prefix of all consensus values
reached in iterations after iterationn*.The Distributed Algorithm is CorrectWe showedthe equivalence of the
distributed and nondeterministic sequential algorithm: For every
computation \(c\) of the distributed algorithm there exists a
computation \(c'\) of
the nondeterministic sequential algorithm -- with appropriate choices of
nondeterministic selections -- such that for all serversqthe sequence of values ofq.vis the same
in \(c\) and \(c'\).Therefore properties of the sequence of values ofq.vfor
each serverqin the sequential algorithm are also
properties in the distributed algorithm.K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

A consensus is reached at the end of an iteration in whichWis a majority of servers.Tagging assignments with the epochs in which
they were assignedWe need some mechanism to determine which servers were written in the
same iteration, and which were written earlier.
So, we make variableq.vhave two fields,(q.v.s, q.v.t)whereq.v.sis the chain atq.vandq.v.tis the epoch in whichq.v.swas assigned its value.
In the sequential algorithm we replace the assignment stepq.v = p.f()byq.v.s, q.v.t = p.f(), p.tWe make the same change the distributed algorithm, replacing the
following assignment in the algorithm for clientpv, t = p.f(), p.tbyv.s, v.t, t = p.f(), p.t, p.tA consensus is reached when the setWof servers in an
iteration is a majority.
For allqin this majority,q.v.tis the
epoch of the iteration.
Therefore, consensus is determined as follows:Consensus: Majority of Servers Agree on a Consensus Values*is a consensus chain in a computation of seq if
during the computation
 there exists a majorityW*of servers and an
epocht*where:
For allqinW*:(q.v.s = s*)and(q.v.t = t*)SpecifyingM: The Number of
Replies Required to ProceedA client needs to receive at leastMcopies of chains
to proceed to the write step.
What is a reasonable value ofM?The smaller the value ofMthe greater the likelihood of
proceeding to the write step.
Again, a reasonable design choice is to require that the client
receive replies from a majority of servers.
If that choice doesn't work we will try a larger value ofM.A client must receive
replies from a majority of servers to proceed to the write step.Specifyingp.f: The Values to be
WrittenConsider a single iteration ofseq.
IfRis not a majority then the iteration terminates
without modifying server values.
Now we consider the case in whichRis a majority.
We design the function so that it returns an extension to a consensus
chain.
Consider two cases.Case 1: All replies are identical.Let each elementq.vinRbes*,
t*.
BecauseRis a majority of servers, from the definition
of consensus it follows thats*is a consensus chain.A client proposes an extension of a consensus chain,
Sop.freturns the consensus,s*, appended
withp.h()wherep.h()is the extension proposed byp.Case 2: Not all replies are identical.p.freturnsq*.v.swhereq*.vis the reply with the largest epoch.The algorithm is given below.def p.f():
  values_read = list(p.copy.values())
  if all(values_read[0] == v for v in values_read):
     # All elements of values_read are identical
     # consensus is any element of values_read.
     consensus = values_read[0].s
     return consensus.append(h())
  else:
     # Not all elements of values_read are identical
     # Compute max_v, the element with largest v.t
     max_v = values_read[0]
     for v in values_read:
        if v.t > max_v.t:  max_v = v
     return max_v.sCorrectness of the Nondeterministic Sequential AlgorithmObservationPart 2 of the specification --
every consensus chain is proposed by a client
--
follows directly from the algorithm
  because the only assignment toq.v.sisp.f().Next,  we prove part 1. We will show that ifcis a
consensus chain after an iteration anddis a
consensus chain after a later iteration thencis a
prefix ofd.Theorem: The Sequential Algorithm is CorrectLets*be a consensus at some point in a computation of
the sequential program.
Then, there exists a majority W* of servers and an epoch t* where: For all q in W*:(q.v.s = s*) and (q.v.t = t*)Let the iteration that executes the transaction with epocht*be then*-th iteration.
Because each assignment toq.vincreasesq.v.twe get the following.Equation 1At the end of then-th iteration,
for allqinW*:For alln\(\geq\)n*: \(\quad\)q.v.t\(\geq\)t*For alln\(<\)n*: \(\quad\)q.v.t\(<\)t*Next we will prove thats*is a prefix of any assignment
to any server variable in any 
iteration after iterationn*.
This can be written as: at all points in the computation, :At all points in the computation, for all serversq:(q.v.t\(<\)t*) \(\; \vee \;\)
(s*\(\leq\)q.v.s)Proof of the InvariantBase Case:n-th Iteration wheren\(\leq\)n*The condition trivially holds for iterationnwheren\(<\)n*because, from equation 1:q.v.t\(<\)t*.Next, consider the case wheren\(=\)n*.
Ifq.vis modified in the iteration thenq.v.s\(=\)s*.
Ifq.vis not modified in the iteration thenq.v.t\(<\)t*.Induction StepAssume that the condition holds before iterationnforn\(>\)n*and 
show that it holds after the iteration.From equation 1 and the induction hypothesis, the following holds
before iterationn.For allqinW*:(q.v.t\(\geq\)t*)
\(\; \wedge \; \)
(s*\(\leq\)q.v.s)In any iteration, server variables are modified only ifRis a majority of servers.
Any two majorities have at least one element in common.
So there is an
element that is in bothRandW*.
Let this element beq*.
Becauseq*is an element ofW*:(q*.v.t\(\geq\)t*)
\(\: \wedge \;\)
(s*\(\leq\)q*.v.s)This gives us the following:Equation 2There exists aq*inRsuch that:(q*.v.t\(\geq\)t*)
\(\: \wedge \;\)
(s*\(\leq\)q*.v.s)Letdbe the value returned by functionp.f.
Consider the statements inp.fand two cases.Case 1: All valuesq.vforqinRare identical.In this case, from equation 2:For allqinR:s*\(\leq\)q.v.s.In functionp.f,consensusis assignedq.v.sfor someqinR.Therefore,s*\(\leq\)consensus.The function returnsd = consensus.append(h()).Therefores*\(\leq\)d.Case 2: Not all valuesq.vforqinRare identical.The function returnsq'.v.twhereq'is inRand:For allqinR: \(\quad\)q'.v.t\(\geq\)q.v.tFrom equation 2:q'.v.t\(\geq\)q*.v.t\(\geq\)t*From the induction hypothesis, andq'.v.t\(\geq\)t*, it follows thats*\(\leq\)q'.v.t.Therefores*\(\leq\)d.
This completes the proof of the invariant.We have shown thats*is a prefix ofallassignments made to server variables after iterationn*.
Therefore,s*is a prefix of all consensus values
reached in iterations after iterationn*.The Distributed Algorithm is CorrectWe showedthe equivalence of the
distributed and nondeterministic sequential algorithm: For every
computation \(c\) of the distributed algorithm there exists a
computation \(c'\) of
the nondeterministic sequential algorithm -- with appropriate choices of
nondeterministic selections -- such that for all serversqthe sequence of values ofq.vis the same
in \(c\) and \(c'\).Therefore properties of the sequence of values ofq.vfor
each serverqin the sequential algorithm are also
properties in the distributed algorithm.K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

Tagging assignments with the epochs in which
they were assigned

We make the same change the distributed algorithm, replacing the
following assignment in the algorithm for clientpv, t = p.f(), p.tbyv.s, v.t, t = p.f(), p.t, p.tA consensus is reached when the setWof servers in an
iteration is a majority.
For allqin this majority,q.v.tis the
epoch of the iteration.
Therefore, consensus is determined as follows:Consensus: Majority of Servers Agree on a Consensus Values*is a consensus chain in a computation of seq if
during the computation
 there exists a majorityW*of servers and an
epocht*where:
For allqinW*:(q.v.s = s*)and(q.v.t = t*)SpecifyingM: The Number of
Replies Required to ProceedA client needs to receive at leastMcopies of chains
to proceed to the write step.
What is a reasonable value ofM?The smaller the value ofMthe greater the likelihood of
proceeding to the write step.
Again, a reasonable design choice is to require that the client
receive replies from a majority of servers.
If that choice doesn't work we will try a larger value ofM.A client must receive
replies from a majority of servers to proceed to the write step.Specifyingp.f: The Values to be
WrittenConsider a single iteration ofseq.
IfRis not a majority then the iteration terminates
without modifying server values.
Now we consider the case in whichRis a majority.
We design the function so that it returns an extension to a consensus
chain.
Consider two cases.Case 1: All replies are identical.Let each elementq.vinRbes*,
t*.
BecauseRis a majority of servers, from the definition
of consensus it follows thats*is a consensus chain.A client proposes an extension of a consensus chain,
Sop.freturns the consensus,s*, appended
withp.h()wherep.h()is the extension proposed byp.Case 2: Not all replies are identical.p.freturnsq*.v.swhereq*.vis the reply with the largest epoch.The algorithm is given below.def p.f():
  values_read = list(p.copy.values())
  if all(values_read[0] == v for v in values_read):
     # All elements of values_read are identical
     # consensus is any element of values_read.
     consensus = values_read[0].s
     return consensus.append(h())
  else:
     # Not all elements of values_read are identical
     # Compute max_v, the element with largest v.t
     max_v = values_read[0]
     for v in values_read:
        if v.t > max_v.t:  max_v = v
     return max_v.sCorrectness of the Nondeterministic Sequential AlgorithmObservationPart 2 of the specification --
every consensus chain is proposed by a client
--
follows directly from the algorithm
  because the only assignment toq.v.sisp.f().Next,  we prove part 1. We will show that ifcis a
consensus chain after an iteration anddis a
consensus chain after a later iteration thencis a
prefix ofd.Theorem: The Sequential Algorithm is CorrectLets*be a consensus at some point in a computation of
the sequential program.
Then, there exists a majority W* of servers and an epoch t* where: For all q in W*:(q.v.s = s*) and (q.v.t = t*)Let the iteration that executes the transaction with epocht*be then*-th iteration.
Because each assignment toq.vincreasesq.v.twe get the following.Equation 1At the end of then-th iteration,
for allqinW*:For alln\(\geq\)n*: \(\quad\)q.v.t\(\geq\)t*For alln\(<\)n*: \(\quad\)q.v.t\(<\)t*Next we will prove thats*is a prefix of any assignment
to any server variable in any 
iteration after iterationn*.
This can be written as: at all points in the computation, :At all points in the computation, for all serversq:(q.v.t\(<\)t*) \(\; \vee \;\)
(s*\(\leq\)q.v.s)Proof of the InvariantBase Case:n-th Iteration wheren\(\leq\)n*The condition trivially holds for iterationnwheren\(<\)n*because, from equation 1:q.v.t\(<\)t*.Next, consider the case wheren\(=\)n*.
Ifq.vis modified in the iteration thenq.v.s\(=\)s*.
Ifq.vis not modified in the iteration thenq.v.t\(<\)t*.Induction StepAssume that the condition holds before iterationnforn\(>\)n*and 
show that it holds after the iteration.From equation 1 and the induction hypothesis, the following holds
before iterationn.For allqinW*:(q.v.t\(\geq\)t*)
\(\; \wedge \; \)
(s*\(\leq\)q.v.s)In any iteration, server variables are modified only ifRis a majority of servers.
Any two majorities have at least one element in common.
So there is an
element that is in bothRandW*.
Let this element beq*.
Becauseq*is an element ofW*:(q*.v.t\(\geq\)t*)
\(\: \wedge \;\)
(s*\(\leq\)q*.v.s)This gives us the following:Equation 2There exists aq*inRsuch that:(q*.v.t\(\geq\)t*)
\(\: \wedge \;\)
(s*\(\leq\)q*.v.s)Letdbe the value returned by functionp.f.
Consider the statements inp.fand two cases.Case 1: All valuesq.vforqinRare identical.In this case, from equation 2:For allqinR:s*\(\leq\)q.v.s.In functionp.f,consensusis assignedq.v.sfor someqinR.Therefore,s*\(\leq\)consensus.The function returnsd = consensus.append(h()).Therefores*\(\leq\)d.Case 2: Not all valuesq.vforqinRare identical.The function returnsq'.v.twhereq'is inRand:For allqinR: \(\quad\)q'.v.t\(\geq\)q.v.tFrom equation 2:q'.v.t\(\geq\)q*.v.t\(\geq\)t*From the induction hypothesis, andq'.v.t\(\geq\)t*, it follows thats*\(\leq\)q'.v.t.Therefores*\(\leq\)d.
This completes the proof of the invariant.We have shown thats*is a prefix ofallassignments made to server variables after iterationn*.
Therefore,s*is a prefix of all consensus values
reached in iterations after iterationn*.The Distributed Algorithm is CorrectWe showedthe equivalence of the
distributed and nondeterministic sequential algorithm: For every
computation \(c\) of the distributed algorithm there exists a
computation \(c'\) of
the nondeterministic sequential algorithm -- with appropriate choices of
nondeterministic selections -- such that for all serversqthe sequence of values ofq.vis the same
in \(c\) and \(c'\).Therefore properties of the sequence of values ofq.vfor
each serverqin the sequential algorithm are also
properties in the distributed algorithm.K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

A consensus is reached when the setWof servers in an
iteration is a majority.
For allqin this majority,q.v.tis the
epoch of the iteration.
Therefore, consensus is determined as follows:Consensus: Majority of Servers Agree on a Consensus Values*is a consensus chain in a computation of seq if
during the computation
 there exists a majorityW*of servers and an
epocht*where:
For allqinW*:(q.v.s = s*)and(q.v.t = t*)SpecifyingM: The Number of
Replies Required to ProceedA client needs to receive at leastMcopies of chains
to proceed to the write step.
What is a reasonable value ofM?The smaller the value ofMthe greater the likelihood of
proceeding to the write step.
Again, a reasonable design choice is to require that the client
receive replies from a majority of servers.
If that choice doesn't work we will try a larger value ofM.A client must receive
replies from a majority of servers to proceed to the write step.Specifyingp.f: The Values to be
WrittenConsider a single iteration ofseq.
IfRis not a majority then the iteration terminates
without modifying server values.
Now we consider the case in whichRis a majority.
We design the function so that it returns an extension to a consensus
chain.
Consider two cases.Case 1: All replies are identical.Let each elementq.vinRbes*,
t*.
BecauseRis a majority of servers, from the definition
of consensus it follows thats*is a consensus chain.A client proposes an extension of a consensus chain,
Sop.freturns the consensus,s*, appended
withp.h()wherep.h()is the extension proposed byp.Case 2: Not all replies are identical.p.freturnsq*.v.swhereq*.vis the reply with the largest epoch.The algorithm is given below.def p.f():
  values_read = list(p.copy.values())
  if all(values_read[0] == v for v in values_read):
     # All elements of values_read are identical
     # consensus is any element of values_read.
     consensus = values_read[0].s
     return consensus.append(h())
  else:
     # Not all elements of values_read are identical
     # Compute max_v, the element with largest v.t
     max_v = values_read[0]
     for v in values_read:
        if v.t > max_v.t:  max_v = v
     return max_v.sCorrectness of the Nondeterministic Sequential AlgorithmObservationPart 2 of the specification --
every consensus chain is proposed by a client
--
follows directly from the algorithm
  because the only assignment toq.v.sisp.f().Next,  we prove part 1. We will show that ifcis a
consensus chain after an iteration anddis a
consensus chain after a later iteration thencis a
prefix ofd.Theorem: The Sequential Algorithm is CorrectLets*be a consensus at some point in a computation of
the sequential program.
Then, there exists a majority W* of servers and an epoch t* where: For all q in W*:(q.v.s = s*) and (q.v.t = t*)Let the iteration that executes the transaction with epocht*be then*-th iteration.
Because each assignment toq.vincreasesq.v.twe get the following.Equation 1At the end of then-th iteration,
for allqinW*:For alln\(\geq\)n*: \(\quad\)q.v.t\(\geq\)t*For alln\(<\)n*: \(\quad\)q.v.t\(<\)t*Next we will prove thats*is a prefix of any assignment
to any server variable in any 
iteration after iterationn*.
This can be written as: at all points in the computation, :At all points in the computation, for all serversq:(q.v.t\(<\)t*) \(\; \vee \;\)
(s*\(\leq\)q.v.s)Proof of the InvariantBase Case:n-th Iteration wheren\(\leq\)n*The condition trivially holds for iterationnwheren\(<\)n*because, from equation 1:q.v.t\(<\)t*.Next, consider the case wheren\(=\)n*.
Ifq.vis modified in the iteration thenq.v.s\(=\)s*.
Ifq.vis not modified in the iteration thenq.v.t\(<\)t*.Induction StepAssume that the condition holds before iterationnforn\(>\)n*and 
show that it holds after the iteration.From equation 1 and the induction hypothesis, the following holds
before iterationn.For allqinW*:(q.v.t\(\geq\)t*)
\(\; \wedge \; \)
(s*\(\leq\)q.v.s)In any iteration, server variables are modified only ifRis a majority of servers.
Any two majorities have at least one element in common.
So there is an
element that is in bothRandW*.
Let this element beq*.
Becauseq*is an element ofW*:(q*.v.t\(\geq\)t*)
\(\: \wedge \;\)
(s*\(\leq\)q*.v.s)This gives us the following:Equation 2There exists aq*inRsuch that:(q*.v.t\(\geq\)t*)
\(\: \wedge \;\)
(s*\(\leq\)q*.v.s)Letdbe the value returned by functionp.f.
Consider the statements inp.fand two cases.Case 1: All valuesq.vforqinRare identical.In this case, from equation 2:For allqinR:s*\(\leq\)q.v.s.In functionp.f,consensusis assignedq.v.sfor someqinR.Therefore,s*\(\leq\)consensus.The function returnsd = consensus.append(h()).Therefores*\(\leq\)d.Case 2: Not all valuesq.vforqinRare identical.The function returnsq'.v.twhereq'is inRand:For allqinR: \(\quad\)q'.v.t\(\geq\)q.v.tFrom equation 2:q'.v.t\(\geq\)q*.v.t\(\geq\)t*From the induction hypothesis, andq'.v.t\(\geq\)t*, it follows thats*\(\leq\)q'.v.t.Therefores*\(\leq\)d.
This completes the proof of the invariant.We have shown thats*is a prefix ofallassignments made to server variables after iterationn*.
Therefore,s*is a prefix of all consensus values
reached in iterations after iterationn*.The Distributed Algorithm is CorrectWe showedthe equivalence of the
distributed and nondeterministic sequential algorithm: For every
computation \(c\) of the distributed algorithm there exists a
computation \(c'\) of
the nondeterministic sequential algorithm -- with appropriate choices of
nondeterministic selections -- such that for all serversqthe sequence of values ofq.vis the same
in \(c\) and \(c'\).Therefore properties of the sequence of values ofq.vfor
each serverqin the sequential algorithm are also
properties in the distributed algorithm.K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

Consensus: Majority of Servers Agree on a Consensus Values*is a consensus chain in a computation of seq if
during the computation
 there exists a majorityW*of servers and an
epocht*where:
For allqinW*:(q.v.s = s*)and(q.v.t = t*)SpecifyingM: The Number of
Replies Required to ProceedA client needs to receive at leastMcopies of chains
to proceed to the write step.
What is a reasonable value ofM?The smaller the value ofMthe greater the likelihood of
proceeding to the write step.
Again, a reasonable design choice is to require that the client
receive replies from a majority of servers.
If that choice doesn't work we will try a larger value ofM.A client must receive
replies from a majority of servers to proceed to the write step.Specifyingp.f: The Values to be
WrittenConsider a single iteration ofseq.
IfRis not a majority then the iteration terminates
without modifying server values.
Now we consider the case in whichRis a majority.
We design the function so that it returns an extension to a consensus
chain.
Consider two cases.Case 1: All replies are identical.Let each elementq.vinRbes*,
t*.
BecauseRis a majority of servers, from the definition
of consensus it follows thats*is a consensus chain.A client proposes an extension of a consensus chain,
Sop.freturns the consensus,s*, appended
withp.h()wherep.h()is the extension proposed byp.Case 2: Not all replies are identical.p.freturnsq*.v.swhereq*.vis the reply with the largest epoch.The algorithm is given below.def p.f():
  values_read = list(p.copy.values())
  if all(values_read[0] == v for v in values_read):
     # All elements of values_read are identical
     # consensus is any element of values_read.
     consensus = values_read[0].s
     return consensus.append(h())
  else:
     # Not all elements of values_read are identical
     # Compute max_v, the element with largest v.t
     max_v = values_read[0]
     for v in values_read:
        if v.t > max_v.t:  max_v = v
     return max_v.sCorrectness of the Nondeterministic Sequential AlgorithmObservationPart 2 of the specification --
every consensus chain is proposed by a client
--
follows directly from the algorithm
  because the only assignment toq.v.sisp.f().Next,  we prove part 1. We will show that ifcis a
consensus chain after an iteration anddis a
consensus chain after a later iteration thencis a
prefix ofd.Theorem: The Sequential Algorithm is CorrectLets*be a consensus at some point in a computation of
the sequential program.
Then, there exists a majority W* of servers and an epoch t* where: For all q in W*:(q.v.s = s*) and (q.v.t = t*)Let the iteration that executes the transaction with epocht*be then*-th iteration.
Because each assignment toq.vincreasesq.v.twe get the following.Equation 1At the end of then-th iteration,
for allqinW*:For alln\(\geq\)n*: \(\quad\)q.v.t\(\geq\)t*For alln\(<\)n*: \(\quad\)q.v.t\(<\)t*Next we will prove thats*is a prefix of any assignment
to any server variable in any 
iteration after iterationn*.
This can be written as: at all points in the computation, :At all points in the computation, for all serversq:(q.v.t\(<\)t*) \(\; \vee \;\)
(s*\(\leq\)q.v.s)Proof of the InvariantBase Case:n-th Iteration wheren\(\leq\)n*The condition trivially holds for iterationnwheren\(<\)n*because, from equation 1:q.v.t\(<\)t*.Next, consider the case wheren\(=\)n*.
Ifq.vis modified in the iteration thenq.v.s\(=\)s*.
Ifq.vis not modified in the iteration thenq.v.t\(<\)t*.Induction StepAssume that the condition holds before iterationnforn\(>\)n*and 
show that it holds after the iteration.From equation 1 and the induction hypothesis, the following holds
before iterationn.For allqinW*:(q.v.t\(\geq\)t*)
\(\; \wedge \; \)
(s*\(\leq\)q.v.s)In any iteration, server variables are modified only ifRis a majority of servers.
Any two majorities have at least one element in common.
So there is an
element that is in bothRandW*.
Let this element beq*.
Becauseq*is an element ofW*:(q*.v.t\(\geq\)t*)
\(\: \wedge \;\)
(s*\(\leq\)q*.v.s)This gives us the following:Equation 2There exists aq*inRsuch that:(q*.v.t\(\geq\)t*)
\(\: \wedge \;\)
(s*\(\leq\)q*.v.s)Letdbe the value returned by functionp.f.
Consider the statements inp.fand two cases.Case 1: All valuesq.vforqinRare identical.In this case, from equation 2:For allqinR:s*\(\leq\)q.v.s.In functionp.f,consensusis assignedq.v.sfor someqinR.Therefore,s*\(\leq\)consensus.The function returnsd = consensus.append(h()).Therefores*\(\leq\)d.Case 2: Not all valuesq.vforqinRare identical.The function returnsq'.v.twhereq'is inRand:For allqinR: \(\quad\)q'.v.t\(\geq\)q.v.tFrom equation 2:q'.v.t\(\geq\)q*.v.t\(\geq\)t*From the induction hypothesis, andq'.v.t\(\geq\)t*, it follows thats*\(\leq\)q'.v.t.Therefores*\(\leq\)d.
This completes the proof of the invariant.We have shown thats*is a prefix ofallassignments made to server variables after iterationn*.
Therefore,s*is a prefix of all consensus values
reached in iterations after iterationn*.The Distributed Algorithm is CorrectWe showedthe equivalence of the
distributed and nondeterministic sequential algorithm: For every
computation \(c\) of the distributed algorithm there exists a
computation \(c'\) of
the nondeterministic sequential algorithm -- with appropriate choices of
nondeterministic selections -- such that for all serversqthe sequence of values ofq.vis the same
in \(c\) and \(c'\).Therefore properties of the sequence of values ofq.vfor
each serverqin the sequential algorithm are also
properties in the distributed algorithm.K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

s*is a consensus chain in a computation of seq if
during the computation
 there exists a majorityW*of servers and an
epocht*where:
For allqinW*:(q.v.s = s*)and(q.v.t = t*)

(q.v.s = s*)and(q.v.t = t*)

The smaller the value ofMthe greater the likelihood of
proceeding to the write step.
Again, a reasonable design choice is to require that the client
receive replies from a majority of servers.
If that choice doesn't work we will try a larger value ofM.A client must receive
replies from a majority of servers to proceed to the write step.Specifyingp.f: The Values to be
WrittenConsider a single iteration ofseq.
IfRis not a majority then the iteration terminates
without modifying server values.
Now we consider the case in whichRis a majority.
We design the function so that it returns an extension to a consensus
chain.
Consider two cases.Case 1: All replies are identical.Let each elementq.vinRbes*,
t*.
BecauseRis a majority of servers, from the definition
of consensus it follows thats*is a consensus chain.A client proposes an extension of a consensus chain,
Sop.freturns the consensus,s*, appended
withp.h()wherep.h()is the extension proposed byp.Case 2: Not all replies are identical.p.freturnsq*.v.swhereq*.vis the reply with the largest epoch.The algorithm is given below.def p.f():
  values_read = list(p.copy.values())
  if all(values_read[0] == v for v in values_read):
     # All elements of values_read are identical
     # consensus is any element of values_read.
     consensus = values_read[0].s
     return consensus.append(h())
  else:
     # Not all elements of values_read are identical
     # Compute max_v, the element with largest v.t
     max_v = values_read[0]
     for v in values_read:
        if v.t > max_v.t:  max_v = v
     return max_v.sCorrectness of the Nondeterministic Sequential AlgorithmObservationPart 2 of the specification --
every consensus chain is proposed by a client
--
follows directly from the algorithm
  because the only assignment toq.v.sisp.f().Next,  we prove part 1. We will show that ifcis a
consensus chain after an iteration anddis a
consensus chain after a later iteration thencis a
prefix ofd.Theorem: The Sequential Algorithm is CorrectLets*be a consensus at some point in a computation of
the sequential program.
Then, there exists a majority W* of servers and an epoch t* where: For all q in W*:(q.v.s = s*) and (q.v.t = t*)Let the iteration that executes the transaction with epocht*be then*-th iteration.
Because each assignment toq.vincreasesq.v.twe get the following.Equation 1At the end of then-th iteration,
for allqinW*:For alln\(\geq\)n*: \(\quad\)q.v.t\(\geq\)t*For alln\(<\)n*: \(\quad\)q.v.t\(<\)t*Next we will prove thats*is a prefix of any assignment
to any server variable in any 
iteration after iterationn*.
This can be written as: at all points in the computation, :At all points in the computation, for all serversq:(q.v.t\(<\)t*) \(\; \vee \;\)
(s*\(\leq\)q.v.s)Proof of the InvariantBase Case:n-th Iteration wheren\(\leq\)n*The condition trivially holds for iterationnwheren\(<\)n*because, from equation 1:q.v.t\(<\)t*.Next, consider the case wheren\(=\)n*.
Ifq.vis modified in the iteration thenq.v.s\(=\)s*.
Ifq.vis not modified in the iteration thenq.v.t\(<\)t*.Induction StepAssume that the condition holds before iterationnforn\(>\)n*and 
show that it holds after the iteration.From equation 1 and the induction hypothesis, the following holds
before iterationn.For allqinW*:(q.v.t\(\geq\)t*)
\(\; \wedge \; \)
(s*\(\leq\)q.v.s)In any iteration, server variables are modified only ifRis a majority of servers.
Any two majorities have at least one element in common.
So there is an
element that is in bothRandW*.
Let this element beq*.
Becauseq*is an element ofW*:(q*.v.t\(\geq\)t*)
\(\: \wedge \;\)
(s*\(\leq\)q*.v.s)This gives us the following:Equation 2There exists aq*inRsuch that:(q*.v.t\(\geq\)t*)
\(\: \wedge \;\)
(s*\(\leq\)q*.v.s)Letdbe the value returned by functionp.f.
Consider the statements inp.fand two cases.Case 1: All valuesq.vforqinRare identical.In this case, from equation 2:For allqinR:s*\(\leq\)q.v.s.In functionp.f,consensusis assignedq.v.sfor someqinR.Therefore,s*\(\leq\)consensus.The function returnsd = consensus.append(h()).Therefores*\(\leq\)d.Case 2: Not all valuesq.vforqinRare identical.The function returnsq'.v.twhereq'is inRand:For allqinR: \(\quad\)q'.v.t\(\geq\)q.v.tFrom equation 2:q'.v.t\(\geq\)q*.v.t\(\geq\)t*From the induction hypothesis, andq'.v.t\(\geq\)t*, it follows thats*\(\leq\)q'.v.t.Therefores*\(\leq\)d.
This completes the proof of the invariant.We have shown thats*is a prefix ofallassignments made to server variables after iterationn*.
Therefore,s*is a prefix of all consensus values
reached in iterations after iterationn*.The Distributed Algorithm is CorrectWe showedthe equivalence of the
distributed and nondeterministic sequential algorithm: For every
computation \(c\) of the distributed algorithm there exists a
computation \(c'\) of
the nondeterministic sequential algorithm -- with appropriate choices of
nondeterministic selections -- such that for all serversqthe sequence of values ofq.vis the same
in \(c\) and \(c'\).Therefore properties of the sequence of values ofq.vfor
each serverqin the sequential algorithm are also
properties in the distributed algorithm.K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

A client must receive
replies from a majority of servers to proceed to the write step.Specifyingp.f: The Values to be
WrittenConsider a single iteration ofseq.
IfRis not a majority then the iteration terminates
without modifying server values.
Now we consider the case in whichRis a majority.
We design the function so that it returns an extension to a consensus
chain.
Consider two cases.Case 1: All replies are identical.Let each elementq.vinRbes*,
t*.
BecauseRis a majority of servers, from the definition
of consensus it follows thats*is a consensus chain.A client proposes an extension of a consensus chain,
Sop.freturns the consensus,s*, appended
withp.h()wherep.h()is the extension proposed byp.Case 2: Not all replies are identical.p.freturnsq*.v.swhereq*.vis the reply with the largest epoch.The algorithm is given below.def p.f():
  values_read = list(p.copy.values())
  if all(values_read[0] == v for v in values_read):
     # All elements of values_read are identical
     # consensus is any element of values_read.
     consensus = values_read[0].s
     return consensus.append(h())
  else:
     # Not all elements of values_read are identical
     # Compute max_v, the element with largest v.t
     max_v = values_read[0]
     for v in values_read:
        if v.t > max_v.t:  max_v = v
     return max_v.sCorrectness of the Nondeterministic Sequential AlgorithmObservationPart 2 of the specification --
every consensus chain is proposed by a client
--
follows directly from the algorithm
  because the only assignment toq.v.sisp.f().Next,  we prove part 1. We will show that ifcis a
consensus chain after an iteration anddis a
consensus chain after a later iteration thencis a
prefix ofd.Theorem: The Sequential Algorithm is CorrectLets*be a consensus at some point in a computation of
the sequential program.
Then, there exists a majority W* of servers and an epoch t* where: For all q in W*:(q.v.s = s*) and (q.v.t = t*)Let the iteration that executes the transaction with epocht*be then*-th iteration.
Because each assignment toq.vincreasesq.v.twe get the following.Equation 1At the end of then-th iteration,
for allqinW*:For alln\(\geq\)n*: \(\quad\)q.v.t\(\geq\)t*For alln\(<\)n*: \(\quad\)q.v.t\(<\)t*Next we will prove thats*is a prefix of any assignment
to any server variable in any 
iteration after iterationn*.
This can be written as: at all points in the computation, :At all points in the computation, for all serversq:(q.v.t\(<\)t*) \(\; \vee \;\)
(s*\(\leq\)q.v.s)Proof of the InvariantBase Case:n-th Iteration wheren\(\leq\)n*The condition trivially holds for iterationnwheren\(<\)n*because, from equation 1:q.v.t\(<\)t*.Next, consider the case wheren\(=\)n*.
Ifq.vis modified in the iteration thenq.v.s\(=\)s*.
Ifq.vis not modified in the iteration thenq.v.t\(<\)t*.Induction StepAssume that the condition holds before iterationnforn\(>\)n*and 
show that it holds after the iteration.From equation 1 and the induction hypothesis, the following holds
before iterationn.For allqinW*:(q.v.t\(\geq\)t*)
\(\; \wedge \; \)
(s*\(\leq\)q.v.s)In any iteration, server variables are modified only ifRis a majority of servers.
Any two majorities have at least one element in common.
So there is an
element that is in bothRandW*.
Let this element beq*.
Becauseq*is an element ofW*:(q*.v.t\(\geq\)t*)
\(\: \wedge \;\)
(s*\(\leq\)q*.v.s)This gives us the following:Equation 2There exists aq*inRsuch that:(q*.v.t\(\geq\)t*)
\(\: \wedge \;\)
(s*\(\leq\)q*.v.s)Letdbe the value returned by functionp.f.
Consider the statements inp.fand two cases.Case 1: All valuesq.vforqinRare identical.In this case, from equation 2:For allqinR:s*\(\leq\)q.v.s.In functionp.f,consensusis assignedq.v.sfor someqinR.Therefore,s*\(\leq\)consensus.The function returnsd = consensus.append(h()).Therefores*\(\leq\)d.Case 2: Not all valuesq.vforqinRare identical.The function returnsq'.v.twhereq'is inRand:For allqinR: \(\quad\)q'.v.t\(\geq\)q.v.tFrom equation 2:q'.v.t\(\geq\)q*.v.t\(\geq\)t*From the induction hypothesis, andq'.v.t\(\geq\)t*, it follows thats*\(\leq\)q'.v.t.Therefores*\(\leq\)d.
This completes the proof of the invariant.We have shown thats*is a prefix ofallassignments made to server variables after iterationn*.
Therefore,s*is a prefix of all consensus values
reached in iterations after iterationn*.The Distributed Algorithm is CorrectWe showedthe equivalence of the
distributed and nondeterministic sequential algorithm: For every
computation \(c\) of the distributed algorithm there exists a
computation \(c'\) of
the nondeterministic sequential algorithm -- with appropriate choices of
nondeterministic selections -- such that for all serversqthe sequence of values ofq.vis the same
in \(c\) and \(c'\).Therefore properties of the sequence of values ofq.vfor
each serverqin the sequential algorithm are also
properties in the distributed algorithm.K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

A client proposes an extension of a consensus chain,
Sop.freturns the consensus,s*, appended
withp.h()wherep.h()is the extension proposed byp.Case 2: Not all replies are identical.p.freturnsq*.v.swhereq*.vis the reply with the largest epoch.The algorithm is given below.def p.f():
  values_read = list(p.copy.values())
  if all(values_read[0] == v for v in values_read):
     # All elements of values_read are identical
     # consensus is any element of values_read.
     consensus = values_read[0].s
     return consensus.append(h())
  else:
     # Not all elements of values_read are identical
     # Compute max_v, the element with largest v.t
     max_v = values_read[0]
     for v in values_read:
        if v.t > max_v.t:  max_v = v
     return max_v.sCorrectness of the Nondeterministic Sequential AlgorithmObservationPart 2 of the specification --
every consensus chain is proposed by a client
--
follows directly from the algorithm
  because the only assignment toq.v.sisp.f().Next,  we prove part 1. We will show that ifcis a
consensus chain after an iteration anddis a
consensus chain after a later iteration thencis a
prefix ofd.Theorem: The Sequential Algorithm is CorrectLets*be a consensus at some point in a computation of
the sequential program.
Then, there exists a majority W* of servers and an epoch t* where: For all q in W*:(q.v.s = s*) and (q.v.t = t*)Let the iteration that executes the transaction with epocht*be then*-th iteration.
Because each assignment toq.vincreasesq.v.twe get the following.Equation 1At the end of then-th iteration,
for allqinW*:For alln\(\geq\)n*: \(\quad\)q.v.t\(\geq\)t*For alln\(<\)n*: \(\quad\)q.v.t\(<\)t*Next we will prove thats*is a prefix of any assignment
to any server variable in any 
iteration after iterationn*.
This can be written as: at all points in the computation, :At all points in the computation, for all serversq:(q.v.t\(<\)t*) \(\; \vee \;\)
(s*\(\leq\)q.v.s)Proof of the InvariantBase Case:n-th Iteration wheren\(\leq\)n*The condition trivially holds for iterationnwheren\(<\)n*because, from equation 1:q.v.t\(<\)t*.Next, consider the case wheren\(=\)n*.
Ifq.vis modified in the iteration thenq.v.s\(=\)s*.
Ifq.vis not modified in the iteration thenq.v.t\(<\)t*.Induction StepAssume that the condition holds before iterationnforn\(>\)n*and 
show that it holds after the iteration.From equation 1 and the induction hypothesis, the following holds
before iterationn.For allqinW*:(q.v.t\(\geq\)t*)
\(\; \wedge \; \)
(s*\(\leq\)q.v.s)In any iteration, server variables are modified only ifRis a majority of servers.
Any two majorities have at least one element in common.
So there is an
element that is in bothRandW*.
Let this element beq*.
Becauseq*is an element ofW*:(q*.v.t\(\geq\)t*)
\(\: \wedge \;\)
(s*\(\leq\)q*.v.s)This gives us the following:Equation 2There exists aq*inRsuch that:(q*.v.t\(\geq\)t*)
\(\: \wedge \;\)
(s*\(\leq\)q*.v.s)Letdbe the value returned by functionp.f.
Consider the statements inp.fand two cases.Case 1: All valuesq.vforqinRare identical.In this case, from equation 2:For allqinR:s*\(\leq\)q.v.s.In functionp.f,consensusis assignedq.v.sfor someqinR.Therefore,s*\(\leq\)consensus.The function returnsd = consensus.append(h()).Therefores*\(\leq\)d.Case 2: Not all valuesq.vforqinRare identical.The function returnsq'.v.twhereq'is inRand:For allqinR: \(\quad\)q'.v.t\(\geq\)q.v.tFrom equation 2:q'.v.t\(\geq\)q*.v.t\(\geq\)t*From the induction hypothesis, andq'.v.t\(\geq\)t*, it follows thats*\(\leq\)q'.v.t.Therefores*\(\leq\)d.
This completes the proof of the invariant.We have shown thats*is a prefix ofallassignments made to server variables after iterationn*.
Therefore,s*is a prefix of all consensus values
reached in iterations after iterationn*.The Distributed Algorithm is CorrectWe showedthe equivalence of the
distributed and nondeterministic sequential algorithm: For every
computation \(c\) of the distributed algorithm there exists a
computation \(c'\) of
the nondeterministic sequential algorithm -- with appropriate choices of
nondeterministic selections -- such that for all serversqthe sequence of values ofq.vis the same
in \(c\) and \(c'\).Therefore properties of the sequence of values ofq.vfor
each serverqin the sequential algorithm are also
properties in the distributed algorithm.K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

The algorithm is given below.def p.f():
  values_read = list(p.copy.values())
  if all(values_read[0] == v for v in values_read):
     # All elements of values_read are identical
     # consensus is any element of values_read.
     consensus = values_read[0].s
     return consensus.append(h())
  else:
     # Not all elements of values_read are identical
     # Compute max_v, the element with largest v.t
     max_v = values_read[0]
     for v in values_read:
        if v.t > max_v.t:  max_v = v
     return max_v.sCorrectness of the Nondeterministic Sequential AlgorithmObservationPart 2 of the specification --
every consensus chain is proposed by a client
--
follows directly from the algorithm
  because the only assignment toq.v.sisp.f().Next,  we prove part 1. We will show that ifcis a
consensus chain after an iteration anddis a
consensus chain after a later iteration thencis a
prefix ofd.Theorem: The Sequential Algorithm is CorrectLets*be a consensus at some point in a computation of
the sequential program.
Then, there exists a majority W* of servers and an epoch t* where: For all q in W*:(q.v.s = s*) and (q.v.t = t*)Let the iteration that executes the transaction with epocht*be then*-th iteration.
Because each assignment toq.vincreasesq.v.twe get the following.Equation 1At the end of then-th iteration,
for allqinW*:For alln\(\geq\)n*: \(\quad\)q.v.t\(\geq\)t*For alln\(<\)n*: \(\quad\)q.v.t\(<\)t*Next we will prove thats*is a prefix of any assignment
to any server variable in any 
iteration after iterationn*.
This can be written as: at all points in the computation, :At all points in the computation, for all serversq:(q.v.t\(<\)t*) \(\; \vee \;\)
(s*\(\leq\)q.v.s)Proof of the InvariantBase Case:n-th Iteration wheren\(\leq\)n*The condition trivially holds for iterationnwheren\(<\)n*because, from equation 1:q.v.t\(<\)t*.Next, consider the case wheren\(=\)n*.
Ifq.vis modified in the iteration thenq.v.s\(=\)s*.
Ifq.vis not modified in the iteration thenq.v.t\(<\)t*.Induction StepAssume that the condition holds before iterationnforn\(>\)n*and 
show that it holds after the iteration.From equation 1 and the induction hypothesis, the following holds
before iterationn.For allqinW*:(q.v.t\(\geq\)t*)
\(\; \wedge \; \)
(s*\(\leq\)q.v.s)In any iteration, server variables are modified only ifRis a majority of servers.
Any two majorities have at least one element in common.
So there is an
element that is in bothRandW*.
Let this element beq*.
Becauseq*is an element ofW*:(q*.v.t\(\geq\)t*)
\(\: \wedge \;\)
(s*\(\leq\)q*.v.s)This gives us the following:Equation 2There exists aq*inRsuch that:(q*.v.t\(\geq\)t*)
\(\: \wedge \;\)
(s*\(\leq\)q*.v.s)Letdbe the value returned by functionp.f.
Consider the statements inp.fand two cases.Case 1: All valuesq.vforqinRare identical.In this case, from equation 2:For allqinR:s*\(\leq\)q.v.s.In functionp.f,consensusis assignedq.v.sfor someqinR.Therefore,s*\(\leq\)consensus.The function returnsd = consensus.append(h()).Therefores*\(\leq\)d.Case 2: Not all valuesq.vforqinRare identical.The function returnsq'.v.twhereq'is inRand:For allqinR: \(\quad\)q'.v.t\(\geq\)q.v.tFrom equation 2:q'.v.t\(\geq\)q*.v.t\(\geq\)t*From the induction hypothesis, andq'.v.t\(\geq\)t*, it follows thats*\(\leq\)q'.v.t.Therefores*\(\leq\)d.
This completes the proof of the invariant.We have shown thats*is a prefix ofallassignments made to server variables after iterationn*.
Therefore,s*is a prefix of all consensus values
reached in iterations after iterationn*.The Distributed Algorithm is CorrectWe showedthe equivalence of the
distributed and nondeterministic sequential algorithm: For every
computation \(c\) of the distributed algorithm there exists a
computation \(c'\) of
the nondeterministic sequential algorithm -- with appropriate choices of
nondeterministic selections -- such that for all serversqthe sequence of values ofq.vis the same
in \(c\) and \(c'\).Therefore properties of the sequence of values ofq.vfor
each serverqin the sequential algorithm are also
properties in the distributed algorithm.K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

Next,  we prove part 1. We will show that ifcis a
consensus chain after an iteration anddis a
consensus chain after a later iteration thencis a
prefix ofd.Theorem: The Sequential Algorithm is CorrectLets*be a consensus at some point in a computation of
the sequential program.
Then, there exists a majority W* of servers and an epoch t* where: For all q in W*:(q.v.s = s*) and (q.v.t = t*)Let the iteration that executes the transaction with epocht*be then*-th iteration.
Because each assignment toq.vincreasesq.v.twe get the following.Equation 1At the end of then-th iteration,
for allqinW*:For alln\(\geq\)n*: \(\quad\)q.v.t\(\geq\)t*For alln\(<\)n*: \(\quad\)q.v.t\(<\)t*Next we will prove thats*is a prefix of any assignment
to any server variable in any 
iteration after iterationn*.
This can be written as: at all points in the computation, :At all points in the computation, for all serversq:(q.v.t\(<\)t*) \(\; \vee \;\)
(s*\(\leq\)q.v.s)Proof of the InvariantBase Case:n-th Iteration wheren\(\leq\)n*The condition trivially holds for iterationnwheren\(<\)n*because, from equation 1:q.v.t\(<\)t*.Next, consider the case wheren\(=\)n*.
Ifq.vis modified in the iteration thenq.v.s\(=\)s*.
Ifq.vis not modified in the iteration thenq.v.t\(<\)t*.Induction StepAssume that the condition holds before iterationnforn\(>\)n*and 
show that it holds after the iteration.From equation 1 and the induction hypothesis, the following holds
before iterationn.For allqinW*:(q.v.t\(\geq\)t*)
\(\; \wedge \; \)
(s*\(\leq\)q.v.s)In any iteration, server variables are modified only ifRis a majority of servers.
Any two majorities have at least one element in common.
So there is an
element that is in bothRandW*.
Let this element beq*.
Becauseq*is an element ofW*:(q*.v.t\(\geq\)t*)
\(\: \wedge \;\)
(s*\(\leq\)q*.v.s)This gives us the following:Equation 2There exists aq*inRsuch that:(q*.v.t\(\geq\)t*)
\(\: \wedge \;\)
(s*\(\leq\)q*.v.s)Letdbe the value returned by functionp.f.
Consider the statements inp.fand two cases.Case 1: All valuesq.vforqinRare identical.In this case, from equation 2:For allqinR:s*\(\leq\)q.v.s.In functionp.f,consensusis assignedq.v.sfor someqinR.Therefore,s*\(\leq\)consensus.The function returnsd = consensus.append(h()).Therefores*\(\leq\)d.Case 2: Not all valuesq.vforqinRare identical.The function returnsq'.v.twhereq'is inRand:For allqinR: \(\quad\)q'.v.t\(\geq\)q.v.tFrom equation 2:q'.v.t\(\geq\)q*.v.t\(\geq\)t*From the induction hypothesis, andq'.v.t\(\geq\)t*, it follows thats*\(\leq\)q'.v.t.Therefores*\(\leq\)d.
This completes the proof of the invariant.We have shown thats*is a prefix ofallassignments made to server variables after iterationn*.
Therefore,s*is a prefix of all consensus values
reached in iterations after iterationn*.The Distributed Algorithm is CorrectWe showedthe equivalence of the
distributed and nondeterministic sequential algorithm: For every
computation \(c\) of the distributed algorithm there exists a
computation \(c'\) of
the nondeterministic sequential algorithm -- with appropriate choices of
nondeterministic selections -- such that for all serversqthe sequence of values ofq.vis the same
in \(c\) and \(c'\).Therefore properties of the sequence of values ofq.vfor
each serverqin the sequential algorithm are also
properties in the distributed algorithm.K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

Lets*be a consensus at some point in a computation of
the sequential program.
Then, there exists a majority W* of servers and an epoch t* where: For all q in W*:(q.v.s = s*) and (q.v.t = t*)Let the iteration that executes the transaction with epocht*be then*-th iteration.
Because each assignment toq.vincreasesq.v.twe get the following.Equation 1At the end of then-th iteration,
for allqinW*:For alln\(\geq\)n*: \(\quad\)q.v.t\(\geq\)t*For alln\(<\)n*: \(\quad\)q.v.t\(<\)t*Next we will prove thats*is a prefix of any assignment
to any server variable in any 
iteration after iterationn*.
This can be written as: at all points in the computation, :At all points in the computation, for all serversq:(q.v.t\(<\)t*) \(\; \vee \;\)
(s*\(\leq\)q.v.s)Proof of the InvariantBase Case:n-th Iteration wheren\(\leq\)n*The condition trivially holds for iterationnwheren\(<\)n*because, from equation 1:q.v.t\(<\)t*.Next, consider the case wheren\(=\)n*.
Ifq.vis modified in the iteration thenq.v.s\(=\)s*.
Ifq.vis not modified in the iteration thenq.v.t\(<\)t*.Induction StepAssume that the condition holds before iterationnforn\(>\)n*and 
show that it holds after the iteration.From equation 1 and the induction hypothesis, the following holds
before iterationn.For allqinW*:(q.v.t\(\geq\)t*)
\(\; \wedge \; \)
(s*\(\leq\)q.v.s)In any iteration, server variables are modified only ifRis a majority of servers.
Any two majorities have at least one element in common.
So there is an
element that is in bothRandW*.
Let this element beq*.
Becauseq*is an element ofW*:(q*.v.t\(\geq\)t*)
\(\: \wedge \;\)
(s*\(\leq\)q*.v.s)This gives us the following:Equation 2There exists aq*inRsuch that:(q*.v.t\(\geq\)t*)
\(\: \wedge \;\)
(s*\(\leq\)q*.v.s)Letdbe the value returned by functionp.f.
Consider the statements inp.fand two cases.Case 1: All valuesq.vforqinRare identical.In this case, from equation 2:For allqinR:s*\(\leq\)q.v.s.In functionp.f,consensusis assignedq.v.sfor someqinR.Therefore,s*\(\leq\)consensus.The function returnsd = consensus.append(h()).Therefores*\(\leq\)d.Case 2: Not all valuesq.vforqinRare identical.The function returnsq'.v.twhereq'is inRand:For allqinR: \(\quad\)q'.v.t\(\geq\)q.v.tFrom equation 2:q'.v.t\(\geq\)q*.v.t\(\geq\)t*From the induction hypothesis, andq'.v.t\(\geq\)t*, it follows thats*\(\leq\)q'.v.t.Therefores*\(\leq\)d.
This completes the proof of the invariant.We have shown thats*is a prefix ofallassignments made to server variables after iterationn*.
Therefore,s*is a prefix of all consensus values
reached in iterations after iterationn*.The Distributed Algorithm is CorrectWe showedthe equivalence of the
distributed and nondeterministic sequential algorithm: For every
computation \(c\) of the distributed algorithm there exists a
computation \(c'\) of
the nondeterministic sequential algorithm -- with appropriate choices of
nondeterministic selections -- such that for all serversqthe sequence of values ofq.vis the same
in \(c\) and \(c'\).Therefore properties of the sequence of values ofq.vfor
each serverqin the sequential algorithm are also
properties in the distributed algorithm.K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

Let the iteration that executes the transaction with epocht*be then*-th iteration.
Because each assignment toq.vincreasesq.v.twe get the following.Equation 1At the end of then-th iteration,
for allqinW*:For alln\(\geq\)n*: \(\quad\)q.v.t\(\geq\)t*For alln\(<\)n*: \(\quad\)q.v.t\(<\)t*Next we will prove thats*is a prefix of any assignment
to any server variable in any 
iteration after iterationn*.
This can be written as: at all points in the computation, :At all points in the computation, for all serversq:(q.v.t\(<\)t*) \(\; \vee \;\)
(s*\(\leq\)q.v.s)Proof of the InvariantBase Case:n-th Iteration wheren\(\leq\)n*The condition trivially holds for iterationnwheren\(<\)n*because, from equation 1:q.v.t\(<\)t*.Next, consider the case wheren\(=\)n*.
Ifq.vis modified in the iteration thenq.v.s\(=\)s*.
Ifq.vis not modified in the iteration thenq.v.t\(<\)t*.Induction StepAssume that the condition holds before iterationnforn\(>\)n*and 
show that it holds after the iteration.From equation 1 and the induction hypothesis, the following holds
before iterationn.For allqinW*:(q.v.t\(\geq\)t*)
\(\; \wedge \; \)
(s*\(\leq\)q.v.s)In any iteration, server variables are modified only ifRis a majority of servers.
Any two majorities have at least one element in common.
So there is an
element that is in bothRandW*.
Let this element beq*.
Becauseq*is an element ofW*:(q*.v.t\(\geq\)t*)
\(\: \wedge \;\)
(s*\(\leq\)q*.v.s)This gives us the following:Equation 2There exists aq*inRsuch that:(q*.v.t\(\geq\)t*)
\(\: \wedge \;\)
(s*\(\leq\)q*.v.s)Letdbe the value returned by functionp.f.
Consider the statements inp.fand two cases.Case 1: All valuesq.vforqinRare identical.In this case, from equation 2:For allqinR:s*\(\leq\)q.v.s.In functionp.f,consensusis assignedq.v.sfor someqinR.Therefore,s*\(\leq\)consensus.The function returnsd = consensus.append(h()).Therefores*\(\leq\)d.Case 2: Not all valuesq.vforqinRare identical.The function returnsq'.v.twhereq'is inRand:For allqinR: \(\quad\)q'.v.t\(\geq\)q.v.tFrom equation 2:q'.v.t\(\geq\)q*.v.t\(\geq\)t*From the induction hypothesis, andq'.v.t\(\geq\)t*, it follows thats*\(\leq\)q'.v.t.Therefores*\(\leq\)d.
This completes the proof of the invariant.We have shown thats*is a prefix ofallassignments made to server variables after iterationn*.
Therefore,s*is a prefix of all consensus values
reached in iterations after iterationn*.The Distributed Algorithm is CorrectWe showedthe equivalence of the
distributed and nondeterministic sequential algorithm: For every
computation \(c\) of the distributed algorithm there exists a
computation \(c'\) of
the nondeterministic sequential algorithm -- with appropriate choices of
nondeterministic selections -- such that for all serversqthe sequence of values ofq.vis the same
in \(c\) and \(c'\).Therefore properties of the sequence of values ofq.vfor
each serverqin the sequential algorithm are also
properties in the distributed algorithm.K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

Equation 1

For alln\(\geq\)n*: \(\quad\)q.v.t\(\geq\)t*For alln\(<\)n*: \(\quad\)q.v.t\(<\)t*Next we will prove thats*is a prefix of any assignment
to any server variable in any 
iteration after iterationn*.
This can be written as: at all points in the computation, :At all points in the computation, for all serversq:(q.v.t\(<\)t*) \(\; \vee \;\)
(s*\(\leq\)q.v.s)Proof of the InvariantBase Case:n-th Iteration wheren\(\leq\)n*The condition trivially holds for iterationnwheren\(<\)n*because, from equation 1:q.v.t\(<\)t*.Next, consider the case wheren\(=\)n*.
Ifq.vis modified in the iteration thenq.v.s\(=\)s*.
Ifq.vis not modified in the iteration thenq.v.t\(<\)t*.Induction StepAssume that the condition holds before iterationnforn\(>\)n*and 
show that it holds after the iteration.From equation 1 and the induction hypothesis, the following holds
before iterationn.For allqinW*:(q.v.t\(\geq\)t*)
\(\; \wedge \; \)
(s*\(\leq\)q.v.s)In any iteration, server variables are modified only ifRis a majority of servers.
Any two majorities have at least one element in common.
So there is an
element that is in bothRandW*.
Let this element beq*.
Becauseq*is an element ofW*:(q*.v.t\(\geq\)t*)
\(\: \wedge \;\)
(s*\(\leq\)q*.v.s)This gives us the following:Equation 2There exists aq*inRsuch that:(q*.v.t\(\geq\)t*)
\(\: \wedge \;\)
(s*\(\leq\)q*.v.s)Letdbe the value returned by functionp.f.
Consider the statements inp.fand two cases.Case 1: All valuesq.vforqinRare identical.In this case, from equation 2:For allqinR:s*\(\leq\)q.v.s.In functionp.f,consensusis assignedq.v.sfor someqinR.Therefore,s*\(\leq\)consensus.The function returnsd = consensus.append(h()).Therefores*\(\leq\)d.Case 2: Not all valuesq.vforqinRare identical.The function returnsq'.v.twhereq'is inRand:For allqinR: \(\quad\)q'.v.t\(\geq\)q.v.tFrom equation 2:q'.v.t\(\geq\)q*.v.t\(\geq\)t*From the induction hypothesis, andq'.v.t\(\geq\)t*, it follows thats*\(\leq\)q'.v.t.Therefores*\(\leq\)d.
This completes the proof of the invariant.We have shown thats*is a prefix ofallassignments made to server variables after iterationn*.
Therefore,s*is a prefix of all consensus values
reached in iterations after iterationn*.The Distributed Algorithm is CorrectWe showedthe equivalence of the
distributed and nondeterministic sequential algorithm: For every
computation \(c\) of the distributed algorithm there exists a
computation \(c'\) of
the nondeterministic sequential algorithm -- with appropriate choices of
nondeterministic selections -- such that for all serversqthe sequence of values ofq.vis the same
in \(c\) and \(c'\).Therefore properties of the sequence of values ofq.vfor
each serverqin the sequential algorithm are also
properties in the distributed algorithm.K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

For alln\(<\)n*: \(\quad\)q.v.t\(<\)t*Next we will prove thats*is a prefix of any assignment
to any server variable in any 
iteration after iterationn*.
This can be written as: at all points in the computation, :At all points in the computation, for all serversq:(q.v.t\(<\)t*) \(\; \vee \;\)
(s*\(\leq\)q.v.s)Proof of the InvariantBase Case:n-th Iteration wheren\(\leq\)n*The condition trivially holds for iterationnwheren\(<\)n*because, from equation 1:q.v.t\(<\)t*.Next, consider the case wheren\(=\)n*.
Ifq.vis modified in the iteration thenq.v.s\(=\)s*.
Ifq.vis not modified in the iteration thenq.v.t\(<\)t*.Induction StepAssume that the condition holds before iterationnforn\(>\)n*and 
show that it holds after the iteration.From equation 1 and the induction hypothesis, the following holds
before iterationn.For allqinW*:(q.v.t\(\geq\)t*)
\(\; \wedge \; \)
(s*\(\leq\)q.v.s)In any iteration, server variables are modified only ifRis a majority of servers.
Any two majorities have at least one element in common.
So there is an
element that is in bothRandW*.
Let this element beq*.
Becauseq*is an element ofW*:(q*.v.t\(\geq\)t*)
\(\: \wedge \;\)
(s*\(\leq\)q*.v.s)This gives us the following:Equation 2There exists aq*inRsuch that:(q*.v.t\(\geq\)t*)
\(\: \wedge \;\)
(s*\(\leq\)q*.v.s)Letdbe the value returned by functionp.f.
Consider the statements inp.fand two cases.Case 1: All valuesq.vforqinRare identical.In this case, from equation 2:For allqinR:s*\(\leq\)q.v.s.In functionp.f,consensusis assignedq.v.sfor someqinR.Therefore,s*\(\leq\)consensus.The function returnsd = consensus.append(h()).Therefores*\(\leq\)d.Case 2: Not all valuesq.vforqinRare identical.The function returnsq'.v.twhereq'is inRand:For allqinR: \(\quad\)q'.v.t\(\geq\)q.v.tFrom equation 2:q'.v.t\(\geq\)q*.v.t\(\geq\)t*From the induction hypothesis, andq'.v.t\(\geq\)t*, it follows thats*\(\leq\)q'.v.t.Therefores*\(\leq\)d.
This completes the proof of the invariant.We have shown thats*is a prefix ofallassignments made to server variables after iterationn*.
Therefore,s*is a prefix of all consensus values
reached in iterations after iterationn*.The Distributed Algorithm is CorrectWe showedthe equivalence of the
distributed and nondeterministic sequential algorithm: For every
computation \(c\) of the distributed algorithm there exists a
computation \(c'\) of
the nondeterministic sequential algorithm -- with appropriate choices of
nondeterministic selections -- such that for all serversqthe sequence of values ofq.vis the same
in \(c\) and \(c'\).Therefore properties of the sequence of values ofq.vfor
each serverqin the sequential algorithm are also
properties in the distributed algorithm.K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

Next we will prove thats*is a prefix of any assignment
to any server variable in any 
iteration after iterationn*.
This can be written as: at all points in the computation, :At all points in the computation, for all serversq:(q.v.t\(<\)t*) \(\; \vee \;\)
(s*\(\leq\)q.v.s)Proof of the InvariantBase Case:n-th Iteration wheren\(\leq\)n*The condition trivially holds for iterationnwheren\(<\)n*because, from equation 1:q.v.t\(<\)t*.Next, consider the case wheren\(=\)n*.
Ifq.vis modified in the iteration thenq.v.s\(=\)s*.
Ifq.vis not modified in the iteration thenq.v.t\(<\)t*.Induction StepAssume that the condition holds before iterationnforn\(>\)n*and 
show that it holds after the iteration.From equation 1 and the induction hypothesis, the following holds
before iterationn.For allqinW*:(q.v.t\(\geq\)t*)
\(\; \wedge \; \)
(s*\(\leq\)q.v.s)In any iteration, server variables are modified only ifRis a majority of servers.
Any two majorities have at least one element in common.
So there is an
element that is in bothRandW*.
Let this element beq*.
Becauseq*is an element ofW*:(q*.v.t\(\geq\)t*)
\(\: \wedge \;\)
(s*\(\leq\)q*.v.s)This gives us the following:Equation 2There exists aq*inRsuch that:(q*.v.t\(\geq\)t*)
\(\: \wedge \;\)
(s*\(\leq\)q*.v.s)Letdbe the value returned by functionp.f.
Consider the statements inp.fand two cases.Case 1: All valuesq.vforqinRare identical.In this case, from equation 2:For allqinR:s*\(\leq\)q.v.s.In functionp.f,consensusis assignedq.v.sfor someqinR.Therefore,s*\(\leq\)consensus.The function returnsd = consensus.append(h()).Therefores*\(\leq\)d.Case 2: Not all valuesq.vforqinRare identical.The function returnsq'.v.twhereq'is inRand:For allqinR: \(\quad\)q'.v.t\(\geq\)q.v.tFrom equation 2:q'.v.t\(\geq\)q*.v.t\(\geq\)t*From the induction hypothesis, andq'.v.t\(\geq\)t*, it follows thats*\(\leq\)q'.v.t.Therefores*\(\leq\)d.
This completes the proof of the invariant.We have shown thats*is a prefix ofallassignments made to server variables after iterationn*.
Therefore,s*is a prefix of all consensus values
reached in iterations after iterationn*.The Distributed Algorithm is CorrectWe showedthe equivalence of the
distributed and nondeterministic sequential algorithm: For every
computation \(c\) of the distributed algorithm there exists a
computation \(c'\) of
the nondeterministic sequential algorithm -- with appropriate choices of
nondeterministic selections -- such that for all serversqthe sequence of values ofq.vis the same
in \(c\) and \(c'\).Therefore properties of the sequence of values ofq.vfor
each serverqin the sequential algorithm are also
properties in the distributed algorithm.K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

At all points in the computation, for all serversq:(q.v.t\(<\)t*) \(\; \vee \;\)
(s*\(\leq\)q.v.s)Proof of the InvariantBase Case:n-th Iteration wheren\(\leq\)n*The condition trivially holds for iterationnwheren\(<\)n*because, from equation 1:q.v.t\(<\)t*.Next, consider the case wheren\(=\)n*.
Ifq.vis modified in the iteration thenq.v.s\(=\)s*.
Ifq.vis not modified in the iteration thenq.v.t\(<\)t*.Induction StepAssume that the condition holds before iterationnforn\(>\)n*and 
show that it holds after the iteration.From equation 1 and the induction hypothesis, the following holds
before iterationn.For allqinW*:(q.v.t\(\geq\)t*)
\(\; \wedge \; \)
(s*\(\leq\)q.v.s)In any iteration, server variables are modified only ifRis a majority of servers.
Any two majorities have at least one element in common.
So there is an
element that is in bothRandW*.
Let this element beq*.
Becauseq*is an element ofW*:(q*.v.t\(\geq\)t*)
\(\: \wedge \;\)
(s*\(\leq\)q*.v.s)This gives us the following:Equation 2There exists aq*inRsuch that:(q*.v.t\(\geq\)t*)
\(\: \wedge \;\)
(s*\(\leq\)q*.v.s)Letdbe the value returned by functionp.f.
Consider the statements inp.fand two cases.Case 1: All valuesq.vforqinRare identical.In this case, from equation 2:For allqinR:s*\(\leq\)q.v.s.In functionp.f,consensusis assignedq.v.sfor someqinR.Therefore,s*\(\leq\)consensus.The function returnsd = consensus.append(h()).Therefores*\(\leq\)d.Case 2: Not all valuesq.vforqinRare identical.The function returnsq'.v.twhereq'is inRand:For allqinR: \(\quad\)q'.v.t\(\geq\)q.v.tFrom equation 2:q'.v.t\(\geq\)q*.v.t\(\geq\)t*From the induction hypothesis, andq'.v.t\(\geq\)t*, it follows thats*\(\leq\)q'.v.t.Therefores*\(\leq\)d.
This completes the proof of the invariant.We have shown thats*is a prefix ofallassignments made to server variables after iterationn*.
Therefore,s*is a prefix of all consensus values
reached in iterations after iterationn*.The Distributed Algorithm is CorrectWe showedthe equivalence of the
distributed and nondeterministic sequential algorithm: For every
computation \(c\) of the distributed algorithm there exists a
computation \(c'\) of
the nondeterministic sequential algorithm -- with appropriate choices of
nondeterministic selections -- such that for all serversqthe sequence of values ofq.vis the same
in \(c\) and \(c'\).Therefore properties of the sequence of values ofq.vfor
each serverqin the sequential algorithm are also
properties in the distributed algorithm.K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

(q.v.t\(<\)t*) \(\; \vee \;\)
(s*\(\leq\)q.v.s)

Base Case:n-th Iteration wheren\(\leq\)n*

The condition trivially holds for iterationnwheren\(<\)n*because, from equation 1:q.v.t\(<\)t*.Next, consider the case wheren\(=\)n*.
Ifq.vis modified in the iteration thenq.v.s\(=\)s*.
Ifq.vis not modified in the iteration thenq.v.t\(<\)t*.Induction StepAssume that the condition holds before iterationnforn\(>\)n*and 
show that it holds after the iteration.From equation 1 and the induction hypothesis, the following holds
before iterationn.For allqinW*:(q.v.t\(\geq\)t*)
\(\; \wedge \; \)
(s*\(\leq\)q.v.s)In any iteration, server variables are modified only ifRis a majority of servers.
Any two majorities have at least one element in common.
So there is an
element that is in bothRandW*.
Let this element beq*.
Becauseq*is an element ofW*:(q*.v.t\(\geq\)t*)
\(\: \wedge \;\)
(s*\(\leq\)q*.v.s)This gives us the following:Equation 2There exists aq*inRsuch that:(q*.v.t\(\geq\)t*)
\(\: \wedge \;\)
(s*\(\leq\)q*.v.s)Letdbe the value returned by functionp.f.
Consider the statements inp.fand two cases.Case 1: All valuesq.vforqinRare identical.In this case, from equation 2:For allqinR:s*\(\leq\)q.v.s.In functionp.f,consensusis assignedq.v.sfor someqinR.Therefore,s*\(\leq\)consensus.The function returnsd = consensus.append(h()).Therefores*\(\leq\)d.Case 2: Not all valuesq.vforqinRare identical.The function returnsq'.v.twhereq'is inRand:For allqinR: \(\quad\)q'.v.t\(\geq\)q.v.tFrom equation 2:q'.v.t\(\geq\)q*.v.t\(\geq\)t*From the induction hypothesis, andq'.v.t\(\geq\)t*, it follows thats*\(\leq\)q'.v.t.Therefores*\(\leq\)d.
This completes the proof of the invariant.We have shown thats*is a prefix ofallassignments made to server variables after iterationn*.
Therefore,s*is a prefix of all consensus values
reached in iterations after iterationn*.The Distributed Algorithm is CorrectWe showedthe equivalence of the
distributed and nondeterministic sequential algorithm: For every
computation \(c\) of the distributed algorithm there exists a
computation \(c'\) of
the nondeterministic sequential algorithm -- with appropriate choices of
nondeterministic selections -- such that for all serversqthe sequence of values ofq.vis the same
in \(c\) and \(c'\).Therefore properties of the sequence of values ofq.vfor
each serverqin the sequential algorithm are also
properties in the distributed algorithm.K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

Next, consider the case wheren\(=\)n*.
Ifq.vis modified in the iteration thenq.v.s\(=\)s*.
Ifq.vis not modified in the iteration thenq.v.t\(<\)t*.Induction StepAssume that the condition holds before iterationnforn\(>\)n*and 
show that it holds after the iteration.From equation 1 and the induction hypothesis, the following holds
before iterationn.For allqinW*:(q.v.t\(\geq\)t*)
\(\; \wedge \; \)
(s*\(\leq\)q.v.s)In any iteration, server variables are modified only ifRis a majority of servers.
Any two majorities have at least one element in common.
So there is an
element that is in bothRandW*.
Let this element beq*.
Becauseq*is an element ofW*:(q*.v.t\(\geq\)t*)
\(\: \wedge \;\)
(s*\(\leq\)q*.v.s)This gives us the following:Equation 2There exists aq*inRsuch that:(q*.v.t\(\geq\)t*)
\(\: \wedge \;\)
(s*\(\leq\)q*.v.s)Letdbe the value returned by functionp.f.
Consider the statements inp.fand two cases.Case 1: All valuesq.vforqinRare identical.In this case, from equation 2:For allqinR:s*\(\leq\)q.v.s.In functionp.f,consensusis assignedq.v.sfor someqinR.Therefore,s*\(\leq\)consensus.The function returnsd = consensus.append(h()).Therefores*\(\leq\)d.Case 2: Not all valuesq.vforqinRare identical.The function returnsq'.v.twhereq'is inRand:For allqinR: \(\quad\)q'.v.t\(\geq\)q.v.tFrom equation 2:q'.v.t\(\geq\)q*.v.t\(\geq\)t*From the induction hypothesis, andq'.v.t\(\geq\)t*, it follows thats*\(\leq\)q'.v.t.Therefores*\(\leq\)d.
This completes the proof of the invariant.We have shown thats*is a prefix ofallassignments made to server variables after iterationn*.
Therefore,s*is a prefix of all consensus values
reached in iterations after iterationn*.The Distributed Algorithm is CorrectWe showedthe equivalence of the
distributed and nondeterministic sequential algorithm: For every
computation \(c\) of the distributed algorithm there exists a
computation \(c'\) of
the nondeterministic sequential algorithm -- with appropriate choices of
nondeterministic selections -- such that for all serversqthe sequence of values ofq.vis the same
in \(c\) and \(c'\).Therefore properties of the sequence of values ofq.vfor
each serverqin the sequential algorithm are also
properties in the distributed algorithm.K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

Induction Step

From equation 1 and the induction hypothesis, the following holds
before iterationn.For allqinW*:(q.v.t\(\geq\)t*)
\(\; \wedge \; \)
(s*\(\leq\)q.v.s)In any iteration, server variables are modified only ifRis a majority of servers.
Any two majorities have at least one element in common.
So there is an
element that is in bothRandW*.
Let this element beq*.
Becauseq*is an element ofW*:(q*.v.t\(\geq\)t*)
\(\: \wedge \;\)
(s*\(\leq\)q*.v.s)This gives us the following:Equation 2There exists aq*inRsuch that:(q*.v.t\(\geq\)t*)
\(\: \wedge \;\)
(s*\(\leq\)q*.v.s)Letdbe the value returned by functionp.f.
Consider the statements inp.fand two cases.Case 1: All valuesq.vforqinRare identical.In this case, from equation 2:For allqinR:s*\(\leq\)q.v.s.In functionp.f,consensusis assignedq.v.sfor someqinR.Therefore,s*\(\leq\)consensus.The function returnsd = consensus.append(h()).Therefores*\(\leq\)d.Case 2: Not all valuesq.vforqinRare identical.The function returnsq'.v.twhereq'is inRand:For allqinR: \(\quad\)q'.v.t\(\geq\)q.v.tFrom equation 2:q'.v.t\(\geq\)q*.v.t\(\geq\)t*From the induction hypothesis, andq'.v.t\(\geq\)t*, it follows thats*\(\leq\)q'.v.t.Therefores*\(\leq\)d.
This completes the proof of the invariant.We have shown thats*is a prefix ofallassignments made to server variables after iterationn*.
Therefore,s*is a prefix of all consensus values
reached in iterations after iterationn*.The Distributed Algorithm is CorrectWe showedthe equivalence of the
distributed and nondeterministic sequential algorithm: For every
computation \(c\) of the distributed algorithm there exists a
computation \(c'\) of
the nondeterministic sequential algorithm -- with appropriate choices of
nondeterministic selections -- such that for all serversqthe sequence of values ofq.vis the same
in \(c\) and \(c'\).Therefore properties of the sequence of values ofq.vfor
each serverqin the sequential algorithm are also
properties in the distributed algorithm.K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

For allqinW*:(q.v.t\(\geq\)t*)
\(\; \wedge \; \)
(s*\(\leq\)q.v.s)In any iteration, server variables are modified only ifRis a majority of servers.
Any two majorities have at least one element in common.
So there is an
element that is in bothRandW*.
Let this element beq*.
Becauseq*is an element ofW*:(q*.v.t\(\geq\)t*)
\(\: \wedge \;\)
(s*\(\leq\)q*.v.s)This gives us the following:Equation 2There exists aq*inRsuch that:(q*.v.t\(\geq\)t*)
\(\: \wedge \;\)
(s*\(\leq\)q*.v.s)Letdbe the value returned by functionp.f.
Consider the statements inp.fand two cases.Case 1: All valuesq.vforqinRare identical.In this case, from equation 2:For allqinR:s*\(\leq\)q.v.s.In functionp.f,consensusis assignedq.v.sfor someqinR.Therefore,s*\(\leq\)consensus.The function returnsd = consensus.append(h()).Therefores*\(\leq\)d.Case 2: Not all valuesq.vforqinRare identical.The function returnsq'.v.twhereq'is inRand:For allqinR: \(\quad\)q'.v.t\(\geq\)q.v.tFrom equation 2:q'.v.t\(\geq\)q*.v.t\(\geq\)t*From the induction hypothesis, andq'.v.t\(\geq\)t*, it follows thats*\(\leq\)q'.v.t.Therefores*\(\leq\)d.
This completes the proof of the invariant.We have shown thats*is a prefix ofallassignments made to server variables after iterationn*.
Therefore,s*is a prefix of all consensus values
reached in iterations after iterationn*.The Distributed Algorithm is CorrectWe showedthe equivalence of the
distributed and nondeterministic sequential algorithm: For every
computation \(c\) of the distributed algorithm there exists a
computation \(c'\) of
the nondeterministic sequential algorithm -- with appropriate choices of
nondeterministic selections -- such that for all serversqthe sequence of values ofq.vis the same
in \(c\) and \(c'\).Therefore properties of the sequence of values ofq.vfor
each serverqin the sequential algorithm are also
properties in the distributed algorithm.K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

In any iteration, server variables are modified only ifRis a majority of servers.
Any two majorities have at least one element in common.
So there is an
element that is in bothRandW*.
Let this element beq*.
Becauseq*is an element ofW*:(q*.v.t\(\geq\)t*)
\(\: \wedge \;\)
(s*\(\leq\)q*.v.s)This gives us the following:Equation 2There exists aq*inRsuch that:(q*.v.t\(\geq\)t*)
\(\: \wedge \;\)
(s*\(\leq\)q*.v.s)Letdbe the value returned by functionp.f.
Consider the statements inp.fand two cases.Case 1: All valuesq.vforqinRare identical.In this case, from equation 2:For allqinR:s*\(\leq\)q.v.s.In functionp.f,consensusis assignedq.v.sfor someqinR.Therefore,s*\(\leq\)consensus.The function returnsd = consensus.append(h()).Therefores*\(\leq\)d.Case 2: Not all valuesq.vforqinRare identical.The function returnsq'.v.twhereq'is inRand:For allqinR: \(\quad\)q'.v.t\(\geq\)q.v.tFrom equation 2:q'.v.t\(\geq\)q*.v.t\(\geq\)t*From the induction hypothesis, andq'.v.t\(\geq\)t*, it follows thats*\(\leq\)q'.v.t.Therefores*\(\leq\)d.
This completes the proof of the invariant.We have shown thats*is a prefix ofallassignments made to server variables after iterationn*.
Therefore,s*is a prefix of all consensus values
reached in iterations after iterationn*.The Distributed Algorithm is CorrectWe showedthe equivalence of the
distributed and nondeterministic sequential algorithm: For every
computation \(c\) of the distributed algorithm there exists a
computation \(c'\) of
the nondeterministic sequential algorithm -- with appropriate choices of
nondeterministic selections -- such that for all serversqthe sequence of values ofq.vis the same
in \(c\) and \(c'\).Therefore properties of the sequence of values ofq.vfor
each serverqin the sequential algorithm are also
properties in the distributed algorithm.K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

(q*.v.t\(\geq\)t*)
\(\: \wedge \;\)
(s*\(\leq\)q*.v.s)This gives us the following:Equation 2There exists aq*inRsuch that:(q*.v.t\(\geq\)t*)
\(\: \wedge \;\)
(s*\(\leq\)q*.v.s)Letdbe the value returned by functionp.f.
Consider the statements inp.fand two cases.Case 1: All valuesq.vforqinRare identical.In this case, from equation 2:For allqinR:s*\(\leq\)q.v.s.In functionp.f,consensusis assignedq.v.sfor someqinR.Therefore,s*\(\leq\)consensus.The function returnsd = consensus.append(h()).Therefores*\(\leq\)d.Case 2: Not all valuesq.vforqinRare identical.The function returnsq'.v.twhereq'is inRand:For allqinR: \(\quad\)q'.v.t\(\geq\)q.v.tFrom equation 2:q'.v.t\(\geq\)q*.v.t\(\geq\)t*From the induction hypothesis, andq'.v.t\(\geq\)t*, it follows thats*\(\leq\)q'.v.t.Therefores*\(\leq\)d.
This completes the proof of the invariant.We have shown thats*is a prefix ofallassignments made to server variables after iterationn*.
Therefore,s*is a prefix of all consensus values
reached in iterations after iterationn*.The Distributed Algorithm is CorrectWe showedthe equivalence of the
distributed and nondeterministic sequential algorithm: For every
computation \(c\) of the distributed algorithm there exists a
computation \(c'\) of
the nondeterministic sequential algorithm -- with appropriate choices of
nondeterministic selections -- such that for all serversqthe sequence of values ofq.vis the same
in \(c\) and \(c'\).Therefore properties of the sequence of values ofq.vfor
each serverqin the sequential algorithm are also
properties in the distributed algorithm.K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

This gives us the following:Equation 2There exists aq*inRsuch that:(q*.v.t\(\geq\)t*)
\(\: \wedge \;\)
(s*\(\leq\)q*.v.s)Letdbe the value returned by functionp.f.
Consider the statements inp.fand two cases.Case 1: All valuesq.vforqinRare identical.In this case, from equation 2:For allqinR:s*\(\leq\)q.v.s.In functionp.f,consensusis assignedq.v.sfor someqinR.Therefore,s*\(\leq\)consensus.The function returnsd = consensus.append(h()).Therefores*\(\leq\)d.Case 2: Not all valuesq.vforqinRare identical.The function returnsq'.v.twhereq'is inRand:For allqinR: \(\quad\)q'.v.t\(\geq\)q.v.tFrom equation 2:q'.v.t\(\geq\)q*.v.t\(\geq\)t*From the induction hypothesis, andq'.v.t\(\geq\)t*, it follows thats*\(\leq\)q'.v.t.Therefores*\(\leq\)d.
This completes the proof of the invariant.We have shown thats*is a prefix ofallassignments made to server variables after iterationn*.
Therefore,s*is a prefix of all consensus values
reached in iterations after iterationn*.The Distributed Algorithm is CorrectWe showedthe equivalence of the
distributed and nondeterministic sequential algorithm: For every
computation \(c\) of the distributed algorithm there exists a
computation \(c'\) of
the nondeterministic sequential algorithm -- with appropriate choices of
nondeterministic selections -- such that for all serversqthe sequence of values ofq.vis the same
in \(c\) and \(c'\).Therefore properties of the sequence of values ofq.vfor
each serverqin the sequential algorithm are also
properties in the distributed algorithm.K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

Equation 2

(q*.v.t\(\geq\)t*)
\(\: \wedge \;\)
(s*\(\leq\)q*.v.s)Letdbe the value returned by functionp.f.
Consider the statements inp.fand two cases.Case 1: All valuesq.vforqinRare identical.In this case, from equation 2:For allqinR:s*\(\leq\)q.v.s.In functionp.f,consensusis assignedq.v.sfor someqinR.Therefore,s*\(\leq\)consensus.The function returnsd = consensus.append(h()).Therefores*\(\leq\)d.Case 2: Not all valuesq.vforqinRare identical.The function returnsq'.v.twhereq'is inRand:For allqinR: \(\quad\)q'.v.t\(\geq\)q.v.tFrom equation 2:q'.v.t\(\geq\)q*.v.t\(\geq\)t*From the induction hypothesis, andq'.v.t\(\geq\)t*, it follows thats*\(\leq\)q'.v.t.Therefores*\(\leq\)d.
This completes the proof of the invariant.We have shown thats*is a prefix ofallassignments made to server variables after iterationn*.
Therefore,s*is a prefix of all consensus values
reached in iterations after iterationn*.The Distributed Algorithm is CorrectWe showedthe equivalence of the
distributed and nondeterministic sequential algorithm: For every
computation \(c\) of the distributed algorithm there exists a
computation \(c'\) of
the nondeterministic sequential algorithm -- with appropriate choices of
nondeterministic selections -- such that for all serversqthe sequence of values ofq.vis the same
in \(c\) and \(c'\).Therefore properties of the sequence of values ofq.vfor
each serverqin the sequential algorithm are also
properties in the distributed algorithm.K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

Letdbe the value returned by functionp.f.
Consider the statements inp.fand two cases.Case 1: All valuesq.vforqinRare identical.In this case, from equation 2:For allqinR:s*\(\leq\)q.v.s.In functionp.f,consensusis assignedq.v.sfor someqinR.Therefore,s*\(\leq\)consensus.The function returnsd = consensus.append(h()).Therefores*\(\leq\)d.Case 2: Not all valuesq.vforqinRare identical.The function returnsq'.v.twhereq'is inRand:For allqinR: \(\quad\)q'.v.t\(\geq\)q.v.tFrom equation 2:q'.v.t\(\geq\)q*.v.t\(\geq\)t*From the induction hypothesis, andq'.v.t\(\geq\)t*, it follows thats*\(\leq\)q'.v.t.Therefores*\(\leq\)d.
This completes the proof of the invariant.We have shown thats*is a prefix ofallassignments made to server variables after iterationn*.
Therefore,s*is a prefix of all consensus values
reached in iterations after iterationn*.The Distributed Algorithm is CorrectWe showedthe equivalence of the
distributed and nondeterministic sequential algorithm: For every
computation \(c\) of the distributed algorithm there exists a
computation \(c'\) of
the nondeterministic sequential algorithm -- with appropriate choices of
nondeterministic selections -- such that for all serversqthe sequence of values ofq.vis the same
in \(c\) and \(c'\).Therefore properties of the sequence of values ofq.vfor
each serverqin the sequential algorithm are also
properties in the distributed algorithm.K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

Case 1: All valuesq.vforqinRare identical.

In this case, from equation 2:For allqinR:s*\(\leq\)q.v.s.In functionp.f,consensusis assignedq.v.sfor someqinR.Therefore,s*\(\leq\)consensus.The function returnsd = consensus.append(h()).Therefores*\(\leq\)d.Case 2: Not all valuesq.vforqinRare identical.The function returnsq'.v.twhereq'is inRand:For allqinR: \(\quad\)q'.v.t\(\geq\)q.v.tFrom equation 2:q'.v.t\(\geq\)q*.v.t\(\geq\)t*From the induction hypothesis, andq'.v.t\(\geq\)t*, it follows thats*\(\leq\)q'.v.t.Therefores*\(\leq\)d.
This completes the proof of the invariant.We have shown thats*is a prefix ofallassignments made to server variables after iterationn*.
Therefore,s*is a prefix of all consensus values
reached in iterations after iterationn*.The Distributed Algorithm is CorrectWe showedthe equivalence of the
distributed and nondeterministic sequential algorithm: For every
computation \(c\) of the distributed algorithm there exists a
computation \(c'\) of
the nondeterministic sequential algorithm -- with appropriate choices of
nondeterministic selections -- such that for all serversqthe sequence of values ofq.vis the same
in \(c\) and \(c'\).Therefore properties of the sequence of values ofq.vfor
each serverqin the sequential algorithm are also
properties in the distributed algorithm.K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

For allqinR:s*\(\leq\)q.v.s.In functionp.f,consensusis assignedq.v.sfor someqinR.Therefore,s*\(\leq\)consensus.The function returnsd = consensus.append(h()).Therefores*\(\leq\)d.Case 2: Not all valuesq.vforqinRare identical.The function returnsq'.v.twhereq'is inRand:For allqinR: \(\quad\)q'.v.t\(\geq\)q.v.tFrom equation 2:q'.v.t\(\geq\)q*.v.t\(\geq\)t*From the induction hypothesis, andq'.v.t\(\geq\)t*, it follows thats*\(\leq\)q'.v.t.Therefores*\(\leq\)d.
This completes the proof of the invariant.We have shown thats*is a prefix ofallassignments made to server variables after iterationn*.
Therefore,s*is a prefix of all consensus values
reached in iterations after iterationn*.The Distributed Algorithm is CorrectWe showedthe equivalence of the
distributed and nondeterministic sequential algorithm: For every
computation \(c\) of the distributed algorithm there exists a
computation \(c'\) of
the nondeterministic sequential algorithm -- with appropriate choices of
nondeterministic selections -- such that for all serversqthe sequence of values ofq.vis the same
in \(c\) and \(c'\).Therefore properties of the sequence of values ofq.vfor
each serverqin the sequential algorithm are also
properties in the distributed algorithm.K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

In functionp.f,consensusis assignedq.v.sfor someqinR.Therefore,s*\(\leq\)consensus.The function returnsd = consensus.append(h()).Therefores*\(\leq\)d.Case 2: Not all valuesq.vforqinRare identical.The function returnsq'.v.twhereq'is inRand:For allqinR: \(\quad\)q'.v.t\(\geq\)q.v.tFrom equation 2:q'.v.t\(\geq\)q*.v.t\(\geq\)t*From the induction hypothesis, andq'.v.t\(\geq\)t*, it follows thats*\(\leq\)q'.v.t.Therefores*\(\leq\)d.
This completes the proof of the invariant.We have shown thats*is a prefix ofallassignments made to server variables after iterationn*.
Therefore,s*is a prefix of all consensus values
reached in iterations after iterationn*.The Distributed Algorithm is CorrectWe showedthe equivalence of the
distributed and nondeterministic sequential algorithm: For every
computation \(c\) of the distributed algorithm there exists a
computation \(c'\) of
the nondeterministic sequential algorithm -- with appropriate choices of
nondeterministic selections -- such that for all serversqthe sequence of values ofq.vis the same
in \(c\) and \(c'\).Therefore properties of the sequence of values ofq.vfor
each serverqin the sequential algorithm are also
properties in the distributed algorithm.K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

Therefore,s*\(\leq\)consensus.The function returnsd = consensus.append(h()).Therefores*\(\leq\)d.Case 2: Not all valuesq.vforqinRare identical.The function returnsq'.v.twhereq'is inRand:For allqinR: \(\quad\)q'.v.t\(\geq\)q.v.tFrom equation 2:q'.v.t\(\geq\)q*.v.t\(\geq\)t*From the induction hypothesis, andq'.v.t\(\geq\)t*, it follows thats*\(\leq\)q'.v.t.Therefores*\(\leq\)d.
This completes the proof of the invariant.We have shown thats*is a prefix ofallassignments made to server variables after iterationn*.
Therefore,s*is a prefix of all consensus values
reached in iterations after iterationn*.The Distributed Algorithm is CorrectWe showedthe equivalence of the
distributed and nondeterministic sequential algorithm: For every
computation \(c\) of the distributed algorithm there exists a
computation \(c'\) of
the nondeterministic sequential algorithm -- with appropriate choices of
nondeterministic selections -- such that for all serversqthe sequence of values ofq.vis the same
in \(c\) and \(c'\).Therefore properties of the sequence of values ofq.vfor
each serverqin the sequential algorithm are also
properties in the distributed algorithm.K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

The function returnsd = consensus.append(h()).Therefores*\(\leq\)d.Case 2: Not all valuesq.vforqinRare identical.The function returnsq'.v.twhereq'is inRand:For allqinR: \(\quad\)q'.v.t\(\geq\)q.v.tFrom equation 2:q'.v.t\(\geq\)q*.v.t\(\geq\)t*From the induction hypothesis, andq'.v.t\(\geq\)t*, it follows thats*\(\leq\)q'.v.t.Therefores*\(\leq\)d.
This completes the proof of the invariant.We have shown thats*is a prefix ofallassignments made to server variables after iterationn*.
Therefore,s*is a prefix of all consensus values
reached in iterations after iterationn*.The Distributed Algorithm is CorrectWe showedthe equivalence of the
distributed and nondeterministic sequential algorithm: For every
computation \(c\) of the distributed algorithm there exists a
computation \(c'\) of
the nondeterministic sequential algorithm -- with appropriate choices of
nondeterministic selections -- such that for all serversqthe sequence of values ofq.vis the same
in \(c\) and \(c'\).Therefore properties of the sequence of values ofq.vfor
each serverqin the sequential algorithm are also
properties in the distributed algorithm.K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

Therefores*\(\leq\)d.Case 2: Not all valuesq.vforqinRare identical.The function returnsq'.v.twhereq'is inRand:For allqinR: \(\quad\)q'.v.t\(\geq\)q.v.tFrom equation 2:q'.v.t\(\geq\)q*.v.t\(\geq\)t*From the induction hypothesis, andq'.v.t\(\geq\)t*, it follows thats*\(\leq\)q'.v.t.Therefores*\(\leq\)d.
This completes the proof of the invariant.We have shown thats*is a prefix ofallassignments made to server variables after iterationn*.
Therefore,s*is a prefix of all consensus values
reached in iterations after iterationn*.The Distributed Algorithm is CorrectWe showedthe equivalence of the
distributed and nondeterministic sequential algorithm: For every
computation \(c\) of the distributed algorithm there exists a
computation \(c'\) of
the nondeterministic sequential algorithm -- with appropriate choices of
nondeterministic selections -- such that for all serversqthe sequence of values ofq.vis the same
in \(c\) and \(c'\).Therefore properties of the sequence of values ofq.vfor
each serverqin the sequential algorithm are also
properties in the distributed algorithm.K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

Case 2: Not all valuesq.vforqinRare identical.

The function returnsq'.v.twhereq'is inRand:For allqinR: \(\quad\)q'.v.t\(\geq\)q.v.tFrom equation 2:q'.v.t\(\geq\)q*.v.t\(\geq\)t*From the induction hypothesis, andq'.v.t\(\geq\)t*, it follows thats*\(\leq\)q'.v.t.Therefores*\(\leq\)d.
This completes the proof of the invariant.We have shown thats*is a prefix ofallassignments made to server variables after iterationn*.
Therefore,s*is a prefix of all consensus values
reached in iterations after iterationn*.The Distributed Algorithm is CorrectWe showedthe equivalence of the
distributed and nondeterministic sequential algorithm: For every
computation \(c\) of the distributed algorithm there exists a
computation \(c'\) of
the nondeterministic sequential algorithm -- with appropriate choices of
nondeterministic selections -- such that for all serversqthe sequence of values ofq.vis the same
in \(c\) and \(c'\).Therefore properties of the sequence of values ofq.vfor
each serverqin the sequential algorithm are also
properties in the distributed algorithm.K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

For allqinR: \(\quad\)q'.v.t\(\geq\)q.v.tFrom equation 2:q'.v.t\(\geq\)q*.v.t\(\geq\)t*From the induction hypothesis, andq'.v.t\(\geq\)t*, it follows thats*\(\leq\)q'.v.t.Therefores*\(\leq\)d.
This completes the proof of the invariant.We have shown thats*is a prefix ofallassignments made to server variables after iterationn*.
Therefore,s*is a prefix of all consensus values
reached in iterations after iterationn*.The Distributed Algorithm is CorrectWe showedthe equivalence of the
distributed and nondeterministic sequential algorithm: For every
computation \(c\) of the distributed algorithm there exists a
computation \(c'\) of
the nondeterministic sequential algorithm -- with appropriate choices of
nondeterministic selections -- such that for all serversqthe sequence of values ofq.vis the same
in \(c\) and \(c'\).Therefore properties of the sequence of values ofq.vfor
each serverqin the sequential algorithm are also
properties in the distributed algorithm.K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

From equation 2:q'.v.t\(\geq\)q*.v.t\(\geq\)t*From the induction hypothesis, andq'.v.t\(\geq\)t*, it follows thats*\(\leq\)q'.v.t.Therefores*\(\leq\)d.
This completes the proof of the invariant.We have shown thats*is a prefix ofallassignments made to server variables after iterationn*.
Therefore,s*is a prefix of all consensus values
reached in iterations after iterationn*.The Distributed Algorithm is CorrectWe showedthe equivalence of the
distributed and nondeterministic sequential algorithm: For every
computation \(c\) of the distributed algorithm there exists a
computation \(c'\) of
the nondeterministic sequential algorithm -- with appropriate choices of
nondeterministic selections -- such that for all serversqthe sequence of values ofq.vis the same
in \(c\) and \(c'\).Therefore properties of the sequence of values ofq.vfor
each serverqin the sequential algorithm are also
properties in the distributed algorithm.K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

From the induction hypothesis, andq'.v.t\(\geq\)t*, it follows thats*\(\leq\)q'.v.t.Therefores*\(\leq\)d.
This completes the proof of the invariant.We have shown thats*is a prefix ofallassignments made to server variables after iterationn*.
Therefore,s*is a prefix of all consensus values
reached in iterations after iterationn*.The Distributed Algorithm is CorrectWe showedthe equivalence of the
distributed and nondeterministic sequential algorithm: For every
computation \(c\) of the distributed algorithm there exists a
computation \(c'\) of
the nondeterministic sequential algorithm -- with appropriate choices of
nondeterministic selections -- such that for all serversqthe sequence of values ofq.vis the same
in \(c\) and \(c'\).Therefore properties of the sequence of values ofq.vfor
each serverqin the sequential algorithm are also
properties in the distributed algorithm.K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

Therefores*\(\leq\)d.
This completes the proof of the invariant.We have shown thats*is a prefix ofallassignments made to server variables after iterationn*.
Therefore,s*is a prefix of all consensus values
reached in iterations after iterationn*.The Distributed Algorithm is CorrectWe showedthe equivalence of the
distributed and nondeterministic sequential algorithm: For every
computation \(c\) of the distributed algorithm there exists a
computation \(c'\) of
the nondeterministic sequential algorithm -- with appropriate choices of
nondeterministic selections -- such that for all serversqthe sequence of values ofq.vis the same
in \(c\) and \(c'\).Therefore properties of the sequence of values ofq.vfor
each serverqin the sequential algorithm are also
properties in the distributed algorithm.K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

We have shown thats*is a prefix ofallassignments made to server variables after iterationn*.
Therefore,s*is a prefix of all consensus values
reached in iterations after iterationn*.The Distributed Algorithm is CorrectWe showedthe equivalence of the
distributed and nondeterministic sequential algorithm: For every
computation \(c\) of the distributed algorithm there exists a
computation \(c'\) of
the nondeterministic sequential algorithm -- with appropriate choices of
nondeterministic selections -- such that for all serversqthe sequence of values ofq.vis the same
in \(c\) and \(c'\).Therefore properties of the sequence of values ofq.vfor
each serverqin the sequential algorithm are also
properties in the distributed algorithm.K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

Therefore properties of the sequence of values ofq.vfor
each serverqin the sequential algorithm are also
properties in the distributed algorithm.K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology
--- END Content from https://kmchandy.github.io/Paxos/StableMajority.html ---


--- START Content from https://kmchandy.github.io/SelfStabilization/SelfStabilization.html ---
TITLE: SelfStabilization/SelfStabilization.html

The literature on self stabilization is extensive. Let's look at
    one example of a self-stabilizing system to get an idea of its
    design.Self Stabilizing Token PassingA ring of agents passes a single token around the ring. An
    agent that holds the token knows that no other agent has the token
    at that point. So, the system can be used to implement mutual
    exclusion.Examples of errors are the disappearance of the single token and
    the creation of additional tokens. A self stabilizing algorithm
    ensures that eventually the system gets back to a safe state,
    i.e., one in which it has exactly one token.Let's begin with a model in which each agent in the ring can read
    the state of its predecessor in the ring. Later, we will modify the
    algorithm to work with message-passing.\(N\) agents, indexed \(j\), are organized in a ring where agent
    \((j+1) \: \textrm{mod} \: N\) can read the state of agent
    \(j\). Hereafter, we will not write "\(\textrm{mod} \; N\);" it is
    to be understood.An agent is eitheridleoractive. The system is
    required to have exactly one active process. The active process
    can be thought of as having the single token in the system.  The
    token is passed from agent \(j\) to agent \(j+1\) when agent \(j\)
    becomes idle and agent \(j+1\) becomes active.For convenience in visualizing diagrams, let's assume that the
    state of an agent is a color. Next we describe the basic algorithm
    that assumes that errors do not occur; later we will modify the
    algorithm to obtain a self-stabilizing algorithm that recover from
    errors.The AlgorithmThe algorithm for agent \(0\) is different from that of
    the other agents. Agent \(0\) has the token exactly when its color
    isthe sameas that of its predecessor. Any other agent holds
    the token exactly when its color isdifferentfrom that of its
predecessor.An agent \(j\) passes the token to
agent \(j+1\) when agent \(j\) changes its color.
Agent \(0\) has the token when all the agents
in the ring have the same color.The diagram below illustrates an example with 4 agents where an
agent's color is either red or blue.
The diagram shows how the token is passed from
each agent to its sucessor.In the figure on the top left, all the agents are red; so agent 0 has
the token. When agent 0 changes its color we get the diagram at the
top center. In this diagram, agent 0 is blue and all the other agents
are red. Agent 0 no longer has the token because its color is
different from that of its predecessor; however agent 1 does have the token
because its color is different from that of its predecessor. The
sequence of diagrams shows what happens when the agent holding the
token --- which is the only active agent --- changes its color.Fig.1: The token is passed by an agent changing colorFaultsLet's look at the state of a system after a fault occurs.  The next
set of diagrams shows how errors --- once they occur --- can propagate
for ever.  These diagrams show a system with three tokens whereas an
error-free system should have exactly one.  The three agents holding
tokens are shown with large yellow numbers and the agent that does not
hold a token is shown with a smaller black number.Fig.2: Errors can propagate foreverThe figure on the top left of the above diagram shows agents 1, 2 and
3 with tokens because their colors are different from those of their
predecessors. The next diagram, top right, shows agents 1 and 3 with
tokens because their colors are different from those of their
predecessors, and agent 0 with a token because its color is the same
as that of its predecessor. The transition to the diagram on the top
right from the one on the top left occurs when agent 3 changes its
color.The sequence of state transitions gets the system to the figure on the
bottom left which is the same as that on the top left with the colors
reversed. This cycle of state transitions can repeat forever, with the
same system always having three tokens. So, this system is not self
stabilizing.A Self-Stabilizing AlgorithmThe solution: add more colors!We will modify the design to have as many colors as there are
agents. In our example, we will have 4 colors because it has 4
agents. The algorithms for all agents, other than agent 0, remains
unchanged. As before agent 0 has the
token when its color is the same as that of its predecessor and agent 0
sends the token by changing agent 0's color. The difference in the
self-stabilizing algorithm is the color to which agent 0 transits.Assume that the 4 colors are numbered 0, 1, 2, 3. If agent 0's color
is \(k\) it makes a transition by changing its color to \(k \:
\textrm{mod} \: N\).The diagram below gives an example of how agent 0's color changes. Its
sequence of colors, 0, 1, 2, 3 are red, blue, green, yellow,
respectively. The diagram on the top left shows a configuration in
which agent 0 holds a token because its color (green --- number 2) is
the same as that of its predecessor. Agent 0 passes the token by
changing its color to 3 (yellow), as show on the diagram on the top
right.The diagram in the middle left shows agent 0 with a token. It passes
the token by changing its color from 1 (blue) to 2 (green), as shown
in the diagram in the middle right.Fig.3: Changes in color of agent 0ProofThe proof has the following three ideas that we first describe
informally.In all states at least one agent holds a token.All trajectories from all states lead to a state in which agent 0
  holds a token.A trajectory from a system state in which agent 0's color is
  different from that of the other agents leads to a state in which
  all agents have the same color (in 
  which case the system is in a safe state).Part 1If all agents have the same color then agent 0 holds a token.
If there is more than one color in the ring then there is at least one
agent, other than agent 0, whose color is different from that of its
predecessor's; so, that agent holds a token.Part 2Agent 0 and agent \(n-1\) will
get the same color at some point because 
agent 0's color will propagate all the way around the ring unless it
gets to agent \(n-1\) sooner.Part 3If agent 0's color is different from the colors of agents \(1, \ldots,
n-1\) then the only
trajectory that leads to agents 0 and agent \(n-1\) having the same
color is for agent 0's color to propagate all the way around the ring.The system always reaches a safe stateLet \(S\) be any state. Let the \(C\) be the set of agent colors in state
\(S\). If \(C\) has all \(N\) colors, then the color of agent \(0\) is
different from the colors of the other agents, and so the result
follows from part 3.If \(C\) has fewer than \(N\) colors then the color of each agent
remains a color in \(C\) until agent 0 gets a color that is not in
\(C\). At this point agent 0's color is different from that of the
other agents and the result follows.ReviewIn this algorithm, does any agent detect that the system is in an
  unsafe state? Can we determine that the system is in an unsafe state
  based solely on the state of any one agent and the state of its
  predecessor? Why?Will this algorithm work if the number of colors is arbitrarily
  large, and more than the number of agents? Why?Will this algorithm work if the number of colors is less than the
  number of agents? Why?K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology



Figure Link: https://kmchandy.github.io/SelfStabilization/Slide01.jpg

Caption: Fig.1: The token is passed by an agent changing color


Examples of errors are the disappearance of the single token and
    the creation of additional tokens. A self stabilizing algorithm
    ensures that eventually the system gets back to a safe state,
    i.e., one in which it has exactly one token.Let's begin with a model in which each agent in the ring can read
    the state of its predecessor in the ring. Later, we will modify the
    algorithm to work with message-passing.\(N\) agents, indexed \(j\), are organized in a ring where agent
    \((j+1) \: \textrm{mod} \: N\) can read the state of agent
    \(j\). Hereafter, we will not write "\(\textrm{mod} \; N\);" it is
    to be understood.An agent is eitheridleoractive. The system is
    required to have exactly one active process. The active process
    can be thought of as having the single token in the system.  The
    token is passed from agent \(j\) to agent \(j+1\) when agent \(j\)
    becomes idle and agent \(j+1\) becomes active.For convenience in visualizing diagrams, let's assume that the
    state of an agent is a color. Next we describe the basic algorithm
    that assumes that errors do not occur; later we will modify the
    algorithm to obtain a self-stabilizing algorithm that recover from
    errors.The AlgorithmThe algorithm for agent \(0\) is different from that of
    the other agents. Agent \(0\) has the token exactly when its color
    isthe sameas that of its predecessor. Any other agent holds
    the token exactly when its color isdifferentfrom that of its
predecessor.An agent \(j\) passes the token to
agent \(j+1\) when agent \(j\) changes its color.
Agent \(0\) has the token when all the agents
in the ring have the same color.The diagram below illustrates an example with 4 agents where an
agent's color is either red or blue.
The diagram shows how the token is passed from
each agent to its sucessor.In the figure on the top left, all the agents are red; so agent 0 has
the token. When agent 0 changes its color we get the diagram at the
top center. In this diagram, agent 0 is blue and all the other agents
are red. Agent 0 no longer has the token because its color is
different from that of its predecessor; however agent 1 does have the token
because its color is different from that of its predecessor. The
sequence of diagrams shows what happens when the agent holding the
token --- which is the only active agent --- changes its color.Fig.1: The token is passed by an agent changing colorFaultsLet's look at the state of a system after a fault occurs.  The next
set of diagrams shows how errors --- once they occur --- can propagate
for ever.  These diagrams show a system with three tokens whereas an
error-free system should have exactly one.  The three agents holding
tokens are shown with large yellow numbers and the agent that does not
hold a token is shown with a smaller black number.Fig.2: Errors can propagate foreverThe figure on the top left of the above diagram shows agents 1, 2 and
3 with tokens because their colors are different from those of their
predecessors. The next diagram, top right, shows agents 1 and 3 with
tokens because their colors are different from those of their
predecessors, and agent 0 with a token because its color is the same
as that of its predecessor. The transition to the diagram on the top
right from the one on the top left occurs when agent 3 changes its
color.The sequence of state transitions gets the system to the figure on the
bottom left which is the same as that on the top left with the colors
reversed. This cycle of state transitions can repeat forever, with the
same system always having three tokens. So, this system is not self
stabilizing.A Self-Stabilizing AlgorithmThe solution: add more colors!We will modify the design to have as many colors as there are
agents. In our example, we will have 4 colors because it has 4
agents. The algorithms for all agents, other than agent 0, remains
unchanged. As before agent 0 has the
token when its color is the same as that of its predecessor and agent 0
sends the token by changing agent 0's color. The difference in the
self-stabilizing algorithm is the color to which agent 0 transits.Assume that the 4 colors are numbered 0, 1, 2, 3. If agent 0's color
is \(k\) it makes a transition by changing its color to \(k \:
\textrm{mod} \: N\).The diagram below gives an example of how agent 0's color changes. Its
sequence of colors, 0, 1, 2, 3 are red, blue, green, yellow,
respectively. The diagram on the top left shows a configuration in
which agent 0 holds a token because its color (green --- number 2) is
the same as that of its predecessor. Agent 0 passes the token by
changing its color to 3 (yellow), as show on the diagram on the top
right.The diagram in the middle left shows agent 0 with a token. It passes
the token by changing its color from 1 (blue) to 2 (green), as shown
in the diagram in the middle right.Fig.3: Changes in color of agent 0ProofThe proof has the following three ideas that we first describe
informally.In all states at least one agent holds a token.All trajectories from all states lead to a state in which agent 0
  holds a token.A trajectory from a system state in which agent 0's color is
  different from that of the other agents leads to a state in which
  all agents have the same color (in 
  which case the system is in a safe state).Part 1If all agents have the same color then agent 0 holds a token.
If there is more than one color in the ring then there is at least one
agent, other than agent 0, whose color is different from that of its
predecessor's; so, that agent holds a token.Part 2Agent 0 and agent \(n-1\) will
get the same color at some point because 
agent 0's color will propagate all the way around the ring unless it
gets to agent \(n-1\) sooner.Part 3If agent 0's color is different from the colors of agents \(1, \ldots,
n-1\) then the only
trajectory that leads to agents 0 and agent \(n-1\) having the same
color is for agent 0's color to propagate all the way around the ring.The system always reaches a safe stateLet \(S\) be any state. Let the \(C\) be the set of agent colors in state
\(S\). If \(C\) has all \(N\) colors, then the color of agent \(0\) is
different from the colors of the other agents, and so the result
follows from part 3.If \(C\) has fewer than \(N\) colors then the color of each agent
remains a color in \(C\) until agent 0 gets a color that is not in
\(C\). At this point agent 0's color is different from that of the
other agents and the result follows.ReviewIn this algorithm, does any agent detect that the system is in an
  unsafe state? Can we determine that the system is in an unsafe state
  based solely on the state of any one agent and the state of its
  predecessor? Why?Will this algorithm work if the number of colors is arbitrarily
  large, and more than the number of agents? Why?Will this algorithm work if the number of colors is less than the
  number of agents? Why?K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology



Figure Link: https://kmchandy.github.io/SelfStabilization/Slide01.jpg

Caption: Fig.1: The token is passed by an agent changing color


Let's begin with a model in which each agent in the ring can read
    the state of its predecessor in the ring. Later, we will modify the
    algorithm to work with message-passing.\(N\) agents, indexed \(j\), are organized in a ring where agent
    \((j+1) \: \textrm{mod} \: N\) can read the state of agent
    \(j\). Hereafter, we will not write "\(\textrm{mod} \; N\);" it is
    to be understood.An agent is eitheridleoractive. The system is
    required to have exactly one active process. The active process
    can be thought of as having the single token in the system.  The
    token is passed from agent \(j\) to agent \(j+1\) when agent \(j\)
    becomes idle and agent \(j+1\) becomes active.For convenience in visualizing diagrams, let's assume that the
    state of an agent is a color. Next we describe the basic algorithm
    that assumes that errors do not occur; later we will modify the
    algorithm to obtain a self-stabilizing algorithm that recover from
    errors.The AlgorithmThe algorithm for agent \(0\) is different from that of
    the other agents. Agent \(0\) has the token exactly when its color
    isthe sameas that of its predecessor. Any other agent holds
    the token exactly when its color isdifferentfrom that of its
predecessor.An agent \(j\) passes the token to
agent \(j+1\) when agent \(j\) changes its color.
Agent \(0\) has the token when all the agents
in the ring have the same color.The diagram below illustrates an example with 4 agents where an
agent's color is either red or blue.
The diagram shows how the token is passed from
each agent to its sucessor.In the figure on the top left, all the agents are red; so agent 0 has
the token. When agent 0 changes its color we get the diagram at the
top center. In this diagram, agent 0 is blue and all the other agents
are red. Agent 0 no longer has the token because its color is
different from that of its predecessor; however agent 1 does have the token
because its color is different from that of its predecessor. The
sequence of diagrams shows what happens when the agent holding the
token --- which is the only active agent --- changes its color.Fig.1: The token is passed by an agent changing colorFaultsLet's look at the state of a system after a fault occurs.  The next
set of diagrams shows how errors --- once they occur --- can propagate
for ever.  These diagrams show a system with three tokens whereas an
error-free system should have exactly one.  The three agents holding
tokens are shown with large yellow numbers and the agent that does not
hold a token is shown with a smaller black number.Fig.2: Errors can propagate foreverThe figure on the top left of the above diagram shows agents 1, 2 and
3 with tokens because their colors are different from those of their
predecessors. The next diagram, top right, shows agents 1 and 3 with
tokens because their colors are different from those of their
predecessors, and agent 0 with a token because its color is the same
as that of its predecessor. The transition to the diagram on the top
right from the one on the top left occurs when agent 3 changes its
color.The sequence of state transitions gets the system to the figure on the
bottom left which is the same as that on the top left with the colors
reversed. This cycle of state transitions can repeat forever, with the
same system always having three tokens. So, this system is not self
stabilizing.A Self-Stabilizing AlgorithmThe solution: add more colors!We will modify the design to have as many colors as there are
agents. In our example, we will have 4 colors because it has 4
agents. The algorithms for all agents, other than agent 0, remains
unchanged. As before agent 0 has the
token when its color is the same as that of its predecessor and agent 0
sends the token by changing agent 0's color. The difference in the
self-stabilizing algorithm is the color to which agent 0 transits.Assume that the 4 colors are numbered 0, 1, 2, 3. If agent 0's color
is \(k\) it makes a transition by changing its color to \(k \:
\textrm{mod} \: N\).The diagram below gives an example of how agent 0's color changes. Its
sequence of colors, 0, 1, 2, 3 are red, blue, green, yellow,
respectively. The diagram on the top left shows a configuration in
which agent 0 holds a token because its color (green --- number 2) is
the same as that of its predecessor. Agent 0 passes the token by
changing its color to 3 (yellow), as show on the diagram on the top
right.The diagram in the middle left shows agent 0 with a token. It passes
the token by changing its color from 1 (blue) to 2 (green), as shown
in the diagram in the middle right.Fig.3: Changes in color of agent 0ProofThe proof has the following three ideas that we first describe
informally.In all states at least one agent holds a token.All trajectories from all states lead to a state in which agent 0
  holds a token.A trajectory from a system state in which agent 0's color is
  different from that of the other agents leads to a state in which
  all agents have the same color (in 
  which case the system is in a safe state).Part 1If all agents have the same color then agent 0 holds a token.
If there is more than one color in the ring then there is at least one
agent, other than agent 0, whose color is different from that of its
predecessor's; so, that agent holds a token.Part 2Agent 0 and agent \(n-1\) will
get the same color at some point because 
agent 0's color will propagate all the way around the ring unless it
gets to agent \(n-1\) sooner.Part 3If agent 0's color is different from the colors of agents \(1, \ldots,
n-1\) then the only
trajectory that leads to agents 0 and agent \(n-1\) having the same
color is for agent 0's color to propagate all the way around the ring.The system always reaches a safe stateLet \(S\) be any state. Let the \(C\) be the set of agent colors in state
\(S\). If \(C\) has all \(N\) colors, then the color of agent \(0\) is
different from the colors of the other agents, and so the result
follows from part 3.If \(C\) has fewer than \(N\) colors then the color of each agent
remains a color in \(C\) until agent 0 gets a color that is not in
\(C\). At this point agent 0's color is different from that of the
other agents and the result follows.ReviewIn this algorithm, does any agent detect that the system is in an
  unsafe state? Can we determine that the system is in an unsafe state
  based solely on the state of any one agent and the state of its
  predecessor? Why?Will this algorithm work if the number of colors is arbitrarily
  large, and more than the number of agents? Why?Will this algorithm work if the number of colors is less than the
  number of agents? Why?K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology



Figure Link: https://kmchandy.github.io/SelfStabilization/Slide01.jpg

Caption: Fig.1: The token is passed by an agent changing color


\(N\) agents, indexed \(j\), are organized in a ring where agent
    \((j+1) \: \textrm{mod} \: N\) can read the state of agent
    \(j\). Hereafter, we will not write "\(\textrm{mod} \; N\);" it is
    to be understood.An agent is eitheridleoractive. The system is
    required to have exactly one active process. The active process
    can be thought of as having the single token in the system.  The
    token is passed from agent \(j\) to agent \(j+1\) when agent \(j\)
    becomes idle and agent \(j+1\) becomes active.For convenience in visualizing diagrams, let's assume that the
    state of an agent is a color. Next we describe the basic algorithm
    that assumes that errors do not occur; later we will modify the
    algorithm to obtain a self-stabilizing algorithm that recover from
    errors.The AlgorithmThe algorithm for agent \(0\) is different from that of
    the other agents. Agent \(0\) has the token exactly when its color
    isthe sameas that of its predecessor. Any other agent holds
    the token exactly when its color isdifferentfrom that of its
predecessor.An agent \(j\) passes the token to
agent \(j+1\) when agent \(j\) changes its color.
Agent \(0\) has the token when all the agents
in the ring have the same color.The diagram below illustrates an example with 4 agents where an
agent's color is either red or blue.
The diagram shows how the token is passed from
each agent to its sucessor.In the figure on the top left, all the agents are red; so agent 0 has
the token. When agent 0 changes its color we get the diagram at the
top center. In this diagram, agent 0 is blue and all the other agents
are red. Agent 0 no longer has the token because its color is
different from that of its predecessor; however agent 1 does have the token
because its color is different from that of its predecessor. The
sequence of diagrams shows what happens when the agent holding the
token --- which is the only active agent --- changes its color.Fig.1: The token is passed by an agent changing colorFaultsLet's look at the state of a system after a fault occurs.  The next
set of diagrams shows how errors --- once they occur --- can propagate
for ever.  These diagrams show a system with three tokens whereas an
error-free system should have exactly one.  The three agents holding
tokens are shown with large yellow numbers and the agent that does not
hold a token is shown with a smaller black number.Fig.2: Errors can propagate foreverThe figure on the top left of the above diagram shows agents 1, 2 and
3 with tokens because their colors are different from those of their
predecessors. The next diagram, top right, shows agents 1 and 3 with
tokens because their colors are different from those of their
predecessors, and agent 0 with a token because its color is the same
as that of its predecessor. The transition to the diagram on the top
right from the one on the top left occurs when agent 3 changes its
color.The sequence of state transitions gets the system to the figure on the
bottom left which is the same as that on the top left with the colors
reversed. This cycle of state transitions can repeat forever, with the
same system always having three tokens. So, this system is not self
stabilizing.A Self-Stabilizing AlgorithmThe solution: add more colors!We will modify the design to have as many colors as there are
agents. In our example, we will have 4 colors because it has 4
agents. The algorithms for all agents, other than agent 0, remains
unchanged. As before agent 0 has the
token when its color is the same as that of its predecessor and agent 0
sends the token by changing agent 0's color. The difference in the
self-stabilizing algorithm is the color to which agent 0 transits.Assume that the 4 colors are numbered 0, 1, 2, 3. If agent 0's color
is \(k\) it makes a transition by changing its color to \(k \:
\textrm{mod} \: N\).The diagram below gives an example of how agent 0's color changes. Its
sequence of colors, 0, 1, 2, 3 are red, blue, green, yellow,
respectively. The diagram on the top left shows a configuration in
which agent 0 holds a token because its color (green --- number 2) is
the same as that of its predecessor. Agent 0 passes the token by
changing its color to 3 (yellow), as show on the diagram on the top
right.The diagram in the middle left shows agent 0 with a token. It passes
the token by changing its color from 1 (blue) to 2 (green), as shown
in the diagram in the middle right.Fig.3: Changes in color of agent 0ProofThe proof has the following three ideas that we first describe
informally.In all states at least one agent holds a token.All trajectories from all states lead to a state in which agent 0
  holds a token.A trajectory from a system state in which agent 0's color is
  different from that of the other agents leads to a state in which
  all agents have the same color (in 
  which case the system is in a safe state).Part 1If all agents have the same color then agent 0 holds a token.
If there is more than one color in the ring then there is at least one
agent, other than agent 0, whose color is different from that of its
predecessor's; so, that agent holds a token.Part 2Agent 0 and agent \(n-1\) will
get the same color at some point because 
agent 0's color will propagate all the way around the ring unless it
gets to agent \(n-1\) sooner.Part 3If agent 0's color is different from the colors of agents \(1, \ldots,
n-1\) then the only
trajectory that leads to agents 0 and agent \(n-1\) having the same
color is for agent 0's color to propagate all the way around the ring.The system always reaches a safe stateLet \(S\) be any state. Let the \(C\) be the set of agent colors in state
\(S\). If \(C\) has all \(N\) colors, then the color of agent \(0\) is
different from the colors of the other agents, and so the result
follows from part 3.If \(C\) has fewer than \(N\) colors then the color of each agent
remains a color in \(C\) until agent 0 gets a color that is not in
\(C\). At this point agent 0's color is different from that of the
other agents and the result follows.ReviewIn this algorithm, does any agent detect that the system is in an
  unsafe state? Can we determine that the system is in an unsafe state
  based solely on the state of any one agent and the state of its
  predecessor? Why?Will this algorithm work if the number of colors is arbitrarily
  large, and more than the number of agents? Why?Will this algorithm work if the number of colors is less than the
  number of agents? Why?K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology



Figure Link: https://kmchandy.github.io/SelfStabilization/Slide01.jpg

Caption: Fig.1: The token is passed by an agent changing color


An agent is eitheridleoractive. The system is
    required to have exactly one active process. The active process
    can be thought of as having the single token in the system.  The
    token is passed from agent \(j\) to agent \(j+1\) when agent \(j\)
    becomes idle and agent \(j+1\) becomes active.For convenience in visualizing diagrams, let's assume that the
    state of an agent is a color. Next we describe the basic algorithm
    that assumes that errors do not occur; later we will modify the
    algorithm to obtain a self-stabilizing algorithm that recover from
    errors.The AlgorithmThe algorithm for agent \(0\) is different from that of
    the other agents. Agent \(0\) has the token exactly when its color
    isthe sameas that of its predecessor. Any other agent holds
    the token exactly when its color isdifferentfrom that of its
predecessor.An agent \(j\) passes the token to
agent \(j+1\) when agent \(j\) changes its color.
Agent \(0\) has the token when all the agents
in the ring have the same color.The diagram below illustrates an example with 4 agents where an
agent's color is either red or blue.
The diagram shows how the token is passed from
each agent to its sucessor.In the figure on the top left, all the agents are red; so agent 0 has
the token. When agent 0 changes its color we get the diagram at the
top center. In this diagram, agent 0 is blue and all the other agents
are red. Agent 0 no longer has the token because its color is
different from that of its predecessor; however agent 1 does have the token
because its color is different from that of its predecessor. The
sequence of diagrams shows what happens when the agent holding the
token --- which is the only active agent --- changes its color.Fig.1: The token is passed by an agent changing colorFaultsLet's look at the state of a system after a fault occurs.  The next
set of diagrams shows how errors --- once they occur --- can propagate
for ever.  These diagrams show a system with three tokens whereas an
error-free system should have exactly one.  The three agents holding
tokens are shown with large yellow numbers and the agent that does not
hold a token is shown with a smaller black number.Fig.2: Errors can propagate foreverThe figure on the top left of the above diagram shows agents 1, 2 and
3 with tokens because their colors are different from those of their
predecessors. The next diagram, top right, shows agents 1 and 3 with
tokens because their colors are different from those of their
predecessors, and agent 0 with a token because its color is the same
as that of its predecessor. The transition to the diagram on the top
right from the one on the top left occurs when agent 3 changes its
color.The sequence of state transitions gets the system to the figure on the
bottom left which is the same as that on the top left with the colors
reversed. This cycle of state transitions can repeat forever, with the
same system always having three tokens. So, this system is not self
stabilizing.A Self-Stabilizing AlgorithmThe solution: add more colors!We will modify the design to have as many colors as there are
agents. In our example, we will have 4 colors because it has 4
agents. The algorithms for all agents, other than agent 0, remains
unchanged. As before agent 0 has the
token when its color is the same as that of its predecessor and agent 0
sends the token by changing agent 0's color. The difference in the
self-stabilizing algorithm is the color to which agent 0 transits.Assume that the 4 colors are numbered 0, 1, 2, 3. If agent 0's color
is \(k\) it makes a transition by changing its color to \(k \:
\textrm{mod} \: N\).The diagram below gives an example of how agent 0's color changes. Its
sequence of colors, 0, 1, 2, 3 are red, blue, green, yellow,
respectively. The diagram on the top left shows a configuration in
which agent 0 holds a token because its color (green --- number 2) is
the same as that of its predecessor. Agent 0 passes the token by
changing its color to 3 (yellow), as show on the diagram on the top
right.The diagram in the middle left shows agent 0 with a token. It passes
the token by changing its color from 1 (blue) to 2 (green), as shown
in the diagram in the middle right.Fig.3: Changes in color of agent 0ProofThe proof has the following three ideas that we first describe
informally.In all states at least one agent holds a token.All trajectories from all states lead to a state in which agent 0
  holds a token.A trajectory from a system state in which agent 0's color is
  different from that of the other agents leads to a state in which
  all agents have the same color (in 
  which case the system is in a safe state).Part 1If all agents have the same color then agent 0 holds a token.
If there is more than one color in the ring then there is at least one
agent, other than agent 0, whose color is different from that of its
predecessor's; so, that agent holds a token.Part 2Agent 0 and agent \(n-1\) will
get the same color at some point because 
agent 0's color will propagate all the way around the ring unless it
gets to agent \(n-1\) sooner.Part 3If agent 0's color is different from the colors of agents \(1, \ldots,
n-1\) then the only
trajectory that leads to agents 0 and agent \(n-1\) having the same
color is for agent 0's color to propagate all the way around the ring.The system always reaches a safe stateLet \(S\) be any state. Let the \(C\) be the set of agent colors in state
\(S\). If \(C\) has all \(N\) colors, then the color of agent \(0\) is
different from the colors of the other agents, and so the result
follows from part 3.If \(C\) has fewer than \(N\) colors then the color of each agent
remains a color in \(C\) until agent 0 gets a color that is not in
\(C\). At this point agent 0's color is different from that of the
other agents and the result follows.ReviewIn this algorithm, does any agent detect that the system is in an
  unsafe state? Can we determine that the system is in an unsafe state
  based solely on the state of any one agent and the state of its
  predecessor? Why?Will this algorithm work if the number of colors is arbitrarily
  large, and more than the number of agents? Why?Will this algorithm work if the number of colors is less than the
  number of agents? Why?K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology



Figure Link: https://kmchandy.github.io/SelfStabilization/Slide01.jpg

Caption: Fig.1: The token is passed by an agent changing color


For convenience in visualizing diagrams, let's assume that the
    state of an agent is a color. Next we describe the basic algorithm
    that assumes that errors do not occur; later we will modify the
    algorithm to obtain a self-stabilizing algorithm that recover from
    errors.The AlgorithmThe algorithm for agent \(0\) is different from that of
    the other agents. Agent \(0\) has the token exactly when its color
    isthe sameas that of its predecessor. Any other agent holds
    the token exactly when its color isdifferentfrom that of its
predecessor.An agent \(j\) passes the token to
agent \(j+1\) when agent \(j\) changes its color.
Agent \(0\) has the token when all the agents
in the ring have the same color.The diagram below illustrates an example with 4 agents where an
agent's color is either red or blue.
The diagram shows how the token is passed from
each agent to its sucessor.In the figure on the top left, all the agents are red; so agent 0 has
the token. When agent 0 changes its color we get the diagram at the
top center. In this diagram, agent 0 is blue and all the other agents
are red. Agent 0 no longer has the token because its color is
different from that of its predecessor; however agent 1 does have the token
because its color is different from that of its predecessor. The
sequence of diagrams shows what happens when the agent holding the
token --- which is the only active agent --- changes its color.Fig.1: The token is passed by an agent changing colorFaultsLet's look at the state of a system after a fault occurs.  The next
set of diagrams shows how errors --- once they occur --- can propagate
for ever.  These diagrams show a system with three tokens whereas an
error-free system should have exactly one.  The three agents holding
tokens are shown with large yellow numbers and the agent that does not
hold a token is shown with a smaller black number.Fig.2: Errors can propagate foreverThe figure on the top left of the above diagram shows agents 1, 2 and
3 with tokens because their colors are different from those of their
predecessors. The next diagram, top right, shows agents 1 and 3 with
tokens because their colors are different from those of their
predecessors, and agent 0 with a token because its color is the same
as that of its predecessor. The transition to the diagram on the top
right from the one on the top left occurs when agent 3 changes its
color.The sequence of state transitions gets the system to the figure on the
bottom left which is the same as that on the top left with the colors
reversed. This cycle of state transitions can repeat forever, with the
same system always having three tokens. So, this system is not self
stabilizing.A Self-Stabilizing AlgorithmThe solution: add more colors!We will modify the design to have as many colors as there are
agents. In our example, we will have 4 colors because it has 4
agents. The algorithms for all agents, other than agent 0, remains
unchanged. As before agent 0 has the
token when its color is the same as that of its predecessor and agent 0
sends the token by changing agent 0's color. The difference in the
self-stabilizing algorithm is the color to which agent 0 transits.Assume that the 4 colors are numbered 0, 1, 2, 3. If agent 0's color
is \(k\) it makes a transition by changing its color to \(k \:
\textrm{mod} \: N\).The diagram below gives an example of how agent 0's color changes. Its
sequence of colors, 0, 1, 2, 3 are red, blue, green, yellow,
respectively. The diagram on the top left shows a configuration in
which agent 0 holds a token because its color (green --- number 2) is
the same as that of its predecessor. Agent 0 passes the token by
changing its color to 3 (yellow), as show on the diagram on the top
right.The diagram in the middle left shows agent 0 with a token. It passes
the token by changing its color from 1 (blue) to 2 (green), as shown
in the diagram in the middle right.Fig.3: Changes in color of agent 0ProofThe proof has the following three ideas that we first describe
informally.In all states at least one agent holds a token.All trajectories from all states lead to a state in which agent 0
  holds a token.A trajectory from a system state in which agent 0's color is
  different from that of the other agents leads to a state in which
  all agents have the same color (in 
  which case the system is in a safe state).Part 1If all agents have the same color then agent 0 holds a token.
If there is more than one color in the ring then there is at least one
agent, other than agent 0, whose color is different from that of its
predecessor's; so, that agent holds a token.Part 2Agent 0 and agent \(n-1\) will
get the same color at some point because 
agent 0's color will propagate all the way around the ring unless it
gets to agent \(n-1\) sooner.Part 3If agent 0's color is different from the colors of agents \(1, \ldots,
n-1\) then the only
trajectory that leads to agents 0 and agent \(n-1\) having the same
color is for agent 0's color to propagate all the way around the ring.The system always reaches a safe stateLet \(S\) be any state. Let the \(C\) be the set of agent colors in state
\(S\). If \(C\) has all \(N\) colors, then the color of agent \(0\) is
different from the colors of the other agents, and so the result
follows from part 3.If \(C\) has fewer than \(N\) colors then the color of each agent
remains a color in \(C\) until agent 0 gets a color that is not in
\(C\). At this point agent 0's color is different from that of the
other agents and the result follows.ReviewIn this algorithm, does any agent detect that the system is in an
  unsafe state? Can we determine that the system is in an unsafe state
  based solely on the state of any one agent and the state of its
  predecessor? Why?Will this algorithm work if the number of colors is arbitrarily
  large, and more than the number of agents? Why?Will this algorithm work if the number of colors is less than the
  number of agents? Why?K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology



Figure Link: https://kmchandy.github.io/SelfStabilization/Slide01.jpg

Caption: Fig.1: The token is passed by an agent changing color


The Algorithm



Figure Link: https://kmchandy.github.io/SelfStabilization/Slide01.jpg

Caption: Fig.1: The token is passed by an agent changing color


An agent \(j\) passes the token to
agent \(j+1\) when agent \(j\) changes its color.
Agent \(0\) has the token when all the agents
in the ring have the same color.The diagram below illustrates an example with 4 agents where an
agent's color is either red or blue.
The diagram shows how the token is passed from
each agent to its sucessor.In the figure on the top left, all the agents are red; so agent 0 has
the token. When agent 0 changes its color we get the diagram at the
top center. In this diagram, agent 0 is blue and all the other agents
are red. Agent 0 no longer has the token because its color is
different from that of its predecessor; however agent 1 does have the token
because its color is different from that of its predecessor. The
sequence of diagrams shows what happens when the agent holding the
token --- which is the only active agent --- changes its color.Fig.1: The token is passed by an agent changing colorFaultsLet's look at the state of a system after a fault occurs.  The next
set of diagrams shows how errors --- once they occur --- can propagate
for ever.  These diagrams show a system with three tokens whereas an
error-free system should have exactly one.  The three agents holding
tokens are shown with large yellow numbers and the agent that does not
hold a token is shown with a smaller black number.Fig.2: Errors can propagate foreverThe figure on the top left of the above diagram shows agents 1, 2 and
3 with tokens because their colors are different from those of their
predecessors. The next diagram, top right, shows agents 1 and 3 with
tokens because their colors are different from those of their
predecessors, and agent 0 with a token because its color is the same
as that of its predecessor. The transition to the diagram on the top
right from the one on the top left occurs when agent 3 changes its
color.The sequence of state transitions gets the system to the figure on the
bottom left which is the same as that on the top left with the colors
reversed. This cycle of state transitions can repeat forever, with the
same system always having three tokens. So, this system is not self
stabilizing.A Self-Stabilizing AlgorithmThe solution: add more colors!We will modify the design to have as many colors as there are
agents. In our example, we will have 4 colors because it has 4
agents. The algorithms for all agents, other than agent 0, remains
unchanged. As before agent 0 has the
token when its color is the same as that of its predecessor and agent 0
sends the token by changing agent 0's color. The difference in the
self-stabilizing algorithm is the color to which agent 0 transits.Assume that the 4 colors are numbered 0, 1, 2, 3. If agent 0's color
is \(k\) it makes a transition by changing its color to \(k \:
\textrm{mod} \: N\).The diagram below gives an example of how agent 0's color changes. Its
sequence of colors, 0, 1, 2, 3 are red, blue, green, yellow,
respectively. The diagram on the top left shows a configuration in
which agent 0 holds a token because its color (green --- number 2) is
the same as that of its predecessor. Agent 0 passes the token by
changing its color to 3 (yellow), as show on the diagram on the top
right.The diagram in the middle left shows agent 0 with a token. It passes
the token by changing its color from 1 (blue) to 2 (green), as shown
in the diagram in the middle right.Fig.3: Changes in color of agent 0ProofThe proof has the following three ideas that we first describe
informally.In all states at least one agent holds a token.All trajectories from all states lead to a state in which agent 0
  holds a token.A trajectory from a system state in which agent 0's color is
  different from that of the other agents leads to a state in which
  all agents have the same color (in 
  which case the system is in a safe state).Part 1If all agents have the same color then agent 0 holds a token.
If there is more than one color in the ring then there is at least one
agent, other than agent 0, whose color is different from that of its
predecessor's; so, that agent holds a token.Part 2Agent 0 and agent \(n-1\) will
get the same color at some point because 
agent 0's color will propagate all the way around the ring unless it
gets to agent \(n-1\) sooner.Part 3If agent 0's color is different from the colors of agents \(1, \ldots,
n-1\) then the only
trajectory that leads to agents 0 and agent \(n-1\) having the same
color is for agent 0's color to propagate all the way around the ring.The system always reaches a safe stateLet \(S\) be any state. Let the \(C\) be the set of agent colors in state
\(S\). If \(C\) has all \(N\) colors, then the color of agent \(0\) is
different from the colors of the other agents, and so the result
follows from part 3.If \(C\) has fewer than \(N\) colors then the color of each agent
remains a color in \(C\) until agent 0 gets a color that is not in
\(C\). At this point agent 0's color is different from that of the
other agents and the result follows.ReviewIn this algorithm, does any agent detect that the system is in an
  unsafe state? Can we determine that the system is in an unsafe state
  based solely on the state of any one agent and the state of its
  predecessor? Why?Will this algorithm work if the number of colors is arbitrarily
  large, and more than the number of agents? Why?Will this algorithm work if the number of colors is less than the
  number of agents? Why?K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology



Figure Link: https://kmchandy.github.io/SelfStabilization/Slide01.jpg

Caption: Fig.1: The token is passed by an agent changing color


The diagram below illustrates an example with 4 agents where an
agent's color is either red or blue.
The diagram shows how the token is passed from
each agent to its sucessor.In the figure on the top left, all the agents are red; so agent 0 has
the token. When agent 0 changes its color we get the diagram at the
top center. In this diagram, agent 0 is blue and all the other agents
are red. Agent 0 no longer has the token because its color is
different from that of its predecessor; however agent 1 does have the token
because its color is different from that of its predecessor. The
sequence of diagrams shows what happens when the agent holding the
token --- which is the only active agent --- changes its color.Fig.1: The token is passed by an agent changing colorFaultsLet's look at the state of a system after a fault occurs.  The next
set of diagrams shows how errors --- once they occur --- can propagate
for ever.  These diagrams show a system with three tokens whereas an
error-free system should have exactly one.  The three agents holding
tokens are shown with large yellow numbers and the agent that does not
hold a token is shown with a smaller black number.Fig.2: Errors can propagate foreverThe figure on the top left of the above diagram shows agents 1, 2 and
3 with tokens because their colors are different from those of their
predecessors. The next diagram, top right, shows agents 1 and 3 with
tokens because their colors are different from those of their
predecessors, and agent 0 with a token because its color is the same
as that of its predecessor. The transition to the diagram on the top
right from the one on the top left occurs when agent 3 changes its
color.The sequence of state transitions gets the system to the figure on the
bottom left which is the same as that on the top left with the colors
reversed. This cycle of state transitions can repeat forever, with the
same system always having three tokens. So, this system is not self
stabilizing.A Self-Stabilizing AlgorithmThe solution: add more colors!We will modify the design to have as many colors as there are
agents. In our example, we will have 4 colors because it has 4
agents. The algorithms for all agents, other than agent 0, remains
unchanged. As before agent 0 has the
token when its color is the same as that of its predecessor and agent 0
sends the token by changing agent 0's color. The difference in the
self-stabilizing algorithm is the color to which agent 0 transits.Assume that the 4 colors are numbered 0, 1, 2, 3. If agent 0's color
is \(k\) it makes a transition by changing its color to \(k \:
\textrm{mod} \: N\).The diagram below gives an example of how agent 0's color changes. Its
sequence of colors, 0, 1, 2, 3 are red, blue, green, yellow,
respectively. The diagram on the top left shows a configuration in
which agent 0 holds a token because its color (green --- number 2) is
the same as that of its predecessor. Agent 0 passes the token by
changing its color to 3 (yellow), as show on the diagram on the top
right.The diagram in the middle left shows agent 0 with a token. It passes
the token by changing its color from 1 (blue) to 2 (green), as shown
in the diagram in the middle right.Fig.3: Changes in color of agent 0ProofThe proof has the following three ideas that we first describe
informally.In all states at least one agent holds a token.All trajectories from all states lead to a state in which agent 0
  holds a token.A trajectory from a system state in which agent 0's color is
  different from that of the other agents leads to a state in which
  all agents have the same color (in 
  which case the system is in a safe state).Part 1If all agents have the same color then agent 0 holds a token.
If there is more than one color in the ring then there is at least one
agent, other than agent 0, whose color is different from that of its
predecessor's; so, that agent holds a token.Part 2Agent 0 and agent \(n-1\) will
get the same color at some point because 
agent 0's color will propagate all the way around the ring unless it
gets to agent \(n-1\) sooner.Part 3If agent 0's color is different from the colors of agents \(1, \ldots,
n-1\) then the only
trajectory that leads to agents 0 and agent \(n-1\) having the same
color is for agent 0's color to propagate all the way around the ring.The system always reaches a safe stateLet \(S\) be any state. Let the \(C\) be the set of agent colors in state
\(S\). If \(C\) has all \(N\) colors, then the color of agent \(0\) is
different from the colors of the other agents, and so the result
follows from part 3.If \(C\) has fewer than \(N\) colors then the color of each agent
remains a color in \(C\) until agent 0 gets a color that is not in
\(C\). At this point agent 0's color is different from that of the
other agents and the result follows.ReviewIn this algorithm, does any agent detect that the system is in an
  unsafe state? Can we determine that the system is in an unsafe state
  based solely on the state of any one agent and the state of its
  predecessor? Why?Will this algorithm work if the number of colors is arbitrarily
  large, and more than the number of agents? Why?Will this algorithm work if the number of colors is less than the
  number of agents? Why?K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology



Figure Link: https://kmchandy.github.io/SelfStabilization/Slide01.jpg

Caption: Fig.1: The token is passed by an agent changing color


In the figure on the top left, all the agents are red; so agent 0 has
the token. When agent 0 changes its color we get the diagram at the
top center. In this diagram, agent 0 is blue and all the other agents
are red. Agent 0 no longer has the token because its color is
different from that of its predecessor; however agent 1 does have the token
because its color is different from that of its predecessor. The
sequence of diagrams shows what happens when the agent holding the
token --- which is the only active agent --- changes its color.Fig.1: The token is passed by an agent changing colorFaultsLet's look at the state of a system after a fault occurs.  The next
set of diagrams shows how errors --- once they occur --- can propagate
for ever.  These diagrams show a system with three tokens whereas an
error-free system should have exactly one.  The three agents holding
tokens are shown with large yellow numbers and the agent that does not
hold a token is shown with a smaller black number.Fig.2: Errors can propagate foreverThe figure on the top left of the above diagram shows agents 1, 2 and
3 with tokens because their colors are different from those of their
predecessors. The next diagram, top right, shows agents 1 and 3 with
tokens because their colors are different from those of their
predecessors, and agent 0 with a token because its color is the same
as that of its predecessor. The transition to the diagram on the top
right from the one on the top left occurs when agent 3 changes its
color.The sequence of state transitions gets the system to the figure on the
bottom left which is the same as that on the top left with the colors
reversed. This cycle of state transitions can repeat forever, with the
same system always having three tokens. So, this system is not self
stabilizing.A Self-Stabilizing AlgorithmThe solution: add more colors!We will modify the design to have as many colors as there are
agents. In our example, we will have 4 colors because it has 4
agents. The algorithms for all agents, other than agent 0, remains
unchanged. As before agent 0 has the
token when its color is the same as that of its predecessor and agent 0
sends the token by changing agent 0's color. The difference in the
self-stabilizing algorithm is the color to which agent 0 transits.Assume that the 4 colors are numbered 0, 1, 2, 3. If agent 0's color
is \(k\) it makes a transition by changing its color to \(k \:
\textrm{mod} \: N\).The diagram below gives an example of how agent 0's color changes. Its
sequence of colors, 0, 1, 2, 3 are red, blue, green, yellow,
respectively. The diagram on the top left shows a configuration in
which agent 0 holds a token because its color (green --- number 2) is
the same as that of its predecessor. Agent 0 passes the token by
changing its color to 3 (yellow), as show on the diagram on the top
right.The diagram in the middle left shows agent 0 with a token. It passes
the token by changing its color from 1 (blue) to 2 (green), as shown
in the diagram in the middle right.Fig.3: Changes in color of agent 0ProofThe proof has the following three ideas that we first describe
informally.In all states at least one agent holds a token.All trajectories from all states lead to a state in which agent 0
  holds a token.A trajectory from a system state in which agent 0's color is
  different from that of the other agents leads to a state in which
  all agents have the same color (in 
  which case the system is in a safe state).Part 1If all agents have the same color then agent 0 holds a token.
If there is more than one color in the ring then there is at least one
agent, other than agent 0, whose color is different from that of its
predecessor's; so, that agent holds a token.Part 2Agent 0 and agent \(n-1\) will
get the same color at some point because 
agent 0's color will propagate all the way around the ring unless it
gets to agent \(n-1\) sooner.Part 3If agent 0's color is different from the colors of agents \(1, \ldots,
n-1\) then the only
trajectory that leads to agents 0 and agent \(n-1\) having the same
color is for agent 0's color to propagate all the way around the ring.The system always reaches a safe stateLet \(S\) be any state. Let the \(C\) be the set of agent colors in state
\(S\). If \(C\) has all \(N\) colors, then the color of agent \(0\) is
different from the colors of the other agents, and so the result
follows from part 3.If \(C\) has fewer than \(N\) colors then the color of each agent
remains a color in \(C\) until agent 0 gets a color that is not in
\(C\). At this point agent 0's color is different from that of the
other agents and the result follows.ReviewIn this algorithm, does any agent detect that the system is in an
  unsafe state? Can we determine that the system is in an unsafe state
  based solely on the state of any one agent and the state of its
  predecessor? Why?Will this algorithm work if the number of colors is arbitrarily
  large, and more than the number of agents? Why?Will this algorithm work if the number of colors is less than the
  number of agents? Why?K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology



Figure Link: https://kmchandy.github.io/SelfStabilization/Slide01.jpg

Caption: Fig.1: The token is passed by an agent changing color


In the figure on the top left, all the agents are red; so agent 0 has
the token. When agent 0 changes its color we get the diagram at the
top center. In this diagram, agent 0 is blue and all the other agents
are red. Agent 0 no longer has the token because its color is
different from that of its predecessor; however agent 1 does have the token
because its color is different from that of its predecessor. The
sequence of diagrams shows what happens when the agent holding the
token --- which is the only active agent --- changes its color.Fig.1: The token is passed by an agent changing colorFaultsLet's look at the state of a system after a fault occurs.  The next
set of diagrams shows how errors --- once they occur --- can propagate
for ever.  These diagrams show a system with three tokens whereas an
error-free system should have exactly one.  The three agents holding
tokens are shown with large yellow numbers and the agent that does not
hold a token is shown with a smaller black number.Fig.2: Errors can propagate foreverThe figure on the top left of the above diagram shows agents 1, 2 and
3 with tokens because their colors are different from those of their
predecessors. The next diagram, top right, shows agents 1 and 3 with
tokens because their colors are different from those of their
predecessors, and agent 0 with a token because its color is the same
as that of its predecessor. The transition to the diagram on the top
right from the one on the top left occurs when agent 3 changes its
color.The sequence of state transitions gets the system to the figure on the
bottom left which is the same as that on the top left with the colors
reversed. This cycle of state transitions can repeat forever, with the
same system always having three tokens. So, this system is not self
stabilizing.A Self-Stabilizing AlgorithmThe solution: add more colors!We will modify the design to have as many colors as there are
agents. In our example, we will have 4 colors because it has 4
agents. The algorithms for all agents, other than agent 0, remains
unchanged. As before agent 0 has the
token when its color is the same as that of its predecessor and agent 0
sends the token by changing agent 0's color. The difference in the
self-stabilizing algorithm is the color to which agent 0 transits.Assume that the 4 colors are numbered 0, 1, 2, 3. If agent 0's color
is \(k\) it makes a transition by changing its color to \(k \:
\textrm{mod} \: N\).The diagram below gives an example of how agent 0's color changes. Its
sequence of colors, 0, 1, 2, 3 are red, blue, green, yellow,
respectively. The diagram on the top left shows a configuration in
which agent 0 holds a token because its color (green --- number 2) is
the same as that of its predecessor. Agent 0 passes the token by
changing its color to 3 (yellow), as show on the diagram on the top
right.The diagram in the middle left shows agent 0 with a token. It passes
the token by changing its color from 1 (blue) to 2 (green), as shown
in the diagram in the middle right.Fig.3: Changes in color of agent 0ProofThe proof has the following three ideas that we first describe
informally.In all states at least one agent holds a token.All trajectories from all states lead to a state in which agent 0
  holds a token.A trajectory from a system state in which agent 0's color is
  different from that of the other agents leads to a state in which
  all agents have the same color (in 
  which case the system is in a safe state).Part 1If all agents have the same color then agent 0 holds a token.
If there is more than one color in the ring then there is at least one
agent, other than agent 0, whose color is different from that of its
predecessor's; so, that agent holds a token.Part 2Agent 0 and agent \(n-1\) will
get the same color at some point because 
agent 0's color will propagate all the way around the ring unless it
gets to agent \(n-1\) sooner.Part 3If agent 0's color is different from the colors of agents \(1, \ldots,
n-1\) then the only
trajectory that leads to agents 0 and agent \(n-1\) having the same
color is for agent 0's color to propagate all the way around the ring.The system always reaches a safe stateLet \(S\) be any state. Let the \(C\) be the set of agent colors in state
\(S\). If \(C\) has all \(N\) colors, then the color of agent \(0\) is
different from the colors of the other agents, and so the result
follows from part 3.If \(C\) has fewer than \(N\) colors then the color of each agent
remains a color in \(C\) until agent 0 gets a color that is not in
\(C\). At this point agent 0's color is different from that of the
other agents and the result follows.ReviewIn this algorithm, does any agent detect that the system is in an
  unsafe state? Can we determine that the system is in an unsafe state
  based solely on the state of any one agent and the state of its
  predecessor? Why?Will this algorithm work if the number of colors is arbitrarily
  large, and more than the number of agents? Why?Will this algorithm work if the number of colors is less than the
  number of agents? Why?K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology



Figure Link: https://kmchandy.github.io/SelfStabilization/Slide01.jpg

Caption: Fig.1: The token is passed by an agent changing color


The figure on the top left of the above diagram shows agents 1, 2 and
3 with tokens because their colors are different from those of their
predecessors. The next diagram, top right, shows agents 1 and 3 with
tokens because their colors are different from those of their
predecessors, and agent 0 with a token because its color is the same
as that of its predecessor. The transition to the diagram on the top
right from the one on the top left occurs when agent 3 changes its
color.The sequence of state transitions gets the system to the figure on the
bottom left which is the same as that on the top left with the colors
reversed. This cycle of state transitions can repeat forever, with the
same system always having three tokens. So, this system is not self
stabilizing.A Self-Stabilizing AlgorithmThe solution: add more colors!We will modify the design to have as many colors as there are
agents. In our example, we will have 4 colors because it has 4
agents. The algorithms for all agents, other than agent 0, remains
unchanged. As before agent 0 has the
token when its color is the same as that of its predecessor and agent 0
sends the token by changing agent 0's color. The difference in the
self-stabilizing algorithm is the color to which agent 0 transits.Assume that the 4 colors are numbered 0, 1, 2, 3. If agent 0's color
is \(k\) it makes a transition by changing its color to \(k \:
\textrm{mod} \: N\).The diagram below gives an example of how agent 0's color changes. Its
sequence of colors, 0, 1, 2, 3 are red, blue, green, yellow,
respectively. The diagram on the top left shows a configuration in
which agent 0 holds a token because its color (green --- number 2) is
the same as that of its predecessor. Agent 0 passes the token by
changing its color to 3 (yellow), as show on the diagram on the top
right.The diagram in the middle left shows agent 0 with a token. It passes
the token by changing its color from 1 (blue) to 2 (green), as shown
in the diagram in the middle right.Fig.3: Changes in color of agent 0ProofThe proof has the following three ideas that we first describe
informally.In all states at least one agent holds a token.All trajectories from all states lead to a state in which agent 0
  holds a token.A trajectory from a system state in which agent 0's color is
  different from that of the other agents leads to a state in which
  all agents have the same color (in 
  which case the system is in a safe state).Part 1If all agents have the same color then agent 0 holds a token.
If there is more than one color in the ring then there is at least one
agent, other than agent 0, whose color is different from that of its
predecessor's; so, that agent holds a token.Part 2Agent 0 and agent \(n-1\) will
get the same color at some point because 
agent 0's color will propagate all the way around the ring unless it
gets to agent \(n-1\) sooner.Part 3If agent 0's color is different from the colors of agents \(1, \ldots,
n-1\) then the only
trajectory that leads to agents 0 and agent \(n-1\) having the same
color is for agent 0's color to propagate all the way around the ring.The system always reaches a safe stateLet \(S\) be any state. Let the \(C\) be the set of agent colors in state
\(S\). If \(C\) has all \(N\) colors, then the color of agent \(0\) is
different from the colors of the other agents, and so the result
follows from part 3.If \(C\) has fewer than \(N\) colors then the color of each agent
remains a color in \(C\) until agent 0 gets a color that is not in
\(C\). At this point agent 0's color is different from that of the
other agents and the result follows.ReviewIn this algorithm, does any agent detect that the system is in an
  unsafe state? Can we determine that the system is in an unsafe state
  based solely on the state of any one agent and the state of its
  predecessor? Why?Will this algorithm work if the number of colors is arbitrarily
  large, and more than the number of agents? Why?Will this algorithm work if the number of colors is less than the
  number of agents? Why?K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology



Figure Link: https://kmchandy.github.io/SelfStabilization/Slide03.jpg

Caption: Fig.3: Changes in color of agent 0


The sequence of state transitions gets the system to the figure on the
bottom left which is the same as that on the top left with the colors
reversed. This cycle of state transitions can repeat forever, with the
same system always having three tokens. So, this system is not self
stabilizing.A Self-Stabilizing AlgorithmThe solution: add more colors!We will modify the design to have as many colors as there are
agents. In our example, we will have 4 colors because it has 4
agents. The algorithms for all agents, other than agent 0, remains
unchanged. As before agent 0 has the
token when its color is the same as that of its predecessor and agent 0
sends the token by changing agent 0's color. The difference in the
self-stabilizing algorithm is the color to which agent 0 transits.Assume that the 4 colors are numbered 0, 1, 2, 3. If agent 0's color
is \(k\) it makes a transition by changing its color to \(k \:
\textrm{mod} \: N\).The diagram below gives an example of how agent 0's color changes. Its
sequence of colors, 0, 1, 2, 3 are red, blue, green, yellow,
respectively. The diagram on the top left shows a configuration in
which agent 0 holds a token because its color (green --- number 2) is
the same as that of its predecessor. Agent 0 passes the token by
changing its color to 3 (yellow), as show on the diagram on the top
right.The diagram in the middle left shows agent 0 with a token. It passes
the token by changing its color from 1 (blue) to 2 (green), as shown
in the diagram in the middle right.Fig.3: Changes in color of agent 0ProofThe proof has the following three ideas that we first describe
informally.In all states at least one agent holds a token.All trajectories from all states lead to a state in which agent 0
  holds a token.A trajectory from a system state in which agent 0's color is
  different from that of the other agents leads to a state in which
  all agents have the same color (in 
  which case the system is in a safe state).Part 1If all agents have the same color then agent 0 holds a token.
If there is more than one color in the ring then there is at least one
agent, other than agent 0, whose color is different from that of its
predecessor's; so, that agent holds a token.Part 2Agent 0 and agent \(n-1\) will
get the same color at some point because 
agent 0's color will propagate all the way around the ring unless it
gets to agent \(n-1\) sooner.Part 3If agent 0's color is different from the colors of agents \(1, \ldots,
n-1\) then the only
trajectory that leads to agents 0 and agent \(n-1\) having the same
color is for agent 0's color to propagate all the way around the ring.The system always reaches a safe stateLet \(S\) be any state. Let the \(C\) be the set of agent colors in state
\(S\). If \(C\) has all \(N\) colors, then the color of agent \(0\) is
different from the colors of the other agents, and so the result
follows from part 3.If \(C\) has fewer than \(N\) colors then the color of each agent
remains a color in \(C\) until agent 0 gets a color that is not in
\(C\). At this point agent 0's color is different from that of the
other agents and the result follows.ReviewIn this algorithm, does any agent detect that the system is in an
  unsafe state? Can we determine that the system is in an unsafe state
  based solely on the state of any one agent and the state of its
  predecessor? Why?Will this algorithm work if the number of colors is arbitrarily
  large, and more than the number of agents? Why?Will this algorithm work if the number of colors is less than the
  number of agents? Why?K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology



Figure Link: https://kmchandy.github.io/SelfStabilization/Slide03.jpg

Caption: Fig.3: Changes in color of agent 0


We will modify the design to have as many colors as there are
agents. In our example, we will have 4 colors because it has 4
agents. The algorithms for all agents, other than agent 0, remains
unchanged. As before agent 0 has the
token when its color is the same as that of its predecessor and agent 0
sends the token by changing agent 0's color. The difference in the
self-stabilizing algorithm is the color to which agent 0 transits.Assume that the 4 colors are numbered 0, 1, 2, 3. If agent 0's color
is \(k\) it makes a transition by changing its color to \(k \:
\textrm{mod} \: N\).The diagram below gives an example of how agent 0's color changes. Its
sequence of colors, 0, 1, 2, 3 are red, blue, green, yellow,
respectively. The diagram on the top left shows a configuration in
which agent 0 holds a token because its color (green --- number 2) is
the same as that of its predecessor. Agent 0 passes the token by
changing its color to 3 (yellow), as show on the diagram on the top
right.The diagram in the middle left shows agent 0 with a token. It passes
the token by changing its color from 1 (blue) to 2 (green), as shown
in the diagram in the middle right.Fig.3: Changes in color of agent 0ProofThe proof has the following three ideas that we first describe
informally.In all states at least one agent holds a token.All trajectories from all states lead to a state in which agent 0
  holds a token.A trajectory from a system state in which agent 0's color is
  different from that of the other agents leads to a state in which
  all agents have the same color (in 
  which case the system is in a safe state).Part 1If all agents have the same color then agent 0 holds a token.
If there is more than one color in the ring then there is at least one
agent, other than agent 0, whose color is different from that of its
predecessor's; so, that agent holds a token.Part 2Agent 0 and agent \(n-1\) will
get the same color at some point because 
agent 0's color will propagate all the way around the ring unless it
gets to agent \(n-1\) sooner.Part 3If agent 0's color is different from the colors of agents \(1, \ldots,
n-1\) then the only
trajectory that leads to agents 0 and agent \(n-1\) having the same
color is for agent 0's color to propagate all the way around the ring.The system always reaches a safe stateLet \(S\) be any state. Let the \(C\) be the set of agent colors in state
\(S\). If \(C\) has all \(N\) colors, then the color of agent \(0\) is
different from the colors of the other agents, and so the result
follows from part 3.If \(C\) has fewer than \(N\) colors then the color of each agent
remains a color in \(C\) until agent 0 gets a color that is not in
\(C\). At this point agent 0's color is different from that of the
other agents and the result follows.ReviewIn this algorithm, does any agent detect that the system is in an
  unsafe state? Can we determine that the system is in an unsafe state
  based solely on the state of any one agent and the state of its
  predecessor? Why?Will this algorithm work if the number of colors is arbitrarily
  large, and more than the number of agents? Why?Will this algorithm work if the number of colors is less than the
  number of agents? Why?K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology



Figure Link: https://kmchandy.github.io/SelfStabilization/Slide03.jpg

Caption: Fig.3: Changes in color of agent 0


Assume that the 4 colors are numbered 0, 1, 2, 3. If agent 0's color
is \(k\) it makes a transition by changing its color to \(k \:
\textrm{mod} \: N\).The diagram below gives an example of how agent 0's color changes. Its
sequence of colors, 0, 1, 2, 3 are red, blue, green, yellow,
respectively. The diagram on the top left shows a configuration in
which agent 0 holds a token because its color (green --- number 2) is
the same as that of its predecessor. Agent 0 passes the token by
changing its color to 3 (yellow), as show on the diagram on the top
right.The diagram in the middle left shows agent 0 with a token. It passes
the token by changing its color from 1 (blue) to 2 (green), as shown
in the diagram in the middle right.Fig.3: Changes in color of agent 0ProofThe proof has the following three ideas that we first describe
informally.In all states at least one agent holds a token.All trajectories from all states lead to a state in which agent 0
  holds a token.A trajectory from a system state in which agent 0's color is
  different from that of the other agents leads to a state in which
  all agents have the same color (in 
  which case the system is in a safe state).Part 1If all agents have the same color then agent 0 holds a token.
If there is more than one color in the ring then there is at least one
agent, other than agent 0, whose color is different from that of its
predecessor's; so, that agent holds a token.Part 2Agent 0 and agent \(n-1\) will
get the same color at some point because 
agent 0's color will propagate all the way around the ring unless it
gets to agent \(n-1\) sooner.Part 3If agent 0's color is different from the colors of agents \(1, \ldots,
n-1\) then the only
trajectory that leads to agents 0 and agent \(n-1\) having the same
color is for agent 0's color to propagate all the way around the ring.The system always reaches a safe stateLet \(S\) be any state. Let the \(C\) be the set of agent colors in state
\(S\). If \(C\) has all \(N\) colors, then the color of agent \(0\) is
different from the colors of the other agents, and so the result
follows from part 3.If \(C\) has fewer than \(N\) colors then the color of each agent
remains a color in \(C\) until agent 0 gets a color that is not in
\(C\). At this point agent 0's color is different from that of the
other agents and the result follows.ReviewIn this algorithm, does any agent detect that the system is in an
  unsafe state? Can we determine that the system is in an unsafe state
  based solely on the state of any one agent and the state of its
  predecessor? Why?Will this algorithm work if the number of colors is arbitrarily
  large, and more than the number of agents? Why?Will this algorithm work if the number of colors is less than the
  number of agents? Why?K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology



Figure Link: https://kmchandy.github.io/SelfStabilization/Slide03.jpg

Caption: Fig.3: Changes in color of agent 0


The diagram below gives an example of how agent 0's color changes. Its
sequence of colors, 0, 1, 2, 3 are red, blue, green, yellow,
respectively. The diagram on the top left shows a configuration in
which agent 0 holds a token because its color (green --- number 2) is
the same as that of its predecessor. Agent 0 passes the token by
changing its color to 3 (yellow), as show on the diagram on the top
right.The diagram in the middle left shows agent 0 with a token. It passes
the token by changing its color from 1 (blue) to 2 (green), as shown
in the diagram in the middle right.Fig.3: Changes in color of agent 0ProofThe proof has the following three ideas that we first describe
informally.In all states at least one agent holds a token.All trajectories from all states lead to a state in which agent 0
  holds a token.A trajectory from a system state in which agent 0's color is
  different from that of the other agents leads to a state in which
  all agents have the same color (in 
  which case the system is in a safe state).Part 1If all agents have the same color then agent 0 holds a token.
If there is more than one color in the ring then there is at least one
agent, other than agent 0, whose color is different from that of its
predecessor's; so, that agent holds a token.Part 2Agent 0 and agent \(n-1\) will
get the same color at some point because 
agent 0's color will propagate all the way around the ring unless it
gets to agent \(n-1\) sooner.Part 3If agent 0's color is different from the colors of agents \(1, \ldots,
n-1\) then the only
trajectory that leads to agents 0 and agent \(n-1\) having the same
color is for agent 0's color to propagate all the way around the ring.The system always reaches a safe stateLet \(S\) be any state. Let the \(C\) be the set of agent colors in state
\(S\). If \(C\) has all \(N\) colors, then the color of agent \(0\) is
different from the colors of the other agents, and so the result
follows from part 3.If \(C\) has fewer than \(N\) colors then the color of each agent
remains a color in \(C\) until agent 0 gets a color that is not in
\(C\). At this point agent 0's color is different from that of the
other agents and the result follows.ReviewIn this algorithm, does any agent detect that the system is in an
  unsafe state? Can we determine that the system is in an unsafe state
  based solely on the state of any one agent and the state of its
  predecessor? Why?Will this algorithm work if the number of colors is arbitrarily
  large, and more than the number of agents? Why?Will this algorithm work if the number of colors is less than the
  number of agents? Why?K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology



Figure Link: https://kmchandy.github.io/SelfStabilization/Slide03.jpg

Caption: Fig.3: Changes in color of agent 0


The diagram in the middle left shows agent 0 with a token. It passes
the token by changing its color from 1 (blue) to 2 (green), as shown
in the diagram in the middle right.Fig.3: Changes in color of agent 0ProofThe proof has the following three ideas that we first describe
informally.In all states at least one agent holds a token.All trajectories from all states lead to a state in which agent 0
  holds a token.A trajectory from a system state in which agent 0's color is
  different from that of the other agents leads to a state in which
  all agents have the same color (in 
  which case the system is in a safe state).Part 1If all agents have the same color then agent 0 holds a token.
If there is more than one color in the ring then there is at least one
agent, other than agent 0, whose color is different from that of its
predecessor's; so, that agent holds a token.Part 2Agent 0 and agent \(n-1\) will
get the same color at some point because 
agent 0's color will propagate all the way around the ring unless it
gets to agent \(n-1\) sooner.Part 3If agent 0's color is different from the colors of agents \(1, \ldots,
n-1\) then the only
trajectory that leads to agents 0 and agent \(n-1\) having the same
color is for agent 0's color to propagate all the way around the ring.The system always reaches a safe stateLet \(S\) be any state. Let the \(C\) be the set of agent colors in state
\(S\). If \(C\) has all \(N\) colors, then the color of agent \(0\) is
different from the colors of the other agents, and so the result
follows from part 3.If \(C\) has fewer than \(N\) colors then the color of each agent
remains a color in \(C\) until agent 0 gets a color that is not in
\(C\). At this point agent 0's color is different from that of the
other agents and the result follows.ReviewIn this algorithm, does any agent detect that the system is in an
  unsafe state? Can we determine that the system is in an unsafe state
  based solely on the state of any one agent and the state of its
  predecessor? Why?Will this algorithm work if the number of colors is arbitrarily
  large, and more than the number of agents? Why?Will this algorithm work if the number of colors is less than the
  number of agents? Why?K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology



Figure Link: https://kmchandy.github.io/SelfStabilization/Slide03.jpg

Caption: Fig.3: Changes in color of agent 0


Part 1

Part 2

Part 3

The system always reaches a safe state

If \(C\) has fewer than \(N\) colors then the color of each agent
remains a color in \(C\) until agent 0 gets a color that is not in
\(C\). At this point agent 0's color is different from that of the
other agents and the result follows.ReviewIn this algorithm, does any agent detect that the system is in an
  unsafe state? Can we determine that the system is in an unsafe state
  based solely on the state of any one agent and the state of its
  predecessor? Why?Will this algorithm work if the number of colors is arbitrarily
  large, and more than the number of agents? Why?Will this algorithm work if the number of colors is less than the
  number of agents? Why?K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology
--- END Content from https://kmchandy.github.io/SelfStabilization/SelfStabilization.html ---


--- START Content from https://kmchandy.github.io/Byzantine/ByzantineWritten.html ---
TITLE: Byzantine/ByzantineWritten.html

The algorithm operates in a sequence of steps calledrounds.
    All messages sent in a round are delivered in the next round.
    Agents execute actions in each round after receiving
    messages sent in the previous round; these actions may include
    sending messages. So, the Byzantine algorithm is synchronous.Byzantine Generals Problem: OverviewA general has \(N\) army units each of which is led by a
    lieutenant general, herafter referred to merely as lieutenant. We
    refer to the general and the lieutenants, collectively, as
    agents. An agent may be either loyal or disloyal. A loyal general
    gives the same command to all lieutenants. A loyal general's
    command is eitherattackorretreat.  A disloyal
    general may give different commands to different lieutenants and
    may give no commands to some. The lieutenants receive commands
    from the general and then communicate among themselves to reach a
    consensus.  Loyal lieutenants follow an algorithm while disloyal
    lieutenants may or may not.The figure below illustrates the difference between loyal and
    disloyal generals.Fig.1: Loyal and disloyal general behaviorByzantine Generals Problem:
SpecificationA loyal general sends attack messages to all lieutenants or sends
retreat messages to all lieutenants.
A disloyal general sends arbitrary messages to lieutenants.Validity: Loyal lieutenants must obey a loyal general. If a loyal general
    gives the command to attack then all loyal lieutenants must
    attack. Likewise, if a loyal general
    gives the command to retreat then all loyal lieutenants must
    retreat.Consensus: Loyal lieutenants come to a consensus: either all of them
    attack or all of them retreat.The specification does not require that traitors be
    discovered. For example, the algorithm doesn't have to determine
    whether the general or a lieutenant is loyal or disloyal.If the only requirement is validity, and consensus isn't
    required, then the solution is trivial: all loyal lieutenants obey
    the general whether the general is loyal or disloyal. If the only
    requirement is consensus then the
    solution is trivial: all loyal lieutenants agree on a predefined
    value, say retreat, regardless of the command issued by the
    general. The conjunction of both requirements makes the problem
  difficult.Oral and Written MessagesThere are two versions of the problem.Written Messages:
    In this version an agent may send copies of messages that it
    receives to other agents but cannot modify the
    messages. Also, an agent cannot forge signatures. So, an agent can
    receive a message M signed by lieutenant \(A\) only if \(A\) sent M to
    some agent.Oral Messages:
    In this version an agent can modify messages and forge
    signatures.So, an agent can receive a message M signed by agent
    \(A\) even if \(A\) never sent M to any agent.The algorithm for written messages is simpler and requires fewer
    messages.Algorithm with Written MessagesA reliable lieutenant who does not get a message from in round 0 treats the
absence of the message as the same as receiving a retreat message.
  Likewise, if a loyal lieutenant gets a message that is not an attack
  or a retreat message then the lieutenant treats the message as a
  retreat message.
  (We use retreat as a default. We could just as well have used attack
  as the default.)Next, we give an overview of the algorithm.Commit to AttackAt each round in the algorithm, a loyal lieutenant has either
  committed to attack or not.
  A lieutenant that has not committed to attack on a round may commit
  to attack on a later round.
  If any loyal lieutenant commits to attack in any round then it remains
  committed to attack thereafter.
  A loyal lieutenant retreats if it has not committed to attack at the
  end of the last round.MessagesA message is an attack message from the general or a commitment to
  attack by a lieutenant. We call commitment messages
  attack messages. An attack message is identified by the
  agent (general or lieutenant) that created the message.Evidence for AttackA loyal lieutenant commits to attack on round \(r \geq 1 \), for the first
  time, if the lieutenant has received an attack message from the
  general and at
  least \(r - 1\) lieutenants. 
  When the lieutenant commits to attack it sends copies of these
  messages, and its own attack message, to all
  other lieutenants.
  So, in round \(r+1\), each lieutenant receives an attack messages from
  the general and at least \(r\)
  lieutenants.
  And so, all loyal lieutenants commit to attack in this round.The AlgorithmWe will use the tactic that is helpful in analyzing distributed
algorithms that operate in rounds.
We will prove properties of a sequential algorithm and then show the
  equivalence of the sequential and distributed algorithms.
The sequential algorithm is given next.Local VariablesAssociated with each lieutenantCare local variablesC.receivedandC.sentwhich are sets of
  agents (general or lieutenants).C.receivedis the set of agents from whichChas received attack messages (or their copies).Csends attack messages (or copies) from agents inC.sent.
The symbolgrepresents the general.Initialization: Round 0A.sent = {}for all lieutenantsA.If the general is loyal and sends attack messages then the set of
  agents from which a lieutenant has received attack messages is the
  singleton set consisting of the general. So, for all
  lieutenantsA:A.received = {g}If the general is loyal and sends retreat messages then for all
  lieutenantsA:A.received = {}If the general is disloyal, thenA.received = {g}for some lieutenantsA,
  andA.received = {}for the others.The algorithm operates in a sequence of rounds with round-numberrstepping from 1 tot+1. Ther-th iteration for loyal agentcconsists of
the following two steps.Roundr > 0Step 1if (|C.received| >= r) AND  (g in C.received):
    C.commit = True
    C.sent = C.received UNION {C}IfC.receivedhas messages that show that:  (1) the number of
agents that have committed to attack on roundris at leastr, and (2) the general has sent an
attack message, thenCcommits to attack and sends these
messages as well as an additional message thatChas
committed to attack.Step 2C.received = (
    (UNION over all loyal agents B of B.sent)
             UNION
     an arbitrary subset of disloyal agents)Creceives the messages sent by loyal lieutenants and messages
sent by disloyal lieutenants.
A disloyal lieutenant,A, can send messages to some lieutenants
thatAis committed to attack, and not send these messages
to other lieutenants.
LieutenantCreceives attack messages from an arbitrary subset
of disloyal lieutenants.LieutenantCdoes not know which agents are loyal and
which are disloyal.C.receivedis the set of all messages sent toCregardless of whether the senders are loyal or
disloyal.Proof of CorrectnessCase 1: General is loyal and sends attack
messagesIn this case, at the start round 1,C.received = {g}.
So, the if-clause in step 1 of round 1 is True, and therefore all
loyal lieutenants commit to attack at the end of round 1.Case 2: General is loyal and sends retreat
messagesA disloyal lieutenant cannot forge the general's signature, and so it is
impossible for any lieutenant to have a copy of an attack message from the
general.
Therefore the if-clause of step 1 is never satisfied, and so no loyal
lieutenant commits to attack in any step.Case 3: General is disloyalPart 1We first show that if any loyal lieutenant commits on roundrthen all loyal lieutenants commit by the end of roundr+1.A loyal lieutenantCcommits on roundrexactly when the setC.receivedhas at leastrattack messages including one from the general.
So, in roundr,C.senthas at leastr+1attack messages including one from the general.
Therefore, the if-clause of step 1 evaluates to True in roundr+1, and so all
loyal lieutenants commit by the end of roundr+1.Therefore, if any loyal lieutenant commits by the end of roundt-1then all loyal lieutenants commit by the end of roundt.Part 2We next show that if no loyal lieutenant has committed by the end of
roundt-1then no loyal lieutenant commits in roundt.C.sent = {}at the end
of roundt-1if no loyal lieutenant has committed to
attack.
Therefore in roundt,C.receivedis an 
arbitrary subset of disloyal lieutenants.
There are at mostt-1disloyal lieutenants because there
are at mosttdisloyal agents, and the general is
disloyal.
So, on roundt, the if-clause of step 1 evaluates to False.Part 3From parts 1 and 2, by the end of
roundt, either all loyal lieutenants commit, or no loyal
lieutenant commits.ExampleThe figure below illustrates a situation in which a loyal lieutenant C commits to
attack on round 3, if it hasn't already committed to attack on rounds
1 and 2.
The figure shows C getting attack messages (red boxes) signed by the
general and lieutenants A and B on round 2. The general and
lieutenants A and B may be disloyal or loyal.Fig.2: Example of a lieutenant committing to attackBecause C is loyal it follows the algorithm, and C
commits to attack at the end of round 3 because C receives a signed attack
 message from the general and signed attack messages from two
 different lieutenants.
So, in round 3, C broadcasts copies of attack messages
signed by the general and attack
messages signed by A and B and attack messages, and also an attack
 message signed by C itself. All loyal
lieutenants commit to attack in round 4 because they receive attack
messages signed by the general and 3 different lieutenants (A, B, C).ReviewAssume that you are explaining the algorithm to someone who hasn't
  taken a course on distributed computing. How would you explain to
  this person that even if the general and 999 lieutenants are
  disloyal, and only 2 lieutenants are loyal, the loyal lieutenants
  reach a consensus after round 100? 
  (Remember the first round is round 0.)Will the algorithm work if at most one lieutenant could modify
  written messages, and all the other lieutenants followed the
  protocol, i.e. these lieutenants could copy but not modify
  messages.K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology



Figure Link: https://kmchandy.github.io/Byzantine/Slide01.jpg

Caption: Fig.1: Loyal and disloyal general behavior


The figure below illustrates the difference between loyal and
    disloyal generals.Fig.1: Loyal and disloyal general behaviorByzantine Generals Problem:
SpecificationA loyal general sends attack messages to all lieutenants or sends
retreat messages to all lieutenants.
A disloyal general sends arbitrary messages to lieutenants.Validity: Loyal lieutenants must obey a loyal general. If a loyal general
    gives the command to attack then all loyal lieutenants must
    attack. Likewise, if a loyal general
    gives the command to retreat then all loyal lieutenants must
    retreat.Consensus: Loyal lieutenants come to a consensus: either all of them
    attack or all of them retreat.The specification does not require that traitors be
    discovered. For example, the algorithm doesn't have to determine
    whether the general or a lieutenant is loyal or disloyal.If the only requirement is validity, and consensus isn't
    required, then the solution is trivial: all loyal lieutenants obey
    the general whether the general is loyal or disloyal. If the only
    requirement is consensus then the
    solution is trivial: all loyal lieutenants agree on a predefined
    value, say retreat, regardless of the command issued by the
    general. The conjunction of both requirements makes the problem
  difficult.Oral and Written MessagesThere are two versions of the problem.Written Messages:
    In this version an agent may send copies of messages that it
    receives to other agents but cannot modify the
    messages. Also, an agent cannot forge signatures. So, an agent can
    receive a message M signed by lieutenant \(A\) only if \(A\) sent M to
    some agent.Oral Messages:
    In this version an agent can modify messages and forge
    signatures.So, an agent can receive a message M signed by agent
    \(A\) even if \(A\) never sent M to any agent.The algorithm for written messages is simpler and requires fewer
    messages.Algorithm with Written MessagesA reliable lieutenant who does not get a message from in round 0 treats the
absence of the message as the same as receiving a retreat message.
  Likewise, if a loyal lieutenant gets a message that is not an attack
  or a retreat message then the lieutenant treats the message as a
  retreat message.
  (We use retreat as a default. We could just as well have used attack
  as the default.)Next, we give an overview of the algorithm.Commit to AttackAt each round in the algorithm, a loyal lieutenant has either
  committed to attack or not.
  A lieutenant that has not committed to attack on a round may commit
  to attack on a later round.
  If any loyal lieutenant commits to attack in any round then it remains
  committed to attack thereafter.
  A loyal lieutenant retreats if it has not committed to attack at the
  end of the last round.MessagesA message is an attack message from the general or a commitment to
  attack by a lieutenant. We call commitment messages
  attack messages. An attack message is identified by the
  agent (general or lieutenant) that created the message.Evidence for AttackA loyal lieutenant commits to attack on round \(r \geq 1 \), for the first
  time, if the lieutenant has received an attack message from the
  general and at
  least \(r - 1\) lieutenants. 
  When the lieutenant commits to attack it sends copies of these
  messages, and its own attack message, to all
  other lieutenants.
  So, in round \(r+1\), each lieutenant receives an attack messages from
  the general and at least \(r\)
  lieutenants.
  And so, all loyal lieutenants commit to attack in this round.The AlgorithmWe will use the tactic that is helpful in analyzing distributed
algorithms that operate in rounds.
We will prove properties of a sequential algorithm and then show the
  equivalence of the sequential and distributed algorithms.
The sequential algorithm is given next.Local VariablesAssociated with each lieutenantCare local variablesC.receivedandC.sentwhich are sets of
  agents (general or lieutenants).C.receivedis the set of agents from whichChas received attack messages (or their copies).Csends attack messages (or copies) from agents inC.sent.
The symbolgrepresents the general.Initialization: Round 0A.sent = {}for all lieutenantsA.If the general is loyal and sends attack messages then the set of
  agents from which a lieutenant has received attack messages is the
  singleton set consisting of the general. So, for all
  lieutenantsA:A.received = {g}If the general is loyal and sends retreat messages then for all
  lieutenantsA:A.received = {}If the general is disloyal, thenA.received = {g}for some lieutenantsA,
  andA.received = {}for the others.The algorithm operates in a sequence of rounds with round-numberrstepping from 1 tot+1. Ther-th iteration for loyal agentcconsists of
the following two steps.Roundr > 0Step 1if (|C.received| >= r) AND  (g in C.received):
    C.commit = True
    C.sent = C.received UNION {C}IfC.receivedhas messages that show that:  (1) the number of
agents that have committed to attack on roundris at leastr, and (2) the general has sent an
attack message, thenCcommits to attack and sends these
messages as well as an additional message thatChas
committed to attack.Step 2C.received = (
    (UNION over all loyal agents B of B.sent)
             UNION
     an arbitrary subset of disloyal agents)Creceives the messages sent by loyal lieutenants and messages
sent by disloyal lieutenants.
A disloyal lieutenant,A, can send messages to some lieutenants
thatAis committed to attack, and not send these messages
to other lieutenants.
LieutenantCreceives attack messages from an arbitrary subset
of disloyal lieutenants.LieutenantCdoes not know which agents are loyal and
which are disloyal.C.receivedis the set of all messages sent toCregardless of whether the senders are loyal or
disloyal.Proof of CorrectnessCase 1: General is loyal and sends attack
messagesIn this case, at the start round 1,C.received = {g}.
So, the if-clause in step 1 of round 1 is True, and therefore all
loyal lieutenants commit to attack at the end of round 1.Case 2: General is loyal and sends retreat
messagesA disloyal lieutenant cannot forge the general's signature, and so it is
impossible for any lieutenant to have a copy of an attack message from the
general.
Therefore the if-clause of step 1 is never satisfied, and so no loyal
lieutenant commits to attack in any step.Case 3: General is disloyalPart 1We first show that if any loyal lieutenant commits on roundrthen all loyal lieutenants commit by the end of roundr+1.A loyal lieutenantCcommits on roundrexactly when the setC.receivedhas at leastrattack messages including one from the general.
So, in roundr,C.senthas at leastr+1attack messages including one from the general.
Therefore, the if-clause of step 1 evaluates to True in roundr+1, and so all
loyal lieutenants commit by the end of roundr+1.Therefore, if any loyal lieutenant commits by the end of roundt-1then all loyal lieutenants commit by the end of roundt.Part 2We next show that if no loyal lieutenant has committed by the end of
roundt-1then no loyal lieutenant commits in roundt.C.sent = {}at the end
of roundt-1if no loyal lieutenant has committed to
attack.
Therefore in roundt,C.receivedis an 
arbitrary subset of disloyal lieutenants.
There are at mostt-1disloyal lieutenants because there
are at mosttdisloyal agents, and the general is
disloyal.
So, on roundt, the if-clause of step 1 evaluates to False.Part 3From parts 1 and 2, by the end of
roundt, either all loyal lieutenants commit, or no loyal
lieutenant commits.ExampleThe figure below illustrates a situation in which a loyal lieutenant C commits to
attack on round 3, if it hasn't already committed to attack on rounds
1 and 2.
The figure shows C getting attack messages (red boxes) signed by the
general and lieutenants A and B on round 2. The general and
lieutenants A and B may be disloyal or loyal.Fig.2: Example of a lieutenant committing to attackBecause C is loyal it follows the algorithm, and C
commits to attack at the end of round 3 because C receives a signed attack
 message from the general and signed attack messages from two
 different lieutenants.
So, in round 3, C broadcasts copies of attack messages
signed by the general and attack
messages signed by A and B and attack messages, and also an attack
 message signed by C itself. All loyal
lieutenants commit to attack in round 4 because they receive attack
messages signed by the general and 3 different lieutenants (A, B, C).ReviewAssume that you are explaining the algorithm to someone who hasn't
  taken a course on distributed computing. How would you explain to
  this person that even if the general and 999 lieutenants are
  disloyal, and only 2 lieutenants are loyal, the loyal lieutenants
  reach a consensus after round 100? 
  (Remember the first round is round 0.)Will the algorithm work if at most one lieutenant could modify
  written messages, and all the other lieutenants followed the
  protocol, i.e. these lieutenants could copy but not modify
  messages.K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology



Figure Link: https://kmchandy.github.io/Byzantine/Slide01.jpg

Caption: Fig.1: Loyal and disloyal general behavior


A loyal general sends attack messages to all lieutenants or sends
retreat messages to all lieutenants.
A disloyal general sends arbitrary messages to lieutenants.Validity: Loyal lieutenants must obey a loyal general. If a loyal general
    gives the command to attack then all loyal lieutenants must
    attack. Likewise, if a loyal general
    gives the command to retreat then all loyal lieutenants must
    retreat.Consensus: Loyal lieutenants come to a consensus: either all of them
    attack or all of them retreat.The specification does not require that traitors be
    discovered. For example, the algorithm doesn't have to determine
    whether the general or a lieutenant is loyal or disloyal.If the only requirement is validity, and consensus isn't
    required, then the solution is trivial: all loyal lieutenants obey
    the general whether the general is loyal or disloyal. If the only
    requirement is consensus then the
    solution is trivial: all loyal lieutenants agree on a predefined
    value, say retreat, regardless of the command issued by the
    general. The conjunction of both requirements makes the problem
  difficult.Oral and Written MessagesThere are two versions of the problem.Written Messages:
    In this version an agent may send copies of messages that it
    receives to other agents but cannot modify the
    messages. Also, an agent cannot forge signatures. So, an agent can
    receive a message M signed by lieutenant \(A\) only if \(A\) sent M to
    some agent.Oral Messages:
    In this version an agent can modify messages and forge
    signatures.So, an agent can receive a message M signed by agent
    \(A\) even if \(A\) never sent M to any agent.The algorithm for written messages is simpler and requires fewer
    messages.Algorithm with Written MessagesA reliable lieutenant who does not get a message from in round 0 treats the
absence of the message as the same as receiving a retreat message.
  Likewise, if a loyal lieutenant gets a message that is not an attack
  or a retreat message then the lieutenant treats the message as a
  retreat message.
  (We use retreat as a default. We could just as well have used attack
  as the default.)Next, we give an overview of the algorithm.Commit to AttackAt each round in the algorithm, a loyal lieutenant has either
  committed to attack or not.
  A lieutenant that has not committed to attack on a round may commit
  to attack on a later round.
  If any loyal lieutenant commits to attack in any round then it remains
  committed to attack thereafter.
  A loyal lieutenant retreats if it has not committed to attack at the
  end of the last round.MessagesA message is an attack message from the general or a commitment to
  attack by a lieutenant. We call commitment messages
  attack messages. An attack message is identified by the
  agent (general or lieutenant) that created the message.Evidence for AttackA loyal lieutenant commits to attack on round \(r \geq 1 \), for the first
  time, if the lieutenant has received an attack message from the
  general and at
  least \(r - 1\) lieutenants. 
  When the lieutenant commits to attack it sends copies of these
  messages, and its own attack message, to all
  other lieutenants.
  So, in round \(r+1\), each lieutenant receives an attack messages from
  the general and at least \(r\)
  lieutenants.
  And so, all loyal lieutenants commit to attack in this round.The AlgorithmWe will use the tactic that is helpful in analyzing distributed
algorithms that operate in rounds.
We will prove properties of a sequential algorithm and then show the
  equivalence of the sequential and distributed algorithms.
The sequential algorithm is given next.Local VariablesAssociated with each lieutenantCare local variablesC.receivedandC.sentwhich are sets of
  agents (general or lieutenants).C.receivedis the set of agents from whichChas received attack messages (or their copies).Csends attack messages (or copies) from agents inC.sent.
The symbolgrepresents the general.Initialization: Round 0A.sent = {}for all lieutenantsA.If the general is loyal and sends attack messages then the set of
  agents from which a lieutenant has received attack messages is the
  singleton set consisting of the general. So, for all
  lieutenantsA:A.received = {g}If the general is loyal and sends retreat messages then for all
  lieutenantsA:A.received = {}If the general is disloyal, thenA.received = {g}for some lieutenantsA,
  andA.received = {}for the others.The algorithm operates in a sequence of rounds with round-numberrstepping from 1 tot+1. Ther-th iteration for loyal agentcconsists of
the following two steps.Roundr > 0Step 1if (|C.received| >= r) AND  (g in C.received):
    C.commit = True
    C.sent = C.received UNION {C}IfC.receivedhas messages that show that:  (1) the number of
agents that have committed to attack on roundris at leastr, and (2) the general has sent an
attack message, thenCcommits to attack and sends these
messages as well as an additional message thatChas
committed to attack.Step 2C.received = (
    (UNION over all loyal agents B of B.sent)
             UNION
     an arbitrary subset of disloyal agents)Creceives the messages sent by loyal lieutenants and messages
sent by disloyal lieutenants.
A disloyal lieutenant,A, can send messages to some lieutenants
thatAis committed to attack, and not send these messages
to other lieutenants.
LieutenantCreceives attack messages from an arbitrary subset
of disloyal lieutenants.LieutenantCdoes not know which agents are loyal and
which are disloyal.C.receivedis the set of all messages sent toCregardless of whether the senders are loyal or
disloyal.Proof of CorrectnessCase 1: General is loyal and sends attack
messagesIn this case, at the start round 1,C.received = {g}.
So, the if-clause in step 1 of round 1 is True, and therefore all
loyal lieutenants commit to attack at the end of round 1.Case 2: General is loyal and sends retreat
messagesA disloyal lieutenant cannot forge the general's signature, and so it is
impossible for any lieutenant to have a copy of an attack message from the
general.
Therefore the if-clause of step 1 is never satisfied, and so no loyal
lieutenant commits to attack in any step.Case 3: General is disloyalPart 1We first show that if any loyal lieutenant commits on roundrthen all loyal lieutenants commit by the end of roundr+1.A loyal lieutenantCcommits on roundrexactly when the setC.receivedhas at leastrattack messages including one from the general.
So, in roundr,C.senthas at leastr+1attack messages including one from the general.
Therefore, the if-clause of step 1 evaluates to True in roundr+1, and so all
loyal lieutenants commit by the end of roundr+1.Therefore, if any loyal lieutenant commits by the end of roundt-1then all loyal lieutenants commit by the end of roundt.Part 2We next show that if no loyal lieutenant has committed by the end of
roundt-1then no loyal lieutenant commits in roundt.C.sent = {}at the end
of roundt-1if no loyal lieutenant has committed to
attack.
Therefore in roundt,C.receivedis an 
arbitrary subset of disloyal lieutenants.
There are at mostt-1disloyal lieutenants because there
are at mosttdisloyal agents, and the general is
disloyal.
So, on roundt, the if-clause of step 1 evaluates to False.Part 3From parts 1 and 2, by the end of
roundt, either all loyal lieutenants commit, or no loyal
lieutenant commits.ExampleThe figure below illustrates a situation in which a loyal lieutenant C commits to
attack on round 3, if it hasn't already committed to attack on rounds
1 and 2.
The figure shows C getting attack messages (red boxes) signed by the
general and lieutenants A and B on round 2. The general and
lieutenants A and B may be disloyal or loyal.Fig.2: Example of a lieutenant committing to attackBecause C is loyal it follows the algorithm, and C
commits to attack at the end of round 3 because C receives a signed attack
 message from the general and signed attack messages from two
 different lieutenants.
So, in round 3, C broadcasts copies of attack messages
signed by the general and attack
messages signed by A and B and attack messages, and also an attack
 message signed by C itself. All loyal
lieutenants commit to attack in round 4 because they receive attack
messages signed by the general and 3 different lieutenants (A, B, C).ReviewAssume that you are explaining the algorithm to someone who hasn't
  taken a course on distributed computing. How would you explain to
  this person that even if the general and 999 lieutenants are
  disloyal, and only 2 lieutenants are loyal, the loyal lieutenants
  reach a consensus after round 100? 
  (Remember the first round is round 0.)Will the algorithm work if at most one lieutenant could modify
  written messages, and all the other lieutenants followed the
  protocol, i.e. these lieutenants could copy but not modify
  messages.K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology



Figure Link: https://kmchandy.github.io/Byzantine/Slide02.jpg

Caption: Fig.2: Example of a lieutenant committing to attack


The specification does not require that traitors be
    discovered. For example, the algorithm doesn't have to determine
    whether the general or a lieutenant is loyal or disloyal.If the only requirement is validity, and consensus isn't
    required, then the solution is trivial: all loyal lieutenants obey
    the general whether the general is loyal or disloyal. If the only
    requirement is consensus then the
    solution is trivial: all loyal lieutenants agree on a predefined
    value, say retreat, regardless of the command issued by the
    general. The conjunction of both requirements makes the problem
  difficult.Oral and Written MessagesThere are two versions of the problem.Written Messages:
    In this version an agent may send copies of messages that it
    receives to other agents but cannot modify the
    messages. Also, an agent cannot forge signatures. So, an agent can
    receive a message M signed by lieutenant \(A\) only if \(A\) sent M to
    some agent.Oral Messages:
    In this version an agent can modify messages and forge
    signatures.So, an agent can receive a message M signed by agent
    \(A\) even if \(A\) never sent M to any agent.The algorithm for written messages is simpler and requires fewer
    messages.Algorithm with Written MessagesA reliable lieutenant who does not get a message from in round 0 treats the
absence of the message as the same as receiving a retreat message.
  Likewise, if a loyal lieutenant gets a message that is not an attack
  or a retreat message then the lieutenant treats the message as a
  retreat message.
  (We use retreat as a default. We could just as well have used attack
  as the default.)Next, we give an overview of the algorithm.Commit to AttackAt each round in the algorithm, a loyal lieutenant has either
  committed to attack or not.
  A lieutenant that has not committed to attack on a round may commit
  to attack on a later round.
  If any loyal lieutenant commits to attack in any round then it remains
  committed to attack thereafter.
  A loyal lieutenant retreats if it has not committed to attack at the
  end of the last round.MessagesA message is an attack message from the general or a commitment to
  attack by a lieutenant. We call commitment messages
  attack messages. An attack message is identified by the
  agent (general or lieutenant) that created the message.Evidence for AttackA loyal lieutenant commits to attack on round \(r \geq 1 \), for the first
  time, if the lieutenant has received an attack message from the
  general and at
  least \(r - 1\) lieutenants. 
  When the lieutenant commits to attack it sends copies of these
  messages, and its own attack message, to all
  other lieutenants.
  So, in round \(r+1\), each lieutenant receives an attack messages from
  the general and at least \(r\)
  lieutenants.
  And so, all loyal lieutenants commit to attack in this round.The AlgorithmWe will use the tactic that is helpful in analyzing distributed
algorithms that operate in rounds.
We will prove properties of a sequential algorithm and then show the
  equivalence of the sequential and distributed algorithms.
The sequential algorithm is given next.Local VariablesAssociated with each lieutenantCare local variablesC.receivedandC.sentwhich are sets of
  agents (general or lieutenants).C.receivedis the set of agents from whichChas received attack messages (or their copies).Csends attack messages (or copies) from agents inC.sent.
The symbolgrepresents the general.Initialization: Round 0A.sent = {}for all lieutenantsA.If the general is loyal and sends attack messages then the set of
  agents from which a lieutenant has received attack messages is the
  singleton set consisting of the general. So, for all
  lieutenantsA:A.received = {g}If the general is loyal and sends retreat messages then for all
  lieutenantsA:A.received = {}If the general is disloyal, thenA.received = {g}for some lieutenantsA,
  andA.received = {}for the others.The algorithm operates in a sequence of rounds with round-numberrstepping from 1 tot+1. Ther-th iteration for loyal agentcconsists of
the following two steps.Roundr > 0Step 1if (|C.received| >= r) AND  (g in C.received):
    C.commit = True
    C.sent = C.received UNION {C}IfC.receivedhas messages that show that:  (1) the number of
agents that have committed to attack on roundris at leastr, and (2) the general has sent an
attack message, thenCcommits to attack and sends these
messages as well as an additional message thatChas
committed to attack.Step 2C.received = (
    (UNION over all loyal agents B of B.sent)
             UNION
     an arbitrary subset of disloyal agents)Creceives the messages sent by loyal lieutenants and messages
sent by disloyal lieutenants.
A disloyal lieutenant,A, can send messages to some lieutenants
thatAis committed to attack, and not send these messages
to other lieutenants.
LieutenantCreceives attack messages from an arbitrary subset
of disloyal lieutenants.LieutenantCdoes not know which agents are loyal and
which are disloyal.C.receivedis the set of all messages sent toCregardless of whether the senders are loyal or
disloyal.Proof of CorrectnessCase 1: General is loyal and sends attack
messagesIn this case, at the start round 1,C.received = {g}.
So, the if-clause in step 1 of round 1 is True, and therefore all
loyal lieutenants commit to attack at the end of round 1.Case 2: General is loyal and sends retreat
messagesA disloyal lieutenant cannot forge the general's signature, and so it is
impossible for any lieutenant to have a copy of an attack message from the
general.
Therefore the if-clause of step 1 is never satisfied, and so no loyal
lieutenant commits to attack in any step.Case 3: General is disloyalPart 1We first show that if any loyal lieutenant commits on roundrthen all loyal lieutenants commit by the end of roundr+1.A loyal lieutenantCcommits on roundrexactly when the setC.receivedhas at leastrattack messages including one from the general.
So, in roundr,C.senthas at leastr+1attack messages including one from the general.
Therefore, the if-clause of step 1 evaluates to True in roundr+1, and so all
loyal lieutenants commit by the end of roundr+1.Therefore, if any loyal lieutenant commits by the end of roundt-1then all loyal lieutenants commit by the end of roundt.Part 2We next show that if no loyal lieutenant has committed by the end of
roundt-1then no loyal lieutenant commits in roundt.C.sent = {}at the end
of roundt-1if no loyal lieutenant has committed to
attack.
Therefore in roundt,C.receivedis an 
arbitrary subset of disloyal lieutenants.
There are at mostt-1disloyal lieutenants because there
are at mosttdisloyal agents, and the general is
disloyal.
So, on roundt, the if-clause of step 1 evaluates to False.Part 3From parts 1 and 2, by the end of
roundt, either all loyal lieutenants commit, or no loyal
lieutenant commits.ExampleThe figure below illustrates a situation in which a loyal lieutenant C commits to
attack on round 3, if it hasn't already committed to attack on rounds
1 and 2.
The figure shows C getting attack messages (red boxes) signed by the
general and lieutenants A and B on round 2. The general and
lieutenants A and B may be disloyal or loyal.Fig.2: Example of a lieutenant committing to attackBecause C is loyal it follows the algorithm, and C
commits to attack at the end of round 3 because C receives a signed attack
 message from the general and signed attack messages from two
 different lieutenants.
So, in round 3, C broadcasts copies of attack messages
signed by the general and attack
messages signed by A and B and attack messages, and also an attack
 message signed by C itself. All loyal
lieutenants commit to attack in round 4 because they receive attack
messages signed by the general and 3 different lieutenants (A, B, C).ReviewAssume that you are explaining the algorithm to someone who hasn't
  taken a course on distributed computing. How would you explain to
  this person that even if the general and 999 lieutenants are
  disloyal, and only 2 lieutenants are loyal, the loyal lieutenants
  reach a consensus after round 100? 
  (Remember the first round is round 0.)Will the algorithm work if at most one lieutenant could modify
  written messages, and all the other lieutenants followed the
  protocol, i.e. these lieutenants could copy but not modify
  messages.K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology



Figure Link: https://kmchandy.github.io/Byzantine/Slide02.jpg

Caption: Fig.2: Example of a lieutenant committing to attack


If the only requirement is validity, and consensus isn't
    required, then the solution is trivial: all loyal lieutenants obey
    the general whether the general is loyal or disloyal. If the only
    requirement is consensus then the
    solution is trivial: all loyal lieutenants agree on a predefined
    value, say retreat, regardless of the command issued by the
    general. The conjunction of both requirements makes the problem
  difficult.Oral and Written MessagesThere are two versions of the problem.Written Messages:
    In this version an agent may send copies of messages that it
    receives to other agents but cannot modify the
    messages. Also, an agent cannot forge signatures. So, an agent can
    receive a message M signed by lieutenant \(A\) only if \(A\) sent M to
    some agent.Oral Messages:
    In this version an agent can modify messages and forge
    signatures.So, an agent can receive a message M signed by agent
    \(A\) even if \(A\) never sent M to any agent.The algorithm for written messages is simpler and requires fewer
    messages.Algorithm with Written MessagesA reliable lieutenant who does not get a message from in round 0 treats the
absence of the message as the same as receiving a retreat message.
  Likewise, if a loyal lieutenant gets a message that is not an attack
  or a retreat message then the lieutenant treats the message as a
  retreat message.
  (We use retreat as a default. We could just as well have used attack
  as the default.)Next, we give an overview of the algorithm.Commit to AttackAt each round in the algorithm, a loyal lieutenant has either
  committed to attack or not.
  A lieutenant that has not committed to attack on a round may commit
  to attack on a later round.
  If any loyal lieutenant commits to attack in any round then it remains
  committed to attack thereafter.
  A loyal lieutenant retreats if it has not committed to attack at the
  end of the last round.MessagesA message is an attack message from the general or a commitment to
  attack by a lieutenant. We call commitment messages
  attack messages. An attack message is identified by the
  agent (general or lieutenant) that created the message.Evidence for AttackA loyal lieutenant commits to attack on round \(r \geq 1 \), for the first
  time, if the lieutenant has received an attack message from the
  general and at
  least \(r - 1\) lieutenants. 
  When the lieutenant commits to attack it sends copies of these
  messages, and its own attack message, to all
  other lieutenants.
  So, in round \(r+1\), each lieutenant receives an attack messages from
  the general and at least \(r\)
  lieutenants.
  And so, all loyal lieutenants commit to attack in this round.The AlgorithmWe will use the tactic that is helpful in analyzing distributed
algorithms that operate in rounds.
We will prove properties of a sequential algorithm and then show the
  equivalence of the sequential and distributed algorithms.
The sequential algorithm is given next.Local VariablesAssociated with each lieutenantCare local variablesC.receivedandC.sentwhich are sets of
  agents (general or lieutenants).C.receivedis the set of agents from whichChas received attack messages (or their copies).Csends attack messages (or copies) from agents inC.sent.
The symbolgrepresents the general.Initialization: Round 0A.sent = {}for all lieutenantsA.If the general is loyal and sends attack messages then the set of
  agents from which a lieutenant has received attack messages is the
  singleton set consisting of the general. So, for all
  lieutenantsA:A.received = {g}If the general is loyal and sends retreat messages then for all
  lieutenantsA:A.received = {}If the general is disloyal, thenA.received = {g}for some lieutenantsA,
  andA.received = {}for the others.The algorithm operates in a sequence of rounds with round-numberrstepping from 1 tot+1. Ther-th iteration for loyal agentcconsists of
the following two steps.Roundr > 0Step 1if (|C.received| >= r) AND  (g in C.received):
    C.commit = True
    C.sent = C.received UNION {C}IfC.receivedhas messages that show that:  (1) the number of
agents that have committed to attack on roundris at leastr, and (2) the general has sent an
attack message, thenCcommits to attack and sends these
messages as well as an additional message thatChas
committed to attack.Step 2C.received = (
    (UNION over all loyal agents B of B.sent)
             UNION
     an arbitrary subset of disloyal agents)Creceives the messages sent by loyal lieutenants and messages
sent by disloyal lieutenants.
A disloyal lieutenant,A, can send messages to some lieutenants
thatAis committed to attack, and not send these messages
to other lieutenants.
LieutenantCreceives attack messages from an arbitrary subset
of disloyal lieutenants.LieutenantCdoes not know which agents are loyal and
which are disloyal.C.receivedis the set of all messages sent toCregardless of whether the senders are loyal or
disloyal.Proof of CorrectnessCase 1: General is loyal and sends attack
messagesIn this case, at the start round 1,C.received = {g}.
So, the if-clause in step 1 of round 1 is True, and therefore all
loyal lieutenants commit to attack at the end of round 1.Case 2: General is loyal and sends retreat
messagesA disloyal lieutenant cannot forge the general's signature, and so it is
impossible for any lieutenant to have a copy of an attack message from the
general.
Therefore the if-clause of step 1 is never satisfied, and so no loyal
lieutenant commits to attack in any step.Case 3: General is disloyalPart 1We first show that if any loyal lieutenant commits on roundrthen all loyal lieutenants commit by the end of roundr+1.A loyal lieutenantCcommits on roundrexactly when the setC.receivedhas at leastrattack messages including one from the general.
So, in roundr,C.senthas at leastr+1attack messages including one from the general.
Therefore, the if-clause of step 1 evaluates to True in roundr+1, and so all
loyal lieutenants commit by the end of roundr+1.Therefore, if any loyal lieutenant commits by the end of roundt-1then all loyal lieutenants commit by the end of roundt.Part 2We next show that if no loyal lieutenant has committed by the end of
roundt-1then no loyal lieutenant commits in roundt.C.sent = {}at the end
of roundt-1if no loyal lieutenant has committed to
attack.
Therefore in roundt,C.receivedis an 
arbitrary subset of disloyal lieutenants.
There are at mostt-1disloyal lieutenants because there
are at mosttdisloyal agents, and the general is
disloyal.
So, on roundt, the if-clause of step 1 evaluates to False.Part 3From parts 1 and 2, by the end of
roundt, either all loyal lieutenants commit, or no loyal
lieutenant commits.ExampleThe figure below illustrates a situation in which a loyal lieutenant C commits to
attack on round 3, if it hasn't already committed to attack on rounds
1 and 2.
The figure shows C getting attack messages (red boxes) signed by the
general and lieutenants A and B on round 2. The general and
lieutenants A and B may be disloyal or loyal.Fig.2: Example of a lieutenant committing to attackBecause C is loyal it follows the algorithm, and C
commits to attack at the end of round 3 because C receives a signed attack
 message from the general and signed attack messages from two
 different lieutenants.
So, in round 3, C broadcasts copies of attack messages
signed by the general and attack
messages signed by A and B and attack messages, and also an attack
 message signed by C itself. All loyal
lieutenants commit to attack in round 4 because they receive attack
messages signed by the general and 3 different lieutenants (A, B, C).ReviewAssume that you are explaining the algorithm to someone who hasn't
  taken a course on distributed computing. How would you explain to
  this person that even if the general and 999 lieutenants are
  disloyal, and only 2 lieutenants are loyal, the loyal lieutenants
  reach a consensus after round 100? 
  (Remember the first round is round 0.)Will the algorithm work if at most one lieutenant could modify
  written messages, and all the other lieutenants followed the
  protocol, i.e. these lieutenants could copy but not modify
  messages.K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology



Figure Link: https://kmchandy.github.io/Byzantine/Slide02.jpg

Caption: Fig.2: Example of a lieutenant committing to attack


A reliable lieutenant who does not get a message from in round 0 treats the
absence of the message as the same as receiving a retreat message.
  Likewise, if a loyal lieutenant gets a message that is not an attack
  or a retreat message then the lieutenant treats the message as a
  retreat message.
  (We use retreat as a default. We could just as well have used attack
  as the default.)Next, we give an overview of the algorithm.Commit to AttackAt each round in the algorithm, a loyal lieutenant has either
  committed to attack or not.
  A lieutenant that has not committed to attack on a round may commit
  to attack on a later round.
  If any loyal lieutenant commits to attack in any round then it remains
  committed to attack thereafter.
  A loyal lieutenant retreats if it has not committed to attack at the
  end of the last round.MessagesA message is an attack message from the general or a commitment to
  attack by a lieutenant. We call commitment messages
  attack messages. An attack message is identified by the
  agent (general or lieutenant) that created the message.Evidence for AttackA loyal lieutenant commits to attack on round \(r \geq 1 \), for the first
  time, if the lieutenant has received an attack message from the
  general and at
  least \(r - 1\) lieutenants. 
  When the lieutenant commits to attack it sends copies of these
  messages, and its own attack message, to all
  other lieutenants.
  So, in round \(r+1\), each lieutenant receives an attack messages from
  the general and at least \(r\)
  lieutenants.
  And so, all loyal lieutenants commit to attack in this round.The AlgorithmWe will use the tactic that is helpful in analyzing distributed
algorithms that operate in rounds.
We will prove properties of a sequential algorithm and then show the
  equivalence of the sequential and distributed algorithms.
The sequential algorithm is given next.Local VariablesAssociated with each lieutenantCare local variablesC.receivedandC.sentwhich are sets of
  agents (general or lieutenants).C.receivedis the set of agents from whichChas received attack messages (or their copies).Csends attack messages (or copies) from agents inC.sent.
The symbolgrepresents the general.Initialization: Round 0A.sent = {}for all lieutenantsA.If the general is loyal and sends attack messages then the set of
  agents from which a lieutenant has received attack messages is the
  singleton set consisting of the general. So, for all
  lieutenantsA:A.received = {g}If the general is loyal and sends retreat messages then for all
  lieutenantsA:A.received = {}If the general is disloyal, thenA.received = {g}for some lieutenantsA,
  andA.received = {}for the others.The algorithm operates in a sequence of rounds with round-numberrstepping from 1 tot+1. Ther-th iteration for loyal agentcconsists of
the following two steps.Roundr > 0Step 1if (|C.received| >= r) AND  (g in C.received):
    C.commit = True
    C.sent = C.received UNION {C}IfC.receivedhas messages that show that:  (1) the number of
agents that have committed to attack on roundris at leastr, and (2) the general has sent an
attack message, thenCcommits to attack and sends these
messages as well as an additional message thatChas
committed to attack.Step 2C.received = (
    (UNION over all loyal agents B of B.sent)
             UNION
     an arbitrary subset of disloyal agents)Creceives the messages sent by loyal lieutenants and messages
sent by disloyal lieutenants.
A disloyal lieutenant,A, can send messages to some lieutenants
thatAis committed to attack, and not send these messages
to other lieutenants.
LieutenantCreceives attack messages from an arbitrary subset
of disloyal lieutenants.LieutenantCdoes not know which agents are loyal and
which are disloyal.C.receivedis the set of all messages sent toCregardless of whether the senders are loyal or
disloyal.Proof of CorrectnessCase 1: General is loyal and sends attack
messagesIn this case, at the start round 1,C.received = {g}.
So, the if-clause in step 1 of round 1 is True, and therefore all
loyal lieutenants commit to attack at the end of round 1.Case 2: General is loyal and sends retreat
messagesA disloyal lieutenant cannot forge the general's signature, and so it is
impossible for any lieutenant to have a copy of an attack message from the
general.
Therefore the if-clause of step 1 is never satisfied, and so no loyal
lieutenant commits to attack in any step.Case 3: General is disloyalPart 1We first show that if any loyal lieutenant commits on roundrthen all loyal lieutenants commit by the end of roundr+1.A loyal lieutenantCcommits on roundrexactly when the setC.receivedhas at leastrattack messages including one from the general.
So, in roundr,C.senthas at leastr+1attack messages including one from the general.
Therefore, the if-clause of step 1 evaluates to True in roundr+1, and so all
loyal lieutenants commit by the end of roundr+1.Therefore, if any loyal lieutenant commits by the end of roundt-1then all loyal lieutenants commit by the end of roundt.Part 2We next show that if no loyal lieutenant has committed by the end of
roundt-1then no loyal lieutenant commits in roundt.C.sent = {}at the end
of roundt-1if no loyal lieutenant has committed to
attack.
Therefore in roundt,C.receivedis an 
arbitrary subset of disloyal lieutenants.
There are at mostt-1disloyal lieutenants because there
are at mosttdisloyal agents, and the general is
disloyal.
So, on roundt, the if-clause of step 1 evaluates to False.Part 3From parts 1 and 2, by the end of
roundt, either all loyal lieutenants commit, or no loyal
lieutenant commits.ExampleThe figure below illustrates a situation in which a loyal lieutenant C commits to
attack on round 3, if it hasn't already committed to attack on rounds
1 and 2.
The figure shows C getting attack messages (red boxes) signed by the
general and lieutenants A and B on round 2. The general and
lieutenants A and B may be disloyal or loyal.Fig.2: Example of a lieutenant committing to attackBecause C is loyal it follows the algorithm, and C
commits to attack at the end of round 3 because C receives a signed attack
 message from the general and signed attack messages from two
 different lieutenants.
So, in round 3, C broadcasts copies of attack messages
signed by the general and attack
messages signed by A and B and attack messages, and also an attack
 message signed by C itself. All loyal
lieutenants commit to attack in round 4 because they receive attack
messages signed by the general and 3 different lieutenants (A, B, C).ReviewAssume that you are explaining the algorithm to someone who hasn't
  taken a course on distributed computing. How would you explain to
  this person that even if the general and 999 lieutenants are
  disloyal, and only 2 lieutenants are loyal, the loyal lieutenants
  reach a consensus after round 100? 
  (Remember the first round is round 0.)Will the algorithm work if at most one lieutenant could modify
  written messages, and all the other lieutenants followed the
  protocol, i.e. these lieutenants could copy but not modify
  messages.K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology



Figure Link: https://kmchandy.github.io/Byzantine/Slide02.jpg

Caption: Fig.2: Example of a lieutenant committing to attack


Next, we give an overview of the algorithm.Commit to AttackAt each round in the algorithm, a loyal lieutenant has either
  committed to attack or not.
  A lieutenant that has not committed to attack on a round may commit
  to attack on a later round.
  If any loyal lieutenant commits to attack in any round then it remains
  committed to attack thereafter.
  A loyal lieutenant retreats if it has not committed to attack at the
  end of the last round.MessagesA message is an attack message from the general or a commitment to
  attack by a lieutenant. We call commitment messages
  attack messages. An attack message is identified by the
  agent (general or lieutenant) that created the message.Evidence for AttackA loyal lieutenant commits to attack on round \(r \geq 1 \), for the first
  time, if the lieutenant has received an attack message from the
  general and at
  least \(r - 1\) lieutenants. 
  When the lieutenant commits to attack it sends copies of these
  messages, and its own attack message, to all
  other lieutenants.
  So, in round \(r+1\), each lieutenant receives an attack messages from
  the general and at least \(r\)
  lieutenants.
  And so, all loyal lieutenants commit to attack in this round.The AlgorithmWe will use the tactic that is helpful in analyzing distributed
algorithms that operate in rounds.
We will prove properties of a sequential algorithm and then show the
  equivalence of the sequential and distributed algorithms.
The sequential algorithm is given next.Local VariablesAssociated with each lieutenantCare local variablesC.receivedandC.sentwhich are sets of
  agents (general or lieutenants).C.receivedis the set of agents from whichChas received attack messages (or their copies).Csends attack messages (or copies) from agents inC.sent.
The symbolgrepresents the general.Initialization: Round 0A.sent = {}for all lieutenantsA.If the general is loyal and sends attack messages then the set of
  agents from which a lieutenant has received attack messages is the
  singleton set consisting of the general. So, for all
  lieutenantsA:A.received = {g}If the general is loyal and sends retreat messages then for all
  lieutenantsA:A.received = {}If the general is disloyal, thenA.received = {g}for some lieutenantsA,
  andA.received = {}for the others.The algorithm operates in a sequence of rounds with round-numberrstepping from 1 tot+1. Ther-th iteration for loyal agentcconsists of
the following two steps.Roundr > 0Step 1if (|C.received| >= r) AND  (g in C.received):
    C.commit = True
    C.sent = C.received UNION {C}IfC.receivedhas messages that show that:  (1) the number of
agents that have committed to attack on roundris at leastr, and (2) the general has sent an
attack message, thenCcommits to attack and sends these
messages as well as an additional message thatChas
committed to attack.Step 2C.received = (
    (UNION over all loyal agents B of B.sent)
             UNION
     an arbitrary subset of disloyal agents)Creceives the messages sent by loyal lieutenants and messages
sent by disloyal lieutenants.
A disloyal lieutenant,A, can send messages to some lieutenants
thatAis committed to attack, and not send these messages
to other lieutenants.
LieutenantCreceives attack messages from an arbitrary subset
of disloyal lieutenants.LieutenantCdoes not know which agents are loyal and
which are disloyal.C.receivedis the set of all messages sent toCregardless of whether the senders are loyal or
disloyal.Proof of CorrectnessCase 1: General is loyal and sends attack
messagesIn this case, at the start round 1,C.received = {g}.
So, the if-clause in step 1 of round 1 is True, and therefore all
loyal lieutenants commit to attack at the end of round 1.Case 2: General is loyal and sends retreat
messagesA disloyal lieutenant cannot forge the general's signature, and so it is
impossible for any lieutenant to have a copy of an attack message from the
general.
Therefore the if-clause of step 1 is never satisfied, and so no loyal
lieutenant commits to attack in any step.Case 3: General is disloyalPart 1We first show that if any loyal lieutenant commits on roundrthen all loyal lieutenants commit by the end of roundr+1.A loyal lieutenantCcommits on roundrexactly when the setC.receivedhas at leastrattack messages including one from the general.
So, in roundr,C.senthas at leastr+1attack messages including one from the general.
Therefore, the if-clause of step 1 evaluates to True in roundr+1, and so all
loyal lieutenants commit by the end of roundr+1.Therefore, if any loyal lieutenant commits by the end of roundt-1then all loyal lieutenants commit by the end of roundt.Part 2We next show that if no loyal lieutenant has committed by the end of
roundt-1then no loyal lieutenant commits in roundt.C.sent = {}at the end
of roundt-1if no loyal lieutenant has committed to
attack.
Therefore in roundt,C.receivedis an 
arbitrary subset of disloyal lieutenants.
There are at mostt-1disloyal lieutenants because there
are at mosttdisloyal agents, and the general is
disloyal.
So, on roundt, the if-clause of step 1 evaluates to False.Part 3From parts 1 and 2, by the end of
roundt, either all loyal lieutenants commit, or no loyal
lieutenant commits.ExampleThe figure below illustrates a situation in which a loyal lieutenant C commits to
attack on round 3, if it hasn't already committed to attack on rounds
1 and 2.
The figure shows C getting attack messages (red boxes) signed by the
general and lieutenants A and B on round 2. The general and
lieutenants A and B may be disloyal or loyal.Fig.2: Example of a lieutenant committing to attackBecause C is loyal it follows the algorithm, and C
commits to attack at the end of round 3 because C receives a signed attack
 message from the general and signed attack messages from two
 different lieutenants.
So, in round 3, C broadcasts copies of attack messages
signed by the general and attack
messages signed by A and B and attack messages, and also an attack
 message signed by C itself. All loyal
lieutenants commit to attack in round 4 because they receive attack
messages signed by the general and 3 different lieutenants (A, B, C).ReviewAssume that you are explaining the algorithm to someone who hasn't
  taken a course on distributed computing. How would you explain to
  this person that even if the general and 999 lieutenants are
  disloyal, and only 2 lieutenants are loyal, the loyal lieutenants
  reach a consensus after round 100? 
  (Remember the first round is round 0.)Will the algorithm work if at most one lieutenant could modify
  written messages, and all the other lieutenants followed the
  protocol, i.e. these lieutenants could copy but not modify
  messages.K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology



Figure Link: https://kmchandy.github.io/Byzantine/Slide02.jpg

Caption: Fig.2: Example of a lieutenant committing to attack


Local Variables



Figure Link: https://kmchandy.github.io/Byzantine/Slide02.jpg

Caption: Fig.2: Example of a lieutenant committing to attack


Step 1



Figure Link: https://kmchandy.github.io/Byzantine/Slide02.jpg

Caption: Fig.2: Example of a lieutenant committing to attack


Step 2



Figure Link: https://kmchandy.github.io/Byzantine/Slide02.jpg

Caption: Fig.2: Example of a lieutenant committing to attack


LieutenantCdoes not know which agents are loyal and
which are disloyal.C.receivedis the set of all messages sent toCregardless of whether the senders are loyal or
disloyal.Proof of CorrectnessCase 1: General is loyal and sends attack
messagesIn this case, at the start round 1,C.received = {g}.
So, the if-clause in step 1 of round 1 is True, and therefore all
loyal lieutenants commit to attack at the end of round 1.Case 2: General is loyal and sends retreat
messagesA disloyal lieutenant cannot forge the general's signature, and so it is
impossible for any lieutenant to have a copy of an attack message from the
general.
Therefore the if-clause of step 1 is never satisfied, and so no loyal
lieutenant commits to attack in any step.Case 3: General is disloyalPart 1We first show that if any loyal lieutenant commits on roundrthen all loyal lieutenants commit by the end of roundr+1.A loyal lieutenantCcommits on roundrexactly when the setC.receivedhas at leastrattack messages including one from the general.
So, in roundr,C.senthas at leastr+1attack messages including one from the general.
Therefore, the if-clause of step 1 evaluates to True in roundr+1, and so all
loyal lieutenants commit by the end of roundr+1.Therefore, if any loyal lieutenant commits by the end of roundt-1then all loyal lieutenants commit by the end of roundt.Part 2We next show that if no loyal lieutenant has committed by the end of
roundt-1then no loyal lieutenant commits in roundt.C.sent = {}at the end
of roundt-1if no loyal lieutenant has committed to
attack.
Therefore in roundt,C.receivedis an 
arbitrary subset of disloyal lieutenants.
There are at mostt-1disloyal lieutenants because there
are at mosttdisloyal agents, and the general is
disloyal.
So, on roundt, the if-clause of step 1 evaluates to False.Part 3From parts 1 and 2, by the end of
roundt, either all loyal lieutenants commit, or no loyal
lieutenant commits.ExampleThe figure below illustrates a situation in which a loyal lieutenant C commits to
attack on round 3, if it hasn't already committed to attack on rounds
1 and 2.
The figure shows C getting attack messages (red boxes) signed by the
general and lieutenants A and B on round 2. The general and
lieutenants A and B may be disloyal or loyal.Fig.2: Example of a lieutenant committing to attackBecause C is loyal it follows the algorithm, and C
commits to attack at the end of round 3 because C receives a signed attack
 message from the general and signed attack messages from two
 different lieutenants.
So, in round 3, C broadcasts copies of attack messages
signed by the general and attack
messages signed by A and B and attack messages, and also an attack
 message signed by C itself. All loyal
lieutenants commit to attack in round 4 because they receive attack
messages signed by the general and 3 different lieutenants (A, B, C).ReviewAssume that you are explaining the algorithm to someone who hasn't
  taken a course on distributed computing. How would you explain to
  this person that even if the general and 999 lieutenants are
  disloyal, and only 2 lieutenants are loyal, the loyal lieutenants
  reach a consensus after round 100? 
  (Remember the first round is round 0.)Will the algorithm work if at most one lieutenant could modify
  written messages, and all the other lieutenants followed the
  protocol, i.e. these lieutenants could copy but not modify
  messages.K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology



Figure Link: https://kmchandy.github.io/Byzantine/Slide02.jpg

Caption: Fig.2: Example of a lieutenant committing to attack


Part 1



Figure Link: https://kmchandy.github.io/Byzantine/Slide02.jpg

Caption: Fig.2: Example of a lieutenant committing to attack


A loyal lieutenantCcommits on roundrexactly when the setC.receivedhas at leastrattack messages including one from the general.
So, in roundr,C.senthas at leastr+1attack messages including one from the general.
Therefore, the if-clause of step 1 evaluates to True in roundr+1, and so all
loyal lieutenants commit by the end of roundr+1.Therefore, if any loyal lieutenant commits by the end of roundt-1then all loyal lieutenants commit by the end of roundt.Part 2We next show that if no loyal lieutenant has committed by the end of
roundt-1then no loyal lieutenant commits in roundt.C.sent = {}at the end
of roundt-1if no loyal lieutenant has committed to
attack.
Therefore in roundt,C.receivedis an 
arbitrary subset of disloyal lieutenants.
There are at mostt-1disloyal lieutenants because there
are at mosttdisloyal agents, and the general is
disloyal.
So, on roundt, the if-clause of step 1 evaluates to False.Part 3From parts 1 and 2, by the end of
roundt, either all loyal lieutenants commit, or no loyal
lieutenant commits.ExampleThe figure below illustrates a situation in which a loyal lieutenant C commits to
attack on round 3, if it hasn't already committed to attack on rounds
1 and 2.
The figure shows C getting attack messages (red boxes) signed by the
general and lieutenants A and B on round 2. The general and
lieutenants A and B may be disloyal or loyal.Fig.2: Example of a lieutenant committing to attackBecause C is loyal it follows the algorithm, and C
commits to attack at the end of round 3 because C receives a signed attack
 message from the general and signed attack messages from two
 different lieutenants.
So, in round 3, C broadcasts copies of attack messages
signed by the general and attack
messages signed by A and B and attack messages, and also an attack
 message signed by C itself. All loyal
lieutenants commit to attack in round 4 because they receive attack
messages signed by the general and 3 different lieutenants (A, B, C).ReviewAssume that you are explaining the algorithm to someone who hasn't
  taken a course on distributed computing. How would you explain to
  this person that even if the general and 999 lieutenants are
  disloyal, and only 2 lieutenants are loyal, the loyal lieutenants
  reach a consensus after round 100? 
  (Remember the first round is round 0.)Will the algorithm work if at most one lieutenant could modify
  written messages, and all the other lieutenants followed the
  protocol, i.e. these lieutenants could copy but not modify
  messages.K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology



Figure Link: https://kmchandy.github.io/Byzantine/Slide02.jpg

Caption: Fig.2: Example of a lieutenant committing to attack


Therefore, if any loyal lieutenant commits by the end of roundt-1then all loyal lieutenants commit by the end of roundt.Part 2We next show that if no loyal lieutenant has committed by the end of
roundt-1then no loyal lieutenant commits in roundt.C.sent = {}at the end
of roundt-1if no loyal lieutenant has committed to
attack.
Therefore in roundt,C.receivedis an 
arbitrary subset of disloyal lieutenants.
There are at mostt-1disloyal lieutenants because there
are at mosttdisloyal agents, and the general is
disloyal.
So, on roundt, the if-clause of step 1 evaluates to False.Part 3From parts 1 and 2, by the end of
roundt, either all loyal lieutenants commit, or no loyal
lieutenant commits.ExampleThe figure below illustrates a situation in which a loyal lieutenant C commits to
attack on round 3, if it hasn't already committed to attack on rounds
1 and 2.
The figure shows C getting attack messages (red boxes) signed by the
general and lieutenants A and B on round 2. The general and
lieutenants A and B may be disloyal or loyal.Fig.2: Example of a lieutenant committing to attackBecause C is loyal it follows the algorithm, and C
commits to attack at the end of round 3 because C receives a signed attack
 message from the general and signed attack messages from two
 different lieutenants.
So, in round 3, C broadcasts copies of attack messages
signed by the general and attack
messages signed by A and B and attack messages, and also an attack
 message signed by C itself. All loyal
lieutenants commit to attack in round 4 because they receive attack
messages signed by the general and 3 different lieutenants (A, B, C).ReviewAssume that you are explaining the algorithm to someone who hasn't
  taken a course on distributed computing. How would you explain to
  this person that even if the general and 999 lieutenants are
  disloyal, and only 2 lieutenants are loyal, the loyal lieutenants
  reach a consensus after round 100? 
  (Remember the first round is round 0.)Will the algorithm work if at most one lieutenant could modify
  written messages, and all the other lieutenants followed the
  protocol, i.e. these lieutenants could copy but not modify
  messages.K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology



Figure Link: https://kmchandy.github.io/Byzantine/Slide02.jpg

Caption: Fig.2: Example of a lieutenant committing to attack


Part 2



Figure Link: https://kmchandy.github.io/Byzantine/Slide02.jpg

Caption: Fig.2: Example of a lieutenant committing to attack


C.sent = {}at the end
of roundt-1if no loyal lieutenant has committed to
attack.
Therefore in roundt,C.receivedis an 
arbitrary subset of disloyal lieutenants.
There are at mostt-1disloyal lieutenants because there
are at mosttdisloyal agents, and the general is
disloyal.
So, on roundt, the if-clause of step 1 evaluates to False.Part 3From parts 1 and 2, by the end of
roundt, either all loyal lieutenants commit, or no loyal
lieutenant commits.ExampleThe figure below illustrates a situation in which a loyal lieutenant C commits to
attack on round 3, if it hasn't already committed to attack on rounds
1 and 2.
The figure shows C getting attack messages (red boxes) signed by the
general and lieutenants A and B on round 2. The general and
lieutenants A and B may be disloyal or loyal.Fig.2: Example of a lieutenant committing to attackBecause C is loyal it follows the algorithm, and C
commits to attack at the end of round 3 because C receives a signed attack
 message from the general and signed attack messages from two
 different lieutenants.
So, in round 3, C broadcasts copies of attack messages
signed by the general and attack
messages signed by A and B and attack messages, and also an attack
 message signed by C itself. All loyal
lieutenants commit to attack in round 4 because they receive attack
messages signed by the general and 3 different lieutenants (A, B, C).ReviewAssume that you are explaining the algorithm to someone who hasn't
  taken a course on distributed computing. How would you explain to
  this person that even if the general and 999 lieutenants are
  disloyal, and only 2 lieutenants are loyal, the loyal lieutenants
  reach a consensus after round 100? 
  (Remember the first round is round 0.)Will the algorithm work if at most one lieutenant could modify
  written messages, and all the other lieutenants followed the
  protocol, i.e. these lieutenants could copy but not modify
  messages.K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology



Figure Link: https://kmchandy.github.io/Byzantine/Slide02.jpg

Caption: Fig.2: Example of a lieutenant committing to attack


Part 3From parts 1 and 2, by the end of
roundt, either all loyal lieutenants commit, or no loyal
lieutenant commits.ExampleThe figure below illustrates a situation in which a loyal lieutenant C commits to
attack on round 3, if it hasn't already committed to attack on rounds
1 and 2.
The figure shows C getting attack messages (red boxes) signed by the
general and lieutenants A and B on round 2. The general and
lieutenants A and B may be disloyal or loyal.Fig.2: Example of a lieutenant committing to attackBecause C is loyal it follows the algorithm, and C
commits to attack at the end of round 3 because C receives a signed attack
 message from the general and signed attack messages from two
 different lieutenants.
So, in round 3, C broadcasts copies of attack messages
signed by the general and attack
messages signed by A and B and attack messages, and also an attack
 message signed by C itself. All loyal
lieutenants commit to attack in round 4 because they receive attack
messages signed by the general and 3 different lieutenants (A, B, C).ReviewAssume that you are explaining the algorithm to someone who hasn't
  taken a course on distributed computing. How would you explain to
  this person that even if the general and 999 lieutenants are
  disloyal, and only 2 lieutenants are loyal, the loyal lieutenants
  reach a consensus after round 100? 
  (Remember the first round is round 0.)Will the algorithm work if at most one lieutenant could modify
  written messages, and all the other lieutenants followed the
  protocol, i.e. these lieutenants could copy but not modify
  messages.K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology



Figure Link: https://kmchandy.github.io/Byzantine/Slide02.jpg

Caption: Fig.2: Example of a lieutenant committing to attack


Part 3



Figure Link: https://kmchandy.github.io/Byzantine/Slide02.jpg

Caption: Fig.2: Example of a lieutenant committing to attack


Because C is loyal it follows the algorithm, and C
commits to attack at the end of round 3 because C receives a signed attack
 message from the general and signed attack messages from two
 different lieutenants.
So, in round 3, C broadcasts copies of attack messages
signed by the general and attack
messages signed by A and B and attack messages, and also an attack
 message signed by C itself. All loyal
lieutenants commit to attack in round 4 because they receive attack
messages signed by the general and 3 different lieutenants (A, B, C).ReviewAssume that you are explaining the algorithm to someone who hasn't
  taken a course on distributed computing. How would you explain to
  this person that even if the general and 999 lieutenants are
  disloyal, and only 2 lieutenants are loyal, the loyal lieutenants
  reach a consensus after round 100? 
  (Remember the first round is round 0.)Will the algorithm work if at most one lieutenant could modify
  written messages, and all the other lieutenants followed the
  protocol, i.e. these lieutenants could copy but not modify
  messages.K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology
--- END Content from https://kmchandy.github.io/Byzantine/ByzantineWritten.html ---


--- START Content from https://kmchandy.github.io/Byzantine/ByzantineOral.html ---
TITLE: Byzantine/ByzantineOral.html

This module describes a Byzantine consensus algorithm in which
messages are not encrypted. An agentxthat receives a message signed by
an agentycannot tell whetherysigned the message or
whether some other agent forgedy's signature and corrupted the
  message.



Figure Link: https://kmchandy.github.io/Byzantine/Byzantine_5/Byzantine_5.001.jpeg

Caption: Fig.1: A Part of the Message Tree for General and 6 Lieutenants


This module describes solutions to the Byzantine problem with oral
messages whereas the previous module studied the problem with written
messages.  For convenience we repeat the problem specification next.An agent is a general or a lieutenant.
  An agent may be loyal or disloyal.
  Let \(N\) be the number of agents and \(t\) the number of disloyal
  agents. 
  
The general sends a command to
each lieutenant where the command is either attack or retreat.
A loyal general sends the same command to all lieutenants whereas a
disloyal general may send different commands to different lieutenants.
Each lieutenant decides to attack or retreat at the end of the
algorithm.If all loyal lieutenants get the same command then each loyal
  lieutenant obeys the commands that it received; if the command is
  attack then each loyal lieutenant attacks, and if the command is
  retreat then each loyal lieutenant retreats.Even if loyal lieutenants receive different commands, all loyal
  lieutenants make the same the decision; either all loyal lieutenants
  attack or all loyal lieutenants retreat.NotationWe use indices \(i, j, k\) for loyal agents; \(x, y\) for
  generic agents who may be loyal or disloyal; and \(e\) for
  disloyal agents.
  Nothing in an agent's id or data identifies the agent as loyal or
  disloyal.
  Moreover, the algorithm does not have to discover which agents act
  disloyally.For a nonempty list \(L\), we use the Python notation \(L_{*}\) to
  refer the last element of the list.
  For example, if \(L = [5, 6]\), then \(L_{*} = 6\).For a list \(L\) and an element \(x\), the notation \(L , x\) represents a list
consisting of \(x\) appended to the tail of \(L\).
For example, if \(L\) is the list \([1, 2]\) then \(L , 3\) is the
  list \([1, 2, 3]\), and \(L, 3, 4\) is the list \([1, 2, 3, 4]\).The general is the agent with index \(0\), and the lieutenants have indices \(1,
\ldots, N-1\).Let \(m[x]\) be the message that the general sends
lieutenant \(x\), and let \(a[x]\) be the decision the lieutenant
  \(x\) makes.SpecificationThe specification has two parts, validity and consensus.Validity: Loyal lieutenants obey a loyal general.If all loyal lieutenants get the same message then each loyal
    lieutenant obeys the message that it receives.\(
  (\forall i, j: m[i] = m[j]) \quad \Rightarrow
\quad (\forall i: a[i] = m[i])
\)Consensus: Loyal lieutenants make the same
decision.\(
\forall i, j: a[i] = a[j]
\)AssumptionsThe oral Byzantine version makes fewer assumptions than the written
  version. The assumptions made are as follows:Synchrony:The algorithm operates in a synchronous fashion in a sequence of
  rounds or synchronous steps. If an agent \(x\) does not send
  a message to an agent \(y\) in a given round then \(y\) can detect
  that \(x\) did not send a message to it in that round.Reliability:If an agent \(y\) sends a message \(m\) to an agent \(y\) in a given
  round then \(z\) receives \(m\) in that round.Receiver knows sender:An agent that receives a message knows which agent sent it. If an
  agent \(z\) receives a message \(m\) from an agent \(y\) in a round
  then \(z\) knows that \(y\) sent \(m\) in that round.Why oral messages are harderIn the written version of the problem, if an agent
\(z\) receives a message \(m\) from any agent where \(m\) is signed by the
general then \(z\) knows that the general did send \(m\).
An agent cannot forge the general's signature and send a false
message.
By contrast, in the oral, or unencrypted version, any agent can
forge any agent's signature and send corrupted messages.Byzantine Generals AlgorithmMessages in the algorithm are eitherattackorretreatmessages.  If an agent \(x\) does not receive a message from an agent
\(y\) on a round then \(x\) treats the absence of the message from
\(y\) in the same way as if \(x\) received aretreatmessage
from \(y\). So, the algorithm only deals withattackandretreatmessages and does not deal with steps that an agent
takes if it does not receive a message.First we describe the flow of message in the algorithm and then
describe the algorithmMessage FlowMessages flow along a tree of height \(t + 1\).
The root node is \(m[0]\) which represents the general's command.
Each node of the tree is of the form \(m[L]\) where \(L[0] = 0\) and
\(L[1, \ldots, ]\) is a list of lieutenants where each lieutenant
appears at most once.The next figure illustrates a part of the messaging tree for \(N = 7,
t = 2\).
(There is insufficient space to show the complete tree.)Fig.1: A Part of the Message Tree for General and 6 LieutenantsEach non-leaf node \(m[L]\) in the tree has a child \(m[L,
x]\) for each lieutenant \(x\) that is not in \(L\).
For example, \(m[0]\) has children \(m[0,1], m[0,2], \ldots, m[0,
N-1]\).\(m[0,x]\) is the message that lieutenant \(x\) receives from the general.
\(m[0, x_{0}, \ldots, x_{k}]\) is the message that
lieutenant \(x_{0}\) receives from the general and forwards to
lieutenant \(x_{1}\),
...
which in turn forwards the message to lieutenant \(x_{n-1}\),
which in turn forwards the message to lieutenant \(x_{n}\).If the general is loyal then it sends the same message to all
lieutenants: \(m[0,x] = m[0]\) for all \(x\).
Loyal lieutenants forward the messages that they receive; however,
disloyal lieutenants may send arbitrary messages.ExampleThe diagram below shows a situation in which the general is loyal and
sends attack messages to all lieutenants.
Lieutenant 1 is disloyal (shown as a dashed circle).
Lieutenant 1 sends retreat messages to some lieutenants and attack
messages to others.
Lieutenant 2 is loyal, and so it broadcasts the message that
it receives.Fig.2: Edges Aggregation Dataflow TreesAggregating PhaseMessages received by a lieutenant are processed by the lieutenant in
steps that are also represented by a tree called theaggregation tree.
The aggregation tree has a node \(a[L]\) for each node \(m[L]\) of the
message tree.The diagram below shows a part of the aggregation tree for \(N=7,
t=2\); these are the processing steps of lieutenant 1.Fig.3: Aggregation Tree: Steps for Lieutenant 1Each node \(a[L, i]\) of the tree has a child \(a[L, x, i]\) for each
\(x\) that is not in \([L, i]\).
For example, \(a[0, 1]\) has children \(a[0, 2, 1], a[0, 3, 1], \ldots
a[0, 6, 1]\).Connections from Messaging to Aggregating NodesThere is an edge directed from each message node \(m(L)\) to the aggregation
node \(a(L)\).
For example, there are edges from \(m[2, 3, 1]\) to \(a[2, 3,
1]\), and from \(m[2, 1]\) to \(a[2, 1]\), and from  \(m[1]\) to
\(a[1]\).Output of Aggregating NodesThe output of an aggregation node is the majority of its inputs.
For example:\(
  a[2, 1] = \textrm{majority}(m[2, 1], a[2, 3, 1], a[2, 4, 1], \ldots, a[2, 6, 1])
\)If there are an equal number of attack and retreat inputs, then the
majority value is defined to be any default value.ExampleThe next diagram shows the data flow -- messaging and aggregation
trees, and the connections between them -- for a system where
\(N=4, t=1\).Fig.4: Dataflow for 4 agents 1 of which is disloyalInductive Generation of the Data FlowThe basic unit of data flow, which is replicated many times, is shown
in the top diagram of the figure below.
The input to the unit is a node \(m[L]\) of the message tree; this
unit is specified by \(L\),and the message \(m[L]\).
The output of the unit are nodes \(a[L, x]\) of the
aggregation tree, for all lieutenants \(x\) not in \(L\).The base case of the induction is a node at depth \(t\).
The input for the base case is \(m[L]\) where \(L\) is a list starting
with \(0\) and followed by \(t\) lieutenants.
For the base case, \(a[L, x] = m[L, x]\) for all \(x\).
The base case is illustrated in the lower diagram.Fig.5: Structure of DataflowThe data flow connecting a message node \(m[L]\) at depth \(d < t\) to
aggregation nodes \(a[L,x]\) is shown below.Fig.6: Structure of DataflowMessage node \(m[L]\) feeds message nodes \(m[L,x]\).
The connections between \(m[L,x]\) and aggregation nodes \(a[L,x,y]\)
are specified by the data flow connecting nodes of depth \(d + 1\),
shown in the diagram by blue dotted lines.The value of an aggregation node is the majority of its inputs.\(
a[L,i] = \textrm{majority}(m[L,i], [\forall x \notin [L, i]: a[L, x, i]])
\)For example,\(
a[0, 1, 2] = \textrm{majority}(m[0,1,2], a[0, 1, 3, 2], a[0, 1, 4, 2],
a[0, 1, 5, 2], \ldots)
\)Proof of ValidityWe prove that for all nodes \(m[L]\) of the message tree, if \(L_{*}\)
is loyal then for all \(i\) not in \(L\):\(
a[L, i] = m[L]
\)The proof is by induction.
The base case is for message nodes at depth \(t\).
We prove that if validity holds for message nodes at depth \(d > 0\)
then it holds for message nodes at depth \(d-1\).Base CaseSee the lower diagram of figure 5. 
For a message node \(m[L]\) at depth \(t\),\(
a[L, i] = m[L, i]
\)If \(L_{*}\) is loyal then \(m[L, i] = m[L]\).
and the result follows.Inductive StepSee figure 6.A node \(L\) at depth \(d\) consists of \(d+1\) agents.
Therefore, there are at least \(3t+1 - (d+1)\) lieutenants that are not in
\(L\).
Because \(d \leq t\) there are at least \(2t\) not in \(L\).
So, at least \(t\) lieutenants not in \(L\) are loyal and at most
\(t\) of them are disloyal.Because \(L_{*}\) is loyal, \(m[L,i] = m[L]\).By the induction assumption, for each loyal lieutenant \(j\) not in \(L\), and for each loyal
lieutenant \(i\) not in \([L, j]\):
\(
a[L, j, i] = m[L, i] = m[L]
\)\(
a[L, i] = \textrm{majority}(m[L,i], [\forall j \notin [L, i]: a[L, j,
i]])
\)The majority is taken over at least \(t + 1\) values equal to
\(m[L]\),
and at most \(t\) values that are different from it. And
therefore \(a[L, i] = m[L]\)ExampleThe next illustrates the proof of validity. Message \(m[L]\) is
shown in red, and the flow of correct messages is shown in red edges
and red nodes. For example, nodes \(m[L, i], m[L, j], m[L, i, j],
m[L, j, i]\) are red because agents \(i, j\) are loyal.A disloyal agent is represented by the symbol \(e\).
The output of a disloyal agent is unknown and is shown in black.
Node \(a[l. i]\) gets more than \(2t\) red inputs and at most \(t\)
black inputs, and hence the majority of its inputs is red.Fig.7: Illustration of ValidityProof of ConsensusWe will prove consensus for nodes at depth \(d\) if the number of
faulty nodes is at most \(t-d\).Base Case \(d = t\)In this case there are no disloyal lieutenants. For each \(i, j\),
\(m[L,i]\) and \(m[L,j]\) are arbitrary; however \(m[L, i] = m[L,
j]\).\(a[L, i, j] = m[L, i]\)
and
\(a[L, j, i] = m[L, j]\).
Therefore the inputs to \(a[L,i]\) and to \(a[L,j]\) are identical and
the result follows.Inductive Step \(d < t\)If the general is loyal, then consensus follows from validity.There are at most \(t - d\) faulty nodes.
If the general is disloyal then there at most \(t-d-1 = t -(d+1)\)
disloyal lieutenants.
Therefore, the induction assumption holds for \(m[L, x]\).
Therefore \(a[L, i, k] = a[L, i, j]\)K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology



Figure Link: https://kmchandy.github.io/Byzantine/Byzantine_5/Byzantine_5.001.jpeg

Caption: Fig.1: A Part of the Message Tree for General and 6 Lieutenants


An agent is a general or a lieutenant.
  An agent may be loyal or disloyal.
  Let \(N\) be the number of agents and \(t\) the number of disloyal
  agents. 
  
The general sends a command to
each lieutenant where the command is either attack or retreat.
A loyal general sends the same command to all lieutenants whereas a
disloyal general may send different commands to different lieutenants.
Each lieutenant decides to attack or retreat at the end of the
algorithm.If all loyal lieutenants get the same command then each loyal
  lieutenant obeys the commands that it received; if the command is
  attack then each loyal lieutenant attacks, and if the command is
  retreat then each loyal lieutenant retreats.Even if loyal lieutenants receive different commands, all loyal
  lieutenants make the same the decision; either all loyal lieutenants
  attack or all loyal lieutenants retreat.NotationWe use indices \(i, j, k\) for loyal agents; \(x, y\) for
  generic agents who may be loyal or disloyal; and \(e\) for
  disloyal agents.
  Nothing in an agent's id or data identifies the agent as loyal or
  disloyal.
  Moreover, the algorithm does not have to discover which agents act
  disloyally.For a nonempty list \(L\), we use the Python notation \(L_{*}\) to
  refer the last element of the list.
  For example, if \(L = [5, 6]\), then \(L_{*} = 6\).For a list \(L\) and an element \(x\), the notation \(L , x\) represents a list
consisting of \(x\) appended to the tail of \(L\).
For example, if \(L\) is the list \([1, 2]\) then \(L , 3\) is the
  list \([1, 2, 3]\), and \(L, 3, 4\) is the list \([1, 2, 3, 4]\).The general is the agent with index \(0\), and the lieutenants have indices \(1,
\ldots, N-1\).Let \(m[x]\) be the message that the general sends
lieutenant \(x\), and let \(a[x]\) be the decision the lieutenant
  \(x\) makes.SpecificationThe specification has two parts, validity and consensus.Validity: Loyal lieutenants obey a loyal general.If all loyal lieutenants get the same message then each loyal
    lieutenant obeys the message that it receives.\(
  (\forall i, j: m[i] = m[j]) \quad \Rightarrow
\quad (\forall i: a[i] = m[i])
\)Consensus: Loyal lieutenants make the same
decision.\(
\forall i, j: a[i] = a[j]
\)AssumptionsThe oral Byzantine version makes fewer assumptions than the written
  version. The assumptions made are as follows:Synchrony:The algorithm operates in a synchronous fashion in a sequence of
  rounds or synchronous steps. If an agent \(x\) does not send
  a message to an agent \(y\) in a given round then \(y\) can detect
  that \(x\) did not send a message to it in that round.Reliability:If an agent \(y\) sends a message \(m\) to an agent \(y\) in a given
  round then \(z\) receives \(m\) in that round.Receiver knows sender:An agent that receives a message knows which agent sent it. If an
  agent \(z\) receives a message \(m\) from an agent \(y\) in a round
  then \(z\) knows that \(y\) sent \(m\) in that round.Why oral messages are harderIn the written version of the problem, if an agent
\(z\) receives a message \(m\) from any agent where \(m\) is signed by the
general then \(z\) knows that the general did send \(m\).
An agent cannot forge the general's signature and send a false
message.
By contrast, in the oral, or unencrypted version, any agent can
forge any agent's signature and send corrupted messages.Byzantine Generals AlgorithmMessages in the algorithm are eitherattackorretreatmessages.  If an agent \(x\) does not receive a message from an agent
\(y\) on a round then \(x\) treats the absence of the message from
\(y\) in the same way as if \(x\) received aretreatmessage
from \(y\). So, the algorithm only deals withattackandretreatmessages and does not deal with steps that an agent
takes if it does not receive a message.First we describe the flow of message in the algorithm and then
describe the algorithmMessage FlowMessages flow along a tree of height \(t + 1\).
The root node is \(m[0]\) which represents the general's command.
Each node of the tree is of the form \(m[L]\) where \(L[0] = 0\) and
\(L[1, \ldots, ]\) is a list of lieutenants where each lieutenant
appears at most once.The next figure illustrates a part of the messaging tree for \(N = 7,
t = 2\).
(There is insufficient space to show the complete tree.)Fig.1: A Part of the Message Tree for General and 6 LieutenantsEach non-leaf node \(m[L]\) in the tree has a child \(m[L,
x]\) for each lieutenant \(x\) that is not in \(L\).
For example, \(m[0]\) has children \(m[0,1], m[0,2], \ldots, m[0,
N-1]\).\(m[0,x]\) is the message that lieutenant \(x\) receives from the general.
\(m[0, x_{0}, \ldots, x_{k}]\) is the message that
lieutenant \(x_{0}\) receives from the general and forwards to
lieutenant \(x_{1}\),
...
which in turn forwards the message to lieutenant \(x_{n-1}\),
which in turn forwards the message to lieutenant \(x_{n}\).If the general is loyal then it sends the same message to all
lieutenants: \(m[0,x] = m[0]\) for all \(x\).
Loyal lieutenants forward the messages that they receive; however,
disloyal lieutenants may send arbitrary messages.ExampleThe diagram below shows a situation in which the general is loyal and
sends attack messages to all lieutenants.
Lieutenant 1 is disloyal (shown as a dashed circle).
Lieutenant 1 sends retreat messages to some lieutenants and attack
messages to others.
Lieutenant 2 is loyal, and so it broadcasts the message that
it receives.Fig.2: Edges Aggregation Dataflow TreesAggregating PhaseMessages received by a lieutenant are processed by the lieutenant in
steps that are also represented by a tree called theaggregation tree.
The aggregation tree has a node \(a[L]\) for each node \(m[L]\) of the
message tree.The diagram below shows a part of the aggregation tree for \(N=7,
t=2\); these are the processing steps of lieutenant 1.Fig.3: Aggregation Tree: Steps for Lieutenant 1Each node \(a[L, i]\) of the tree has a child \(a[L, x, i]\) for each
\(x\) that is not in \([L, i]\).
For example, \(a[0, 1]\) has children \(a[0, 2, 1], a[0, 3, 1], \ldots
a[0, 6, 1]\).Connections from Messaging to Aggregating NodesThere is an edge directed from each message node \(m(L)\) to the aggregation
node \(a(L)\).
For example, there are edges from \(m[2, 3, 1]\) to \(a[2, 3,
1]\), and from \(m[2, 1]\) to \(a[2, 1]\), and from  \(m[1]\) to
\(a[1]\).Output of Aggregating NodesThe output of an aggregation node is the majority of its inputs.
For example:\(
  a[2, 1] = \textrm{majority}(m[2, 1], a[2, 3, 1], a[2, 4, 1], \ldots, a[2, 6, 1])
\)If there are an equal number of attack and retreat inputs, then the
majority value is defined to be any default value.ExampleThe next diagram shows the data flow -- messaging and aggregation
trees, and the connections between them -- for a system where
\(N=4, t=1\).Fig.4: Dataflow for 4 agents 1 of which is disloyalInductive Generation of the Data FlowThe basic unit of data flow, which is replicated many times, is shown
in the top diagram of the figure below.
The input to the unit is a node \(m[L]\) of the message tree; this
unit is specified by \(L\),and the message \(m[L]\).
The output of the unit are nodes \(a[L, x]\) of the
aggregation tree, for all lieutenants \(x\) not in \(L\).The base case of the induction is a node at depth \(t\).
The input for the base case is \(m[L]\) where \(L\) is a list starting
with \(0\) and followed by \(t\) lieutenants.
For the base case, \(a[L, x] = m[L, x]\) for all \(x\).
The base case is illustrated in the lower diagram.Fig.5: Structure of DataflowThe data flow connecting a message node \(m[L]\) at depth \(d < t\) to
aggregation nodes \(a[L,x]\) is shown below.Fig.6: Structure of DataflowMessage node \(m[L]\) feeds message nodes \(m[L,x]\).
The connections between \(m[L,x]\) and aggregation nodes \(a[L,x,y]\)
are specified by the data flow connecting nodes of depth \(d + 1\),
shown in the diagram by blue dotted lines.The value of an aggregation node is the majority of its inputs.\(
a[L,i] = \textrm{majority}(m[L,i], [\forall x \notin [L, i]: a[L, x, i]])
\)For example,\(
a[0, 1, 2] = \textrm{majority}(m[0,1,2], a[0, 1, 3, 2], a[0, 1, 4, 2],
a[0, 1, 5, 2], \ldots)
\)Proof of ValidityWe prove that for all nodes \(m[L]\) of the message tree, if \(L_{*}\)
is loyal then for all \(i\) not in \(L\):\(
a[L, i] = m[L]
\)The proof is by induction.
The base case is for message nodes at depth \(t\).
We prove that if validity holds for message nodes at depth \(d > 0\)
then it holds for message nodes at depth \(d-1\).Base CaseSee the lower diagram of figure 5. 
For a message node \(m[L]\) at depth \(t\),\(
a[L, i] = m[L, i]
\)If \(L_{*}\) is loyal then \(m[L, i] = m[L]\).
and the result follows.Inductive StepSee figure 6.A node \(L\) at depth \(d\) consists of \(d+1\) agents.
Therefore, there are at least \(3t+1 - (d+1)\) lieutenants that are not in
\(L\).
Because \(d \leq t\) there are at least \(2t\) not in \(L\).
So, at least \(t\) lieutenants not in \(L\) are loyal and at most
\(t\) of them are disloyal.Because \(L_{*}\) is loyal, \(m[L,i] = m[L]\).By the induction assumption, for each loyal lieutenant \(j\) not in \(L\), and for each loyal
lieutenant \(i\) not in \([L, j]\):
\(
a[L, j, i] = m[L, i] = m[L]
\)\(
a[L, i] = \textrm{majority}(m[L,i], [\forall j \notin [L, i]: a[L, j,
i]])
\)The majority is taken over at least \(t + 1\) values equal to
\(m[L]\),
and at most \(t\) values that are different from it. And
therefore \(a[L, i] = m[L]\)ExampleThe next illustrates the proof of validity. Message \(m[L]\) is
shown in red, and the flow of correct messages is shown in red edges
and red nodes. For example, nodes \(m[L, i], m[L, j], m[L, i, j],
m[L, j, i]\) are red because agents \(i, j\) are loyal.A disloyal agent is represented by the symbol \(e\).
The output of a disloyal agent is unknown and is shown in black.
Node \(a[l. i]\) gets more than \(2t\) red inputs and at most \(t\)
black inputs, and hence the majority of its inputs is red.Fig.7: Illustration of ValidityProof of ConsensusWe will prove consensus for nodes at depth \(d\) if the number of
faulty nodes is at most \(t-d\).Base Case \(d = t\)In this case there are no disloyal lieutenants. For each \(i, j\),
\(m[L,i]\) and \(m[L,j]\) are arbitrary; however \(m[L, i] = m[L,
j]\).\(a[L, i, j] = m[L, i]\)
and
\(a[L, j, i] = m[L, j]\).
Therefore the inputs to \(a[L,i]\) and to \(a[L,j]\) are identical and
the result follows.Inductive Step \(d < t\)If the general is loyal, then consensus follows from validity.There are at most \(t - d\) faulty nodes.
If the general is disloyal then there at most \(t-d-1 = t -(d+1)\)
disloyal lieutenants.
Therefore, the induction assumption holds for \(m[L, x]\).
Therefore \(a[L, i, k] = a[L, i, j]\)K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology



Figure Link: https://kmchandy.github.io/Byzantine/Byzantine_5/Byzantine_5.001.jpeg

Caption: Fig.1: A Part of the Message Tree for General and 6 Lieutenants


If all loyal lieutenants get the same command then each loyal
  lieutenant obeys the commands that it received; if the command is
  attack then each loyal lieutenant attacks, and if the command is
  retreat then each loyal lieutenant retreats.Even if loyal lieutenants receive different commands, all loyal
  lieutenants make the same the decision; either all loyal lieutenants
  attack or all loyal lieutenants retreat.NotationWe use indices \(i, j, k\) for loyal agents; \(x, y\) for
  generic agents who may be loyal or disloyal; and \(e\) for
  disloyal agents.
  Nothing in an agent's id or data identifies the agent as loyal or
  disloyal.
  Moreover, the algorithm does not have to discover which agents act
  disloyally.For a nonempty list \(L\), we use the Python notation \(L_{*}\) to
  refer the last element of the list.
  For example, if \(L = [5, 6]\), then \(L_{*} = 6\).For a list \(L\) and an element \(x\), the notation \(L , x\) represents a list
consisting of \(x\) appended to the tail of \(L\).
For example, if \(L\) is the list \([1, 2]\) then \(L , 3\) is the
  list \([1, 2, 3]\), and \(L, 3, 4\) is the list \([1, 2, 3, 4]\).The general is the agent with index \(0\), and the lieutenants have indices \(1,
\ldots, N-1\).Let \(m[x]\) be the message that the general sends
lieutenant \(x\), and let \(a[x]\) be the decision the lieutenant
  \(x\) makes.SpecificationThe specification has two parts, validity and consensus.Validity: Loyal lieutenants obey a loyal general.If all loyal lieutenants get the same message then each loyal
    lieutenant obeys the message that it receives.\(
  (\forall i, j: m[i] = m[j]) \quad \Rightarrow
\quad (\forall i: a[i] = m[i])
\)Consensus: Loyal lieutenants make the same
decision.\(
\forall i, j: a[i] = a[j]
\)AssumptionsThe oral Byzantine version makes fewer assumptions than the written
  version. The assumptions made are as follows:Synchrony:The algorithm operates in a synchronous fashion in a sequence of
  rounds or synchronous steps. If an agent \(x\) does not send
  a message to an agent \(y\) in a given round then \(y\) can detect
  that \(x\) did not send a message to it in that round.Reliability:If an agent \(y\) sends a message \(m\) to an agent \(y\) in a given
  round then \(z\) receives \(m\) in that round.Receiver knows sender:An agent that receives a message knows which agent sent it. If an
  agent \(z\) receives a message \(m\) from an agent \(y\) in a round
  then \(z\) knows that \(y\) sent \(m\) in that round.Why oral messages are harderIn the written version of the problem, if an agent
\(z\) receives a message \(m\) from any agent where \(m\) is signed by the
general then \(z\) knows that the general did send \(m\).
An agent cannot forge the general's signature and send a false
message.
By contrast, in the oral, or unencrypted version, any agent can
forge any agent's signature and send corrupted messages.Byzantine Generals AlgorithmMessages in the algorithm are eitherattackorretreatmessages.  If an agent \(x\) does not receive a message from an agent
\(y\) on a round then \(x\) treats the absence of the message from
\(y\) in the same way as if \(x\) received aretreatmessage
from \(y\). So, the algorithm only deals withattackandretreatmessages and does not deal with steps that an agent
takes if it does not receive a message.First we describe the flow of message in the algorithm and then
describe the algorithmMessage FlowMessages flow along a tree of height \(t + 1\).
The root node is \(m[0]\) which represents the general's command.
Each node of the tree is of the form \(m[L]\) where \(L[0] = 0\) and
\(L[1, \ldots, ]\) is a list of lieutenants where each lieutenant
appears at most once.The next figure illustrates a part of the messaging tree for \(N = 7,
t = 2\).
(There is insufficient space to show the complete tree.)Fig.1: A Part of the Message Tree for General and 6 LieutenantsEach non-leaf node \(m[L]\) in the tree has a child \(m[L,
x]\) for each lieutenant \(x\) that is not in \(L\).
For example, \(m[0]\) has children \(m[0,1], m[0,2], \ldots, m[0,
N-1]\).\(m[0,x]\) is the message that lieutenant \(x\) receives from the general.
\(m[0, x_{0}, \ldots, x_{k}]\) is the message that
lieutenant \(x_{0}\) receives from the general and forwards to
lieutenant \(x_{1}\),
...
which in turn forwards the message to lieutenant \(x_{n-1}\),
which in turn forwards the message to lieutenant \(x_{n}\).If the general is loyal then it sends the same message to all
lieutenants: \(m[0,x] = m[0]\) for all \(x\).
Loyal lieutenants forward the messages that they receive; however,
disloyal lieutenants may send arbitrary messages.ExampleThe diagram below shows a situation in which the general is loyal and
sends attack messages to all lieutenants.
Lieutenant 1 is disloyal (shown as a dashed circle).
Lieutenant 1 sends retreat messages to some lieutenants and attack
messages to others.
Lieutenant 2 is loyal, and so it broadcasts the message that
it receives.Fig.2: Edges Aggregation Dataflow TreesAggregating PhaseMessages received by a lieutenant are processed by the lieutenant in
steps that are also represented by a tree called theaggregation tree.
The aggregation tree has a node \(a[L]\) for each node \(m[L]\) of the
message tree.The diagram below shows a part of the aggregation tree for \(N=7,
t=2\); these are the processing steps of lieutenant 1.Fig.3: Aggregation Tree: Steps for Lieutenant 1Each node \(a[L, i]\) of the tree has a child \(a[L, x, i]\) for each
\(x\) that is not in \([L, i]\).
For example, \(a[0, 1]\) has children \(a[0, 2, 1], a[0, 3, 1], \ldots
a[0, 6, 1]\).Connections from Messaging to Aggregating NodesThere is an edge directed from each message node \(m(L)\) to the aggregation
node \(a(L)\).
For example, there are edges from \(m[2, 3, 1]\) to \(a[2, 3,
1]\), and from \(m[2, 1]\) to \(a[2, 1]\), and from  \(m[1]\) to
\(a[1]\).Output of Aggregating NodesThe output of an aggregation node is the majority of its inputs.
For example:\(
  a[2, 1] = \textrm{majority}(m[2, 1], a[2, 3, 1], a[2, 4, 1], \ldots, a[2, 6, 1])
\)If there are an equal number of attack and retreat inputs, then the
majority value is defined to be any default value.ExampleThe next diagram shows the data flow -- messaging and aggregation
trees, and the connections between them -- for a system where
\(N=4, t=1\).Fig.4: Dataflow for 4 agents 1 of which is disloyalInductive Generation of the Data FlowThe basic unit of data flow, which is replicated many times, is shown
in the top diagram of the figure below.
The input to the unit is a node \(m[L]\) of the message tree; this
unit is specified by \(L\),and the message \(m[L]\).
The output of the unit are nodes \(a[L, x]\) of the
aggregation tree, for all lieutenants \(x\) not in \(L\).The base case of the induction is a node at depth \(t\).
The input for the base case is \(m[L]\) where \(L\) is a list starting
with \(0\) and followed by \(t\) lieutenants.
For the base case, \(a[L, x] = m[L, x]\) for all \(x\).
The base case is illustrated in the lower diagram.Fig.5: Structure of DataflowThe data flow connecting a message node \(m[L]\) at depth \(d < t\) to
aggregation nodes \(a[L,x]\) is shown below.Fig.6: Structure of DataflowMessage node \(m[L]\) feeds message nodes \(m[L,x]\).
The connections between \(m[L,x]\) and aggregation nodes \(a[L,x,y]\)
are specified by the data flow connecting nodes of depth \(d + 1\),
shown in the diagram by blue dotted lines.The value of an aggregation node is the majority of its inputs.\(
a[L,i] = \textrm{majority}(m[L,i], [\forall x \notin [L, i]: a[L, x, i]])
\)For example,\(
a[0, 1, 2] = \textrm{majority}(m[0,1,2], a[0, 1, 3, 2], a[0, 1, 4, 2],
a[0, 1, 5, 2], \ldots)
\)Proof of ValidityWe prove that for all nodes \(m[L]\) of the message tree, if \(L_{*}\)
is loyal then for all \(i\) not in \(L\):\(
a[L, i] = m[L]
\)The proof is by induction.
The base case is for message nodes at depth \(t\).
We prove that if validity holds for message nodes at depth \(d > 0\)
then it holds for message nodes at depth \(d-1\).Base CaseSee the lower diagram of figure 5. 
For a message node \(m[L]\) at depth \(t\),\(
a[L, i] = m[L, i]
\)If \(L_{*}\) is loyal then \(m[L, i] = m[L]\).
and the result follows.Inductive StepSee figure 6.A node \(L\) at depth \(d\) consists of \(d+1\) agents.
Therefore, there are at least \(3t+1 - (d+1)\) lieutenants that are not in
\(L\).
Because \(d \leq t\) there are at least \(2t\) not in \(L\).
So, at least \(t\) lieutenants not in \(L\) are loyal and at most
\(t\) of them are disloyal.Because \(L_{*}\) is loyal, \(m[L,i] = m[L]\).By the induction assumption, for each loyal lieutenant \(j\) not in \(L\), and for each loyal
lieutenant \(i\) not in \([L, j]\):
\(
a[L, j, i] = m[L, i] = m[L]
\)\(
a[L, i] = \textrm{majority}(m[L,i], [\forall j \notin [L, i]: a[L, j,
i]])
\)The majority is taken over at least \(t + 1\) values equal to
\(m[L]\),
and at most \(t\) values that are different from it. And
therefore \(a[L, i] = m[L]\)ExampleThe next illustrates the proof of validity. Message \(m[L]\) is
shown in red, and the flow of correct messages is shown in red edges
and red nodes. For example, nodes \(m[L, i], m[L, j], m[L, i, j],
m[L, j, i]\) are red because agents \(i, j\) are loyal.A disloyal agent is represented by the symbol \(e\).
The output of a disloyal agent is unknown and is shown in black.
Node \(a[l. i]\) gets more than \(2t\) red inputs and at most \(t\)
black inputs, and hence the majority of its inputs is red.Fig.7: Illustration of ValidityProof of ConsensusWe will prove consensus for nodes at depth \(d\) if the number of
faulty nodes is at most \(t-d\).Base Case \(d = t\)In this case there are no disloyal lieutenants. For each \(i, j\),
\(m[L,i]\) and \(m[L,j]\) are arbitrary; however \(m[L, i] = m[L,
j]\).\(a[L, i, j] = m[L, i]\)
and
\(a[L, j, i] = m[L, j]\).
Therefore the inputs to \(a[L,i]\) and to \(a[L,j]\) are identical and
the result follows.Inductive Step \(d < t\)If the general is loyal, then consensus follows from validity.There are at most \(t - d\) faulty nodes.
If the general is disloyal then there at most \(t-d-1 = t -(d+1)\)
disloyal lieutenants.
Therefore, the induction assumption holds for \(m[L, x]\).
Therefore \(a[L, i, k] = a[L, i, j]\)K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology



Figure Link: https://kmchandy.github.io/Byzantine/Byzantine_5/Byzantine_5.001.jpeg

Caption: Fig.1: A Part of the Message Tree for General and 6 Lieutenants


Even if loyal lieutenants receive different commands, all loyal
  lieutenants make the same the decision; either all loyal lieutenants
  attack or all loyal lieutenants retreat.NotationWe use indices \(i, j, k\) for loyal agents; \(x, y\) for
  generic agents who may be loyal or disloyal; and \(e\) for
  disloyal agents.
  Nothing in an agent's id or data identifies the agent as loyal or
  disloyal.
  Moreover, the algorithm does not have to discover which agents act
  disloyally.For a nonempty list \(L\), we use the Python notation \(L_{*}\) to
  refer the last element of the list.
  For example, if \(L = [5, 6]\), then \(L_{*} = 6\).For a list \(L\) and an element \(x\), the notation \(L , x\) represents a list
consisting of \(x\) appended to the tail of \(L\).
For example, if \(L\) is the list \([1, 2]\) then \(L , 3\) is the
  list \([1, 2, 3]\), and \(L, 3, 4\) is the list \([1, 2, 3, 4]\).The general is the agent with index \(0\), and the lieutenants have indices \(1,
\ldots, N-1\).Let \(m[x]\) be the message that the general sends
lieutenant \(x\), and let \(a[x]\) be the decision the lieutenant
  \(x\) makes.SpecificationThe specification has two parts, validity and consensus.Validity: Loyal lieutenants obey a loyal general.If all loyal lieutenants get the same message then each loyal
    lieutenant obeys the message that it receives.\(
  (\forall i, j: m[i] = m[j]) \quad \Rightarrow
\quad (\forall i: a[i] = m[i])
\)Consensus: Loyal lieutenants make the same
decision.\(
\forall i, j: a[i] = a[j]
\)AssumptionsThe oral Byzantine version makes fewer assumptions than the written
  version. The assumptions made are as follows:Synchrony:The algorithm operates in a synchronous fashion in a sequence of
  rounds or synchronous steps. If an agent \(x\) does not send
  a message to an agent \(y\) in a given round then \(y\) can detect
  that \(x\) did not send a message to it in that round.Reliability:If an agent \(y\) sends a message \(m\) to an agent \(y\) in a given
  round then \(z\) receives \(m\) in that round.Receiver knows sender:An agent that receives a message knows which agent sent it. If an
  agent \(z\) receives a message \(m\) from an agent \(y\) in a round
  then \(z\) knows that \(y\) sent \(m\) in that round.Why oral messages are harderIn the written version of the problem, if an agent
\(z\) receives a message \(m\) from any agent where \(m\) is signed by the
general then \(z\) knows that the general did send \(m\).
An agent cannot forge the general's signature and send a false
message.
By contrast, in the oral, or unencrypted version, any agent can
forge any agent's signature and send corrupted messages.Byzantine Generals AlgorithmMessages in the algorithm are eitherattackorretreatmessages.  If an agent \(x\) does not receive a message from an agent
\(y\) on a round then \(x\) treats the absence of the message from
\(y\) in the same way as if \(x\) received aretreatmessage
from \(y\). So, the algorithm only deals withattackandretreatmessages and does not deal with steps that an agent
takes if it does not receive a message.First we describe the flow of message in the algorithm and then
describe the algorithmMessage FlowMessages flow along a tree of height \(t + 1\).
The root node is \(m[0]\) which represents the general's command.
Each node of the tree is of the form \(m[L]\) where \(L[0] = 0\) and
\(L[1, \ldots, ]\) is a list of lieutenants where each lieutenant
appears at most once.The next figure illustrates a part of the messaging tree for \(N = 7,
t = 2\).
(There is insufficient space to show the complete tree.)Fig.1: A Part of the Message Tree for General and 6 LieutenantsEach non-leaf node \(m[L]\) in the tree has a child \(m[L,
x]\) for each lieutenant \(x\) that is not in \(L\).
For example, \(m[0]\) has children \(m[0,1], m[0,2], \ldots, m[0,
N-1]\).\(m[0,x]\) is the message that lieutenant \(x\) receives from the general.
\(m[0, x_{0}, \ldots, x_{k}]\) is the message that
lieutenant \(x_{0}\) receives from the general and forwards to
lieutenant \(x_{1}\),
...
which in turn forwards the message to lieutenant \(x_{n-1}\),
which in turn forwards the message to lieutenant \(x_{n}\).If the general is loyal then it sends the same message to all
lieutenants: \(m[0,x] = m[0]\) for all \(x\).
Loyal lieutenants forward the messages that they receive; however,
disloyal lieutenants may send arbitrary messages.ExampleThe diagram below shows a situation in which the general is loyal and
sends attack messages to all lieutenants.
Lieutenant 1 is disloyal (shown as a dashed circle).
Lieutenant 1 sends retreat messages to some lieutenants and attack
messages to others.
Lieutenant 2 is loyal, and so it broadcasts the message that
it receives.Fig.2: Edges Aggregation Dataflow TreesAggregating PhaseMessages received by a lieutenant are processed by the lieutenant in
steps that are also represented by a tree called theaggregation tree.
The aggregation tree has a node \(a[L]\) for each node \(m[L]\) of the
message tree.The diagram below shows a part of the aggregation tree for \(N=7,
t=2\); these are the processing steps of lieutenant 1.Fig.3: Aggregation Tree: Steps for Lieutenant 1Each node \(a[L, i]\) of the tree has a child \(a[L, x, i]\) for each
\(x\) that is not in \([L, i]\).
For example, \(a[0, 1]\) has children \(a[0, 2, 1], a[0, 3, 1], \ldots
a[0, 6, 1]\).Connections from Messaging to Aggregating NodesThere is an edge directed from each message node \(m(L)\) to the aggregation
node \(a(L)\).
For example, there are edges from \(m[2, 3, 1]\) to \(a[2, 3,
1]\), and from \(m[2, 1]\) to \(a[2, 1]\), and from  \(m[1]\) to
\(a[1]\).Output of Aggregating NodesThe output of an aggregation node is the majority of its inputs.
For example:\(
  a[2, 1] = \textrm{majority}(m[2, 1], a[2, 3, 1], a[2, 4, 1], \ldots, a[2, 6, 1])
\)If there are an equal number of attack and retreat inputs, then the
majority value is defined to be any default value.ExampleThe next diagram shows the data flow -- messaging and aggregation
trees, and the connections between them -- for a system where
\(N=4, t=1\).Fig.4: Dataflow for 4 agents 1 of which is disloyalInductive Generation of the Data FlowThe basic unit of data flow, which is replicated many times, is shown
in the top diagram of the figure below.
The input to the unit is a node \(m[L]\) of the message tree; this
unit is specified by \(L\),and the message \(m[L]\).
The output of the unit are nodes \(a[L, x]\) of the
aggregation tree, for all lieutenants \(x\) not in \(L\).The base case of the induction is a node at depth \(t\).
The input for the base case is \(m[L]\) where \(L\) is a list starting
with \(0\) and followed by \(t\) lieutenants.
For the base case, \(a[L, x] = m[L, x]\) for all \(x\).
The base case is illustrated in the lower diagram.Fig.5: Structure of DataflowThe data flow connecting a message node \(m[L]\) at depth \(d < t\) to
aggregation nodes \(a[L,x]\) is shown below.Fig.6: Structure of DataflowMessage node \(m[L]\) feeds message nodes \(m[L,x]\).
The connections between \(m[L,x]\) and aggregation nodes \(a[L,x,y]\)
are specified by the data flow connecting nodes of depth \(d + 1\),
shown in the diagram by blue dotted lines.The value of an aggregation node is the majority of its inputs.\(
a[L,i] = \textrm{majority}(m[L,i], [\forall x \notin [L, i]: a[L, x, i]])
\)For example,\(
a[0, 1, 2] = \textrm{majority}(m[0,1,2], a[0, 1, 3, 2], a[0, 1, 4, 2],
a[0, 1, 5, 2], \ldots)
\)Proof of ValidityWe prove that for all nodes \(m[L]\) of the message tree, if \(L_{*}\)
is loyal then for all \(i\) not in \(L\):\(
a[L, i] = m[L]
\)The proof is by induction.
The base case is for message nodes at depth \(t\).
We prove that if validity holds for message nodes at depth \(d > 0\)
then it holds for message nodes at depth \(d-1\).Base CaseSee the lower diagram of figure 5. 
For a message node \(m[L]\) at depth \(t\),\(
a[L, i] = m[L, i]
\)If \(L_{*}\) is loyal then \(m[L, i] = m[L]\).
and the result follows.Inductive StepSee figure 6.A node \(L\) at depth \(d\) consists of \(d+1\) agents.
Therefore, there are at least \(3t+1 - (d+1)\) lieutenants that are not in
\(L\).
Because \(d \leq t\) there are at least \(2t\) not in \(L\).
So, at least \(t\) lieutenants not in \(L\) are loyal and at most
\(t\) of them are disloyal.Because \(L_{*}\) is loyal, \(m[L,i] = m[L]\).By the induction assumption, for each loyal lieutenant \(j\) not in \(L\), and for each loyal
lieutenant \(i\) not in \([L, j]\):
\(
a[L, j, i] = m[L, i] = m[L]
\)\(
a[L, i] = \textrm{majority}(m[L,i], [\forall j \notin [L, i]: a[L, j,
i]])
\)The majority is taken over at least \(t + 1\) values equal to
\(m[L]\),
and at most \(t\) values that are different from it. And
therefore \(a[L, i] = m[L]\)ExampleThe next illustrates the proof of validity. Message \(m[L]\) is
shown in red, and the flow of correct messages is shown in red edges
and red nodes. For example, nodes \(m[L, i], m[L, j], m[L, i, j],
m[L, j, i]\) are red because agents \(i, j\) are loyal.A disloyal agent is represented by the symbol \(e\).
The output of a disloyal agent is unknown and is shown in black.
Node \(a[l. i]\) gets more than \(2t\) red inputs and at most \(t\)
black inputs, and hence the majority of its inputs is red.Fig.7: Illustration of ValidityProof of ConsensusWe will prove consensus for nodes at depth \(d\) if the number of
faulty nodes is at most \(t-d\).Base Case \(d = t\)In this case there are no disloyal lieutenants. For each \(i, j\),
\(m[L,i]\) and \(m[L,j]\) are arbitrary; however \(m[L, i] = m[L,
j]\).\(a[L, i, j] = m[L, i]\)
and
\(a[L, j, i] = m[L, j]\).
Therefore the inputs to \(a[L,i]\) and to \(a[L,j]\) are identical and
the result follows.Inductive Step \(d < t\)If the general is loyal, then consensus follows from validity.There are at most \(t - d\) faulty nodes.
If the general is disloyal then there at most \(t-d-1 = t -(d+1)\)
disloyal lieutenants.
Therefore, the induction assumption holds for \(m[L, x]\).
Therefore \(a[L, i, k] = a[L, i, j]\)K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology



Figure Link: https://kmchandy.github.io/Byzantine/Byzantine_5/Byzantine_5.001.jpeg

Caption: Fig.1: A Part of the Message Tree for General and 6 Lieutenants


For a nonempty list \(L\), we use the Python notation \(L_{*}\) to
  refer the last element of the list.
  For example, if \(L = [5, 6]\), then \(L_{*} = 6\).For a list \(L\) and an element \(x\), the notation \(L , x\) represents a list
consisting of \(x\) appended to the tail of \(L\).
For example, if \(L\) is the list \([1, 2]\) then \(L , 3\) is the
  list \([1, 2, 3]\), and \(L, 3, 4\) is the list \([1, 2, 3, 4]\).The general is the agent with index \(0\), and the lieutenants have indices \(1,
\ldots, N-1\).Let \(m[x]\) be the message that the general sends
lieutenant \(x\), and let \(a[x]\) be the decision the lieutenant
  \(x\) makes.SpecificationThe specification has two parts, validity and consensus.Validity: Loyal lieutenants obey a loyal general.If all loyal lieutenants get the same message then each loyal
    lieutenant obeys the message that it receives.\(
  (\forall i, j: m[i] = m[j]) \quad \Rightarrow
\quad (\forall i: a[i] = m[i])
\)Consensus: Loyal lieutenants make the same
decision.\(
\forall i, j: a[i] = a[j]
\)AssumptionsThe oral Byzantine version makes fewer assumptions than the written
  version. The assumptions made are as follows:Synchrony:The algorithm operates in a synchronous fashion in a sequence of
  rounds or synchronous steps. If an agent \(x\) does not send
  a message to an agent \(y\) in a given round then \(y\) can detect
  that \(x\) did not send a message to it in that round.Reliability:If an agent \(y\) sends a message \(m\) to an agent \(y\) in a given
  round then \(z\) receives \(m\) in that round.Receiver knows sender:An agent that receives a message knows which agent sent it. If an
  agent \(z\) receives a message \(m\) from an agent \(y\) in a round
  then \(z\) knows that \(y\) sent \(m\) in that round.Why oral messages are harderIn the written version of the problem, if an agent
\(z\) receives a message \(m\) from any agent where \(m\) is signed by the
general then \(z\) knows that the general did send \(m\).
An agent cannot forge the general's signature and send a false
message.
By contrast, in the oral, or unencrypted version, any agent can
forge any agent's signature and send corrupted messages.Byzantine Generals AlgorithmMessages in the algorithm are eitherattackorretreatmessages.  If an agent \(x\) does not receive a message from an agent
\(y\) on a round then \(x\) treats the absence of the message from
\(y\) in the same way as if \(x\) received aretreatmessage
from \(y\). So, the algorithm only deals withattackandretreatmessages and does not deal with steps that an agent
takes if it does not receive a message.First we describe the flow of message in the algorithm and then
describe the algorithmMessage FlowMessages flow along a tree of height \(t + 1\).
The root node is \(m[0]\) which represents the general's command.
Each node of the tree is of the form \(m[L]\) where \(L[0] = 0\) and
\(L[1, \ldots, ]\) is a list of lieutenants where each lieutenant
appears at most once.The next figure illustrates a part of the messaging tree for \(N = 7,
t = 2\).
(There is insufficient space to show the complete tree.)Fig.1: A Part of the Message Tree for General and 6 LieutenantsEach non-leaf node \(m[L]\) in the tree has a child \(m[L,
x]\) for each lieutenant \(x\) that is not in \(L\).
For example, \(m[0]\) has children \(m[0,1], m[0,2], \ldots, m[0,
N-1]\).\(m[0,x]\) is the message that lieutenant \(x\) receives from the general.
\(m[0, x_{0}, \ldots, x_{k}]\) is the message that
lieutenant \(x_{0}\) receives from the general and forwards to
lieutenant \(x_{1}\),
...
which in turn forwards the message to lieutenant \(x_{n-1}\),
which in turn forwards the message to lieutenant \(x_{n}\).If the general is loyal then it sends the same message to all
lieutenants: \(m[0,x] = m[0]\) for all \(x\).
Loyal lieutenants forward the messages that they receive; however,
disloyal lieutenants may send arbitrary messages.ExampleThe diagram below shows a situation in which the general is loyal and
sends attack messages to all lieutenants.
Lieutenant 1 is disloyal (shown as a dashed circle).
Lieutenant 1 sends retreat messages to some lieutenants and attack
messages to others.
Lieutenant 2 is loyal, and so it broadcasts the message that
it receives.Fig.2: Edges Aggregation Dataflow TreesAggregating PhaseMessages received by a lieutenant are processed by the lieutenant in
steps that are also represented by a tree called theaggregation tree.
The aggregation tree has a node \(a[L]\) for each node \(m[L]\) of the
message tree.The diagram below shows a part of the aggregation tree for \(N=7,
t=2\); these are the processing steps of lieutenant 1.Fig.3: Aggregation Tree: Steps for Lieutenant 1Each node \(a[L, i]\) of the tree has a child \(a[L, x, i]\) for each
\(x\) that is not in \([L, i]\).
For example, \(a[0, 1]\) has children \(a[0, 2, 1], a[0, 3, 1], \ldots
a[0, 6, 1]\).Connections from Messaging to Aggregating NodesThere is an edge directed from each message node \(m(L)\) to the aggregation
node \(a(L)\).
For example, there are edges from \(m[2, 3, 1]\) to \(a[2, 3,
1]\), and from \(m[2, 1]\) to \(a[2, 1]\), and from  \(m[1]\) to
\(a[1]\).Output of Aggregating NodesThe output of an aggregation node is the majority of its inputs.
For example:\(
  a[2, 1] = \textrm{majority}(m[2, 1], a[2, 3, 1], a[2, 4, 1], \ldots, a[2, 6, 1])
\)If there are an equal number of attack and retreat inputs, then the
majority value is defined to be any default value.ExampleThe next diagram shows the data flow -- messaging and aggregation
trees, and the connections between them -- for a system where
\(N=4, t=1\).Fig.4: Dataflow for 4 agents 1 of which is disloyalInductive Generation of the Data FlowThe basic unit of data flow, which is replicated many times, is shown
in the top diagram of the figure below.
The input to the unit is a node \(m[L]\) of the message tree; this
unit is specified by \(L\),and the message \(m[L]\).
The output of the unit are nodes \(a[L, x]\) of the
aggregation tree, for all lieutenants \(x\) not in \(L\).The base case of the induction is a node at depth \(t\).
The input for the base case is \(m[L]\) where \(L\) is a list starting
with \(0\) and followed by \(t\) lieutenants.
For the base case, \(a[L, x] = m[L, x]\) for all \(x\).
The base case is illustrated in the lower diagram.Fig.5: Structure of DataflowThe data flow connecting a message node \(m[L]\) at depth \(d < t\) to
aggregation nodes \(a[L,x]\) is shown below.Fig.6: Structure of DataflowMessage node \(m[L]\) feeds message nodes \(m[L,x]\).
The connections between \(m[L,x]\) and aggregation nodes \(a[L,x,y]\)
are specified by the data flow connecting nodes of depth \(d + 1\),
shown in the diagram by blue dotted lines.The value of an aggregation node is the majority of its inputs.\(
a[L,i] = \textrm{majority}(m[L,i], [\forall x \notin [L, i]: a[L, x, i]])
\)For example,\(
a[0, 1, 2] = \textrm{majority}(m[0,1,2], a[0, 1, 3, 2], a[0, 1, 4, 2],
a[0, 1, 5, 2], \ldots)
\)Proof of ValidityWe prove that for all nodes \(m[L]\) of the message tree, if \(L_{*}\)
is loyal then for all \(i\) not in \(L\):\(
a[L, i] = m[L]
\)The proof is by induction.
The base case is for message nodes at depth \(t\).
We prove that if validity holds for message nodes at depth \(d > 0\)
then it holds for message nodes at depth \(d-1\).Base CaseSee the lower diagram of figure 5. 
For a message node \(m[L]\) at depth \(t\),\(
a[L, i] = m[L, i]
\)If \(L_{*}\) is loyal then \(m[L, i] = m[L]\).
and the result follows.Inductive StepSee figure 6.A node \(L\) at depth \(d\) consists of \(d+1\) agents.
Therefore, there are at least \(3t+1 - (d+1)\) lieutenants that are not in
\(L\).
Because \(d \leq t\) there are at least \(2t\) not in \(L\).
So, at least \(t\) lieutenants not in \(L\) are loyal and at most
\(t\) of them are disloyal.Because \(L_{*}\) is loyal, \(m[L,i] = m[L]\).By the induction assumption, for each loyal lieutenant \(j\) not in \(L\), and for each loyal
lieutenant \(i\) not in \([L, j]\):
\(
a[L, j, i] = m[L, i] = m[L]
\)\(
a[L, i] = \textrm{majority}(m[L,i], [\forall j \notin [L, i]: a[L, j,
i]])
\)The majority is taken over at least \(t + 1\) values equal to
\(m[L]\),
and at most \(t\) values that are different from it. And
therefore \(a[L, i] = m[L]\)ExampleThe next illustrates the proof of validity. Message \(m[L]\) is
shown in red, and the flow of correct messages is shown in red edges
and red nodes. For example, nodes \(m[L, i], m[L, j], m[L, i, j],
m[L, j, i]\) are red because agents \(i, j\) are loyal.A disloyal agent is represented by the symbol \(e\).
The output of a disloyal agent is unknown and is shown in black.
Node \(a[l. i]\) gets more than \(2t\) red inputs and at most \(t\)
black inputs, and hence the majority of its inputs is red.Fig.7: Illustration of ValidityProof of ConsensusWe will prove consensus for nodes at depth \(d\) if the number of
faulty nodes is at most \(t-d\).Base Case \(d = t\)In this case there are no disloyal lieutenants. For each \(i, j\),
\(m[L,i]\) and \(m[L,j]\) are arbitrary; however \(m[L, i] = m[L,
j]\).\(a[L, i, j] = m[L, i]\)
and
\(a[L, j, i] = m[L, j]\).
Therefore the inputs to \(a[L,i]\) and to \(a[L,j]\) are identical and
the result follows.Inductive Step \(d < t\)If the general is loyal, then consensus follows from validity.There are at most \(t - d\) faulty nodes.
If the general is disloyal then there at most \(t-d-1 = t -(d+1)\)
disloyal lieutenants.
Therefore, the induction assumption holds for \(m[L, x]\).
Therefore \(a[L, i, k] = a[L, i, j]\)K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology



Figure Link: https://kmchandy.github.io/Byzantine/Byzantine_5/Byzantine_5.001.jpeg

Caption: Fig.1: A Part of the Message Tree for General and 6 Lieutenants


For a list \(L\) and an element \(x\), the notation \(L , x\) represents a list
consisting of \(x\) appended to the tail of \(L\).
For example, if \(L\) is the list \([1, 2]\) then \(L , 3\) is the
  list \([1, 2, 3]\), and \(L, 3, 4\) is the list \([1, 2, 3, 4]\).The general is the agent with index \(0\), and the lieutenants have indices \(1,
\ldots, N-1\).Let \(m[x]\) be the message that the general sends
lieutenant \(x\), and let \(a[x]\) be the decision the lieutenant
  \(x\) makes.SpecificationThe specification has two parts, validity and consensus.Validity: Loyal lieutenants obey a loyal general.If all loyal lieutenants get the same message then each loyal
    lieutenant obeys the message that it receives.\(
  (\forall i, j: m[i] = m[j]) \quad \Rightarrow
\quad (\forall i: a[i] = m[i])
\)Consensus: Loyal lieutenants make the same
decision.\(
\forall i, j: a[i] = a[j]
\)AssumptionsThe oral Byzantine version makes fewer assumptions than the written
  version. The assumptions made are as follows:Synchrony:The algorithm operates in a synchronous fashion in a sequence of
  rounds or synchronous steps. If an agent \(x\) does not send
  a message to an agent \(y\) in a given round then \(y\) can detect
  that \(x\) did not send a message to it in that round.Reliability:If an agent \(y\) sends a message \(m\) to an agent \(y\) in a given
  round then \(z\) receives \(m\) in that round.Receiver knows sender:An agent that receives a message knows which agent sent it. If an
  agent \(z\) receives a message \(m\) from an agent \(y\) in a round
  then \(z\) knows that \(y\) sent \(m\) in that round.Why oral messages are harderIn the written version of the problem, if an agent
\(z\) receives a message \(m\) from any agent where \(m\) is signed by the
general then \(z\) knows that the general did send \(m\).
An agent cannot forge the general's signature and send a false
message.
By contrast, in the oral, or unencrypted version, any agent can
forge any agent's signature and send corrupted messages.Byzantine Generals AlgorithmMessages in the algorithm are eitherattackorretreatmessages.  If an agent \(x\) does not receive a message from an agent
\(y\) on a round then \(x\) treats the absence of the message from
\(y\) in the same way as if \(x\) received aretreatmessage
from \(y\). So, the algorithm only deals withattackandretreatmessages and does not deal with steps that an agent
takes if it does not receive a message.First we describe the flow of message in the algorithm and then
describe the algorithmMessage FlowMessages flow along a tree of height \(t + 1\).
The root node is \(m[0]\) which represents the general's command.
Each node of the tree is of the form \(m[L]\) where \(L[0] = 0\) and
\(L[1, \ldots, ]\) is a list of lieutenants where each lieutenant
appears at most once.The next figure illustrates a part of the messaging tree for \(N = 7,
t = 2\).
(There is insufficient space to show the complete tree.)Fig.1: A Part of the Message Tree for General and 6 LieutenantsEach non-leaf node \(m[L]\) in the tree has a child \(m[L,
x]\) for each lieutenant \(x\) that is not in \(L\).
For example, \(m[0]\) has children \(m[0,1], m[0,2], \ldots, m[0,
N-1]\).\(m[0,x]\) is the message that lieutenant \(x\) receives from the general.
\(m[0, x_{0}, \ldots, x_{k}]\) is the message that
lieutenant \(x_{0}\) receives from the general and forwards to
lieutenant \(x_{1}\),
...
which in turn forwards the message to lieutenant \(x_{n-1}\),
which in turn forwards the message to lieutenant \(x_{n}\).If the general is loyal then it sends the same message to all
lieutenants: \(m[0,x] = m[0]\) for all \(x\).
Loyal lieutenants forward the messages that they receive; however,
disloyal lieutenants may send arbitrary messages.ExampleThe diagram below shows a situation in which the general is loyal and
sends attack messages to all lieutenants.
Lieutenant 1 is disloyal (shown as a dashed circle).
Lieutenant 1 sends retreat messages to some lieutenants and attack
messages to others.
Lieutenant 2 is loyal, and so it broadcasts the message that
it receives.Fig.2: Edges Aggregation Dataflow TreesAggregating PhaseMessages received by a lieutenant are processed by the lieutenant in
steps that are also represented by a tree called theaggregation tree.
The aggregation tree has a node \(a[L]\) for each node \(m[L]\) of the
message tree.The diagram below shows a part of the aggregation tree for \(N=7,
t=2\); these are the processing steps of lieutenant 1.Fig.3: Aggregation Tree: Steps for Lieutenant 1Each node \(a[L, i]\) of the tree has a child \(a[L, x, i]\) for each
\(x\) that is not in \([L, i]\).
For example, \(a[0, 1]\) has children \(a[0, 2, 1], a[0, 3, 1], \ldots
a[0, 6, 1]\).Connections from Messaging to Aggregating NodesThere is an edge directed from each message node \(m(L)\) to the aggregation
node \(a(L)\).
For example, there are edges from \(m[2, 3, 1]\) to \(a[2, 3,
1]\), and from \(m[2, 1]\) to \(a[2, 1]\), and from  \(m[1]\) to
\(a[1]\).Output of Aggregating NodesThe output of an aggregation node is the majority of its inputs.
For example:\(
  a[2, 1] = \textrm{majority}(m[2, 1], a[2, 3, 1], a[2, 4, 1], \ldots, a[2, 6, 1])
\)If there are an equal number of attack and retreat inputs, then the
majority value is defined to be any default value.ExampleThe next diagram shows the data flow -- messaging and aggregation
trees, and the connections between them -- for a system where
\(N=4, t=1\).Fig.4: Dataflow for 4 agents 1 of which is disloyalInductive Generation of the Data FlowThe basic unit of data flow, which is replicated many times, is shown
in the top diagram of the figure below.
The input to the unit is a node \(m[L]\) of the message tree; this
unit is specified by \(L\),and the message \(m[L]\).
The output of the unit are nodes \(a[L, x]\) of the
aggregation tree, for all lieutenants \(x\) not in \(L\).The base case of the induction is a node at depth \(t\).
The input for the base case is \(m[L]\) where \(L\) is a list starting
with \(0\) and followed by \(t\) lieutenants.
For the base case, \(a[L, x] = m[L, x]\) for all \(x\).
The base case is illustrated in the lower diagram.Fig.5: Structure of DataflowThe data flow connecting a message node \(m[L]\) at depth \(d < t\) to
aggregation nodes \(a[L,x]\) is shown below.Fig.6: Structure of DataflowMessage node \(m[L]\) feeds message nodes \(m[L,x]\).
The connections between \(m[L,x]\) and aggregation nodes \(a[L,x,y]\)
are specified by the data flow connecting nodes of depth \(d + 1\),
shown in the diagram by blue dotted lines.The value of an aggregation node is the majority of its inputs.\(
a[L,i] = \textrm{majority}(m[L,i], [\forall x \notin [L, i]: a[L, x, i]])
\)For example,\(
a[0, 1, 2] = \textrm{majority}(m[0,1,2], a[0, 1, 3, 2], a[0, 1, 4, 2],
a[0, 1, 5, 2], \ldots)
\)Proof of ValidityWe prove that for all nodes \(m[L]\) of the message tree, if \(L_{*}\)
is loyal then for all \(i\) not in \(L\):\(
a[L, i] = m[L]
\)The proof is by induction.
The base case is for message nodes at depth \(t\).
We prove that if validity holds for message nodes at depth \(d > 0\)
then it holds for message nodes at depth \(d-1\).Base CaseSee the lower diagram of figure 5. 
For a message node \(m[L]\) at depth \(t\),\(
a[L, i] = m[L, i]
\)If \(L_{*}\) is loyal then \(m[L, i] = m[L]\).
and the result follows.Inductive StepSee figure 6.A node \(L\) at depth \(d\) consists of \(d+1\) agents.
Therefore, there are at least \(3t+1 - (d+1)\) lieutenants that are not in
\(L\).
Because \(d \leq t\) there are at least \(2t\) not in \(L\).
So, at least \(t\) lieutenants not in \(L\) are loyal and at most
\(t\) of them are disloyal.Because \(L_{*}\) is loyal, \(m[L,i] = m[L]\).By the induction assumption, for each loyal lieutenant \(j\) not in \(L\), and for each loyal
lieutenant \(i\) not in \([L, j]\):
\(
a[L, j, i] = m[L, i] = m[L]
\)\(
a[L, i] = \textrm{majority}(m[L,i], [\forall j \notin [L, i]: a[L, j,
i]])
\)The majority is taken over at least \(t + 1\) values equal to
\(m[L]\),
and at most \(t\) values that are different from it. And
therefore \(a[L, i] = m[L]\)ExampleThe next illustrates the proof of validity. Message \(m[L]\) is
shown in red, and the flow of correct messages is shown in red edges
and red nodes. For example, nodes \(m[L, i], m[L, j], m[L, i, j],
m[L, j, i]\) are red because agents \(i, j\) are loyal.A disloyal agent is represented by the symbol \(e\).
The output of a disloyal agent is unknown and is shown in black.
Node \(a[l. i]\) gets more than \(2t\) red inputs and at most \(t\)
black inputs, and hence the majority of its inputs is red.Fig.7: Illustration of ValidityProof of ConsensusWe will prove consensus for nodes at depth \(d\) if the number of
faulty nodes is at most \(t-d\).Base Case \(d = t\)In this case there are no disloyal lieutenants. For each \(i, j\),
\(m[L,i]\) and \(m[L,j]\) are arbitrary; however \(m[L, i] = m[L,
j]\).\(a[L, i, j] = m[L, i]\)
and
\(a[L, j, i] = m[L, j]\).
Therefore the inputs to \(a[L,i]\) and to \(a[L,j]\) are identical and
the result follows.Inductive Step \(d < t\)If the general is loyal, then consensus follows from validity.There are at most \(t - d\) faulty nodes.
If the general is disloyal then there at most \(t-d-1 = t -(d+1)\)
disloyal lieutenants.
Therefore, the induction assumption holds for \(m[L, x]\).
Therefore \(a[L, i, k] = a[L, i, j]\)K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology



Figure Link: https://kmchandy.github.io/Byzantine/Byzantine_5/Byzantine_5.001.jpeg

Caption: Fig.1: A Part of the Message Tree for General and 6 Lieutenants


The general is the agent with index \(0\), and the lieutenants have indices \(1,
\ldots, N-1\).Let \(m[x]\) be the message that the general sends
lieutenant \(x\), and let \(a[x]\) be the decision the lieutenant
  \(x\) makes.SpecificationThe specification has two parts, validity and consensus.Validity: Loyal lieutenants obey a loyal general.If all loyal lieutenants get the same message then each loyal
    lieutenant obeys the message that it receives.\(
  (\forall i, j: m[i] = m[j]) \quad \Rightarrow
\quad (\forall i: a[i] = m[i])
\)Consensus: Loyal lieutenants make the same
decision.\(
\forall i, j: a[i] = a[j]
\)AssumptionsThe oral Byzantine version makes fewer assumptions than the written
  version. The assumptions made are as follows:Synchrony:The algorithm operates in a synchronous fashion in a sequence of
  rounds or synchronous steps. If an agent \(x\) does not send
  a message to an agent \(y\) in a given round then \(y\) can detect
  that \(x\) did not send a message to it in that round.Reliability:If an agent \(y\) sends a message \(m\) to an agent \(y\) in a given
  round then \(z\) receives \(m\) in that round.Receiver knows sender:An agent that receives a message knows which agent sent it. If an
  agent \(z\) receives a message \(m\) from an agent \(y\) in a round
  then \(z\) knows that \(y\) sent \(m\) in that round.Why oral messages are harderIn the written version of the problem, if an agent
\(z\) receives a message \(m\) from any agent where \(m\) is signed by the
general then \(z\) knows that the general did send \(m\).
An agent cannot forge the general's signature and send a false
message.
By contrast, in the oral, or unencrypted version, any agent can
forge any agent's signature and send corrupted messages.Byzantine Generals AlgorithmMessages in the algorithm are eitherattackorretreatmessages.  If an agent \(x\) does not receive a message from an agent
\(y\) on a round then \(x\) treats the absence of the message from
\(y\) in the same way as if \(x\) received aretreatmessage
from \(y\). So, the algorithm only deals withattackandretreatmessages and does not deal with steps that an agent
takes if it does not receive a message.First we describe the flow of message in the algorithm and then
describe the algorithmMessage FlowMessages flow along a tree of height \(t + 1\).
The root node is \(m[0]\) which represents the general's command.
Each node of the tree is of the form \(m[L]\) where \(L[0] = 0\) and
\(L[1, \ldots, ]\) is a list of lieutenants where each lieutenant
appears at most once.The next figure illustrates a part of the messaging tree for \(N = 7,
t = 2\).
(There is insufficient space to show the complete tree.)Fig.1: A Part of the Message Tree for General and 6 LieutenantsEach non-leaf node \(m[L]\) in the tree has a child \(m[L,
x]\) for each lieutenant \(x\) that is not in \(L\).
For example, \(m[0]\) has children \(m[0,1], m[0,2], \ldots, m[0,
N-1]\).\(m[0,x]\) is the message that lieutenant \(x\) receives from the general.
\(m[0, x_{0}, \ldots, x_{k}]\) is the message that
lieutenant \(x_{0}\) receives from the general and forwards to
lieutenant \(x_{1}\),
...
which in turn forwards the message to lieutenant \(x_{n-1}\),
which in turn forwards the message to lieutenant \(x_{n}\).If the general is loyal then it sends the same message to all
lieutenants: \(m[0,x] = m[0]\) for all \(x\).
Loyal lieutenants forward the messages that they receive; however,
disloyal lieutenants may send arbitrary messages.ExampleThe diagram below shows a situation in which the general is loyal and
sends attack messages to all lieutenants.
Lieutenant 1 is disloyal (shown as a dashed circle).
Lieutenant 1 sends retreat messages to some lieutenants and attack
messages to others.
Lieutenant 2 is loyal, and so it broadcasts the message that
it receives.Fig.2: Edges Aggregation Dataflow TreesAggregating PhaseMessages received by a lieutenant are processed by the lieutenant in
steps that are also represented by a tree called theaggregation tree.
The aggregation tree has a node \(a[L]\) for each node \(m[L]\) of the
message tree.The diagram below shows a part of the aggregation tree for \(N=7,
t=2\); these are the processing steps of lieutenant 1.Fig.3: Aggregation Tree: Steps for Lieutenant 1Each node \(a[L, i]\) of the tree has a child \(a[L, x, i]\) for each
\(x\) that is not in \([L, i]\).
For example, \(a[0, 1]\) has children \(a[0, 2, 1], a[0, 3, 1], \ldots
a[0, 6, 1]\).Connections from Messaging to Aggregating NodesThere is an edge directed from each message node \(m(L)\) to the aggregation
node \(a(L)\).
For example, there are edges from \(m[2, 3, 1]\) to \(a[2, 3,
1]\), and from \(m[2, 1]\) to \(a[2, 1]\), and from  \(m[1]\) to
\(a[1]\).Output of Aggregating NodesThe output of an aggregation node is the majority of its inputs.
For example:\(
  a[2, 1] = \textrm{majority}(m[2, 1], a[2, 3, 1], a[2, 4, 1], \ldots, a[2, 6, 1])
\)If there are an equal number of attack and retreat inputs, then the
majority value is defined to be any default value.ExampleThe next diagram shows the data flow -- messaging and aggregation
trees, and the connections between them -- for a system where
\(N=4, t=1\).Fig.4: Dataflow for 4 agents 1 of which is disloyalInductive Generation of the Data FlowThe basic unit of data flow, which is replicated many times, is shown
in the top diagram of the figure below.
The input to the unit is a node \(m[L]\) of the message tree; this
unit is specified by \(L\),and the message \(m[L]\).
The output of the unit are nodes \(a[L, x]\) of the
aggregation tree, for all lieutenants \(x\) not in \(L\).The base case of the induction is a node at depth \(t\).
The input for the base case is \(m[L]\) where \(L\) is a list starting
with \(0\) and followed by \(t\) lieutenants.
For the base case, \(a[L, x] = m[L, x]\) for all \(x\).
The base case is illustrated in the lower diagram.Fig.5: Structure of DataflowThe data flow connecting a message node \(m[L]\) at depth \(d < t\) to
aggregation nodes \(a[L,x]\) is shown below.Fig.6: Structure of DataflowMessage node \(m[L]\) feeds message nodes \(m[L,x]\).
The connections between \(m[L,x]\) and aggregation nodes \(a[L,x,y]\)
are specified by the data flow connecting nodes of depth \(d + 1\),
shown in the diagram by blue dotted lines.The value of an aggregation node is the majority of its inputs.\(
a[L,i] = \textrm{majority}(m[L,i], [\forall x \notin [L, i]: a[L, x, i]])
\)For example,\(
a[0, 1, 2] = \textrm{majority}(m[0,1,2], a[0, 1, 3, 2], a[0, 1, 4, 2],
a[0, 1, 5, 2], \ldots)
\)Proof of ValidityWe prove that for all nodes \(m[L]\) of the message tree, if \(L_{*}\)
is loyal then for all \(i\) not in \(L\):\(
a[L, i] = m[L]
\)The proof is by induction.
The base case is for message nodes at depth \(t\).
We prove that if validity holds for message nodes at depth \(d > 0\)
then it holds for message nodes at depth \(d-1\).Base CaseSee the lower diagram of figure 5. 
For a message node \(m[L]\) at depth \(t\),\(
a[L, i] = m[L, i]
\)If \(L_{*}\) is loyal then \(m[L, i] = m[L]\).
and the result follows.Inductive StepSee figure 6.A node \(L\) at depth \(d\) consists of \(d+1\) agents.
Therefore, there are at least \(3t+1 - (d+1)\) lieutenants that are not in
\(L\).
Because \(d \leq t\) there are at least \(2t\) not in \(L\).
So, at least \(t\) lieutenants not in \(L\) are loyal and at most
\(t\) of them are disloyal.Because \(L_{*}\) is loyal, \(m[L,i] = m[L]\).By the induction assumption, for each loyal lieutenant \(j\) not in \(L\), and for each loyal
lieutenant \(i\) not in \([L, j]\):
\(
a[L, j, i] = m[L, i] = m[L]
\)\(
a[L, i] = \textrm{majority}(m[L,i], [\forall j \notin [L, i]: a[L, j,
i]])
\)The majority is taken over at least \(t + 1\) values equal to
\(m[L]\),
and at most \(t\) values that are different from it. And
therefore \(a[L, i] = m[L]\)ExampleThe next illustrates the proof of validity. Message \(m[L]\) is
shown in red, and the flow of correct messages is shown in red edges
and red nodes. For example, nodes \(m[L, i], m[L, j], m[L, i, j],
m[L, j, i]\) are red because agents \(i, j\) are loyal.A disloyal agent is represented by the symbol \(e\).
The output of a disloyal agent is unknown and is shown in black.
Node \(a[l. i]\) gets more than \(2t\) red inputs and at most \(t\)
black inputs, and hence the majority of its inputs is red.Fig.7: Illustration of ValidityProof of ConsensusWe will prove consensus for nodes at depth \(d\) if the number of
faulty nodes is at most \(t-d\).Base Case \(d = t\)In this case there are no disloyal lieutenants. For each \(i, j\),
\(m[L,i]\) and \(m[L,j]\) are arbitrary; however \(m[L, i] = m[L,
j]\).\(a[L, i, j] = m[L, i]\)
and
\(a[L, j, i] = m[L, j]\).
Therefore the inputs to \(a[L,i]\) and to \(a[L,j]\) are identical and
the result follows.Inductive Step \(d < t\)If the general is loyal, then consensus follows from validity.There are at most \(t - d\) faulty nodes.
If the general is disloyal then there at most \(t-d-1 = t -(d+1)\)
disloyal lieutenants.
Therefore, the induction assumption holds for \(m[L, x]\).
Therefore \(a[L, i, k] = a[L, i, j]\)K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology



Figure Link: https://kmchandy.github.io/Byzantine/Byzantine_5/Byzantine_5.001.jpeg

Caption: Fig.1: A Part of the Message Tree for General and 6 Lieutenants


Let \(m[x]\) be the message that the general sends
lieutenant \(x\), and let \(a[x]\) be the decision the lieutenant
  \(x\) makes.SpecificationThe specification has two parts, validity and consensus.Validity: Loyal lieutenants obey a loyal general.If all loyal lieutenants get the same message then each loyal
    lieutenant obeys the message that it receives.\(
  (\forall i, j: m[i] = m[j]) \quad \Rightarrow
\quad (\forall i: a[i] = m[i])
\)Consensus: Loyal lieutenants make the same
decision.\(
\forall i, j: a[i] = a[j]
\)AssumptionsThe oral Byzantine version makes fewer assumptions than the written
  version. The assumptions made are as follows:Synchrony:The algorithm operates in a synchronous fashion in a sequence of
  rounds or synchronous steps. If an agent \(x\) does not send
  a message to an agent \(y\) in a given round then \(y\) can detect
  that \(x\) did not send a message to it in that round.Reliability:If an agent \(y\) sends a message \(m\) to an agent \(y\) in a given
  round then \(z\) receives \(m\) in that round.Receiver knows sender:An agent that receives a message knows which agent sent it. If an
  agent \(z\) receives a message \(m\) from an agent \(y\) in a round
  then \(z\) knows that \(y\) sent \(m\) in that round.Why oral messages are harderIn the written version of the problem, if an agent
\(z\) receives a message \(m\) from any agent where \(m\) is signed by the
general then \(z\) knows that the general did send \(m\).
An agent cannot forge the general's signature and send a false
message.
By contrast, in the oral, or unencrypted version, any agent can
forge any agent's signature and send corrupted messages.Byzantine Generals AlgorithmMessages in the algorithm are eitherattackorretreatmessages.  If an agent \(x\) does not receive a message from an agent
\(y\) on a round then \(x\) treats the absence of the message from
\(y\) in the same way as if \(x\) received aretreatmessage
from \(y\). So, the algorithm only deals withattackandretreatmessages and does not deal with steps that an agent
takes if it does not receive a message.First we describe the flow of message in the algorithm and then
describe the algorithmMessage FlowMessages flow along a tree of height \(t + 1\).
The root node is \(m[0]\) which represents the general's command.
Each node of the tree is of the form \(m[L]\) where \(L[0] = 0\) and
\(L[1, \ldots, ]\) is a list of lieutenants where each lieutenant
appears at most once.The next figure illustrates a part of the messaging tree for \(N = 7,
t = 2\).
(There is insufficient space to show the complete tree.)Fig.1: A Part of the Message Tree for General and 6 LieutenantsEach non-leaf node \(m[L]\) in the tree has a child \(m[L,
x]\) for each lieutenant \(x\) that is not in \(L\).
For example, \(m[0]\) has children \(m[0,1], m[0,2], \ldots, m[0,
N-1]\).\(m[0,x]\) is the message that lieutenant \(x\) receives from the general.
\(m[0, x_{0}, \ldots, x_{k}]\) is the message that
lieutenant \(x_{0}\) receives from the general and forwards to
lieutenant \(x_{1}\),
...
which in turn forwards the message to lieutenant \(x_{n-1}\),
which in turn forwards the message to lieutenant \(x_{n}\).If the general is loyal then it sends the same message to all
lieutenants: \(m[0,x] = m[0]\) for all \(x\).
Loyal lieutenants forward the messages that they receive; however,
disloyal lieutenants may send arbitrary messages.ExampleThe diagram below shows a situation in which the general is loyal and
sends attack messages to all lieutenants.
Lieutenant 1 is disloyal (shown as a dashed circle).
Lieutenant 1 sends retreat messages to some lieutenants and attack
messages to others.
Lieutenant 2 is loyal, and so it broadcasts the message that
it receives.Fig.2: Edges Aggregation Dataflow TreesAggregating PhaseMessages received by a lieutenant are processed by the lieutenant in
steps that are also represented by a tree called theaggregation tree.
The aggregation tree has a node \(a[L]\) for each node \(m[L]\) of the
message tree.The diagram below shows a part of the aggregation tree for \(N=7,
t=2\); these are the processing steps of lieutenant 1.Fig.3: Aggregation Tree: Steps for Lieutenant 1Each node \(a[L, i]\) of the tree has a child \(a[L, x, i]\) for each
\(x\) that is not in \([L, i]\).
For example, \(a[0, 1]\) has children \(a[0, 2, 1], a[0, 3, 1], \ldots
a[0, 6, 1]\).Connections from Messaging to Aggregating NodesThere is an edge directed from each message node \(m(L)\) to the aggregation
node \(a(L)\).
For example, there are edges from \(m[2, 3, 1]\) to \(a[2, 3,
1]\), and from \(m[2, 1]\) to \(a[2, 1]\), and from  \(m[1]\) to
\(a[1]\).Output of Aggregating NodesThe output of an aggregation node is the majority of its inputs.
For example:\(
  a[2, 1] = \textrm{majority}(m[2, 1], a[2, 3, 1], a[2, 4, 1], \ldots, a[2, 6, 1])
\)If there are an equal number of attack and retreat inputs, then the
majority value is defined to be any default value.ExampleThe next diagram shows the data flow -- messaging and aggregation
trees, and the connections between them -- for a system where
\(N=4, t=1\).Fig.4: Dataflow for 4 agents 1 of which is disloyalInductive Generation of the Data FlowThe basic unit of data flow, which is replicated many times, is shown
in the top diagram of the figure below.
The input to the unit is a node \(m[L]\) of the message tree; this
unit is specified by \(L\),and the message \(m[L]\).
The output of the unit are nodes \(a[L, x]\) of the
aggregation tree, for all lieutenants \(x\) not in \(L\).The base case of the induction is a node at depth \(t\).
The input for the base case is \(m[L]\) where \(L\) is a list starting
with \(0\) and followed by \(t\) lieutenants.
For the base case, \(a[L, x] = m[L, x]\) for all \(x\).
The base case is illustrated in the lower diagram.Fig.5: Structure of DataflowThe data flow connecting a message node \(m[L]\) at depth \(d < t\) to
aggregation nodes \(a[L,x]\) is shown below.Fig.6: Structure of DataflowMessage node \(m[L]\) feeds message nodes \(m[L,x]\).
The connections between \(m[L,x]\) and aggregation nodes \(a[L,x,y]\)
are specified by the data flow connecting nodes of depth \(d + 1\),
shown in the diagram by blue dotted lines.The value of an aggregation node is the majority of its inputs.\(
a[L,i] = \textrm{majority}(m[L,i], [\forall x \notin [L, i]: a[L, x, i]])
\)For example,\(
a[0, 1, 2] = \textrm{majority}(m[0,1,2], a[0, 1, 3, 2], a[0, 1, 4, 2],
a[0, 1, 5, 2], \ldots)
\)Proof of ValidityWe prove that for all nodes \(m[L]\) of the message tree, if \(L_{*}\)
is loyal then for all \(i\) not in \(L\):\(
a[L, i] = m[L]
\)The proof is by induction.
The base case is for message nodes at depth \(t\).
We prove that if validity holds for message nodes at depth \(d > 0\)
then it holds for message nodes at depth \(d-1\).Base CaseSee the lower diagram of figure 5. 
For a message node \(m[L]\) at depth \(t\),\(
a[L, i] = m[L, i]
\)If \(L_{*}\) is loyal then \(m[L, i] = m[L]\).
and the result follows.Inductive StepSee figure 6.A node \(L\) at depth \(d\) consists of \(d+1\) agents.
Therefore, there are at least \(3t+1 - (d+1)\) lieutenants that are not in
\(L\).
Because \(d \leq t\) there are at least \(2t\) not in \(L\).
So, at least \(t\) lieutenants not in \(L\) are loyal and at most
\(t\) of them are disloyal.Because \(L_{*}\) is loyal, \(m[L,i] = m[L]\).By the induction assumption, for each loyal lieutenant \(j\) not in \(L\), and for each loyal
lieutenant \(i\) not in \([L, j]\):
\(
a[L, j, i] = m[L, i] = m[L]
\)\(
a[L, i] = \textrm{majority}(m[L,i], [\forall j \notin [L, i]: a[L, j,
i]])
\)The majority is taken over at least \(t + 1\) values equal to
\(m[L]\),
and at most \(t\) values that are different from it. And
therefore \(a[L, i] = m[L]\)ExampleThe next illustrates the proof of validity. Message \(m[L]\) is
shown in red, and the flow of correct messages is shown in red edges
and red nodes. For example, nodes \(m[L, i], m[L, j], m[L, i, j],
m[L, j, i]\) are red because agents \(i, j\) are loyal.A disloyal agent is represented by the symbol \(e\).
The output of a disloyal agent is unknown and is shown in black.
Node \(a[l. i]\) gets more than \(2t\) red inputs and at most \(t\)
black inputs, and hence the majority of its inputs is red.Fig.7: Illustration of ValidityProof of ConsensusWe will prove consensus for nodes at depth \(d\) if the number of
faulty nodes is at most \(t-d\).Base Case \(d = t\)In this case there are no disloyal lieutenants. For each \(i, j\),
\(m[L,i]\) and \(m[L,j]\) are arbitrary; however \(m[L, i] = m[L,
j]\).\(a[L, i, j] = m[L, i]\)
and
\(a[L, j, i] = m[L, j]\).
Therefore the inputs to \(a[L,i]\) and to \(a[L,j]\) are identical and
the result follows.Inductive Step \(d < t\)If the general is loyal, then consensus follows from validity.There are at most \(t - d\) faulty nodes.
If the general is disloyal then there at most \(t-d-1 = t -(d+1)\)
disloyal lieutenants.
Therefore, the induction assumption holds for \(m[L, x]\).
Therefore \(a[L, i, k] = a[L, i, j]\)K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology



Figure Link: https://kmchandy.github.io/Byzantine/Byzantine_5/Byzantine_5.001.jpeg

Caption: Fig.1: A Part of the Message Tree for General and 6 Lieutenants


If all loyal lieutenants get the same message then each loyal
    lieutenant obeys the message that it receives.\(
  (\forall i, j: m[i] = m[j]) \quad \Rightarrow
\quad (\forall i: a[i] = m[i])
\)Consensus: Loyal lieutenants make the same
decision.\(
\forall i, j: a[i] = a[j]
\)AssumptionsThe oral Byzantine version makes fewer assumptions than the written
  version. The assumptions made are as follows:Synchrony:The algorithm operates in a synchronous fashion in a sequence of
  rounds or synchronous steps. If an agent \(x\) does not send
  a message to an agent \(y\) in a given round then \(y\) can detect
  that \(x\) did not send a message to it in that round.Reliability:If an agent \(y\) sends a message \(m\) to an agent \(y\) in a given
  round then \(z\) receives \(m\) in that round.Receiver knows sender:An agent that receives a message knows which agent sent it. If an
  agent \(z\) receives a message \(m\) from an agent \(y\) in a round
  then \(z\) knows that \(y\) sent \(m\) in that round.Why oral messages are harderIn the written version of the problem, if an agent
\(z\) receives a message \(m\) from any agent where \(m\) is signed by the
general then \(z\) knows that the general did send \(m\).
An agent cannot forge the general's signature and send a false
message.
By contrast, in the oral, or unencrypted version, any agent can
forge any agent's signature and send corrupted messages.Byzantine Generals AlgorithmMessages in the algorithm are eitherattackorretreatmessages.  If an agent \(x\) does not receive a message from an agent
\(y\) on a round then \(x\) treats the absence of the message from
\(y\) in the same way as if \(x\) received aretreatmessage
from \(y\). So, the algorithm only deals withattackandretreatmessages and does not deal with steps that an agent
takes if it does not receive a message.First we describe the flow of message in the algorithm and then
describe the algorithmMessage FlowMessages flow along a tree of height \(t + 1\).
The root node is \(m[0]\) which represents the general's command.
Each node of the tree is of the form \(m[L]\) where \(L[0] = 0\) and
\(L[1, \ldots, ]\) is a list of lieutenants where each lieutenant
appears at most once.The next figure illustrates a part of the messaging tree for \(N = 7,
t = 2\).
(There is insufficient space to show the complete tree.)Fig.1: A Part of the Message Tree for General and 6 LieutenantsEach non-leaf node \(m[L]\) in the tree has a child \(m[L,
x]\) for each lieutenant \(x\) that is not in \(L\).
For example, \(m[0]\) has children \(m[0,1], m[0,2], \ldots, m[0,
N-1]\).\(m[0,x]\) is the message that lieutenant \(x\) receives from the general.
\(m[0, x_{0}, \ldots, x_{k}]\) is the message that
lieutenant \(x_{0}\) receives from the general and forwards to
lieutenant \(x_{1}\),
...
which in turn forwards the message to lieutenant \(x_{n-1}\),
which in turn forwards the message to lieutenant \(x_{n}\).If the general is loyal then it sends the same message to all
lieutenants: \(m[0,x] = m[0]\) for all \(x\).
Loyal lieutenants forward the messages that they receive; however,
disloyal lieutenants may send arbitrary messages.ExampleThe diagram below shows a situation in which the general is loyal and
sends attack messages to all lieutenants.
Lieutenant 1 is disloyal (shown as a dashed circle).
Lieutenant 1 sends retreat messages to some lieutenants and attack
messages to others.
Lieutenant 2 is loyal, and so it broadcasts the message that
it receives.Fig.2: Edges Aggregation Dataflow TreesAggregating PhaseMessages received by a lieutenant are processed by the lieutenant in
steps that are also represented by a tree called theaggregation tree.
The aggregation tree has a node \(a[L]\) for each node \(m[L]\) of the
message tree.The diagram below shows a part of the aggregation tree for \(N=7,
t=2\); these are the processing steps of lieutenant 1.Fig.3: Aggregation Tree: Steps for Lieutenant 1Each node \(a[L, i]\) of the tree has a child \(a[L, x, i]\) for each
\(x\) that is not in \([L, i]\).
For example, \(a[0, 1]\) has children \(a[0, 2, 1], a[0, 3, 1], \ldots
a[0, 6, 1]\).Connections from Messaging to Aggregating NodesThere is an edge directed from each message node \(m(L)\) to the aggregation
node \(a(L)\).
For example, there are edges from \(m[2, 3, 1]\) to \(a[2, 3,
1]\), and from \(m[2, 1]\) to \(a[2, 1]\), and from  \(m[1]\) to
\(a[1]\).Output of Aggregating NodesThe output of an aggregation node is the majority of its inputs.
For example:\(
  a[2, 1] = \textrm{majority}(m[2, 1], a[2, 3, 1], a[2, 4, 1], \ldots, a[2, 6, 1])
\)If there are an equal number of attack and retreat inputs, then the
majority value is defined to be any default value.ExampleThe next diagram shows the data flow -- messaging and aggregation
trees, and the connections between them -- for a system where
\(N=4, t=1\).Fig.4: Dataflow for 4 agents 1 of which is disloyalInductive Generation of the Data FlowThe basic unit of data flow, which is replicated many times, is shown
in the top diagram of the figure below.
The input to the unit is a node \(m[L]\) of the message tree; this
unit is specified by \(L\),and the message \(m[L]\).
The output of the unit are nodes \(a[L, x]\) of the
aggregation tree, for all lieutenants \(x\) not in \(L\).The base case of the induction is a node at depth \(t\).
The input for the base case is \(m[L]\) where \(L\) is a list starting
with \(0\) and followed by \(t\) lieutenants.
For the base case, \(a[L, x] = m[L, x]\) for all \(x\).
The base case is illustrated in the lower diagram.Fig.5: Structure of DataflowThe data flow connecting a message node \(m[L]\) at depth \(d < t\) to
aggregation nodes \(a[L,x]\) is shown below.Fig.6: Structure of DataflowMessage node \(m[L]\) feeds message nodes \(m[L,x]\).
The connections between \(m[L,x]\) and aggregation nodes \(a[L,x,y]\)
are specified by the data flow connecting nodes of depth \(d + 1\),
shown in the diagram by blue dotted lines.The value of an aggregation node is the majority of its inputs.\(
a[L,i] = \textrm{majority}(m[L,i], [\forall x \notin [L, i]: a[L, x, i]])
\)For example,\(
a[0, 1, 2] = \textrm{majority}(m[0,1,2], a[0, 1, 3, 2], a[0, 1, 4, 2],
a[0, 1, 5, 2], \ldots)
\)Proof of ValidityWe prove that for all nodes \(m[L]\) of the message tree, if \(L_{*}\)
is loyal then for all \(i\) not in \(L\):\(
a[L, i] = m[L]
\)The proof is by induction.
The base case is for message nodes at depth \(t\).
We prove that if validity holds for message nodes at depth \(d > 0\)
then it holds for message nodes at depth \(d-1\).Base CaseSee the lower diagram of figure 5. 
For a message node \(m[L]\) at depth \(t\),\(
a[L, i] = m[L, i]
\)If \(L_{*}\) is loyal then \(m[L, i] = m[L]\).
and the result follows.Inductive StepSee figure 6.A node \(L\) at depth \(d\) consists of \(d+1\) agents.
Therefore, there are at least \(3t+1 - (d+1)\) lieutenants that are not in
\(L\).
Because \(d \leq t\) there are at least \(2t\) not in \(L\).
So, at least \(t\) lieutenants not in \(L\) are loyal and at most
\(t\) of them are disloyal.Because \(L_{*}\) is loyal, \(m[L,i] = m[L]\).By the induction assumption, for each loyal lieutenant \(j\) not in \(L\), and for each loyal
lieutenant \(i\) not in \([L, j]\):
\(
a[L, j, i] = m[L, i] = m[L]
\)\(
a[L, i] = \textrm{majority}(m[L,i], [\forall j \notin [L, i]: a[L, j,
i]])
\)The majority is taken over at least \(t + 1\) values equal to
\(m[L]\),
and at most \(t\) values that are different from it. And
therefore \(a[L, i] = m[L]\)ExampleThe next illustrates the proof of validity. Message \(m[L]\) is
shown in red, and the flow of correct messages is shown in red edges
and red nodes. For example, nodes \(m[L, i], m[L, j], m[L, i, j],
m[L, j, i]\) are red because agents \(i, j\) are loyal.A disloyal agent is represented by the symbol \(e\).
The output of a disloyal agent is unknown and is shown in black.
Node \(a[l. i]\) gets more than \(2t\) red inputs and at most \(t\)
black inputs, and hence the majority of its inputs is red.Fig.7: Illustration of ValidityProof of ConsensusWe will prove consensus for nodes at depth \(d\) if the number of
faulty nodes is at most \(t-d\).Base Case \(d = t\)In this case there are no disloyal lieutenants. For each \(i, j\),
\(m[L,i]\) and \(m[L,j]\) are arbitrary; however \(m[L, i] = m[L,
j]\).\(a[L, i, j] = m[L, i]\)
and
\(a[L, j, i] = m[L, j]\).
Therefore the inputs to \(a[L,i]\) and to \(a[L,j]\) are identical and
the result follows.Inductive Step \(d < t\)If the general is loyal, then consensus follows from validity.There are at most \(t - d\) faulty nodes.
If the general is disloyal then there at most \(t-d-1 = t -(d+1)\)
disloyal lieutenants.
Therefore, the induction assumption holds for \(m[L, x]\).
Therefore \(a[L, i, k] = a[L, i, j]\)K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology



Figure Link: https://kmchandy.github.io/Byzantine/Byzantine_5/Byzantine_5.001.jpeg

Caption: Fig.1: A Part of the Message Tree for General and 6 Lieutenants


\(
  (\forall i, j: m[i] = m[j]) \quad \Rightarrow
\quad (\forall i: a[i] = m[i])
\)Consensus: Loyal lieutenants make the same
decision.\(
\forall i, j: a[i] = a[j]
\)AssumptionsThe oral Byzantine version makes fewer assumptions than the written
  version. The assumptions made are as follows:Synchrony:The algorithm operates in a synchronous fashion in a sequence of
  rounds or synchronous steps. If an agent \(x\) does not send
  a message to an agent \(y\) in a given round then \(y\) can detect
  that \(x\) did not send a message to it in that round.Reliability:If an agent \(y\) sends a message \(m\) to an agent \(y\) in a given
  round then \(z\) receives \(m\) in that round.Receiver knows sender:An agent that receives a message knows which agent sent it. If an
  agent \(z\) receives a message \(m\) from an agent \(y\) in a round
  then \(z\) knows that \(y\) sent \(m\) in that round.Why oral messages are harderIn the written version of the problem, if an agent
\(z\) receives a message \(m\) from any agent where \(m\) is signed by the
general then \(z\) knows that the general did send \(m\).
An agent cannot forge the general's signature and send a false
message.
By contrast, in the oral, or unencrypted version, any agent can
forge any agent's signature and send corrupted messages.Byzantine Generals AlgorithmMessages in the algorithm are eitherattackorretreatmessages.  If an agent \(x\) does not receive a message from an agent
\(y\) on a round then \(x\) treats the absence of the message from
\(y\) in the same way as if \(x\) received aretreatmessage
from \(y\). So, the algorithm only deals withattackandretreatmessages and does not deal with steps that an agent
takes if it does not receive a message.First we describe the flow of message in the algorithm and then
describe the algorithmMessage FlowMessages flow along a tree of height \(t + 1\).
The root node is \(m[0]\) which represents the general's command.
Each node of the tree is of the form \(m[L]\) where \(L[0] = 0\) and
\(L[1, \ldots, ]\) is a list of lieutenants where each lieutenant
appears at most once.The next figure illustrates a part of the messaging tree for \(N = 7,
t = 2\).
(There is insufficient space to show the complete tree.)Fig.1: A Part of the Message Tree for General and 6 LieutenantsEach non-leaf node \(m[L]\) in the tree has a child \(m[L,
x]\) for each lieutenant \(x\) that is not in \(L\).
For example, \(m[0]\) has children \(m[0,1], m[0,2], \ldots, m[0,
N-1]\).\(m[0,x]\) is the message that lieutenant \(x\) receives from the general.
\(m[0, x_{0}, \ldots, x_{k}]\) is the message that
lieutenant \(x_{0}\) receives from the general and forwards to
lieutenant \(x_{1}\),
...
which in turn forwards the message to lieutenant \(x_{n-1}\),
which in turn forwards the message to lieutenant \(x_{n}\).If the general is loyal then it sends the same message to all
lieutenants: \(m[0,x] = m[0]\) for all \(x\).
Loyal lieutenants forward the messages that they receive; however,
disloyal lieutenants may send arbitrary messages.ExampleThe diagram below shows a situation in which the general is loyal and
sends attack messages to all lieutenants.
Lieutenant 1 is disloyal (shown as a dashed circle).
Lieutenant 1 sends retreat messages to some lieutenants and attack
messages to others.
Lieutenant 2 is loyal, and so it broadcasts the message that
it receives.Fig.2: Edges Aggregation Dataflow TreesAggregating PhaseMessages received by a lieutenant are processed by the lieutenant in
steps that are also represented by a tree called theaggregation tree.
The aggregation tree has a node \(a[L]\) for each node \(m[L]\) of the
message tree.The diagram below shows a part of the aggregation tree for \(N=7,
t=2\); these are the processing steps of lieutenant 1.Fig.3: Aggregation Tree: Steps for Lieutenant 1Each node \(a[L, i]\) of the tree has a child \(a[L, x, i]\) for each
\(x\) that is not in \([L, i]\).
For example, \(a[0, 1]\) has children \(a[0, 2, 1], a[0, 3, 1], \ldots
a[0, 6, 1]\).Connections from Messaging to Aggregating NodesThere is an edge directed from each message node \(m(L)\) to the aggregation
node \(a(L)\).
For example, there are edges from \(m[2, 3, 1]\) to \(a[2, 3,
1]\), and from \(m[2, 1]\) to \(a[2, 1]\), and from  \(m[1]\) to
\(a[1]\).Output of Aggregating NodesThe output of an aggregation node is the majority of its inputs.
For example:\(
  a[2, 1] = \textrm{majority}(m[2, 1], a[2, 3, 1], a[2, 4, 1], \ldots, a[2, 6, 1])
\)If there are an equal number of attack and retreat inputs, then the
majority value is defined to be any default value.ExampleThe next diagram shows the data flow -- messaging and aggregation
trees, and the connections between them -- for a system where
\(N=4, t=1\).Fig.4: Dataflow for 4 agents 1 of which is disloyalInductive Generation of the Data FlowThe basic unit of data flow, which is replicated many times, is shown
in the top diagram of the figure below.
The input to the unit is a node \(m[L]\) of the message tree; this
unit is specified by \(L\),and the message \(m[L]\).
The output of the unit are nodes \(a[L, x]\) of the
aggregation tree, for all lieutenants \(x\) not in \(L\).The base case of the induction is a node at depth \(t\).
The input for the base case is \(m[L]\) where \(L\) is a list starting
with \(0\) and followed by \(t\) lieutenants.
For the base case, \(a[L, x] = m[L, x]\) for all \(x\).
The base case is illustrated in the lower diagram.Fig.5: Structure of DataflowThe data flow connecting a message node \(m[L]\) at depth \(d < t\) to
aggregation nodes \(a[L,x]\) is shown below.Fig.6: Structure of DataflowMessage node \(m[L]\) feeds message nodes \(m[L,x]\).
The connections between \(m[L,x]\) and aggregation nodes \(a[L,x,y]\)
are specified by the data flow connecting nodes of depth \(d + 1\),
shown in the diagram by blue dotted lines.The value of an aggregation node is the majority of its inputs.\(
a[L,i] = \textrm{majority}(m[L,i], [\forall x \notin [L, i]: a[L, x, i]])
\)For example,\(
a[0, 1, 2] = \textrm{majority}(m[0,1,2], a[0, 1, 3, 2], a[0, 1, 4, 2],
a[0, 1, 5, 2], \ldots)
\)Proof of ValidityWe prove that for all nodes \(m[L]\) of the message tree, if \(L_{*}\)
is loyal then for all \(i\) not in \(L\):\(
a[L, i] = m[L]
\)The proof is by induction.
The base case is for message nodes at depth \(t\).
We prove that if validity holds for message nodes at depth \(d > 0\)
then it holds for message nodes at depth \(d-1\).Base CaseSee the lower diagram of figure 5. 
For a message node \(m[L]\) at depth \(t\),\(
a[L, i] = m[L, i]
\)If \(L_{*}\) is loyal then \(m[L, i] = m[L]\).
and the result follows.Inductive StepSee figure 6.A node \(L\) at depth \(d\) consists of \(d+1\) agents.
Therefore, there are at least \(3t+1 - (d+1)\) lieutenants that are not in
\(L\).
Because \(d \leq t\) there are at least \(2t\) not in \(L\).
So, at least \(t\) lieutenants not in \(L\) are loyal and at most
\(t\) of them are disloyal.Because \(L_{*}\) is loyal, \(m[L,i] = m[L]\).By the induction assumption, for each loyal lieutenant \(j\) not in \(L\), and for each loyal
lieutenant \(i\) not in \([L, j]\):
\(
a[L, j, i] = m[L, i] = m[L]
\)\(
a[L, i] = \textrm{majority}(m[L,i], [\forall j \notin [L, i]: a[L, j,
i]])
\)The majority is taken over at least \(t + 1\) values equal to
\(m[L]\),
and at most \(t\) values that are different from it. And
therefore \(a[L, i] = m[L]\)ExampleThe next illustrates the proof of validity. Message \(m[L]\) is
shown in red, and the flow of correct messages is shown in red edges
and red nodes. For example, nodes \(m[L, i], m[L, j], m[L, i, j],
m[L, j, i]\) are red because agents \(i, j\) are loyal.A disloyal agent is represented by the symbol \(e\).
The output of a disloyal agent is unknown and is shown in black.
Node \(a[l. i]\) gets more than \(2t\) red inputs and at most \(t\)
black inputs, and hence the majority of its inputs is red.Fig.7: Illustration of ValidityProof of ConsensusWe will prove consensus for nodes at depth \(d\) if the number of
faulty nodes is at most \(t-d\).Base Case \(d = t\)In this case there are no disloyal lieutenants. For each \(i, j\),
\(m[L,i]\) and \(m[L,j]\) are arbitrary; however \(m[L, i] = m[L,
j]\).\(a[L, i, j] = m[L, i]\)
and
\(a[L, j, i] = m[L, j]\).
Therefore the inputs to \(a[L,i]\) and to \(a[L,j]\) are identical and
the result follows.Inductive Step \(d < t\)If the general is loyal, then consensus follows from validity.There are at most \(t - d\) faulty nodes.
If the general is disloyal then there at most \(t-d-1 = t -(d+1)\)
disloyal lieutenants.
Therefore, the induction assumption holds for \(m[L, x]\).
Therefore \(a[L, i, k] = a[L, i, j]\)K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology



Figure Link: https://kmchandy.github.io/Byzantine/Byzantine_5/Byzantine_5.001.jpeg

Caption: Fig.1: A Part of the Message Tree for General and 6 Lieutenants


First we describe the flow of message in the algorithm and then
describe the algorithmMessage FlowMessages flow along a tree of height \(t + 1\).
The root node is \(m[0]\) which represents the general's command.
Each node of the tree is of the form \(m[L]\) where \(L[0] = 0\) and
\(L[1, \ldots, ]\) is a list of lieutenants where each lieutenant
appears at most once.The next figure illustrates a part of the messaging tree for \(N = 7,
t = 2\).
(There is insufficient space to show the complete tree.)Fig.1: A Part of the Message Tree for General and 6 LieutenantsEach non-leaf node \(m[L]\) in the tree has a child \(m[L,
x]\) for each lieutenant \(x\) that is not in \(L\).
For example, \(m[0]\) has children \(m[0,1], m[0,2], \ldots, m[0,
N-1]\).\(m[0,x]\) is the message that lieutenant \(x\) receives from the general.
\(m[0, x_{0}, \ldots, x_{k}]\) is the message that
lieutenant \(x_{0}\) receives from the general and forwards to
lieutenant \(x_{1}\),
...
which in turn forwards the message to lieutenant \(x_{n-1}\),
which in turn forwards the message to lieutenant \(x_{n}\).If the general is loyal then it sends the same message to all
lieutenants: \(m[0,x] = m[0]\) for all \(x\).
Loyal lieutenants forward the messages that they receive; however,
disloyal lieutenants may send arbitrary messages.ExampleThe diagram below shows a situation in which the general is loyal and
sends attack messages to all lieutenants.
Lieutenant 1 is disloyal (shown as a dashed circle).
Lieutenant 1 sends retreat messages to some lieutenants and attack
messages to others.
Lieutenant 2 is loyal, and so it broadcasts the message that
it receives.Fig.2: Edges Aggregation Dataflow TreesAggregating PhaseMessages received by a lieutenant are processed by the lieutenant in
steps that are also represented by a tree called theaggregation tree.
The aggregation tree has a node \(a[L]\) for each node \(m[L]\) of the
message tree.The diagram below shows a part of the aggregation tree for \(N=7,
t=2\); these are the processing steps of lieutenant 1.Fig.3: Aggregation Tree: Steps for Lieutenant 1Each node \(a[L, i]\) of the tree has a child \(a[L, x, i]\) for each
\(x\) that is not in \([L, i]\).
For example, \(a[0, 1]\) has children \(a[0, 2, 1], a[0, 3, 1], \ldots
a[0, 6, 1]\).Connections from Messaging to Aggregating NodesThere is an edge directed from each message node \(m(L)\) to the aggregation
node \(a(L)\).
For example, there are edges from \(m[2, 3, 1]\) to \(a[2, 3,
1]\), and from \(m[2, 1]\) to \(a[2, 1]\), and from  \(m[1]\) to
\(a[1]\).Output of Aggregating NodesThe output of an aggregation node is the majority of its inputs.
For example:\(
  a[2, 1] = \textrm{majority}(m[2, 1], a[2, 3, 1], a[2, 4, 1], \ldots, a[2, 6, 1])
\)If there are an equal number of attack and retreat inputs, then the
majority value is defined to be any default value.ExampleThe next diagram shows the data flow -- messaging and aggregation
trees, and the connections between them -- for a system where
\(N=4, t=1\).Fig.4: Dataflow for 4 agents 1 of which is disloyalInductive Generation of the Data FlowThe basic unit of data flow, which is replicated many times, is shown
in the top diagram of the figure below.
The input to the unit is a node \(m[L]\) of the message tree; this
unit is specified by \(L\),and the message \(m[L]\).
The output of the unit are nodes \(a[L, x]\) of the
aggregation tree, for all lieutenants \(x\) not in \(L\).The base case of the induction is a node at depth \(t\).
The input for the base case is \(m[L]\) where \(L\) is a list starting
with \(0\) and followed by \(t\) lieutenants.
For the base case, \(a[L, x] = m[L, x]\) for all \(x\).
The base case is illustrated in the lower diagram.Fig.5: Structure of DataflowThe data flow connecting a message node \(m[L]\) at depth \(d < t\) to
aggregation nodes \(a[L,x]\) is shown below.Fig.6: Structure of DataflowMessage node \(m[L]\) feeds message nodes \(m[L,x]\).
The connections between \(m[L,x]\) and aggregation nodes \(a[L,x,y]\)
are specified by the data flow connecting nodes of depth \(d + 1\),
shown in the diagram by blue dotted lines.The value of an aggregation node is the majority of its inputs.\(
a[L,i] = \textrm{majority}(m[L,i], [\forall x \notin [L, i]: a[L, x, i]])
\)For example,\(
a[0, 1, 2] = \textrm{majority}(m[0,1,2], a[0, 1, 3, 2], a[0, 1, 4, 2],
a[0, 1, 5, 2], \ldots)
\)Proof of ValidityWe prove that for all nodes \(m[L]\) of the message tree, if \(L_{*}\)
is loyal then for all \(i\) not in \(L\):\(
a[L, i] = m[L]
\)The proof is by induction.
The base case is for message nodes at depth \(t\).
We prove that if validity holds for message nodes at depth \(d > 0\)
then it holds for message nodes at depth \(d-1\).Base CaseSee the lower diagram of figure 5. 
For a message node \(m[L]\) at depth \(t\),\(
a[L, i] = m[L, i]
\)If \(L_{*}\) is loyal then \(m[L, i] = m[L]\).
and the result follows.Inductive StepSee figure 6.A node \(L\) at depth \(d\) consists of \(d+1\) agents.
Therefore, there are at least \(3t+1 - (d+1)\) lieutenants that are not in
\(L\).
Because \(d \leq t\) there are at least \(2t\) not in \(L\).
So, at least \(t\) lieutenants not in \(L\) are loyal and at most
\(t\) of them are disloyal.Because \(L_{*}\) is loyal, \(m[L,i] = m[L]\).By the induction assumption, for each loyal lieutenant \(j\) not in \(L\), and for each loyal
lieutenant \(i\) not in \([L, j]\):
\(
a[L, j, i] = m[L, i] = m[L]
\)\(
a[L, i] = \textrm{majority}(m[L,i], [\forall j \notin [L, i]: a[L, j,
i]])
\)The majority is taken over at least \(t + 1\) values equal to
\(m[L]\),
and at most \(t\) values that are different from it. And
therefore \(a[L, i] = m[L]\)ExampleThe next illustrates the proof of validity. Message \(m[L]\) is
shown in red, and the flow of correct messages is shown in red edges
and red nodes. For example, nodes \(m[L, i], m[L, j], m[L, i, j],
m[L, j, i]\) are red because agents \(i, j\) are loyal.A disloyal agent is represented by the symbol \(e\).
The output of a disloyal agent is unknown and is shown in black.
Node \(a[l. i]\) gets more than \(2t\) red inputs and at most \(t\)
black inputs, and hence the majority of its inputs is red.Fig.7: Illustration of ValidityProof of ConsensusWe will prove consensus for nodes at depth \(d\) if the number of
faulty nodes is at most \(t-d\).Base Case \(d = t\)In this case there are no disloyal lieutenants. For each \(i, j\),
\(m[L,i]\) and \(m[L,j]\) are arbitrary; however \(m[L, i] = m[L,
j]\).\(a[L, i, j] = m[L, i]\)
and
\(a[L, j, i] = m[L, j]\).
Therefore the inputs to \(a[L,i]\) and to \(a[L,j]\) are identical and
the result follows.Inductive Step \(d < t\)If the general is loyal, then consensus follows from validity.There are at most \(t - d\) faulty nodes.
If the general is disloyal then there at most \(t-d-1 = t -(d+1)\)
disloyal lieutenants.
Therefore, the induction assumption holds for \(m[L, x]\).
Therefore \(a[L, i, k] = a[L, i, j]\)K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology



Figure Link: https://kmchandy.github.io/Byzantine/Byzantine_5/Byzantine_5.001.jpeg

Caption: Fig.1: A Part of the Message Tree for General and 6 Lieutenants


The next figure illustrates a part of the messaging tree for \(N = 7,
t = 2\).
(There is insufficient space to show the complete tree.)Fig.1: A Part of the Message Tree for General and 6 LieutenantsEach non-leaf node \(m[L]\) in the tree has a child \(m[L,
x]\) for each lieutenant \(x\) that is not in \(L\).
For example, \(m[0]\) has children \(m[0,1], m[0,2], \ldots, m[0,
N-1]\).\(m[0,x]\) is the message that lieutenant \(x\) receives from the general.
\(m[0, x_{0}, \ldots, x_{k}]\) is the message that
lieutenant \(x_{0}\) receives from the general and forwards to
lieutenant \(x_{1}\),
...
which in turn forwards the message to lieutenant \(x_{n-1}\),
which in turn forwards the message to lieutenant \(x_{n}\).If the general is loyal then it sends the same message to all
lieutenants: \(m[0,x] = m[0]\) for all \(x\).
Loyal lieutenants forward the messages that they receive; however,
disloyal lieutenants may send arbitrary messages.ExampleThe diagram below shows a situation in which the general is loyal and
sends attack messages to all lieutenants.
Lieutenant 1 is disloyal (shown as a dashed circle).
Lieutenant 1 sends retreat messages to some lieutenants and attack
messages to others.
Lieutenant 2 is loyal, and so it broadcasts the message that
it receives.Fig.2: Edges Aggregation Dataflow TreesAggregating PhaseMessages received by a lieutenant are processed by the lieutenant in
steps that are also represented by a tree called theaggregation tree.
The aggregation tree has a node \(a[L]\) for each node \(m[L]\) of the
message tree.The diagram below shows a part of the aggregation tree for \(N=7,
t=2\); these are the processing steps of lieutenant 1.Fig.3: Aggregation Tree: Steps for Lieutenant 1Each node \(a[L, i]\) of the tree has a child \(a[L, x, i]\) for each
\(x\) that is not in \([L, i]\).
For example, \(a[0, 1]\) has children \(a[0, 2, 1], a[0, 3, 1], \ldots
a[0, 6, 1]\).Connections from Messaging to Aggregating NodesThere is an edge directed from each message node \(m(L)\) to the aggregation
node \(a(L)\).
For example, there are edges from \(m[2, 3, 1]\) to \(a[2, 3,
1]\), and from \(m[2, 1]\) to \(a[2, 1]\), and from  \(m[1]\) to
\(a[1]\).Output of Aggregating NodesThe output of an aggregation node is the majority of its inputs.
For example:\(
  a[2, 1] = \textrm{majority}(m[2, 1], a[2, 3, 1], a[2, 4, 1], \ldots, a[2, 6, 1])
\)If there are an equal number of attack and retreat inputs, then the
majority value is defined to be any default value.ExampleThe next diagram shows the data flow -- messaging and aggregation
trees, and the connections between them -- for a system where
\(N=4, t=1\).Fig.4: Dataflow for 4 agents 1 of which is disloyalInductive Generation of the Data FlowThe basic unit of data flow, which is replicated many times, is shown
in the top diagram of the figure below.
The input to the unit is a node \(m[L]\) of the message tree; this
unit is specified by \(L\),and the message \(m[L]\).
The output of the unit are nodes \(a[L, x]\) of the
aggregation tree, for all lieutenants \(x\) not in \(L\).The base case of the induction is a node at depth \(t\).
The input for the base case is \(m[L]\) where \(L\) is a list starting
with \(0\) and followed by \(t\) lieutenants.
For the base case, \(a[L, x] = m[L, x]\) for all \(x\).
The base case is illustrated in the lower diagram.Fig.5: Structure of DataflowThe data flow connecting a message node \(m[L]\) at depth \(d < t\) to
aggregation nodes \(a[L,x]\) is shown below.Fig.6: Structure of DataflowMessage node \(m[L]\) feeds message nodes \(m[L,x]\).
The connections between \(m[L,x]\) and aggregation nodes \(a[L,x,y]\)
are specified by the data flow connecting nodes of depth \(d + 1\),
shown in the diagram by blue dotted lines.The value of an aggregation node is the majority of its inputs.\(
a[L,i] = \textrm{majority}(m[L,i], [\forall x \notin [L, i]: a[L, x, i]])
\)For example,\(
a[0, 1, 2] = \textrm{majority}(m[0,1,2], a[0, 1, 3, 2], a[0, 1, 4, 2],
a[0, 1, 5, 2], \ldots)
\)Proof of ValidityWe prove that for all nodes \(m[L]\) of the message tree, if \(L_{*}\)
is loyal then for all \(i\) not in \(L\):\(
a[L, i] = m[L]
\)The proof is by induction.
The base case is for message nodes at depth \(t\).
We prove that if validity holds for message nodes at depth \(d > 0\)
then it holds for message nodes at depth \(d-1\).Base CaseSee the lower diagram of figure 5. 
For a message node \(m[L]\) at depth \(t\),\(
a[L, i] = m[L, i]
\)If \(L_{*}\) is loyal then \(m[L, i] = m[L]\).
and the result follows.Inductive StepSee figure 6.A node \(L\) at depth \(d\) consists of \(d+1\) agents.
Therefore, there are at least \(3t+1 - (d+1)\) lieutenants that are not in
\(L\).
Because \(d \leq t\) there are at least \(2t\) not in \(L\).
So, at least \(t\) lieutenants not in \(L\) are loyal and at most
\(t\) of them are disloyal.Because \(L_{*}\) is loyal, \(m[L,i] = m[L]\).By the induction assumption, for each loyal lieutenant \(j\) not in \(L\), and for each loyal
lieutenant \(i\) not in \([L, j]\):
\(
a[L, j, i] = m[L, i] = m[L]
\)\(
a[L, i] = \textrm{majority}(m[L,i], [\forall j \notin [L, i]: a[L, j,
i]])
\)The majority is taken over at least \(t + 1\) values equal to
\(m[L]\),
and at most \(t\) values that are different from it. And
therefore \(a[L, i] = m[L]\)ExampleThe next illustrates the proof of validity. Message \(m[L]\) is
shown in red, and the flow of correct messages is shown in red edges
and red nodes. For example, nodes \(m[L, i], m[L, j], m[L, i, j],
m[L, j, i]\) are red because agents \(i, j\) are loyal.A disloyal agent is represented by the symbol \(e\).
The output of a disloyal agent is unknown and is shown in black.
Node \(a[l. i]\) gets more than \(2t\) red inputs and at most \(t\)
black inputs, and hence the majority of its inputs is red.Fig.7: Illustration of ValidityProof of ConsensusWe will prove consensus for nodes at depth \(d\) if the number of
faulty nodes is at most \(t-d\).Base Case \(d = t\)In this case there are no disloyal lieutenants. For each \(i, j\),
\(m[L,i]\) and \(m[L,j]\) are arbitrary; however \(m[L, i] = m[L,
j]\).\(a[L, i, j] = m[L, i]\)
and
\(a[L, j, i] = m[L, j]\).
Therefore the inputs to \(a[L,i]\) and to \(a[L,j]\) are identical and
the result follows.Inductive Step \(d < t\)If the general is loyal, then consensus follows from validity.There are at most \(t - d\) faulty nodes.
If the general is disloyal then there at most \(t-d-1 = t -(d+1)\)
disloyal lieutenants.
Therefore, the induction assumption holds for \(m[L, x]\).
Therefore \(a[L, i, k] = a[L, i, j]\)K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology



Figure Link: https://kmchandy.github.io/Byzantine/Byzantine_5/Byzantine_5.001.jpeg

Caption: Fig.1: A Part of the Message Tree for General and 6 Lieutenants


Each non-leaf node \(m[L]\) in the tree has a child \(m[L,
x]\) for each lieutenant \(x\) that is not in \(L\).
For example, \(m[0]\) has children \(m[0,1], m[0,2], \ldots, m[0,
N-1]\).\(m[0,x]\) is the message that lieutenant \(x\) receives from the general.
\(m[0, x_{0}, \ldots, x_{k}]\) is the message that
lieutenant \(x_{0}\) receives from the general and forwards to
lieutenant \(x_{1}\),
...
which in turn forwards the message to lieutenant \(x_{n-1}\),
which in turn forwards the message to lieutenant \(x_{n}\).If the general is loyal then it sends the same message to all
lieutenants: \(m[0,x] = m[0]\) for all \(x\).
Loyal lieutenants forward the messages that they receive; however,
disloyal lieutenants may send arbitrary messages.ExampleThe diagram below shows a situation in which the general is loyal and
sends attack messages to all lieutenants.
Lieutenant 1 is disloyal (shown as a dashed circle).
Lieutenant 1 sends retreat messages to some lieutenants and attack
messages to others.
Lieutenant 2 is loyal, and so it broadcasts the message that
it receives.Fig.2: Edges Aggregation Dataflow TreesAggregating PhaseMessages received by a lieutenant are processed by the lieutenant in
steps that are also represented by a tree called theaggregation tree.
The aggregation tree has a node \(a[L]\) for each node \(m[L]\) of the
message tree.The diagram below shows a part of the aggregation tree for \(N=7,
t=2\); these are the processing steps of lieutenant 1.Fig.3: Aggregation Tree: Steps for Lieutenant 1Each node \(a[L, i]\) of the tree has a child \(a[L, x, i]\) for each
\(x\) that is not in \([L, i]\).
For example, \(a[0, 1]\) has children \(a[0, 2, 1], a[0, 3, 1], \ldots
a[0, 6, 1]\).Connections from Messaging to Aggregating NodesThere is an edge directed from each message node \(m(L)\) to the aggregation
node \(a(L)\).
For example, there are edges from \(m[2, 3, 1]\) to \(a[2, 3,
1]\), and from \(m[2, 1]\) to \(a[2, 1]\), and from  \(m[1]\) to
\(a[1]\).Output of Aggregating NodesThe output of an aggregation node is the majority of its inputs.
For example:\(
  a[2, 1] = \textrm{majority}(m[2, 1], a[2, 3, 1], a[2, 4, 1], \ldots, a[2, 6, 1])
\)If there are an equal number of attack and retreat inputs, then the
majority value is defined to be any default value.ExampleThe next diagram shows the data flow -- messaging and aggregation
trees, and the connections between them -- for a system where
\(N=4, t=1\).Fig.4: Dataflow for 4 agents 1 of which is disloyalInductive Generation of the Data FlowThe basic unit of data flow, which is replicated many times, is shown
in the top diagram of the figure below.
The input to the unit is a node \(m[L]\) of the message tree; this
unit is specified by \(L\),and the message \(m[L]\).
The output of the unit are nodes \(a[L, x]\) of the
aggregation tree, for all lieutenants \(x\) not in \(L\).The base case of the induction is a node at depth \(t\).
The input for the base case is \(m[L]\) where \(L\) is a list starting
with \(0\) and followed by \(t\) lieutenants.
For the base case, \(a[L, x] = m[L, x]\) for all \(x\).
The base case is illustrated in the lower diagram.Fig.5: Structure of DataflowThe data flow connecting a message node \(m[L]\) at depth \(d < t\) to
aggregation nodes \(a[L,x]\) is shown below.Fig.6: Structure of DataflowMessage node \(m[L]\) feeds message nodes \(m[L,x]\).
The connections between \(m[L,x]\) and aggregation nodes \(a[L,x,y]\)
are specified by the data flow connecting nodes of depth \(d + 1\),
shown in the diagram by blue dotted lines.The value of an aggregation node is the majority of its inputs.\(
a[L,i] = \textrm{majority}(m[L,i], [\forall x \notin [L, i]: a[L, x, i]])
\)For example,\(
a[0, 1, 2] = \textrm{majority}(m[0,1,2], a[0, 1, 3, 2], a[0, 1, 4, 2],
a[0, 1, 5, 2], \ldots)
\)Proof of ValidityWe prove that for all nodes \(m[L]\) of the message tree, if \(L_{*}\)
is loyal then for all \(i\) not in \(L\):\(
a[L, i] = m[L]
\)The proof is by induction.
The base case is for message nodes at depth \(t\).
We prove that if validity holds for message nodes at depth \(d > 0\)
then it holds for message nodes at depth \(d-1\).Base CaseSee the lower diagram of figure 5. 
For a message node \(m[L]\) at depth \(t\),\(
a[L, i] = m[L, i]
\)If \(L_{*}\) is loyal then \(m[L, i] = m[L]\).
and the result follows.Inductive StepSee figure 6.A node \(L\) at depth \(d\) consists of \(d+1\) agents.
Therefore, there are at least \(3t+1 - (d+1)\) lieutenants that are not in
\(L\).
Because \(d \leq t\) there are at least \(2t\) not in \(L\).
So, at least \(t\) lieutenants not in \(L\) are loyal and at most
\(t\) of them are disloyal.Because \(L_{*}\) is loyal, \(m[L,i] = m[L]\).By the induction assumption, for each loyal lieutenant \(j\) not in \(L\), and for each loyal
lieutenant \(i\) not in \([L, j]\):
\(
a[L, j, i] = m[L, i] = m[L]
\)\(
a[L, i] = \textrm{majority}(m[L,i], [\forall j \notin [L, i]: a[L, j,
i]])
\)The majority is taken over at least \(t + 1\) values equal to
\(m[L]\),
and at most \(t\) values that are different from it. And
therefore \(a[L, i] = m[L]\)ExampleThe next illustrates the proof of validity. Message \(m[L]\) is
shown in red, and the flow of correct messages is shown in red edges
and red nodes. For example, nodes \(m[L, i], m[L, j], m[L, i, j],
m[L, j, i]\) are red because agents \(i, j\) are loyal.A disloyal agent is represented by the symbol \(e\).
The output of a disloyal agent is unknown and is shown in black.
Node \(a[l. i]\) gets more than \(2t\) red inputs and at most \(t\)
black inputs, and hence the majority of its inputs is red.Fig.7: Illustration of ValidityProof of ConsensusWe will prove consensus for nodes at depth \(d\) if the number of
faulty nodes is at most \(t-d\).Base Case \(d = t\)In this case there are no disloyal lieutenants. For each \(i, j\),
\(m[L,i]\) and \(m[L,j]\) are arbitrary; however \(m[L, i] = m[L,
j]\).\(a[L, i, j] = m[L, i]\)
and
\(a[L, j, i] = m[L, j]\).
Therefore the inputs to \(a[L,i]\) and to \(a[L,j]\) are identical and
the result follows.Inductive Step \(d < t\)If the general is loyal, then consensus follows from validity.There are at most \(t - d\) faulty nodes.
If the general is disloyal then there at most \(t-d-1 = t -(d+1)\)
disloyal lieutenants.
Therefore, the induction assumption holds for \(m[L, x]\).
Therefore \(a[L, i, k] = a[L, i, j]\)K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology



Figure Link: https://kmchandy.github.io/Byzantine/Byzantine_5/Byzantine_5.002.jpeg

Caption: Fig.2: Edges Aggregation Dataflow Trees


\(m[0,x]\) is the message that lieutenant \(x\) receives from the general.
\(m[0, x_{0}, \ldots, x_{k}]\) is the message that
lieutenant \(x_{0}\) receives from the general and forwards to
lieutenant \(x_{1}\),
...
which in turn forwards the message to lieutenant \(x_{n-1}\),
which in turn forwards the message to lieutenant \(x_{n}\).If the general is loyal then it sends the same message to all
lieutenants: \(m[0,x] = m[0]\) for all \(x\).
Loyal lieutenants forward the messages that they receive; however,
disloyal lieutenants may send arbitrary messages.ExampleThe diagram below shows a situation in which the general is loyal and
sends attack messages to all lieutenants.
Lieutenant 1 is disloyal (shown as a dashed circle).
Lieutenant 1 sends retreat messages to some lieutenants and attack
messages to others.
Lieutenant 2 is loyal, and so it broadcasts the message that
it receives.Fig.2: Edges Aggregation Dataflow TreesAggregating PhaseMessages received by a lieutenant are processed by the lieutenant in
steps that are also represented by a tree called theaggregation tree.
The aggregation tree has a node \(a[L]\) for each node \(m[L]\) of the
message tree.The diagram below shows a part of the aggregation tree for \(N=7,
t=2\); these are the processing steps of lieutenant 1.Fig.3: Aggregation Tree: Steps for Lieutenant 1Each node \(a[L, i]\) of the tree has a child \(a[L, x, i]\) for each
\(x\) that is not in \([L, i]\).
For example, \(a[0, 1]\) has children \(a[0, 2, 1], a[0, 3, 1], \ldots
a[0, 6, 1]\).Connections from Messaging to Aggregating NodesThere is an edge directed from each message node \(m(L)\) to the aggregation
node \(a(L)\).
For example, there are edges from \(m[2, 3, 1]\) to \(a[2, 3,
1]\), and from \(m[2, 1]\) to \(a[2, 1]\), and from  \(m[1]\) to
\(a[1]\).Output of Aggregating NodesThe output of an aggregation node is the majority of its inputs.
For example:\(
  a[2, 1] = \textrm{majority}(m[2, 1], a[2, 3, 1], a[2, 4, 1], \ldots, a[2, 6, 1])
\)If there are an equal number of attack and retreat inputs, then the
majority value is defined to be any default value.ExampleThe next diagram shows the data flow -- messaging and aggregation
trees, and the connections between them -- for a system where
\(N=4, t=1\).Fig.4: Dataflow for 4 agents 1 of which is disloyalInductive Generation of the Data FlowThe basic unit of data flow, which is replicated many times, is shown
in the top diagram of the figure below.
The input to the unit is a node \(m[L]\) of the message tree; this
unit is specified by \(L\),and the message \(m[L]\).
The output of the unit are nodes \(a[L, x]\) of the
aggregation tree, for all lieutenants \(x\) not in \(L\).The base case of the induction is a node at depth \(t\).
The input for the base case is \(m[L]\) where \(L\) is a list starting
with \(0\) and followed by \(t\) lieutenants.
For the base case, \(a[L, x] = m[L, x]\) for all \(x\).
The base case is illustrated in the lower diagram.Fig.5: Structure of DataflowThe data flow connecting a message node \(m[L]\) at depth \(d < t\) to
aggregation nodes \(a[L,x]\) is shown below.Fig.6: Structure of DataflowMessage node \(m[L]\) feeds message nodes \(m[L,x]\).
The connections between \(m[L,x]\) and aggregation nodes \(a[L,x,y]\)
are specified by the data flow connecting nodes of depth \(d + 1\),
shown in the diagram by blue dotted lines.The value of an aggregation node is the majority of its inputs.\(
a[L,i] = \textrm{majority}(m[L,i], [\forall x \notin [L, i]: a[L, x, i]])
\)For example,\(
a[0, 1, 2] = \textrm{majority}(m[0,1,2], a[0, 1, 3, 2], a[0, 1, 4, 2],
a[0, 1, 5, 2], \ldots)
\)Proof of ValidityWe prove that for all nodes \(m[L]\) of the message tree, if \(L_{*}\)
is loyal then for all \(i\) not in \(L\):\(
a[L, i] = m[L]
\)The proof is by induction.
The base case is for message nodes at depth \(t\).
We prove that if validity holds for message nodes at depth \(d > 0\)
then it holds for message nodes at depth \(d-1\).Base CaseSee the lower diagram of figure 5. 
For a message node \(m[L]\) at depth \(t\),\(
a[L, i] = m[L, i]
\)If \(L_{*}\) is loyal then \(m[L, i] = m[L]\).
and the result follows.Inductive StepSee figure 6.A node \(L\) at depth \(d\) consists of \(d+1\) agents.
Therefore, there are at least \(3t+1 - (d+1)\) lieutenants that are not in
\(L\).
Because \(d \leq t\) there are at least \(2t\) not in \(L\).
So, at least \(t\) lieutenants not in \(L\) are loyal and at most
\(t\) of them are disloyal.Because \(L_{*}\) is loyal, \(m[L,i] = m[L]\).By the induction assumption, for each loyal lieutenant \(j\) not in \(L\), and for each loyal
lieutenant \(i\) not in \([L, j]\):
\(
a[L, j, i] = m[L, i] = m[L]
\)\(
a[L, i] = \textrm{majority}(m[L,i], [\forall j \notin [L, i]: a[L, j,
i]])
\)The majority is taken over at least \(t + 1\) values equal to
\(m[L]\),
and at most \(t\) values that are different from it. And
therefore \(a[L, i] = m[L]\)ExampleThe next illustrates the proof of validity. Message \(m[L]\) is
shown in red, and the flow of correct messages is shown in red edges
and red nodes. For example, nodes \(m[L, i], m[L, j], m[L, i, j],
m[L, j, i]\) are red because agents \(i, j\) are loyal.A disloyal agent is represented by the symbol \(e\).
The output of a disloyal agent is unknown and is shown in black.
Node \(a[l. i]\) gets more than \(2t\) red inputs and at most \(t\)
black inputs, and hence the majority of its inputs is red.Fig.7: Illustration of ValidityProof of ConsensusWe will prove consensus for nodes at depth \(d\) if the number of
faulty nodes is at most \(t-d\).Base Case \(d = t\)In this case there are no disloyal lieutenants. For each \(i, j\),
\(m[L,i]\) and \(m[L,j]\) are arbitrary; however \(m[L, i] = m[L,
j]\).\(a[L, i, j] = m[L, i]\)
and
\(a[L, j, i] = m[L, j]\).
Therefore the inputs to \(a[L,i]\) and to \(a[L,j]\) are identical and
the result follows.Inductive Step \(d < t\)If the general is loyal, then consensus follows from validity.There are at most \(t - d\) faulty nodes.
If the general is disloyal then there at most \(t-d-1 = t -(d+1)\)
disloyal lieutenants.
Therefore, the induction assumption holds for \(m[L, x]\).
Therefore \(a[L, i, k] = a[L, i, j]\)K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology



Figure Link: https://kmchandy.github.io/Byzantine/Byzantine_5/Byzantine_5.002.jpeg

Caption: Fig.2: Edges Aggregation Dataflow Trees


If the general is loyal then it sends the same message to all
lieutenants: \(m[0,x] = m[0]\) for all \(x\).
Loyal lieutenants forward the messages that they receive; however,
disloyal lieutenants may send arbitrary messages.ExampleThe diagram below shows a situation in which the general is loyal and
sends attack messages to all lieutenants.
Lieutenant 1 is disloyal (shown as a dashed circle).
Lieutenant 1 sends retreat messages to some lieutenants and attack
messages to others.
Lieutenant 2 is loyal, and so it broadcasts the message that
it receives.Fig.2: Edges Aggregation Dataflow TreesAggregating PhaseMessages received by a lieutenant are processed by the lieutenant in
steps that are also represented by a tree called theaggregation tree.
The aggregation tree has a node \(a[L]\) for each node \(m[L]\) of the
message tree.The diagram below shows a part of the aggregation tree for \(N=7,
t=2\); these are the processing steps of lieutenant 1.Fig.3: Aggregation Tree: Steps for Lieutenant 1Each node \(a[L, i]\) of the tree has a child \(a[L, x, i]\) for each
\(x\) that is not in \([L, i]\).
For example, \(a[0, 1]\) has children \(a[0, 2, 1], a[0, 3, 1], \ldots
a[0, 6, 1]\).Connections from Messaging to Aggregating NodesThere is an edge directed from each message node \(m(L)\) to the aggregation
node \(a(L)\).
For example, there are edges from \(m[2, 3, 1]\) to \(a[2, 3,
1]\), and from \(m[2, 1]\) to \(a[2, 1]\), and from  \(m[1]\) to
\(a[1]\).Output of Aggregating NodesThe output of an aggregation node is the majority of its inputs.
For example:\(
  a[2, 1] = \textrm{majority}(m[2, 1], a[2, 3, 1], a[2, 4, 1], \ldots, a[2, 6, 1])
\)If there are an equal number of attack and retreat inputs, then the
majority value is defined to be any default value.ExampleThe next diagram shows the data flow -- messaging and aggregation
trees, and the connections between them -- for a system where
\(N=4, t=1\).Fig.4: Dataflow for 4 agents 1 of which is disloyalInductive Generation of the Data FlowThe basic unit of data flow, which is replicated many times, is shown
in the top diagram of the figure below.
The input to the unit is a node \(m[L]\) of the message tree; this
unit is specified by \(L\),and the message \(m[L]\).
The output of the unit are nodes \(a[L, x]\) of the
aggregation tree, for all lieutenants \(x\) not in \(L\).The base case of the induction is a node at depth \(t\).
The input for the base case is \(m[L]\) where \(L\) is a list starting
with \(0\) and followed by \(t\) lieutenants.
For the base case, \(a[L, x] = m[L, x]\) for all \(x\).
The base case is illustrated in the lower diagram.Fig.5: Structure of DataflowThe data flow connecting a message node \(m[L]\) at depth \(d < t\) to
aggregation nodes \(a[L,x]\) is shown below.Fig.6: Structure of DataflowMessage node \(m[L]\) feeds message nodes \(m[L,x]\).
The connections between \(m[L,x]\) and aggregation nodes \(a[L,x,y]\)
are specified by the data flow connecting nodes of depth \(d + 1\),
shown in the diagram by blue dotted lines.The value of an aggregation node is the majority of its inputs.\(
a[L,i] = \textrm{majority}(m[L,i], [\forall x \notin [L, i]: a[L, x, i]])
\)For example,\(
a[0, 1, 2] = \textrm{majority}(m[0,1,2], a[0, 1, 3, 2], a[0, 1, 4, 2],
a[0, 1, 5, 2], \ldots)
\)Proof of ValidityWe prove that for all nodes \(m[L]\) of the message tree, if \(L_{*}\)
is loyal then for all \(i\) not in \(L\):\(
a[L, i] = m[L]
\)The proof is by induction.
The base case is for message nodes at depth \(t\).
We prove that if validity holds for message nodes at depth \(d > 0\)
then it holds for message nodes at depth \(d-1\).Base CaseSee the lower diagram of figure 5. 
For a message node \(m[L]\) at depth \(t\),\(
a[L, i] = m[L, i]
\)If \(L_{*}\) is loyal then \(m[L, i] = m[L]\).
and the result follows.Inductive StepSee figure 6.A node \(L\) at depth \(d\) consists of \(d+1\) agents.
Therefore, there are at least \(3t+1 - (d+1)\) lieutenants that are not in
\(L\).
Because \(d \leq t\) there are at least \(2t\) not in \(L\).
So, at least \(t\) lieutenants not in \(L\) are loyal and at most
\(t\) of them are disloyal.Because \(L_{*}\) is loyal, \(m[L,i] = m[L]\).By the induction assumption, for each loyal lieutenant \(j\) not in \(L\), and for each loyal
lieutenant \(i\) not in \([L, j]\):
\(
a[L, j, i] = m[L, i] = m[L]
\)\(
a[L, i] = \textrm{majority}(m[L,i], [\forall j \notin [L, i]: a[L, j,
i]])
\)The majority is taken over at least \(t + 1\) values equal to
\(m[L]\),
and at most \(t\) values that are different from it. And
therefore \(a[L, i] = m[L]\)ExampleThe next illustrates the proof of validity. Message \(m[L]\) is
shown in red, and the flow of correct messages is shown in red edges
and red nodes. For example, nodes \(m[L, i], m[L, j], m[L, i, j],
m[L, j, i]\) are red because agents \(i, j\) are loyal.A disloyal agent is represented by the symbol \(e\).
The output of a disloyal agent is unknown and is shown in black.
Node \(a[l. i]\) gets more than \(2t\) red inputs and at most \(t\)
black inputs, and hence the majority of its inputs is red.Fig.7: Illustration of ValidityProof of ConsensusWe will prove consensus for nodes at depth \(d\) if the number of
faulty nodes is at most \(t-d\).Base Case \(d = t\)In this case there are no disloyal lieutenants. For each \(i, j\),
\(m[L,i]\) and \(m[L,j]\) are arbitrary; however \(m[L, i] = m[L,
j]\).\(a[L, i, j] = m[L, i]\)
and
\(a[L, j, i] = m[L, j]\).
Therefore the inputs to \(a[L,i]\) and to \(a[L,j]\) are identical and
the result follows.Inductive Step \(d < t\)If the general is loyal, then consensus follows from validity.There are at most \(t - d\) faulty nodes.
If the general is disloyal then there at most \(t-d-1 = t -(d+1)\)
disloyal lieutenants.
Therefore, the induction assumption holds for \(m[L, x]\).
Therefore \(a[L, i, k] = a[L, i, j]\)K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology



Figure Link: https://kmchandy.github.io/Byzantine/Byzantine_5/Byzantine_5.002.jpeg

Caption: Fig.2: Edges Aggregation Dataflow Trees


The diagram below shows a part of the aggregation tree for \(N=7,
t=2\); these are the processing steps of lieutenant 1.Fig.3: Aggregation Tree: Steps for Lieutenant 1Each node \(a[L, i]\) of the tree has a child \(a[L, x, i]\) for each
\(x\) that is not in \([L, i]\).
For example, \(a[0, 1]\) has children \(a[0, 2, 1], a[0, 3, 1], \ldots
a[0, 6, 1]\).Connections from Messaging to Aggregating NodesThere is an edge directed from each message node \(m(L)\) to the aggregation
node \(a(L)\).
For example, there are edges from \(m[2, 3, 1]\) to \(a[2, 3,
1]\), and from \(m[2, 1]\) to \(a[2, 1]\), and from  \(m[1]\) to
\(a[1]\).Output of Aggregating NodesThe output of an aggregation node is the majority of its inputs.
For example:\(
  a[2, 1] = \textrm{majority}(m[2, 1], a[2, 3, 1], a[2, 4, 1], \ldots, a[2, 6, 1])
\)If there are an equal number of attack and retreat inputs, then the
majority value is defined to be any default value.ExampleThe next diagram shows the data flow -- messaging and aggregation
trees, and the connections between them -- for a system where
\(N=4, t=1\).Fig.4: Dataflow for 4 agents 1 of which is disloyalInductive Generation of the Data FlowThe basic unit of data flow, which is replicated many times, is shown
in the top diagram of the figure below.
The input to the unit is a node \(m[L]\) of the message tree; this
unit is specified by \(L\),and the message \(m[L]\).
The output of the unit are nodes \(a[L, x]\) of the
aggregation tree, for all lieutenants \(x\) not in \(L\).The base case of the induction is a node at depth \(t\).
The input for the base case is \(m[L]\) where \(L\) is a list starting
with \(0\) and followed by \(t\) lieutenants.
For the base case, \(a[L, x] = m[L, x]\) for all \(x\).
The base case is illustrated in the lower diagram.Fig.5: Structure of DataflowThe data flow connecting a message node \(m[L]\) at depth \(d < t\) to
aggregation nodes \(a[L,x]\) is shown below.Fig.6: Structure of DataflowMessage node \(m[L]\) feeds message nodes \(m[L,x]\).
The connections between \(m[L,x]\) and aggregation nodes \(a[L,x,y]\)
are specified by the data flow connecting nodes of depth \(d + 1\),
shown in the diagram by blue dotted lines.The value of an aggregation node is the majority of its inputs.\(
a[L,i] = \textrm{majority}(m[L,i], [\forall x \notin [L, i]: a[L, x, i]])
\)For example,\(
a[0, 1, 2] = \textrm{majority}(m[0,1,2], a[0, 1, 3, 2], a[0, 1, 4, 2],
a[0, 1, 5, 2], \ldots)
\)Proof of ValidityWe prove that for all nodes \(m[L]\) of the message tree, if \(L_{*}\)
is loyal then for all \(i\) not in \(L\):\(
a[L, i] = m[L]
\)The proof is by induction.
The base case is for message nodes at depth \(t\).
We prove that if validity holds for message nodes at depth \(d > 0\)
then it holds for message nodes at depth \(d-1\).Base CaseSee the lower diagram of figure 5. 
For a message node \(m[L]\) at depth \(t\),\(
a[L, i] = m[L, i]
\)If \(L_{*}\) is loyal then \(m[L, i] = m[L]\).
and the result follows.Inductive StepSee figure 6.A node \(L\) at depth \(d\) consists of \(d+1\) agents.
Therefore, there are at least \(3t+1 - (d+1)\) lieutenants that are not in
\(L\).
Because \(d \leq t\) there are at least \(2t\) not in \(L\).
So, at least \(t\) lieutenants not in \(L\) are loyal and at most
\(t\) of them are disloyal.Because \(L_{*}\) is loyal, \(m[L,i] = m[L]\).By the induction assumption, for each loyal lieutenant \(j\) not in \(L\), and for each loyal
lieutenant \(i\) not in \([L, j]\):
\(
a[L, j, i] = m[L, i] = m[L]
\)\(
a[L, i] = \textrm{majority}(m[L,i], [\forall j \notin [L, i]: a[L, j,
i]])
\)The majority is taken over at least \(t + 1\) values equal to
\(m[L]\),
and at most \(t\) values that are different from it. And
therefore \(a[L, i] = m[L]\)ExampleThe next illustrates the proof of validity. Message \(m[L]\) is
shown in red, and the flow of correct messages is shown in red edges
and red nodes. For example, nodes \(m[L, i], m[L, j], m[L, i, j],
m[L, j, i]\) are red because agents \(i, j\) are loyal.A disloyal agent is represented by the symbol \(e\).
The output of a disloyal agent is unknown and is shown in black.
Node \(a[l. i]\) gets more than \(2t\) red inputs and at most \(t\)
black inputs, and hence the majority of its inputs is red.Fig.7: Illustration of ValidityProof of ConsensusWe will prove consensus for nodes at depth \(d\) if the number of
faulty nodes is at most \(t-d\).Base Case \(d = t\)In this case there are no disloyal lieutenants. For each \(i, j\),
\(m[L,i]\) and \(m[L,j]\) are arbitrary; however \(m[L, i] = m[L,
j]\).\(a[L, i, j] = m[L, i]\)
and
\(a[L, j, i] = m[L, j]\).
Therefore the inputs to \(a[L,i]\) and to \(a[L,j]\) are identical and
the result follows.Inductive Step \(d < t\)If the general is loyal, then consensus follows from validity.There are at most \(t - d\) faulty nodes.
If the general is disloyal then there at most \(t-d-1 = t -(d+1)\)
disloyal lieutenants.
Therefore, the induction assumption holds for \(m[L, x]\).
Therefore \(a[L, i, k] = a[L, i, j]\)K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology



Figure Link: https://kmchandy.github.io/Byzantine/Byzantine_5/Byzantine_5.003.jpeg

Caption: Fig.3: Aggregation Tree: Steps for Lieutenant 1


Each node \(a[L, i]\) of the tree has a child \(a[L, x, i]\) for each
\(x\) that is not in \([L, i]\).
For example, \(a[0, 1]\) has children \(a[0, 2, 1], a[0, 3, 1], \ldots
a[0, 6, 1]\).Connections from Messaging to Aggregating NodesThere is an edge directed from each message node \(m(L)\) to the aggregation
node \(a(L)\).
For example, there are edges from \(m[2, 3, 1]\) to \(a[2, 3,
1]\), and from \(m[2, 1]\) to \(a[2, 1]\), and from  \(m[1]\) to
\(a[1]\).Output of Aggregating NodesThe output of an aggregation node is the majority of its inputs.
For example:\(
  a[2, 1] = \textrm{majority}(m[2, 1], a[2, 3, 1], a[2, 4, 1], \ldots, a[2, 6, 1])
\)If there are an equal number of attack and retreat inputs, then the
majority value is defined to be any default value.ExampleThe next diagram shows the data flow -- messaging and aggregation
trees, and the connections between them -- for a system where
\(N=4, t=1\).Fig.4: Dataflow for 4 agents 1 of which is disloyalInductive Generation of the Data FlowThe basic unit of data flow, which is replicated many times, is shown
in the top diagram of the figure below.
The input to the unit is a node \(m[L]\) of the message tree; this
unit is specified by \(L\),and the message \(m[L]\).
The output of the unit are nodes \(a[L, x]\) of the
aggregation tree, for all lieutenants \(x\) not in \(L\).The base case of the induction is a node at depth \(t\).
The input for the base case is \(m[L]\) where \(L\) is a list starting
with \(0\) and followed by \(t\) lieutenants.
For the base case, \(a[L, x] = m[L, x]\) for all \(x\).
The base case is illustrated in the lower diagram.Fig.5: Structure of DataflowThe data flow connecting a message node \(m[L]\) at depth \(d < t\) to
aggregation nodes \(a[L,x]\) is shown below.Fig.6: Structure of DataflowMessage node \(m[L]\) feeds message nodes \(m[L,x]\).
The connections between \(m[L,x]\) and aggregation nodes \(a[L,x,y]\)
are specified by the data flow connecting nodes of depth \(d + 1\),
shown in the diagram by blue dotted lines.The value of an aggregation node is the majority of its inputs.\(
a[L,i] = \textrm{majority}(m[L,i], [\forall x \notin [L, i]: a[L, x, i]])
\)For example,\(
a[0, 1, 2] = \textrm{majority}(m[0,1,2], a[0, 1, 3, 2], a[0, 1, 4, 2],
a[0, 1, 5, 2], \ldots)
\)Proof of ValidityWe prove that for all nodes \(m[L]\) of the message tree, if \(L_{*}\)
is loyal then for all \(i\) not in \(L\):\(
a[L, i] = m[L]
\)The proof is by induction.
The base case is for message nodes at depth \(t\).
We prove that if validity holds for message nodes at depth \(d > 0\)
then it holds for message nodes at depth \(d-1\).Base CaseSee the lower diagram of figure 5. 
For a message node \(m[L]\) at depth \(t\),\(
a[L, i] = m[L, i]
\)If \(L_{*}\) is loyal then \(m[L, i] = m[L]\).
and the result follows.Inductive StepSee figure 6.A node \(L\) at depth \(d\) consists of \(d+1\) agents.
Therefore, there are at least \(3t+1 - (d+1)\) lieutenants that are not in
\(L\).
Because \(d \leq t\) there are at least \(2t\) not in \(L\).
So, at least \(t\) lieutenants not in \(L\) are loyal and at most
\(t\) of them are disloyal.Because \(L_{*}\) is loyal, \(m[L,i] = m[L]\).By the induction assumption, for each loyal lieutenant \(j\) not in \(L\), and for each loyal
lieutenant \(i\) not in \([L, j]\):
\(
a[L, j, i] = m[L, i] = m[L]
\)\(
a[L, i] = \textrm{majority}(m[L,i], [\forall j \notin [L, i]: a[L, j,
i]])
\)The majority is taken over at least \(t + 1\) values equal to
\(m[L]\),
and at most \(t\) values that are different from it. And
therefore \(a[L, i] = m[L]\)ExampleThe next illustrates the proof of validity. Message \(m[L]\) is
shown in red, and the flow of correct messages is shown in red edges
and red nodes. For example, nodes \(m[L, i], m[L, j], m[L, i, j],
m[L, j, i]\) are red because agents \(i, j\) are loyal.A disloyal agent is represented by the symbol \(e\).
The output of a disloyal agent is unknown and is shown in black.
Node \(a[l. i]\) gets more than \(2t\) red inputs and at most \(t\)
black inputs, and hence the majority of its inputs is red.Fig.7: Illustration of ValidityProof of ConsensusWe will prove consensus for nodes at depth \(d\) if the number of
faulty nodes is at most \(t-d\).Base Case \(d = t\)In this case there are no disloyal lieutenants. For each \(i, j\),
\(m[L,i]\) and \(m[L,j]\) are arbitrary; however \(m[L, i] = m[L,
j]\).\(a[L, i, j] = m[L, i]\)
and
\(a[L, j, i] = m[L, j]\).
Therefore the inputs to \(a[L,i]\) and to \(a[L,j]\) are identical and
the result follows.Inductive Step \(d < t\)If the general is loyal, then consensus follows from validity.There are at most \(t - d\) faulty nodes.
If the general is disloyal then there at most \(t-d-1 = t -(d+1)\)
disloyal lieutenants.
Therefore, the induction assumption holds for \(m[L, x]\).
Therefore \(a[L, i, k] = a[L, i, j]\)K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology



Figure Link: https://kmchandy.github.io/Byzantine/Byzantine_5/Byzantine_5.004.jpeg

Caption: Fig.4: Dataflow for 4 agents 1 of which is disloyal


Connections from Messaging to Aggregating Nodes



Figure Link: https://kmchandy.github.io/Byzantine/Byzantine_5/Byzantine_5.004.jpeg

Caption: Fig.4: Dataflow for 4 agents 1 of which is disloyal


Output of Aggregating Nodes



Figure Link: https://kmchandy.github.io/Byzantine/Byzantine_5/Byzantine_5.004.jpeg

Caption: Fig.4: Dataflow for 4 agents 1 of which is disloyal


\(
  a[2, 1] = \textrm{majority}(m[2, 1], a[2, 3, 1], a[2, 4, 1], \ldots, a[2, 6, 1])
\)If there are an equal number of attack and retreat inputs, then the
majority value is defined to be any default value.ExampleThe next diagram shows the data flow -- messaging and aggregation
trees, and the connections between them -- for a system where
\(N=4, t=1\).Fig.4: Dataflow for 4 agents 1 of which is disloyalInductive Generation of the Data FlowThe basic unit of data flow, which is replicated many times, is shown
in the top diagram of the figure below.
The input to the unit is a node \(m[L]\) of the message tree; this
unit is specified by \(L\),and the message \(m[L]\).
The output of the unit are nodes \(a[L, x]\) of the
aggregation tree, for all lieutenants \(x\) not in \(L\).The base case of the induction is a node at depth \(t\).
The input for the base case is \(m[L]\) where \(L\) is a list starting
with \(0\) and followed by \(t\) lieutenants.
For the base case, \(a[L, x] = m[L, x]\) for all \(x\).
The base case is illustrated in the lower diagram.Fig.5: Structure of DataflowThe data flow connecting a message node \(m[L]\) at depth \(d < t\) to
aggregation nodes \(a[L,x]\) is shown below.Fig.6: Structure of DataflowMessage node \(m[L]\) feeds message nodes \(m[L,x]\).
The connections between \(m[L,x]\) and aggregation nodes \(a[L,x,y]\)
are specified by the data flow connecting nodes of depth \(d + 1\),
shown in the diagram by blue dotted lines.The value of an aggregation node is the majority of its inputs.\(
a[L,i] = \textrm{majority}(m[L,i], [\forall x \notin [L, i]: a[L, x, i]])
\)For example,\(
a[0, 1, 2] = \textrm{majority}(m[0,1,2], a[0, 1, 3, 2], a[0, 1, 4, 2],
a[0, 1, 5, 2], \ldots)
\)Proof of ValidityWe prove that for all nodes \(m[L]\) of the message tree, if \(L_{*}\)
is loyal then for all \(i\) not in \(L\):\(
a[L, i] = m[L]
\)The proof is by induction.
The base case is for message nodes at depth \(t\).
We prove that if validity holds for message nodes at depth \(d > 0\)
then it holds for message nodes at depth \(d-1\).Base CaseSee the lower diagram of figure 5. 
For a message node \(m[L]\) at depth \(t\),\(
a[L, i] = m[L, i]
\)If \(L_{*}\) is loyal then \(m[L, i] = m[L]\).
and the result follows.Inductive StepSee figure 6.A node \(L\) at depth \(d\) consists of \(d+1\) agents.
Therefore, there are at least \(3t+1 - (d+1)\) lieutenants that are not in
\(L\).
Because \(d \leq t\) there are at least \(2t\) not in \(L\).
So, at least \(t\) lieutenants not in \(L\) are loyal and at most
\(t\) of them are disloyal.Because \(L_{*}\) is loyal, \(m[L,i] = m[L]\).By the induction assumption, for each loyal lieutenant \(j\) not in \(L\), and for each loyal
lieutenant \(i\) not in \([L, j]\):
\(
a[L, j, i] = m[L, i] = m[L]
\)\(
a[L, i] = \textrm{majority}(m[L,i], [\forall j \notin [L, i]: a[L, j,
i]])
\)The majority is taken over at least \(t + 1\) values equal to
\(m[L]\),
and at most \(t\) values that are different from it. And
therefore \(a[L, i] = m[L]\)ExampleThe next illustrates the proof of validity. Message \(m[L]\) is
shown in red, and the flow of correct messages is shown in red edges
and red nodes. For example, nodes \(m[L, i], m[L, j], m[L, i, j],
m[L, j, i]\) are red because agents \(i, j\) are loyal.A disloyal agent is represented by the symbol \(e\).
The output of a disloyal agent is unknown and is shown in black.
Node \(a[l. i]\) gets more than \(2t\) red inputs and at most \(t\)
black inputs, and hence the majority of its inputs is red.Fig.7: Illustration of ValidityProof of ConsensusWe will prove consensus for nodes at depth \(d\) if the number of
faulty nodes is at most \(t-d\).Base Case \(d = t\)In this case there are no disloyal lieutenants. For each \(i, j\),
\(m[L,i]\) and \(m[L,j]\) are arbitrary; however \(m[L, i] = m[L,
j]\).\(a[L, i, j] = m[L, i]\)
and
\(a[L, j, i] = m[L, j]\).
Therefore the inputs to \(a[L,i]\) and to \(a[L,j]\) are identical and
the result follows.Inductive Step \(d < t\)If the general is loyal, then consensus follows from validity.There are at most \(t - d\) faulty nodes.
If the general is disloyal then there at most \(t-d-1 = t -(d+1)\)
disloyal lieutenants.
Therefore, the induction assumption holds for \(m[L, x]\).
Therefore \(a[L, i, k] = a[L, i, j]\)K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology



Figure Link: https://kmchandy.github.io/Byzantine/Byzantine_5/Byzantine_5.004.jpeg

Caption: Fig.4: Dataflow for 4 agents 1 of which is disloyal


If there are an equal number of attack and retreat inputs, then the
majority value is defined to be any default value.ExampleThe next diagram shows the data flow -- messaging and aggregation
trees, and the connections between them -- for a system where
\(N=4, t=1\).Fig.4: Dataflow for 4 agents 1 of which is disloyalInductive Generation of the Data FlowThe basic unit of data flow, which is replicated many times, is shown
in the top diagram of the figure below.
The input to the unit is a node \(m[L]\) of the message tree; this
unit is specified by \(L\),and the message \(m[L]\).
The output of the unit are nodes \(a[L, x]\) of the
aggregation tree, for all lieutenants \(x\) not in \(L\).The base case of the induction is a node at depth \(t\).
The input for the base case is \(m[L]\) where \(L\) is a list starting
with \(0\) and followed by \(t\) lieutenants.
For the base case, \(a[L, x] = m[L, x]\) for all \(x\).
The base case is illustrated in the lower diagram.Fig.5: Structure of DataflowThe data flow connecting a message node \(m[L]\) at depth \(d < t\) to
aggregation nodes \(a[L,x]\) is shown below.Fig.6: Structure of DataflowMessage node \(m[L]\) feeds message nodes \(m[L,x]\).
The connections between \(m[L,x]\) and aggregation nodes \(a[L,x,y]\)
are specified by the data flow connecting nodes of depth \(d + 1\),
shown in the diagram by blue dotted lines.The value of an aggregation node is the majority of its inputs.\(
a[L,i] = \textrm{majority}(m[L,i], [\forall x \notin [L, i]: a[L, x, i]])
\)For example,\(
a[0, 1, 2] = \textrm{majority}(m[0,1,2], a[0, 1, 3, 2], a[0, 1, 4, 2],
a[0, 1, 5, 2], \ldots)
\)Proof of ValidityWe prove that for all nodes \(m[L]\) of the message tree, if \(L_{*}\)
is loyal then for all \(i\) not in \(L\):\(
a[L, i] = m[L]
\)The proof is by induction.
The base case is for message nodes at depth \(t\).
We prove that if validity holds for message nodes at depth \(d > 0\)
then it holds for message nodes at depth \(d-1\).Base CaseSee the lower diagram of figure 5. 
For a message node \(m[L]\) at depth \(t\),\(
a[L, i] = m[L, i]
\)If \(L_{*}\) is loyal then \(m[L, i] = m[L]\).
and the result follows.Inductive StepSee figure 6.A node \(L\) at depth \(d\) consists of \(d+1\) agents.
Therefore, there are at least \(3t+1 - (d+1)\) lieutenants that are not in
\(L\).
Because \(d \leq t\) there are at least \(2t\) not in \(L\).
So, at least \(t\) lieutenants not in \(L\) are loyal and at most
\(t\) of them are disloyal.Because \(L_{*}\) is loyal, \(m[L,i] = m[L]\).By the induction assumption, for each loyal lieutenant \(j\) not in \(L\), and for each loyal
lieutenant \(i\) not in \([L, j]\):
\(
a[L, j, i] = m[L, i] = m[L]
\)\(
a[L, i] = \textrm{majority}(m[L,i], [\forall j \notin [L, i]: a[L, j,
i]])
\)The majority is taken over at least \(t + 1\) values equal to
\(m[L]\),
and at most \(t\) values that are different from it. And
therefore \(a[L, i] = m[L]\)ExampleThe next illustrates the proof of validity. Message \(m[L]\) is
shown in red, and the flow of correct messages is shown in red edges
and red nodes. For example, nodes \(m[L, i], m[L, j], m[L, i, j],
m[L, j, i]\) are red because agents \(i, j\) are loyal.A disloyal agent is represented by the symbol \(e\).
The output of a disloyal agent is unknown and is shown in black.
Node \(a[l. i]\) gets more than \(2t\) red inputs and at most \(t\)
black inputs, and hence the majority of its inputs is red.Fig.7: Illustration of ValidityProof of ConsensusWe will prove consensus for nodes at depth \(d\) if the number of
faulty nodes is at most \(t-d\).Base Case \(d = t\)In this case there are no disloyal lieutenants. For each \(i, j\),
\(m[L,i]\) and \(m[L,j]\) are arbitrary; however \(m[L, i] = m[L,
j]\).\(a[L, i, j] = m[L, i]\)
and
\(a[L, j, i] = m[L, j]\).
Therefore the inputs to \(a[L,i]\) and to \(a[L,j]\) are identical and
the result follows.Inductive Step \(d < t\)If the general is loyal, then consensus follows from validity.There are at most \(t - d\) faulty nodes.
If the general is disloyal then there at most \(t-d-1 = t -(d+1)\)
disloyal lieutenants.
Therefore, the induction assumption holds for \(m[L, x]\).
Therefore \(a[L, i, k] = a[L, i, j]\)K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology



Figure Link: https://kmchandy.github.io/Byzantine/Byzantine_5/Byzantine_5.004.jpeg

Caption: Fig.4: Dataflow for 4 agents 1 of which is disloyal


The base case of the induction is a node at depth \(t\).
The input for the base case is \(m[L]\) where \(L\) is a list starting
with \(0\) and followed by \(t\) lieutenants.
For the base case, \(a[L, x] = m[L, x]\) for all \(x\).
The base case is illustrated in the lower diagram.Fig.5: Structure of DataflowThe data flow connecting a message node \(m[L]\) at depth \(d < t\) to
aggregation nodes \(a[L,x]\) is shown below.Fig.6: Structure of DataflowMessage node \(m[L]\) feeds message nodes \(m[L,x]\).
The connections between \(m[L,x]\) and aggregation nodes \(a[L,x,y]\)
are specified by the data flow connecting nodes of depth \(d + 1\),
shown in the diagram by blue dotted lines.The value of an aggregation node is the majority of its inputs.\(
a[L,i] = \textrm{majority}(m[L,i], [\forall x \notin [L, i]: a[L, x, i]])
\)For example,\(
a[0, 1, 2] = \textrm{majority}(m[0,1,2], a[0, 1, 3, 2], a[0, 1, 4, 2],
a[0, 1, 5, 2], \ldots)
\)Proof of ValidityWe prove that for all nodes \(m[L]\) of the message tree, if \(L_{*}\)
is loyal then for all \(i\) not in \(L\):\(
a[L, i] = m[L]
\)The proof is by induction.
The base case is for message nodes at depth \(t\).
We prove that if validity holds for message nodes at depth \(d > 0\)
then it holds for message nodes at depth \(d-1\).Base CaseSee the lower diagram of figure 5. 
For a message node \(m[L]\) at depth \(t\),\(
a[L, i] = m[L, i]
\)If \(L_{*}\) is loyal then \(m[L, i] = m[L]\).
and the result follows.Inductive StepSee figure 6.A node \(L\) at depth \(d\) consists of \(d+1\) agents.
Therefore, there are at least \(3t+1 - (d+1)\) lieutenants that are not in
\(L\).
Because \(d \leq t\) there are at least \(2t\) not in \(L\).
So, at least \(t\) lieutenants not in \(L\) are loyal and at most
\(t\) of them are disloyal.Because \(L_{*}\) is loyal, \(m[L,i] = m[L]\).By the induction assumption, for each loyal lieutenant \(j\) not in \(L\), and for each loyal
lieutenant \(i\) not in \([L, j]\):
\(
a[L, j, i] = m[L, i] = m[L]
\)\(
a[L, i] = \textrm{majority}(m[L,i], [\forall j \notin [L, i]: a[L, j,
i]])
\)The majority is taken over at least \(t + 1\) values equal to
\(m[L]\),
and at most \(t\) values that are different from it. And
therefore \(a[L, i] = m[L]\)ExampleThe next illustrates the proof of validity. Message \(m[L]\) is
shown in red, and the flow of correct messages is shown in red edges
and red nodes. For example, nodes \(m[L, i], m[L, j], m[L, i, j],
m[L, j, i]\) are red because agents \(i, j\) are loyal.A disloyal agent is represented by the symbol \(e\).
The output of a disloyal agent is unknown and is shown in black.
Node \(a[l. i]\) gets more than \(2t\) red inputs and at most \(t\)
black inputs, and hence the majority of its inputs is red.Fig.7: Illustration of ValidityProof of ConsensusWe will prove consensus for nodes at depth \(d\) if the number of
faulty nodes is at most \(t-d\).Base Case \(d = t\)In this case there are no disloyal lieutenants. For each \(i, j\),
\(m[L,i]\) and \(m[L,j]\) are arbitrary; however \(m[L, i] = m[L,
j]\).\(a[L, i, j] = m[L, i]\)
and
\(a[L, j, i] = m[L, j]\).
Therefore the inputs to \(a[L,i]\) and to \(a[L,j]\) are identical and
the result follows.Inductive Step \(d < t\)If the general is loyal, then consensus follows from validity.There are at most \(t - d\) faulty nodes.
If the general is disloyal then there at most \(t-d-1 = t -(d+1)\)
disloyal lieutenants.
Therefore, the induction assumption holds for \(m[L, x]\).
Therefore \(a[L, i, k] = a[L, i, j]\)K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology



Figure Link: https://kmchandy.github.io/Byzantine/Byzantine_5/Byzantine_5.005.jpeg

Caption: Fig.5: Structure of Dataflow


The value of an aggregation node is the majority of its inputs.\(
a[L,i] = \textrm{majority}(m[L,i], [\forall x \notin [L, i]: a[L, x, i]])
\)For example,\(
a[0, 1, 2] = \textrm{majority}(m[0,1,2], a[0, 1, 3, 2], a[0, 1, 4, 2],
a[0, 1, 5, 2], \ldots)
\)Proof of ValidityWe prove that for all nodes \(m[L]\) of the message tree, if \(L_{*}\)
is loyal then for all \(i\) not in \(L\):\(
a[L, i] = m[L]
\)The proof is by induction.
The base case is for message nodes at depth \(t\).
We prove that if validity holds for message nodes at depth \(d > 0\)
then it holds for message nodes at depth \(d-1\).Base CaseSee the lower diagram of figure 5. 
For a message node \(m[L]\) at depth \(t\),\(
a[L, i] = m[L, i]
\)If \(L_{*}\) is loyal then \(m[L, i] = m[L]\).
and the result follows.Inductive StepSee figure 6.A node \(L\) at depth \(d\) consists of \(d+1\) agents.
Therefore, there are at least \(3t+1 - (d+1)\) lieutenants that are not in
\(L\).
Because \(d \leq t\) there are at least \(2t\) not in \(L\).
So, at least \(t\) lieutenants not in \(L\) are loyal and at most
\(t\) of them are disloyal.Because \(L_{*}\) is loyal, \(m[L,i] = m[L]\).By the induction assumption, for each loyal lieutenant \(j\) not in \(L\), and for each loyal
lieutenant \(i\) not in \([L, j]\):
\(
a[L, j, i] = m[L, i] = m[L]
\)\(
a[L, i] = \textrm{majority}(m[L,i], [\forall j \notin [L, i]: a[L, j,
i]])
\)The majority is taken over at least \(t + 1\) values equal to
\(m[L]\),
and at most \(t\) values that are different from it. And
therefore \(a[L, i] = m[L]\)ExampleThe next illustrates the proof of validity. Message \(m[L]\) is
shown in red, and the flow of correct messages is shown in red edges
and red nodes. For example, nodes \(m[L, i], m[L, j], m[L, i, j],
m[L, j, i]\) are red because agents \(i, j\) are loyal.A disloyal agent is represented by the symbol \(e\).
The output of a disloyal agent is unknown and is shown in black.
Node \(a[l. i]\) gets more than \(2t\) red inputs and at most \(t\)
black inputs, and hence the majority of its inputs is red.Fig.7: Illustration of ValidityProof of ConsensusWe will prove consensus for nodes at depth \(d\) if the number of
faulty nodes is at most \(t-d\).Base Case \(d = t\)In this case there are no disloyal lieutenants. For each \(i, j\),
\(m[L,i]\) and \(m[L,j]\) are arbitrary; however \(m[L, i] = m[L,
j]\).\(a[L, i, j] = m[L, i]\)
and
\(a[L, j, i] = m[L, j]\).
Therefore the inputs to \(a[L,i]\) and to \(a[L,j]\) are identical and
the result follows.Inductive Step \(d < t\)If the general is loyal, then consensus follows from validity.There are at most \(t - d\) faulty nodes.
If the general is disloyal then there at most \(t-d-1 = t -(d+1)\)
disloyal lieutenants.
Therefore, the induction assumption holds for \(m[L, x]\).
Therefore \(a[L, i, k] = a[L, i, j]\)K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology



Figure Link: https://kmchandy.github.io/Byzantine/Byzantine_5/Byzantine_5.007.jpeg

Caption: Fig.7: Illustration of Validity


\(
a[L,i] = \textrm{majority}(m[L,i], [\forall x \notin [L, i]: a[L, x, i]])
\)For example,\(
a[0, 1, 2] = \textrm{majority}(m[0,1,2], a[0, 1, 3, 2], a[0, 1, 4, 2],
a[0, 1, 5, 2], \ldots)
\)Proof of ValidityWe prove that for all nodes \(m[L]\) of the message tree, if \(L_{*}\)
is loyal then for all \(i\) not in \(L\):\(
a[L, i] = m[L]
\)The proof is by induction.
The base case is for message nodes at depth \(t\).
We prove that if validity holds for message nodes at depth \(d > 0\)
then it holds for message nodes at depth \(d-1\).Base CaseSee the lower diagram of figure 5. 
For a message node \(m[L]\) at depth \(t\),\(
a[L, i] = m[L, i]
\)If \(L_{*}\) is loyal then \(m[L, i] = m[L]\).
and the result follows.Inductive StepSee figure 6.A node \(L\) at depth \(d\) consists of \(d+1\) agents.
Therefore, there are at least \(3t+1 - (d+1)\) lieutenants that are not in
\(L\).
Because \(d \leq t\) there are at least \(2t\) not in \(L\).
So, at least \(t\) lieutenants not in \(L\) are loyal and at most
\(t\) of them are disloyal.Because \(L_{*}\) is loyal, \(m[L,i] = m[L]\).By the induction assumption, for each loyal lieutenant \(j\) not in \(L\), and for each loyal
lieutenant \(i\) not in \([L, j]\):
\(
a[L, j, i] = m[L, i] = m[L]
\)\(
a[L, i] = \textrm{majority}(m[L,i], [\forall j \notin [L, i]: a[L, j,
i]])
\)The majority is taken over at least \(t + 1\) values equal to
\(m[L]\),
and at most \(t\) values that are different from it. And
therefore \(a[L, i] = m[L]\)ExampleThe next illustrates the proof of validity. Message \(m[L]\) is
shown in red, and the flow of correct messages is shown in red edges
and red nodes. For example, nodes \(m[L, i], m[L, j], m[L, i, j],
m[L, j, i]\) are red because agents \(i, j\) are loyal.A disloyal agent is represented by the symbol \(e\).
The output of a disloyal agent is unknown and is shown in black.
Node \(a[l. i]\) gets more than \(2t\) red inputs and at most \(t\)
black inputs, and hence the majority of its inputs is red.Fig.7: Illustration of ValidityProof of ConsensusWe will prove consensus for nodes at depth \(d\) if the number of
faulty nodes is at most \(t-d\).Base Case \(d = t\)In this case there are no disloyal lieutenants. For each \(i, j\),
\(m[L,i]\) and \(m[L,j]\) are arbitrary; however \(m[L, i] = m[L,
j]\).\(a[L, i, j] = m[L, i]\)
and
\(a[L, j, i] = m[L, j]\).
Therefore the inputs to \(a[L,i]\) and to \(a[L,j]\) are identical and
the result follows.Inductive Step \(d < t\)If the general is loyal, then consensus follows from validity.There are at most \(t - d\) faulty nodes.
If the general is disloyal then there at most \(t-d-1 = t -(d+1)\)
disloyal lieutenants.
Therefore, the induction assumption holds for \(m[L, x]\).
Therefore \(a[L, i, k] = a[L, i, j]\)K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology



Figure Link: https://kmchandy.github.io/Byzantine/Byzantine_5/Byzantine_5.007.jpeg

Caption: Fig.7: Illustration of Validity


For example,\(
a[0, 1, 2] = \textrm{majority}(m[0,1,2], a[0, 1, 3, 2], a[0, 1, 4, 2],
a[0, 1, 5, 2], \ldots)
\)Proof of ValidityWe prove that for all nodes \(m[L]\) of the message tree, if \(L_{*}\)
is loyal then for all \(i\) not in \(L\):\(
a[L, i] = m[L]
\)The proof is by induction.
The base case is for message nodes at depth \(t\).
We prove that if validity holds for message nodes at depth \(d > 0\)
then it holds for message nodes at depth \(d-1\).Base CaseSee the lower diagram of figure 5. 
For a message node \(m[L]\) at depth \(t\),\(
a[L, i] = m[L, i]
\)If \(L_{*}\) is loyal then \(m[L, i] = m[L]\).
and the result follows.Inductive StepSee figure 6.A node \(L\) at depth \(d\) consists of \(d+1\) agents.
Therefore, there are at least \(3t+1 - (d+1)\) lieutenants that are not in
\(L\).
Because \(d \leq t\) there are at least \(2t\) not in \(L\).
So, at least \(t\) lieutenants not in \(L\) are loyal and at most
\(t\) of them are disloyal.Because \(L_{*}\) is loyal, \(m[L,i] = m[L]\).By the induction assumption, for each loyal lieutenant \(j\) not in \(L\), and for each loyal
lieutenant \(i\) not in \([L, j]\):
\(
a[L, j, i] = m[L, i] = m[L]
\)\(
a[L, i] = \textrm{majority}(m[L,i], [\forall j \notin [L, i]: a[L, j,
i]])
\)The majority is taken over at least \(t + 1\) values equal to
\(m[L]\),
and at most \(t\) values that are different from it. And
therefore \(a[L, i] = m[L]\)ExampleThe next illustrates the proof of validity. Message \(m[L]\) is
shown in red, and the flow of correct messages is shown in red edges
and red nodes. For example, nodes \(m[L, i], m[L, j], m[L, i, j],
m[L, j, i]\) are red because agents \(i, j\) are loyal.A disloyal agent is represented by the symbol \(e\).
The output of a disloyal agent is unknown and is shown in black.
Node \(a[l. i]\) gets more than \(2t\) red inputs and at most \(t\)
black inputs, and hence the majority of its inputs is red.Fig.7: Illustration of ValidityProof of ConsensusWe will prove consensus for nodes at depth \(d\) if the number of
faulty nodes is at most \(t-d\).Base Case \(d = t\)In this case there are no disloyal lieutenants. For each \(i, j\),
\(m[L,i]\) and \(m[L,j]\) are arbitrary; however \(m[L, i] = m[L,
j]\).\(a[L, i, j] = m[L, i]\)
and
\(a[L, j, i] = m[L, j]\).
Therefore the inputs to \(a[L,i]\) and to \(a[L,j]\) are identical and
the result follows.Inductive Step \(d < t\)If the general is loyal, then consensus follows from validity.There are at most \(t - d\) faulty nodes.
If the general is disloyal then there at most \(t-d-1 = t -(d+1)\)
disloyal lieutenants.
Therefore, the induction assumption holds for \(m[L, x]\).
Therefore \(a[L, i, k] = a[L, i, j]\)K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology



Figure Link: https://kmchandy.github.io/Byzantine/Byzantine_5/Byzantine_5.007.jpeg

Caption: Fig.7: Illustration of Validity


\(
a[0, 1, 2] = \textrm{majority}(m[0,1,2], a[0, 1, 3, 2], a[0, 1, 4, 2],
a[0, 1, 5, 2], \ldots)
\)Proof of ValidityWe prove that for all nodes \(m[L]\) of the message tree, if \(L_{*}\)
is loyal then for all \(i\) not in \(L\):\(
a[L, i] = m[L]
\)The proof is by induction.
The base case is for message nodes at depth \(t\).
We prove that if validity holds for message nodes at depth \(d > 0\)
then it holds for message nodes at depth \(d-1\).Base CaseSee the lower diagram of figure 5. 
For a message node \(m[L]\) at depth \(t\),\(
a[L, i] = m[L, i]
\)If \(L_{*}\) is loyal then \(m[L, i] = m[L]\).
and the result follows.Inductive StepSee figure 6.A node \(L\) at depth \(d\) consists of \(d+1\) agents.
Therefore, there are at least \(3t+1 - (d+1)\) lieutenants that are not in
\(L\).
Because \(d \leq t\) there are at least \(2t\) not in \(L\).
So, at least \(t\) lieutenants not in \(L\) are loyal and at most
\(t\) of them are disloyal.Because \(L_{*}\) is loyal, \(m[L,i] = m[L]\).By the induction assumption, for each loyal lieutenant \(j\) not in \(L\), and for each loyal
lieutenant \(i\) not in \([L, j]\):
\(
a[L, j, i] = m[L, i] = m[L]
\)\(
a[L, i] = \textrm{majority}(m[L,i], [\forall j \notin [L, i]: a[L, j,
i]])
\)The majority is taken over at least \(t + 1\) values equal to
\(m[L]\),
and at most \(t\) values that are different from it. And
therefore \(a[L, i] = m[L]\)ExampleThe next illustrates the proof of validity. Message \(m[L]\) is
shown in red, and the flow of correct messages is shown in red edges
and red nodes. For example, nodes \(m[L, i], m[L, j], m[L, i, j],
m[L, j, i]\) are red because agents \(i, j\) are loyal.A disloyal agent is represented by the symbol \(e\).
The output of a disloyal agent is unknown and is shown in black.
Node \(a[l. i]\) gets more than \(2t\) red inputs and at most \(t\)
black inputs, and hence the majority of its inputs is red.Fig.7: Illustration of ValidityProof of ConsensusWe will prove consensus for nodes at depth \(d\) if the number of
faulty nodes is at most \(t-d\).Base Case \(d = t\)In this case there are no disloyal lieutenants. For each \(i, j\),
\(m[L,i]\) and \(m[L,j]\) are arbitrary; however \(m[L, i] = m[L,
j]\).\(a[L, i, j] = m[L, i]\)
and
\(a[L, j, i] = m[L, j]\).
Therefore the inputs to \(a[L,i]\) and to \(a[L,j]\) are identical and
the result follows.Inductive Step \(d < t\)If the general is loyal, then consensus follows from validity.There are at most \(t - d\) faulty nodes.
If the general is disloyal then there at most \(t-d-1 = t -(d+1)\)
disloyal lieutenants.
Therefore, the induction assumption holds for \(m[L, x]\).
Therefore \(a[L, i, k] = a[L, i, j]\)K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology



Figure Link: https://kmchandy.github.io/Byzantine/Byzantine_5/Byzantine_5.007.jpeg

Caption: Fig.7: Illustration of Validity


\(
a[L, i] = m[L]
\)The proof is by induction.
The base case is for message nodes at depth \(t\).
We prove that if validity holds for message nodes at depth \(d > 0\)
then it holds for message nodes at depth \(d-1\).Base CaseSee the lower diagram of figure 5. 
For a message node \(m[L]\) at depth \(t\),\(
a[L, i] = m[L, i]
\)If \(L_{*}\) is loyal then \(m[L, i] = m[L]\).
and the result follows.Inductive StepSee figure 6.A node \(L\) at depth \(d\) consists of \(d+1\) agents.
Therefore, there are at least \(3t+1 - (d+1)\) lieutenants that are not in
\(L\).
Because \(d \leq t\) there are at least \(2t\) not in \(L\).
So, at least \(t\) lieutenants not in \(L\) are loyal and at most
\(t\) of them are disloyal.Because \(L_{*}\) is loyal, \(m[L,i] = m[L]\).By the induction assumption, for each loyal lieutenant \(j\) not in \(L\), and for each loyal
lieutenant \(i\) not in \([L, j]\):
\(
a[L, j, i] = m[L, i] = m[L]
\)\(
a[L, i] = \textrm{majority}(m[L,i], [\forall j \notin [L, i]: a[L, j,
i]])
\)The majority is taken over at least \(t + 1\) values equal to
\(m[L]\),
and at most \(t\) values that are different from it. And
therefore \(a[L, i] = m[L]\)ExampleThe next illustrates the proof of validity. Message \(m[L]\) is
shown in red, and the flow of correct messages is shown in red edges
and red nodes. For example, nodes \(m[L, i], m[L, j], m[L, i, j],
m[L, j, i]\) are red because agents \(i, j\) are loyal.A disloyal agent is represented by the symbol \(e\).
The output of a disloyal agent is unknown and is shown in black.
Node \(a[l. i]\) gets more than \(2t\) red inputs and at most \(t\)
black inputs, and hence the majority of its inputs is red.Fig.7: Illustration of ValidityProof of ConsensusWe will prove consensus for nodes at depth \(d\) if the number of
faulty nodes is at most \(t-d\).Base Case \(d = t\)In this case there are no disloyal lieutenants. For each \(i, j\),
\(m[L,i]\) and \(m[L,j]\) are arbitrary; however \(m[L, i] = m[L,
j]\).\(a[L, i, j] = m[L, i]\)
and
\(a[L, j, i] = m[L, j]\).
Therefore the inputs to \(a[L,i]\) and to \(a[L,j]\) are identical and
the result follows.Inductive Step \(d < t\)If the general is loyal, then consensus follows from validity.There are at most \(t - d\) faulty nodes.
If the general is disloyal then there at most \(t-d-1 = t -(d+1)\)
disloyal lieutenants.
Therefore, the induction assumption holds for \(m[L, x]\).
Therefore \(a[L, i, k] = a[L, i, j]\)K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology



Figure Link: https://kmchandy.github.io/Byzantine/Byzantine_5/Byzantine_5.007.jpeg

Caption: Fig.7: Illustration of Validity


The proof is by induction.
The base case is for message nodes at depth \(t\).
We prove that if validity holds for message nodes at depth \(d > 0\)
then it holds for message nodes at depth \(d-1\).Base CaseSee the lower diagram of figure 5. 
For a message node \(m[L]\) at depth \(t\),\(
a[L, i] = m[L, i]
\)If \(L_{*}\) is loyal then \(m[L, i] = m[L]\).
and the result follows.Inductive StepSee figure 6.A node \(L\) at depth \(d\) consists of \(d+1\) agents.
Therefore, there are at least \(3t+1 - (d+1)\) lieutenants that are not in
\(L\).
Because \(d \leq t\) there are at least \(2t\) not in \(L\).
So, at least \(t\) lieutenants not in \(L\) are loyal and at most
\(t\) of them are disloyal.Because \(L_{*}\) is loyal, \(m[L,i] = m[L]\).By the induction assumption, for each loyal lieutenant \(j\) not in \(L\), and for each loyal
lieutenant \(i\) not in \([L, j]\):
\(
a[L, j, i] = m[L, i] = m[L]
\)\(
a[L, i] = \textrm{majority}(m[L,i], [\forall j \notin [L, i]: a[L, j,
i]])
\)The majority is taken over at least \(t + 1\) values equal to
\(m[L]\),
and at most \(t\) values that are different from it. And
therefore \(a[L, i] = m[L]\)ExampleThe next illustrates the proof of validity. Message \(m[L]\) is
shown in red, and the flow of correct messages is shown in red edges
and red nodes. For example, nodes \(m[L, i], m[L, j], m[L, i, j],
m[L, j, i]\) are red because agents \(i, j\) are loyal.A disloyal agent is represented by the symbol \(e\).
The output of a disloyal agent is unknown and is shown in black.
Node \(a[l. i]\) gets more than \(2t\) red inputs and at most \(t\)
black inputs, and hence the majority of its inputs is red.Fig.7: Illustration of ValidityProof of ConsensusWe will prove consensus for nodes at depth \(d\) if the number of
faulty nodes is at most \(t-d\).Base Case \(d = t\)In this case there are no disloyal lieutenants. For each \(i, j\),
\(m[L,i]\) and \(m[L,j]\) are arbitrary; however \(m[L, i] = m[L,
j]\).\(a[L, i, j] = m[L, i]\)
and
\(a[L, j, i] = m[L, j]\).
Therefore the inputs to \(a[L,i]\) and to \(a[L,j]\) are identical and
the result follows.Inductive Step \(d < t\)If the general is loyal, then consensus follows from validity.There are at most \(t - d\) faulty nodes.
If the general is disloyal then there at most \(t-d-1 = t -(d+1)\)
disloyal lieutenants.
Therefore, the induction assumption holds for \(m[L, x]\).
Therefore \(a[L, i, k] = a[L, i, j]\)K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology



Figure Link: https://kmchandy.github.io/Byzantine/Byzantine_5/Byzantine_5.007.jpeg

Caption: Fig.7: Illustration of Validity


\(
a[L, i] = m[L, i]
\)If \(L_{*}\) is loyal then \(m[L, i] = m[L]\).
and the result follows.Inductive StepSee figure 6.A node \(L\) at depth \(d\) consists of \(d+1\) agents.
Therefore, there are at least \(3t+1 - (d+1)\) lieutenants that are not in
\(L\).
Because \(d \leq t\) there are at least \(2t\) not in \(L\).
So, at least \(t\) lieutenants not in \(L\) are loyal and at most
\(t\) of them are disloyal.Because \(L_{*}\) is loyal, \(m[L,i] = m[L]\).By the induction assumption, for each loyal lieutenant \(j\) not in \(L\), and for each loyal
lieutenant \(i\) not in \([L, j]\):
\(
a[L, j, i] = m[L, i] = m[L]
\)\(
a[L, i] = \textrm{majority}(m[L,i], [\forall j \notin [L, i]: a[L, j,
i]])
\)The majority is taken over at least \(t + 1\) values equal to
\(m[L]\),
and at most \(t\) values that are different from it. And
therefore \(a[L, i] = m[L]\)ExampleThe next illustrates the proof of validity. Message \(m[L]\) is
shown in red, and the flow of correct messages is shown in red edges
and red nodes. For example, nodes \(m[L, i], m[L, j], m[L, i, j],
m[L, j, i]\) are red because agents \(i, j\) are loyal.A disloyal agent is represented by the symbol \(e\).
The output of a disloyal agent is unknown and is shown in black.
Node \(a[l. i]\) gets more than \(2t\) red inputs and at most \(t\)
black inputs, and hence the majority of its inputs is red.Fig.7: Illustration of ValidityProof of ConsensusWe will prove consensus for nodes at depth \(d\) if the number of
faulty nodes is at most \(t-d\).Base Case \(d = t\)In this case there are no disloyal lieutenants. For each \(i, j\),
\(m[L,i]\) and \(m[L,j]\) are arbitrary; however \(m[L, i] = m[L,
j]\).\(a[L, i, j] = m[L, i]\)
and
\(a[L, j, i] = m[L, j]\).
Therefore the inputs to \(a[L,i]\) and to \(a[L,j]\) are identical and
the result follows.Inductive Step \(d < t\)If the general is loyal, then consensus follows from validity.There are at most \(t - d\) faulty nodes.
If the general is disloyal then there at most \(t-d-1 = t -(d+1)\)
disloyal lieutenants.
Therefore, the induction assumption holds for \(m[L, x]\).
Therefore \(a[L, i, k] = a[L, i, j]\)K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology



Figure Link: https://kmchandy.github.io/Byzantine/Byzantine_5/Byzantine_5.007.jpeg

Caption: Fig.7: Illustration of Validity


If \(L_{*}\) is loyal then \(m[L, i] = m[L]\).
and the result follows.Inductive StepSee figure 6.A node \(L\) at depth \(d\) consists of \(d+1\) agents.
Therefore, there are at least \(3t+1 - (d+1)\) lieutenants that are not in
\(L\).
Because \(d \leq t\) there are at least \(2t\) not in \(L\).
So, at least \(t\) lieutenants not in \(L\) are loyal and at most
\(t\) of them are disloyal.Because \(L_{*}\) is loyal, \(m[L,i] = m[L]\).By the induction assumption, for each loyal lieutenant \(j\) not in \(L\), and for each loyal
lieutenant \(i\) not in \([L, j]\):
\(
a[L, j, i] = m[L, i] = m[L]
\)\(
a[L, i] = \textrm{majority}(m[L,i], [\forall j \notin [L, i]: a[L, j,
i]])
\)The majority is taken over at least \(t + 1\) values equal to
\(m[L]\),
and at most \(t\) values that are different from it. And
therefore \(a[L, i] = m[L]\)ExampleThe next illustrates the proof of validity. Message \(m[L]\) is
shown in red, and the flow of correct messages is shown in red edges
and red nodes. For example, nodes \(m[L, i], m[L, j], m[L, i, j],
m[L, j, i]\) are red because agents \(i, j\) are loyal.A disloyal agent is represented by the symbol \(e\).
The output of a disloyal agent is unknown and is shown in black.
Node \(a[l. i]\) gets more than \(2t\) red inputs and at most \(t\)
black inputs, and hence the majority of its inputs is red.Fig.7: Illustration of ValidityProof of ConsensusWe will prove consensus for nodes at depth \(d\) if the number of
faulty nodes is at most \(t-d\).Base Case \(d = t\)In this case there are no disloyal lieutenants. For each \(i, j\),
\(m[L,i]\) and \(m[L,j]\) are arbitrary; however \(m[L, i] = m[L,
j]\).\(a[L, i, j] = m[L, i]\)
and
\(a[L, j, i] = m[L, j]\).
Therefore the inputs to \(a[L,i]\) and to \(a[L,j]\) are identical and
the result follows.Inductive Step \(d < t\)If the general is loyal, then consensus follows from validity.There are at most \(t - d\) faulty nodes.
If the general is disloyal then there at most \(t-d-1 = t -(d+1)\)
disloyal lieutenants.
Therefore, the induction assumption holds for \(m[L, x]\).
Therefore \(a[L, i, k] = a[L, i, j]\)K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology



Figure Link: https://kmchandy.github.io/Byzantine/Byzantine_5/Byzantine_5.007.jpeg

Caption: Fig.7: Illustration of Validity


See figure 6.A node \(L\) at depth \(d\) consists of \(d+1\) agents.
Therefore, there are at least \(3t+1 - (d+1)\) lieutenants that are not in
\(L\).
Because \(d \leq t\) there are at least \(2t\) not in \(L\).
So, at least \(t\) lieutenants not in \(L\) are loyal and at most
\(t\) of them are disloyal.Because \(L_{*}\) is loyal, \(m[L,i] = m[L]\).By the induction assumption, for each loyal lieutenant \(j\) not in \(L\), and for each loyal
lieutenant \(i\) not in \([L, j]\):
\(
a[L, j, i] = m[L, i] = m[L]
\)\(
a[L, i] = \textrm{majority}(m[L,i], [\forall j \notin [L, i]: a[L, j,
i]])
\)The majority is taken over at least \(t + 1\) values equal to
\(m[L]\),
and at most \(t\) values that are different from it. And
therefore \(a[L, i] = m[L]\)ExampleThe next illustrates the proof of validity. Message \(m[L]\) is
shown in red, and the flow of correct messages is shown in red edges
and red nodes. For example, nodes \(m[L, i], m[L, j], m[L, i, j],
m[L, j, i]\) are red because agents \(i, j\) are loyal.A disloyal agent is represented by the symbol \(e\).
The output of a disloyal agent is unknown and is shown in black.
Node \(a[l. i]\) gets more than \(2t\) red inputs and at most \(t\)
black inputs, and hence the majority of its inputs is red.Fig.7: Illustration of ValidityProof of ConsensusWe will prove consensus for nodes at depth \(d\) if the number of
faulty nodes is at most \(t-d\).Base Case \(d = t\)In this case there are no disloyal lieutenants. For each \(i, j\),
\(m[L,i]\) and \(m[L,j]\) are arbitrary; however \(m[L, i] = m[L,
j]\).\(a[L, i, j] = m[L, i]\)
and
\(a[L, j, i] = m[L, j]\).
Therefore the inputs to \(a[L,i]\) and to \(a[L,j]\) are identical and
the result follows.Inductive Step \(d < t\)If the general is loyal, then consensus follows from validity.There are at most \(t - d\) faulty nodes.
If the general is disloyal then there at most \(t-d-1 = t -(d+1)\)
disloyal lieutenants.
Therefore, the induction assumption holds for \(m[L, x]\).
Therefore \(a[L, i, k] = a[L, i, j]\)K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology



Figure Link: https://kmchandy.github.io/Byzantine/Byzantine_5/Byzantine_5.007.jpeg

Caption: Fig.7: Illustration of Validity


A node \(L\) at depth \(d\) consists of \(d+1\) agents.
Therefore, there are at least \(3t+1 - (d+1)\) lieutenants that are not in
\(L\).
Because \(d \leq t\) there are at least \(2t\) not in \(L\).
So, at least \(t\) lieutenants not in \(L\) are loyal and at most
\(t\) of them are disloyal.Because \(L_{*}\) is loyal, \(m[L,i] = m[L]\).By the induction assumption, for each loyal lieutenant \(j\) not in \(L\), and for each loyal
lieutenant \(i\) not in \([L, j]\):
\(
a[L, j, i] = m[L, i] = m[L]
\)\(
a[L, i] = \textrm{majority}(m[L,i], [\forall j \notin [L, i]: a[L, j,
i]])
\)The majority is taken over at least \(t + 1\) values equal to
\(m[L]\),
and at most \(t\) values that are different from it. And
therefore \(a[L, i] = m[L]\)ExampleThe next illustrates the proof of validity. Message \(m[L]\) is
shown in red, and the flow of correct messages is shown in red edges
and red nodes. For example, nodes \(m[L, i], m[L, j], m[L, i, j],
m[L, j, i]\) are red because agents \(i, j\) are loyal.A disloyal agent is represented by the symbol \(e\).
The output of a disloyal agent is unknown and is shown in black.
Node \(a[l. i]\) gets more than \(2t\) red inputs and at most \(t\)
black inputs, and hence the majority of its inputs is red.Fig.7: Illustration of ValidityProof of ConsensusWe will prove consensus for nodes at depth \(d\) if the number of
faulty nodes is at most \(t-d\).Base Case \(d = t\)In this case there are no disloyal lieutenants. For each \(i, j\),
\(m[L,i]\) and \(m[L,j]\) are arbitrary; however \(m[L, i] = m[L,
j]\).\(a[L, i, j] = m[L, i]\)
and
\(a[L, j, i] = m[L, j]\).
Therefore the inputs to \(a[L,i]\) and to \(a[L,j]\) are identical and
the result follows.Inductive Step \(d < t\)If the general is loyal, then consensus follows from validity.There are at most \(t - d\) faulty nodes.
If the general is disloyal then there at most \(t-d-1 = t -(d+1)\)
disloyal lieutenants.
Therefore, the induction assumption holds for \(m[L, x]\).
Therefore \(a[L, i, k] = a[L, i, j]\)K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology



Figure Link: https://kmchandy.github.io/Byzantine/Byzantine_5/Byzantine_5.007.jpeg

Caption: Fig.7: Illustration of Validity


Because \(L_{*}\) is loyal, \(m[L,i] = m[L]\).By the induction assumption, for each loyal lieutenant \(j\) not in \(L\), and for each loyal
lieutenant \(i\) not in \([L, j]\):
\(
a[L, j, i] = m[L, i] = m[L]
\)\(
a[L, i] = \textrm{majority}(m[L,i], [\forall j \notin [L, i]: a[L, j,
i]])
\)The majority is taken over at least \(t + 1\) values equal to
\(m[L]\),
and at most \(t\) values that are different from it. And
therefore \(a[L, i] = m[L]\)ExampleThe next illustrates the proof of validity. Message \(m[L]\) is
shown in red, and the flow of correct messages is shown in red edges
and red nodes. For example, nodes \(m[L, i], m[L, j], m[L, i, j],
m[L, j, i]\) are red because agents \(i, j\) are loyal.A disloyal agent is represented by the symbol \(e\).
The output of a disloyal agent is unknown and is shown in black.
Node \(a[l. i]\) gets more than \(2t\) red inputs and at most \(t\)
black inputs, and hence the majority of its inputs is red.Fig.7: Illustration of ValidityProof of ConsensusWe will prove consensus for nodes at depth \(d\) if the number of
faulty nodes is at most \(t-d\).Base Case \(d = t\)In this case there are no disloyal lieutenants. For each \(i, j\),
\(m[L,i]\) and \(m[L,j]\) are arbitrary; however \(m[L, i] = m[L,
j]\).\(a[L, i, j] = m[L, i]\)
and
\(a[L, j, i] = m[L, j]\).
Therefore the inputs to \(a[L,i]\) and to \(a[L,j]\) are identical and
the result follows.Inductive Step \(d < t\)If the general is loyal, then consensus follows from validity.There are at most \(t - d\) faulty nodes.
If the general is disloyal then there at most \(t-d-1 = t -(d+1)\)
disloyal lieutenants.
Therefore, the induction assumption holds for \(m[L, x]\).
Therefore \(a[L, i, k] = a[L, i, j]\)K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology



Figure Link: https://kmchandy.github.io/Byzantine/Byzantine_5/Byzantine_5.007.jpeg

Caption: Fig.7: Illustration of Validity


By the induction assumption, for each loyal lieutenant \(j\) not in \(L\), and for each loyal
lieutenant \(i\) not in \([L, j]\):
\(
a[L, j, i] = m[L, i] = m[L]
\)\(
a[L, i] = \textrm{majority}(m[L,i], [\forall j \notin [L, i]: a[L, j,
i]])
\)The majority is taken over at least \(t + 1\) values equal to
\(m[L]\),
and at most \(t\) values that are different from it. And
therefore \(a[L, i] = m[L]\)ExampleThe next illustrates the proof of validity. Message \(m[L]\) is
shown in red, and the flow of correct messages is shown in red edges
and red nodes. For example, nodes \(m[L, i], m[L, j], m[L, i, j],
m[L, j, i]\) are red because agents \(i, j\) are loyal.A disloyal agent is represented by the symbol \(e\).
The output of a disloyal agent is unknown and is shown in black.
Node \(a[l. i]\) gets more than \(2t\) red inputs and at most \(t\)
black inputs, and hence the majority of its inputs is red.Fig.7: Illustration of ValidityProof of ConsensusWe will prove consensus for nodes at depth \(d\) if the number of
faulty nodes is at most \(t-d\).Base Case \(d = t\)In this case there are no disloyal lieutenants. For each \(i, j\),
\(m[L,i]\) and \(m[L,j]\) are arbitrary; however \(m[L, i] = m[L,
j]\).\(a[L, i, j] = m[L, i]\)
and
\(a[L, j, i] = m[L, j]\).
Therefore the inputs to \(a[L,i]\) and to \(a[L,j]\) are identical and
the result follows.Inductive Step \(d < t\)If the general is loyal, then consensus follows from validity.There are at most \(t - d\) faulty nodes.
If the general is disloyal then there at most \(t-d-1 = t -(d+1)\)
disloyal lieutenants.
Therefore, the induction assumption holds for \(m[L, x]\).
Therefore \(a[L, i, k] = a[L, i, j]\)K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology



Figure Link: https://kmchandy.github.io/Byzantine/Byzantine_5/Byzantine_5.007.jpeg

Caption: Fig.7: Illustration of Validity


\(
a[L, i] = \textrm{majority}(m[L,i], [\forall j \notin [L, i]: a[L, j,
i]])
\)The majority is taken over at least \(t + 1\) values equal to
\(m[L]\),
and at most \(t\) values that are different from it. And
therefore \(a[L, i] = m[L]\)ExampleThe next illustrates the proof of validity. Message \(m[L]\) is
shown in red, and the flow of correct messages is shown in red edges
and red nodes. For example, nodes \(m[L, i], m[L, j], m[L, i, j],
m[L, j, i]\) are red because agents \(i, j\) are loyal.A disloyal agent is represented by the symbol \(e\).
The output of a disloyal agent is unknown and is shown in black.
Node \(a[l. i]\) gets more than \(2t\) red inputs and at most \(t\)
black inputs, and hence the majority of its inputs is red.Fig.7: Illustration of ValidityProof of ConsensusWe will prove consensus for nodes at depth \(d\) if the number of
faulty nodes is at most \(t-d\).Base Case \(d = t\)In this case there are no disloyal lieutenants. For each \(i, j\),
\(m[L,i]\) and \(m[L,j]\) are arbitrary; however \(m[L, i] = m[L,
j]\).\(a[L, i, j] = m[L, i]\)
and
\(a[L, j, i] = m[L, j]\).
Therefore the inputs to \(a[L,i]\) and to \(a[L,j]\) are identical and
the result follows.Inductive Step \(d < t\)If the general is loyal, then consensus follows from validity.There are at most \(t - d\) faulty nodes.
If the general is disloyal then there at most \(t-d-1 = t -(d+1)\)
disloyal lieutenants.
Therefore, the induction assumption holds for \(m[L, x]\).
Therefore \(a[L, i, k] = a[L, i, j]\)K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology



Figure Link: https://kmchandy.github.io/Byzantine/Byzantine_5/Byzantine_5.007.jpeg

Caption: Fig.7: Illustration of Validity


The majority is taken over at least \(t + 1\) values equal to
\(m[L]\),
and at most \(t\) values that are different from it. And
therefore \(a[L, i] = m[L]\)ExampleThe next illustrates the proof of validity. Message \(m[L]\) is
shown in red, and the flow of correct messages is shown in red edges
and red nodes. For example, nodes \(m[L, i], m[L, j], m[L, i, j],
m[L, j, i]\) are red because agents \(i, j\) are loyal.A disloyal agent is represented by the symbol \(e\).
The output of a disloyal agent is unknown and is shown in black.
Node \(a[l. i]\) gets more than \(2t\) red inputs and at most \(t\)
black inputs, and hence the majority of its inputs is red.Fig.7: Illustration of ValidityProof of ConsensusWe will prove consensus for nodes at depth \(d\) if the number of
faulty nodes is at most \(t-d\).Base Case \(d = t\)In this case there are no disloyal lieutenants. For each \(i, j\),
\(m[L,i]\) and \(m[L,j]\) are arbitrary; however \(m[L, i] = m[L,
j]\).\(a[L, i, j] = m[L, i]\)
and
\(a[L, j, i] = m[L, j]\).
Therefore the inputs to \(a[L,i]\) and to \(a[L,j]\) are identical and
the result follows.Inductive Step \(d < t\)If the general is loyal, then consensus follows from validity.There are at most \(t - d\) faulty nodes.
If the general is disloyal then there at most \(t-d-1 = t -(d+1)\)
disloyal lieutenants.
Therefore, the induction assumption holds for \(m[L, x]\).
Therefore \(a[L, i, k] = a[L, i, j]\)K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology



Figure Link: https://kmchandy.github.io/Byzantine/Byzantine_5/Byzantine_5.007.jpeg

Caption: Fig.7: Illustration of Validity


A disloyal agent is represented by the symbol \(e\).
The output of a disloyal agent is unknown and is shown in black.
Node \(a[l. i]\) gets more than \(2t\) red inputs and at most \(t\)
black inputs, and hence the majority of its inputs is red.Fig.7: Illustration of ValidityProof of ConsensusWe will prove consensus for nodes at depth \(d\) if the number of
faulty nodes is at most \(t-d\).Base Case \(d = t\)In this case there are no disloyal lieutenants. For each \(i, j\),
\(m[L,i]\) and \(m[L,j]\) are arbitrary; however \(m[L, i] = m[L,
j]\).\(a[L, i, j] = m[L, i]\)
and
\(a[L, j, i] = m[L, j]\).
Therefore the inputs to \(a[L,i]\) and to \(a[L,j]\) are identical and
the result follows.Inductive Step \(d < t\)If the general is loyal, then consensus follows from validity.There are at most \(t - d\) faulty nodes.
If the general is disloyal then there at most \(t-d-1 = t -(d+1)\)
disloyal lieutenants.
Therefore, the induction assumption holds for \(m[L, x]\).
Therefore \(a[L, i, k] = a[L, i, j]\)K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology



Figure Link: https://kmchandy.github.io/Byzantine/Byzantine_5/Byzantine_5.007.jpeg

Caption: Fig.7: Illustration of Validity


\(a[L, i, j] = m[L, i]\)
and
\(a[L, j, i] = m[L, j]\).
Therefore the inputs to \(a[L,i]\) and to \(a[L,j]\) are identical and
the result follows.Inductive Step \(d < t\)If the general is loyal, then consensus follows from validity.There are at most \(t - d\) faulty nodes.
If the general is disloyal then there at most \(t-d-1 = t -(d+1)\)
disloyal lieutenants.
Therefore, the induction assumption holds for \(m[L, x]\).
Therefore \(a[L, i, k] = a[L, i, j]\)K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

There are at most \(t - d\) faulty nodes.
If the general is disloyal then there at most \(t-d-1 = t -(d+1)\)
disloyal lieutenants.
Therefore, the induction assumption holds for \(m[L, x]\).
Therefore \(a[L, i, k] = a[L, i, j]\)K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology
--- END Content from https://kmchandy.github.io/Byzantine/ByzantineOral.html ---


--- START Content from https://kmchandy.github.io/Crypto/CryptoCurrencyIntroduction.html ---
TITLE: Crypto/CryptoCurrencyIntroduction.html

Let's look at the following problem: Given \(H\), find any
    colliding pair \(x, y\).Consider a hash function \(H\) that outputs \(n\)-bit numbers and
    whose input is \(m\) bit strings. As a specific example lets
    assume that \(m\) is a large number and \(n = 4\). We can
    find a collision in the following way.Let \(D\) be an
    array of size \(2^{n} = 16\). Initially \(D\) contains null values. Repeat the following
    iteration until a collision is found.
Pick a random input \(x\).
If \(D[H(x)]\) is null then set \(D[H(x)] = x\) else there is a collision between
\(H(x)\) and \(x\).By the pigeon-hole principle, we will find a collision in at most
    \(2^{n} + 1 = 17\) iterations.
    This  brute-force algorithm uses space \(2^{n}\) and finds a
    collision in at most  \(2^{n} + 1\) steps.
From theBirthday Paradoxa collision will be found with high probability
in \(2^{n/2}\) iterations though
the worse-case time is \(2^{n}+1\). If \(n = 256\) then a collision
will be found with high probability in \(2^{128}\) iterations; 
however, executing \(2^{128}\) steps is still intractable.For \(H\) to be
    collision resistant the output of \(H\) 
    must be \(n\)-bits for large \(n\). For example \(n = 256\) in the
    SHA-256 hash function.Commitment using HashesYou bet that your soccer friend, Megan, cannot predict the winner
    of the 2022 World Cup. Megan puts the name of the predicted
    winner, \(W\), in an envelope and
    gives it to a trusted third party. After the World Cup is over,
    the third party reveals Megan's prediction, and at that point you
    can find out whether Megan's prediction was accurate.The trusted third party provides two services:Hiding: You can't find out Megan's prediction until the third party reveals it.Binding: Megan can't change her prediction after giving it to the third
  party.Can we use a hash function instead of a trusted third party?HidingLet's try the following idea. Megan commits to \(W\) in the following
way. She announces a hash function, \(H\),
and the hash, \(y\), where \(y = H(W)\). You know \(H\) and \(y\). 
After the World Cup is over, she reveals her prediction, \(W\). At
this point you can verify that \(y = H(W)\).Does the hash function provide the services of the third party?
Can you discover Megan's prediction before she reveals it?It's easy. There are only 32 teams playing. Compute \(H(x)\) where
    \(x\) runs over each of the 32 teams. One of those teams has to be
\(W\). You can discover her prediction in at most 210 steps.Let's try another algorithm.  Megan selects a secret value \(r\) which
she keeps to herself. Instead
of giving you \(H(W)\), she gives you \(y\) where \(y = H(r + W)\) and
where \(+\) indicates concatenation of strings. Can
you discover \(W\) from \(H\) and \(y\) without knowing \(r\)?A brute-force solution is to try every combination of \(r\) and
\(W\). If \(r\) is obtained from a distribution that is spread out,
then finding \(W\) without knowing \(r\)  take so
much time that it is practically impossible.Hiding: Given \(H, y\), where \(y = H(r + W)\) and \(r\) is a secret,
discovering \(W\) is intractable.BindingDoes the hash function \(H\) and the secret \(r\) provide both
services of the trusted third party? Is Megan bound to her prediction
or can she change her "prediction" after knowing the winner of the
World Cup?Suppose Megan has values \(r\) and \(r'\) such that H(r + 'Brazil') =
H(r' + 'Italy'). After the World Cup is over, she can announce that
her secret is \(r\) if Brazil wins, and announce that it is \(r'\) if
Italy wins.A hash function \(H\) isbindingif finding pairs \((x,y)\) and \((x',y')\)
where \(y \neq y'\) such that:
\(H(x + y) = H(x' + y')\)
is intractable.Suppose you give Megan a hash function that is binding. Then she cannot find
  (in reasonable time) values \(r_{j}\) to match country \(C_{j}\)
such that\(H(r_{0} + C_{0}) = H(r_{1} + C_{1})  = H(r_{2} + C_{2})  = \ldots\).and so she can't wait for the winner \(C_{j}\) to be announced before announcing
her secret \(r_{j}\).In summary, we can use a hash function that is hiding and binding to
play the role of a trusted third party in a commitment.Puzzle FriendlyThe concept ofpuzzle friendlyis related tohiding.
Let \(r\) be a value picked from a spread-out distribution. Let
\(H\) map arbitrary length strings to \(n\)-bit strings.
Consider the following problem:
    Given \(H\), \(r\), and an \(n\)-bit value \(y\), compute any \(x\) such that
\(H(r+x) = y\).In this problem, as opposed to thehidingproblem, we are given \(r\) and not \(x\),The hash function \(H\) is said to bepuzzle friendlyexactly when any algorithm to solve this problem is about as slow
as a brute-force algorithm which checks \(H(r+x) = y\) for random values
of \(x\).
The number of steps taken by any
algorithm that solves this problem is not significantly lower than
\(2^{n}\).Now, let's look at the following related problem.  Given \(H\), \(r\),
and a set \(Y\) of \(n\)-bit strings, compute any \(x\) such that
\(H(r+x) \in Y\).  If \(Y\) consists of a single element \(y\) then
this problem is the same as that in the previous paragraph.  If \(Y\)
is a set of all \(n\)-bit strings then this problem is trivial because
any \(x\) solves the problem.  The probability that a random value
hashes to an element of \(Y\) is proportional to the cardinality of
\(Y\).  The cardinality of \(Y\) controls the expected time to
solve the puzzle.The hash function \(H\) ispuzzle friendlywhen
given \(H\), \(r\), and a set \(Y\) of \(n\)-bit values, the time
required to compute any \(x\) such that \(H(r+x) \in Y\) is
not significantly lower than \(2^{n} / |Y|\), where \(|Y|\) is the
cardinality of \(Y\).A Cryptographic Hash FunctionA cryptographic hash function is one that is collision resistant,
hiding and puzzle-friendly.Hashing Inputs of Arbitrary LengthLet \(f\) be a function that operates on input strings
of fixed length and produces output strings of fixed length. Let the
input and output strings of \(f\) have lengths \(M + N\) and \(M\),
respectively. We look at functions where \(N > 0\), and since the
output is smaller than the input, \(f\) is called acompression
function.We can use function \(f\) to define a function \(g\) whose inputs are strings of arbitrary
lengths and whose outputs are strings of length \(M\).
Example code for \(g\) is given below whereInitialValueis a given constant string of length \(M\).def g(y):
    output = InitialValue
    // pad y so that it's length is a multiple of N
    if len(y)%N > 0:  y = y + "0"*(N - len(y)%N)
    // partition y into blocks of size N
    blocks = [y[i: i+N] for i in range(0, len(y), N)]
    // Apply function f to the concatenation of the
    // previous output (length M) with each block
    // (length N) to get the next output (length M).
    for block in blocks: output = f(output+block)
    return outputHash PointersA hash pointer to an item \(D\) of data is a pair \((ptr, H(D))\)
where \(ptr\) is a pointer that 
points to \(D\), and \(H\) is a cryptographic hash function.Any data structure with pointers can be converted into a
data structure with hash pointers: merely replace a pointer \(ptr\) to
\(D\) by \((ptr, H(D))\).Tamper-Evident Data StructuresSingle BlockA simple example of a tamper-evident structure is a single block of
data D which is pointed to by a hash pointer consisting of a regular
pointer and a hash H(D).Fig.1: Hash Pointer points to a Tamper-Evident Block
    of DataAssume that a malicious agent cannot modifyboththe hash
pointer and the data that it points to. If an 
agent
    changes D to D' then the tampering can be discovered because the
    hash pointer won't match the data that it is pointing to:
    \(H(D') \neq H(D)\).Tamper-Evident Linked ListLet's look at linear linked list to which elements can be appended but
not deleted. The \(i\)-th element appended to the list points to the
\((i-1)\)-th element. 
Let's replace the pointers in the list by hashed
pointers.Fig.2: Hash Pointer points to a Tamper-Evident List
    of DataThe \(j\)-th element of the list, \(j > 0\), consists of data,
\(D_{j}\), and a hash pointer that points to the \(j - 1\)-th
  element of the list. The hash pointer in the \(j\)-th element
consists of a regular pointer, \(ptr_{j}\), which points to the \(j - 1\)-th
  element of the list, and a hashed value, \(HA_{j}\), which is a cryptographic hash of theentire\(j-1\)th element consisting  of \(D_{j-1}\), \(ptr_{j-1}\)
  and \(HA_{j-1}\).
The \(0\)-th element is called thegenesiselement and has
default values. The list is accessed by a hash pointer that points to
the last element of the list; let this pointer be \(ptr_{n}, HA_{n}\).Assume that agents cannot modify \(ptr_{n}, HA_{n}\). Then,
can any agent determine whether the list has been tampered with?Suppose the agent modifies \(D_{j}\), \(ptr_{j}\) or
\(HA_{j}\) for any \(j < n\). Any agent can detect this tampering because the
hash value \(HA_{j+1}\) will no longer match the \(j\)-th element of
the list. If
the malicious agent also modifies \(HA_{j+1}\) then 
hash value \(HA_{j+2}\) will no longer match the \(j+1\)-th element.By induction on \(j\), any agent can detect tampering with the list
provided malicious agents do not also modify the hash pointer to the
last element of the last.Tamper-Evident Acyclic Graphs and Merkle
TreesThe idea described in the previous paragraph to convert linear linked
lists can be used to convert directed acyclic graphs, in which nodes are
connected by pointers, into tamper-evident graphs. A specific case of
a directed acyclic graph is a rooted tree.AMerkle treeis a special case of a binary balanced tree in which
data items are stored only in the leaves.
Nodes that are not leaves 
contain only hash pointers to nodes in the next level down.
To prove that an element at the leaf is a member of the tree we need only the
\(log_{2}(n)\) hash pointers on the path from the root to that
leaf. By contrast, to prove that an element is a member of a linear
list we need to inspect \(O(n)\) elements, on average.Keys and Signed MessagesYou can create a random public-key, private-key pairby calling a function on your computer.With high probability, nobody else has this
specific pair of keys. Each individual's private key is a secret held
by that individual. Public keys are accessible by everybody.Sending messages securelyKeys are used to send messages securely.
Kamala sends a secure message to Joe by encrypting the message with
Joe's public key; Joe decrypts the message using Joe's private key.
An agent cannot decrypt the encrypted message without Joe's private key.Signing messagesSuppose Kamala needs to send a signed message to Joe while
ensuring that nobody can forge her signature. She encrypts the messageMwith her private key to get an encrypted messageM',
and sends the pair(M, M')securely to Joe, i.e., she encrypts(M, M')with Joe's public key, and sends the resulting
encrypted message to Joe.  When Joe receives the message, Joe decrypts
it using his private key to get(M, M'). Then Joe decryptsM'using Kamala's public key to get the decrypted messageM''. IfM'' = Mthen Joe knows that Kamala sentMbecause only an agent with Kamala's private signature could
have sent that message.Cryptocurrency Managed by a Trusted
AgentLet's start with a digital currency managed by a trusted agent that we will
call a bank. Later, we will
look at a consensus algorithm --- very different from Paxos and Byzantine
Generals --- which will allow cryptocurrencies without trusted
agent.The bank maintains a tamper-evident linear listLof transactions that we call a
tamper-evidentledger. 
Any agent can get a copy of the ledger.
This tamper-evident ledger is the foundation of the currency.A transaction is one of
two types:createorpay.
In a pay transaction,payersgive coins that they possess topayees. An agent can be both a 
payer and payee of the same transaction.
In acreatetransaction the bank creates coins that it gives to
agents --- the payees of the transaction; the bank acts as the payer.For this system to be trusted the bank must follow some protocol that determines
how and when the bank creates coins. We won't discuss these protocols.Apaytransaction is signed by all payers of the transaction.
Acreatetransaction is signed by the bank.
We discussed digital signatures and keys earlier.Each element of the tamper-evident ledger has:a unique id;the type of the transaction, either create or pay;list of payers: only for pay transactions --- a list
  indicating the agents who pay coins into the transaction and the
  amounts that they put in;array of value-payee pairs: for both create and pay transactions --- an
  array of pairs(value, payee public key), where each
  pair in the array indicates that coins of the specified value are
  given to the payee with the specified public key.Example of a create transactionAn example of acreatetransaction is:(3146, create,  [(2.1, 7xxxx...), (3.2, 8xxxx)]).The id of this transaction is 3146, the type of the transaction is
create, and the array of value-payee pairs is[(2.1, 7xxxx...), (3.2, 8xxx)]In this transaction the bank creates a coin of value 2.1 and gives it
to the agent with public key 7xxxx..., and the bank also
creates a coin of value 3.2 and gives it
to the agent with public key 8xxxx...The pair:(transaction id, index into array of value-payee pairs)uniquely identifies a (value, payee) tuple.For example (3146, 0) --- 
transaction id 3146, and array index 0 --- identifies value-payee[0] of
transaction 3146 which is specified by the 2-tuple: (2.1, 7xxxx...). So, the
transaction id and index, (3146, 0), tells everybody that the agent
with public key 7xxxx received 2.1 units of coin in transaction 3146.When this
transaction is in a tamper-evident ledger, every agent from that point
onwards knows that agent 7xxxx received 2.1 coins. Any modifications of this
record can be detected.Likewise, (3146, 1) --- 
transaction id 3146, and array index 1 --- identifies value-payee[1] which
is the 2-tuple (3.2, 8xxxx...).Pay transactionCoins are transferred from payers to payees in a pay transaction.Coins flowing into a pay transaction from payersThe payers are identified by a list of 2-tuples, where each 2-tuple is(transaction id, index into array of value-payee pairs)wheretransaction idis the id of the transaction in the
tamper-evident ledger. As we said earlier, this pair uniquely identifies an agent and a
value that this agent acquired in this transaction.
For example the pair --- transaction id, index --- such as (3146, 0)
identifies the 2-tuple (2.1, 7xxxx...); this 2-tuple asserts
that the agent
with public key 7xxxx received 2.1 units of coin in 
transaction 3146. Theentire amountspecified in the 2-tuple (2.1 in our
example) is value that flows into thispaytransaction from the agent
with public key 7xxxx.Suppose the payers in a pay transaction are identified by the list:[(3146, 0), (7359, 3)]and suppose pair 3 in transaction 7359 is (3.2, 8xxxx). Then the total
amount of coins flowing into this pay transaction is 2.1 + 3.2, and
this amount is disbursed to payees.Fig.3: Coins flowing into and out of a pay transaction.Coins flowing out of a pay transaction to payeesThe outflow of coins is specified by an array of value-payee pairs,
exactly as in a create transaction.
The transaction clears all coins: the total inflow from payers is
equal to the total outflow to payees in a transaction.The system may provide incentives, such as payment of coins, to
managers (e.g. banks) of cryptocurrencies. In this case, one of the
payees is the bank itself.Managing amounts spent in a transactionA transaction-id, index pair --- such as (3146, 0)
identifies a 2-tuple such as (2.1, 7xxxx...); this 2-tuple asserts
that the agent
with public key 7xxxx received 2.1 units of coin in 
transaction 3146. Theentire amountspecified in the 2-tuple (2.1 in our
example) is value that flows into the transaction. What should this
agent do if it
wants to put in more than 2.1 coins into the transaction? Or less than
2.1 coins?To put in more value, the bank identifies other transaction-id, index
pairs in which this agent received coins. For example, say that (4539, 2)
identifies a 2-tuple (3.2, 7xxxx), and assume that the payers in this transaction
are specified by the pairs (3146, 0) and (4539, 2). The pair (3146, 0)
asserts that agent (7xxxx) received 2.1 coins and the pair (4539, 2)
asserts that the same agent received 3.2 coins. So the total amount of 
coins flowing into this
transaction from this agent (7xxxx) includes 2.1 + 3.2.To put in less value, the agent acts as both payer and payee; the net
value that this agent pays out to other agents is the difference
between the amount that this agent puts in and takes out. For example,
if agent with public key 7xxxx wants to put in 1.9 coins into this
transaction its payer information can be given by the transaction-id,
index pair (3146, 0) which asserts that the agent received 2.1 coins
and the same agent is a payee that withdraws 0.2 coins.Preventing Double SpendingHow does the system prevent an agent from using the
same coin twice?For example, the transaction id, index pair
(3146, 0) identifies the 2-tuple (2.1, 7xxxx...); this tuple
asserts that the agent
with public key 7xxxx received 2.1 units of coin in 
transaction 3146. Why can't the agent with public key 7xxxx use the
2.1 coins that it received to buy items from Amazon and later use
the same 2.1 coins to buy more items from Walmart?The bank checks that the agent hasn't already spent the coin that it
is putting into a transaction.Before permitting the transaction that double-spends the 2.1 coins
with Walmart, the bank inspects the tamper-evident ledger
for all transactions after transaction 3146 and before the Walmart
transaction to ensure that the agent (7xxxx) hasn't already spent the
2.1 coins that it got in transaction 3146. The transaction that spends
the coins at Amazon will show up in the ledger, and so the transaction
with Walmart will not be allowed.Every agent that has the bank's hash pointer to the end of the
tamper-evident ledger can inspect the ledger to check that
double-spending hasn't occurred.The bank signs a valid transaction and appends it to the
tamper-evident ledger. All agents can see the bank's signature and
verify that nobody (not even the bank) has tampered with the
tamper-evident ledger.Example of a pay transactionAn example of the specification of a pay transaction is:(9431, pay,
  [(3146, 0), (4731, 2)],
  [(0.7, 7xxxx...), (4.6, 9xxxx...)]
).The id of this transaction is 9431; the type of the transaction is
pay; the payers into the transaction are identified by the pairs of
(transaction-id, index): (3146, 0), and (4731, 2); and the
payee array is [(0.7, 7xxxx...), (4.6, 9xxxx...)].Transaction validityThe bank appends a transaction toLif and only if the
transaction is valid. The bank checks for validity by carrying out the
following steps:The bank verifies that the payers into the transaction signed the
  transaction.The bank checks that the total value of coins paid out from the
  transaction does not
  exceed the total value paid in to the transaction. (If the value paid
  in exceeds the value paid out then the bank takes the difference as
  a transaction fee. More about fees later.)The bank verifies that the payers' claims to have received coins in
  previous transactions is genuine. For example,
  if the agent with
  public key 7xxxx... claims to have received coins worth 2.1 in the
  transaction with id 10, and payee array index 0, then the bank
  verifies this claim by that transaction.The bank ensures that coins paid into the transaction haven't
  already been spent.Optimizations: Blocks and Block ChainVerifying large numbers of small transactions requires more
computation than verifying small numbers of blocks of many transactions.
Ablock chainis a tamper-evident ledger in which
each element of the ledger is a block consisting of many transactions.
A block of transactions can be aggregated into a single large
transaction by aggregating all the payers and payees of the smaller
transactions.The amount of computation decreases as the number of transactions in a
block increases. The time required to fill a block with transactions
is larger when the number of transactions to fill the block increases.Checking the Trusted AgentConsider a system in which the trusted agent broadcasts its current
copy of the tamper-evident ledger to all agents.
Every agent can inspect its copy of the tamper-evident ledger to determine whether the
ledger has been tampered with. So, every agent can validate its trust
in the trusted agent; however, this validation
suffers from a crucial problem: Agents may only
have copies ofold, staleversions of the ledger.
By the time that an agent receives a copy of the ledger, the trusted
agent may have added more transactions to the ledger.Fig.4: Old Copy is a Prefix of the Block ChainAn old copy of the ledger can differ from the current copy in only one
way: the current copy may have transactions appended to the end
of the old copy. So all agents can validatepastbehavior of the
trusted agent. An agent cannot, however, treat its copy of the ledger
as the master copy because the agent may not have the transactions
added most recently to the ledger.In the next module we will see how the Bitcoin algorithm addresses
this problem.Advantages of this cryptocurrencyAny agent can get a copy of the tamper-evident ledger and verify that all
transactions in the ledger are valid. Any agent can verify that
the only way in which the ledger is modified is that elements are
append to its tail; all that the agent needs to do is to check that
the pointer to the tail is modified only by appending
elements. Because the ledger is tamper-evident, an agent can check
that the ledger doesn't change while the hash pointer to the end
of the ledger doesn't change.The bank can't forge a transaction because all payers sign the
transaction. Agents can remain anonymous because an agent's only
public information is the agent's public key, and an agent can create
multiple public keys. Every agent can verify the correctness of every
transaction.Disadvantages of this cryptocurrencyUsers may not trust the bank. Transactions are not private because the
bank has a record of all transactions. And the bank is a single point
of failure.Next we'll look at Bitcoin's algorithms for
implementing a cryptocurrency without trusted agents.ReviewWhat iscollision resistance? Why is it relevant to
  cryptocurrencies?Consider the example ofhidingin which Megan hides her
  predictions for the winner of the World Cup. Suppose you could
  choose two different random number generators for creating the
  secret \(r\); in one case the bits of \(r\) are uncorrelated, and in
  the other they are highly correlated. Which generator would you use
  and why?What isbinding? Why is it relevant to hiding information?What ispuzzle friendly?What is atamper-evident data structure? How would you
  implement a tamper-evident linear list? A tamper-evident tree?Describe the algorithm for cryptocurrency with a trusted agent to
  someone who knows absolutely nothing about cryptocurrency.How could the algorithm for cryptocurrency be used for a collection
  of distributed agents to keep track of a sequence of events, other
  than buying/selling currency?K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology



Figure Link: https://kmchandy.github.io/Crypto/CryptoCurrencyIntroduction/Slide1.jpg

Caption: Fig.1: Hash Pointer points to a Tamper-Evident Block
    of Data


Consider a hash function \(H\) that outputs \(n\)-bit numbers and
    whose input is \(m\) bit strings. As a specific example lets
    assume that \(m\) is a large number and \(n = 4\). We can
    find a collision in the following way.Let \(D\) be an
    array of size \(2^{n} = 16\). Initially \(D\) contains null values. Repeat the following
    iteration until a collision is found.
Pick a random input \(x\).
If \(D[H(x)]\) is null then set \(D[H(x)] = x\) else there is a collision between
\(H(x)\) and \(x\).By the pigeon-hole principle, we will find a collision in at most
    \(2^{n} + 1 = 17\) iterations.
    This  brute-force algorithm uses space \(2^{n}\) and finds a
    collision in at most  \(2^{n} + 1\) steps.
From theBirthday Paradoxa collision will be found with high probability
in \(2^{n/2}\) iterations though
the worse-case time is \(2^{n}+1\). If \(n = 256\) then a collision
will be found with high probability in \(2^{128}\) iterations; 
however, executing \(2^{128}\) steps is still intractable.For \(H\) to be
    collision resistant the output of \(H\) 
    must be \(n\)-bits for large \(n\). For example \(n = 256\) in the
    SHA-256 hash function.Commitment using HashesYou bet that your soccer friend, Megan, cannot predict the winner
    of the 2022 World Cup. Megan puts the name of the predicted
    winner, \(W\), in an envelope and
    gives it to a trusted third party. After the World Cup is over,
    the third party reveals Megan's prediction, and at that point you
    can find out whether Megan's prediction was accurate.The trusted third party provides two services:Hiding: You can't find out Megan's prediction until the third party reveals it.Binding: Megan can't change her prediction after giving it to the third
  party.Can we use a hash function instead of a trusted third party?HidingLet's try the following idea. Megan commits to \(W\) in the following
way. She announces a hash function, \(H\),
and the hash, \(y\), where \(y = H(W)\). You know \(H\) and \(y\). 
After the World Cup is over, she reveals her prediction, \(W\). At
this point you can verify that \(y = H(W)\).Does the hash function provide the services of the third party?
Can you discover Megan's prediction before she reveals it?It's easy. There are only 32 teams playing. Compute \(H(x)\) where
    \(x\) runs over each of the 32 teams. One of those teams has to be
\(W\). You can discover her prediction in at most 210 steps.Let's try another algorithm.  Megan selects a secret value \(r\) which
she keeps to herself. Instead
of giving you \(H(W)\), she gives you \(y\) where \(y = H(r + W)\) and
where \(+\) indicates concatenation of strings. Can
you discover \(W\) from \(H\) and \(y\) without knowing \(r\)?A brute-force solution is to try every combination of \(r\) and
\(W\). If \(r\) is obtained from a distribution that is spread out,
then finding \(W\) without knowing \(r\)  take so
much time that it is practically impossible.Hiding: Given \(H, y\), where \(y = H(r + W)\) and \(r\) is a secret,
discovering \(W\) is intractable.BindingDoes the hash function \(H\) and the secret \(r\) provide both
services of the trusted third party? Is Megan bound to her prediction
or can she change her "prediction" after knowing the winner of the
World Cup?Suppose Megan has values \(r\) and \(r'\) such that H(r + 'Brazil') =
H(r' + 'Italy'). After the World Cup is over, she can announce that
her secret is \(r\) if Brazil wins, and announce that it is \(r'\) if
Italy wins.A hash function \(H\) isbindingif finding pairs \((x,y)\) and \((x',y')\)
where \(y \neq y'\) such that:
\(H(x + y) = H(x' + y')\)
is intractable.Suppose you give Megan a hash function that is binding. Then she cannot find
  (in reasonable time) values \(r_{j}\) to match country \(C_{j}\)
such that\(H(r_{0} + C_{0}) = H(r_{1} + C_{1})  = H(r_{2} + C_{2})  = \ldots\).and so she can't wait for the winner \(C_{j}\) to be announced before announcing
her secret \(r_{j}\).In summary, we can use a hash function that is hiding and binding to
play the role of a trusted third party in a commitment.Puzzle FriendlyThe concept ofpuzzle friendlyis related tohiding.
Let \(r\) be a value picked from a spread-out distribution. Let
\(H\) map arbitrary length strings to \(n\)-bit strings.
Consider the following problem:
    Given \(H\), \(r\), and an \(n\)-bit value \(y\), compute any \(x\) such that
\(H(r+x) = y\).In this problem, as opposed to thehidingproblem, we are given \(r\) and not \(x\),The hash function \(H\) is said to bepuzzle friendlyexactly when any algorithm to solve this problem is about as slow
as a brute-force algorithm which checks \(H(r+x) = y\) for random values
of \(x\).
The number of steps taken by any
algorithm that solves this problem is not significantly lower than
\(2^{n}\).Now, let's look at the following related problem.  Given \(H\), \(r\),
and a set \(Y\) of \(n\)-bit strings, compute any \(x\) such that
\(H(r+x) \in Y\).  If \(Y\) consists of a single element \(y\) then
this problem is the same as that in the previous paragraph.  If \(Y\)
is a set of all \(n\)-bit strings then this problem is trivial because
any \(x\) solves the problem.  The probability that a random value
hashes to an element of \(Y\) is proportional to the cardinality of
\(Y\).  The cardinality of \(Y\) controls the expected time to
solve the puzzle.The hash function \(H\) ispuzzle friendlywhen
given \(H\), \(r\), and a set \(Y\) of \(n\)-bit values, the time
required to compute any \(x\) such that \(H(r+x) \in Y\) is
not significantly lower than \(2^{n} / |Y|\), where \(|Y|\) is the
cardinality of \(Y\).A Cryptographic Hash FunctionA cryptographic hash function is one that is collision resistant,
hiding and puzzle-friendly.Hashing Inputs of Arbitrary LengthLet \(f\) be a function that operates on input strings
of fixed length and produces output strings of fixed length. Let the
input and output strings of \(f\) have lengths \(M + N\) and \(M\),
respectively. We look at functions where \(N > 0\), and since the
output is smaller than the input, \(f\) is called acompression
function.We can use function \(f\) to define a function \(g\) whose inputs are strings of arbitrary
lengths and whose outputs are strings of length \(M\).
Example code for \(g\) is given below whereInitialValueis a given constant string of length \(M\).def g(y):
    output = InitialValue
    // pad y so that it's length is a multiple of N
    if len(y)%N > 0:  y = y + "0"*(N - len(y)%N)
    // partition y into blocks of size N
    blocks = [y[i: i+N] for i in range(0, len(y), N)]
    // Apply function f to the concatenation of the
    // previous output (length M) with each block
    // (length N) to get the next output (length M).
    for block in blocks: output = f(output+block)
    return outputHash PointersA hash pointer to an item \(D\) of data is a pair \((ptr, H(D))\)
where \(ptr\) is a pointer that 
points to \(D\), and \(H\) is a cryptographic hash function.Any data structure with pointers can be converted into a
data structure with hash pointers: merely replace a pointer \(ptr\) to
\(D\) by \((ptr, H(D))\).Tamper-Evident Data StructuresSingle BlockA simple example of a tamper-evident structure is a single block of
data D which is pointed to by a hash pointer consisting of a regular
pointer and a hash H(D).Fig.1: Hash Pointer points to a Tamper-Evident Block
    of DataAssume that a malicious agent cannot modifyboththe hash
pointer and the data that it points to. If an 
agent
    changes D to D' then the tampering can be discovered because the
    hash pointer won't match the data that it is pointing to:
    \(H(D') \neq H(D)\).Tamper-Evident Linked ListLet's look at linear linked list to which elements can be appended but
not deleted. The \(i\)-th element appended to the list points to the
\((i-1)\)-th element. 
Let's replace the pointers in the list by hashed
pointers.Fig.2: Hash Pointer points to a Tamper-Evident List
    of DataThe \(j\)-th element of the list, \(j > 0\), consists of data,
\(D_{j}\), and a hash pointer that points to the \(j - 1\)-th
  element of the list. The hash pointer in the \(j\)-th element
consists of a regular pointer, \(ptr_{j}\), which points to the \(j - 1\)-th
  element of the list, and a hashed value, \(HA_{j}\), which is a cryptographic hash of theentire\(j-1\)th element consisting  of \(D_{j-1}\), \(ptr_{j-1}\)
  and \(HA_{j-1}\).
The \(0\)-th element is called thegenesiselement and has
default values. The list is accessed by a hash pointer that points to
the last element of the list; let this pointer be \(ptr_{n}, HA_{n}\).Assume that agents cannot modify \(ptr_{n}, HA_{n}\). Then,
can any agent determine whether the list has been tampered with?Suppose the agent modifies \(D_{j}\), \(ptr_{j}\) or
\(HA_{j}\) for any \(j < n\). Any agent can detect this tampering because the
hash value \(HA_{j+1}\) will no longer match the \(j\)-th element of
the list. If
the malicious agent also modifies \(HA_{j+1}\) then 
hash value \(HA_{j+2}\) will no longer match the \(j+1\)-th element.By induction on \(j\), any agent can detect tampering with the list
provided malicious agents do not also modify the hash pointer to the
last element of the last.Tamper-Evident Acyclic Graphs and Merkle
TreesThe idea described in the previous paragraph to convert linear linked
lists can be used to convert directed acyclic graphs, in which nodes are
connected by pointers, into tamper-evident graphs. A specific case of
a directed acyclic graph is a rooted tree.AMerkle treeis a special case of a binary balanced tree in which
data items are stored only in the leaves.
Nodes that are not leaves 
contain only hash pointers to nodes in the next level down.
To prove that an element at the leaf is a member of the tree we need only the
\(log_{2}(n)\) hash pointers on the path from the root to that
leaf. By contrast, to prove that an element is a member of a linear
list we need to inspect \(O(n)\) elements, on average.Keys and Signed MessagesYou can create a random public-key, private-key pairby calling a function on your computer.With high probability, nobody else has this
specific pair of keys. Each individual's private key is a secret held
by that individual. Public keys are accessible by everybody.Sending messages securelyKeys are used to send messages securely.
Kamala sends a secure message to Joe by encrypting the message with
Joe's public key; Joe decrypts the message using Joe's private key.
An agent cannot decrypt the encrypted message without Joe's private key.Signing messagesSuppose Kamala needs to send a signed message to Joe while
ensuring that nobody can forge her signature. She encrypts the messageMwith her private key to get an encrypted messageM',
and sends the pair(M, M')securely to Joe, i.e., she encrypts(M, M')with Joe's public key, and sends the resulting
encrypted message to Joe.  When Joe receives the message, Joe decrypts
it using his private key to get(M, M'). Then Joe decryptsM'using Kamala's public key to get the decrypted messageM''. IfM'' = Mthen Joe knows that Kamala sentMbecause only an agent with Kamala's private signature could
have sent that message.Cryptocurrency Managed by a Trusted
AgentLet's start with a digital currency managed by a trusted agent that we will
call a bank. Later, we will
look at a consensus algorithm --- very different from Paxos and Byzantine
Generals --- which will allow cryptocurrencies without trusted
agent.The bank maintains a tamper-evident linear listLof transactions that we call a
tamper-evidentledger. 
Any agent can get a copy of the ledger.
This tamper-evident ledger is the foundation of the currency.A transaction is one of
two types:createorpay.
In a pay transaction,payersgive coins that they possess topayees. An agent can be both a 
payer and payee of the same transaction.
In acreatetransaction the bank creates coins that it gives to
agents --- the payees of the transaction; the bank acts as the payer.For this system to be trusted the bank must follow some protocol that determines
how and when the bank creates coins. We won't discuss these protocols.Apaytransaction is signed by all payers of the transaction.
Acreatetransaction is signed by the bank.
We discussed digital signatures and keys earlier.Each element of the tamper-evident ledger has:a unique id;the type of the transaction, either create or pay;list of payers: only for pay transactions --- a list
  indicating the agents who pay coins into the transaction and the
  amounts that they put in;array of value-payee pairs: for both create and pay transactions --- an
  array of pairs(value, payee public key), where each
  pair in the array indicates that coins of the specified value are
  given to the payee with the specified public key.Example of a create transactionAn example of acreatetransaction is:(3146, create,  [(2.1, 7xxxx...), (3.2, 8xxxx)]).The id of this transaction is 3146, the type of the transaction is
create, and the array of value-payee pairs is[(2.1, 7xxxx...), (3.2, 8xxx)]In this transaction the bank creates a coin of value 2.1 and gives it
to the agent with public key 7xxxx..., and the bank also
creates a coin of value 3.2 and gives it
to the agent with public key 8xxxx...The pair:(transaction id, index into array of value-payee pairs)uniquely identifies a (value, payee) tuple.For example (3146, 0) --- 
transaction id 3146, and array index 0 --- identifies value-payee[0] of
transaction 3146 which is specified by the 2-tuple: (2.1, 7xxxx...). So, the
transaction id and index, (3146, 0), tells everybody that the agent
with public key 7xxxx received 2.1 units of coin in transaction 3146.When this
transaction is in a tamper-evident ledger, every agent from that point
onwards knows that agent 7xxxx received 2.1 coins. Any modifications of this
record can be detected.Likewise, (3146, 1) --- 
transaction id 3146, and array index 1 --- identifies value-payee[1] which
is the 2-tuple (3.2, 8xxxx...).Pay transactionCoins are transferred from payers to payees in a pay transaction.Coins flowing into a pay transaction from payersThe payers are identified by a list of 2-tuples, where each 2-tuple is(transaction id, index into array of value-payee pairs)wheretransaction idis the id of the transaction in the
tamper-evident ledger. As we said earlier, this pair uniquely identifies an agent and a
value that this agent acquired in this transaction.
For example the pair --- transaction id, index --- such as (3146, 0)
identifies the 2-tuple (2.1, 7xxxx...); this 2-tuple asserts
that the agent
with public key 7xxxx received 2.1 units of coin in 
transaction 3146. Theentire amountspecified in the 2-tuple (2.1 in our
example) is value that flows into thispaytransaction from the agent
with public key 7xxxx.Suppose the payers in a pay transaction are identified by the list:[(3146, 0), (7359, 3)]and suppose pair 3 in transaction 7359 is (3.2, 8xxxx). Then the total
amount of coins flowing into this pay transaction is 2.1 + 3.2, and
this amount is disbursed to payees.Fig.3: Coins flowing into and out of a pay transaction.Coins flowing out of a pay transaction to payeesThe outflow of coins is specified by an array of value-payee pairs,
exactly as in a create transaction.
The transaction clears all coins: the total inflow from payers is
equal to the total outflow to payees in a transaction.The system may provide incentives, such as payment of coins, to
managers (e.g. banks) of cryptocurrencies. In this case, one of the
payees is the bank itself.Managing amounts spent in a transactionA transaction-id, index pair --- such as (3146, 0)
identifies a 2-tuple such as (2.1, 7xxxx...); this 2-tuple asserts
that the agent
with public key 7xxxx received 2.1 units of coin in 
transaction 3146. Theentire amountspecified in the 2-tuple (2.1 in our
example) is value that flows into the transaction. What should this
agent do if it
wants to put in more than 2.1 coins into the transaction? Or less than
2.1 coins?To put in more value, the bank identifies other transaction-id, index
pairs in which this agent received coins. For example, say that (4539, 2)
identifies a 2-tuple (3.2, 7xxxx), and assume that the payers in this transaction
are specified by the pairs (3146, 0) and (4539, 2). The pair (3146, 0)
asserts that agent (7xxxx) received 2.1 coins and the pair (4539, 2)
asserts that the same agent received 3.2 coins. So the total amount of 
coins flowing into this
transaction from this agent (7xxxx) includes 2.1 + 3.2.To put in less value, the agent acts as both payer and payee; the net
value that this agent pays out to other agents is the difference
between the amount that this agent puts in and takes out. For example,
if agent with public key 7xxxx wants to put in 1.9 coins into this
transaction its payer information can be given by the transaction-id,
index pair (3146, 0) which asserts that the agent received 2.1 coins
and the same agent is a payee that withdraws 0.2 coins.Preventing Double SpendingHow does the system prevent an agent from using the
same coin twice?For example, the transaction id, index pair
(3146, 0) identifies the 2-tuple (2.1, 7xxxx...); this tuple
asserts that the agent
with public key 7xxxx received 2.1 units of coin in 
transaction 3146. Why can't the agent with public key 7xxxx use the
2.1 coins that it received to buy items from Amazon and later use
the same 2.1 coins to buy more items from Walmart?The bank checks that the agent hasn't already spent the coin that it
is putting into a transaction.Before permitting the transaction that double-spends the 2.1 coins
with Walmart, the bank inspects the tamper-evident ledger
for all transactions after transaction 3146 and before the Walmart
transaction to ensure that the agent (7xxxx) hasn't already spent the
2.1 coins that it got in transaction 3146. The transaction that spends
the coins at Amazon will show up in the ledger, and so the transaction
with Walmart will not be allowed.Every agent that has the bank's hash pointer to the end of the
tamper-evident ledger can inspect the ledger to check that
double-spending hasn't occurred.The bank signs a valid transaction and appends it to the
tamper-evident ledger. All agents can see the bank's signature and
verify that nobody (not even the bank) has tampered with the
tamper-evident ledger.Example of a pay transactionAn example of the specification of a pay transaction is:(9431, pay,
  [(3146, 0), (4731, 2)],
  [(0.7, 7xxxx...), (4.6, 9xxxx...)]
).The id of this transaction is 9431; the type of the transaction is
pay; the payers into the transaction are identified by the pairs of
(transaction-id, index): (3146, 0), and (4731, 2); and the
payee array is [(0.7, 7xxxx...), (4.6, 9xxxx...)].Transaction validityThe bank appends a transaction toLif and only if the
transaction is valid. The bank checks for validity by carrying out the
following steps:The bank verifies that the payers into the transaction signed the
  transaction.The bank checks that the total value of coins paid out from the
  transaction does not
  exceed the total value paid in to the transaction. (If the value paid
  in exceeds the value paid out then the bank takes the difference as
  a transaction fee. More about fees later.)The bank verifies that the payers' claims to have received coins in
  previous transactions is genuine. For example,
  if the agent with
  public key 7xxxx... claims to have received coins worth 2.1 in the
  transaction with id 10, and payee array index 0, then the bank
  verifies this claim by that transaction.The bank ensures that coins paid into the transaction haven't
  already been spent.Optimizations: Blocks and Block ChainVerifying large numbers of small transactions requires more
computation than verifying small numbers of blocks of many transactions.
Ablock chainis a tamper-evident ledger in which
each element of the ledger is a block consisting of many transactions.
A block of transactions can be aggregated into a single large
transaction by aggregating all the payers and payees of the smaller
transactions.The amount of computation decreases as the number of transactions in a
block increases. The time required to fill a block with transactions
is larger when the number of transactions to fill the block increases.Checking the Trusted AgentConsider a system in which the trusted agent broadcasts its current
copy of the tamper-evident ledger to all agents.
Every agent can inspect its copy of the tamper-evident ledger to determine whether the
ledger has been tampered with. So, every agent can validate its trust
in the trusted agent; however, this validation
suffers from a crucial problem: Agents may only
have copies ofold, staleversions of the ledger.
By the time that an agent receives a copy of the ledger, the trusted
agent may have added more transactions to the ledger.Fig.4: Old Copy is a Prefix of the Block ChainAn old copy of the ledger can differ from the current copy in only one
way: the current copy may have transactions appended to the end
of the old copy. So all agents can validatepastbehavior of the
trusted agent. An agent cannot, however, treat its copy of the ledger
as the master copy because the agent may not have the transactions
added most recently to the ledger.In the next module we will see how the Bitcoin algorithm addresses
this problem.Advantages of this cryptocurrencyAny agent can get a copy of the tamper-evident ledger and verify that all
transactions in the ledger are valid. Any agent can verify that
the only way in which the ledger is modified is that elements are
append to its tail; all that the agent needs to do is to check that
the pointer to the tail is modified only by appending
elements. Because the ledger is tamper-evident, an agent can check
that the ledger doesn't change while the hash pointer to the end
of the ledger doesn't change.The bank can't forge a transaction because all payers sign the
transaction. Agents can remain anonymous because an agent's only
public information is the agent's public key, and an agent can create
multiple public keys. Every agent can verify the correctness of every
transaction.Disadvantages of this cryptocurrencyUsers may not trust the bank. Transactions are not private because the
bank has a record of all transactions. And the bank is a single point
of failure.Next we'll look at Bitcoin's algorithms for
implementing a cryptocurrency without trusted agents.ReviewWhat iscollision resistance? Why is it relevant to
  cryptocurrencies?Consider the example ofhidingin which Megan hides her
  predictions for the winner of the World Cup. Suppose you could
  choose two different random number generators for creating the
  secret \(r\); in one case the bits of \(r\) are uncorrelated, and in
  the other they are highly correlated. Which generator would you use
  and why?What isbinding? Why is it relevant to hiding information?What ispuzzle friendly?What is atamper-evident data structure? How would you
  implement a tamper-evident linear list? A tamper-evident tree?Describe the algorithm for cryptocurrency with a trusted agent to
  someone who knows absolutely nothing about cryptocurrency.How could the algorithm for cryptocurrency be used for a collection
  of distributed agents to keep track of a sequence of events, other
  than buying/selling currency?K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology



Figure Link: https://kmchandy.github.io/Crypto/CryptoCurrencyIntroduction/Slide1.jpg

Caption: Fig.1: Hash Pointer points to a Tamper-Evident Block
    of Data


Let \(D\) be an
    array of size \(2^{n} = 16\). Initially \(D\) contains null values. Repeat the following
    iteration until a collision is found.
Pick a random input \(x\).
If \(D[H(x)]\) is null then set \(D[H(x)] = x\) else there is a collision between
\(H(x)\) and \(x\).By the pigeon-hole principle, we will find a collision in at most
    \(2^{n} + 1 = 17\) iterations.
    This  brute-force algorithm uses space \(2^{n}\) and finds a
    collision in at most  \(2^{n} + 1\) steps.
From theBirthday Paradoxa collision will be found with high probability
in \(2^{n/2}\) iterations though
the worse-case time is \(2^{n}+1\). If \(n = 256\) then a collision
will be found with high probability in \(2^{128}\) iterations; 
however, executing \(2^{128}\) steps is still intractable.For \(H\) to be
    collision resistant the output of \(H\) 
    must be \(n\)-bits for large \(n\). For example \(n = 256\) in the
    SHA-256 hash function.Commitment using HashesYou bet that your soccer friend, Megan, cannot predict the winner
    of the 2022 World Cup. Megan puts the name of the predicted
    winner, \(W\), in an envelope and
    gives it to a trusted third party. After the World Cup is over,
    the third party reveals Megan's prediction, and at that point you
    can find out whether Megan's prediction was accurate.The trusted third party provides two services:Hiding: You can't find out Megan's prediction until the third party reveals it.Binding: Megan can't change her prediction after giving it to the third
  party.Can we use a hash function instead of a trusted third party?HidingLet's try the following idea. Megan commits to \(W\) in the following
way. She announces a hash function, \(H\),
and the hash, \(y\), where \(y = H(W)\). You know \(H\) and \(y\). 
After the World Cup is over, she reveals her prediction, \(W\). At
this point you can verify that \(y = H(W)\).Does the hash function provide the services of the third party?
Can you discover Megan's prediction before she reveals it?It's easy. There are only 32 teams playing. Compute \(H(x)\) where
    \(x\) runs over each of the 32 teams. One of those teams has to be
\(W\). You can discover her prediction in at most 210 steps.Let's try another algorithm.  Megan selects a secret value \(r\) which
she keeps to herself. Instead
of giving you \(H(W)\), she gives you \(y\) where \(y = H(r + W)\) and
where \(+\) indicates concatenation of strings. Can
you discover \(W\) from \(H\) and \(y\) without knowing \(r\)?A brute-force solution is to try every combination of \(r\) and
\(W\). If \(r\) is obtained from a distribution that is spread out,
then finding \(W\) without knowing \(r\)  take so
much time that it is practically impossible.Hiding: Given \(H, y\), where \(y = H(r + W)\) and \(r\) is a secret,
discovering \(W\) is intractable.BindingDoes the hash function \(H\) and the secret \(r\) provide both
services of the trusted third party? Is Megan bound to her prediction
or can she change her "prediction" after knowing the winner of the
World Cup?Suppose Megan has values \(r\) and \(r'\) such that H(r + 'Brazil') =
H(r' + 'Italy'). After the World Cup is over, she can announce that
her secret is \(r\) if Brazil wins, and announce that it is \(r'\) if
Italy wins.A hash function \(H\) isbindingif finding pairs \((x,y)\) and \((x',y')\)
where \(y \neq y'\) such that:
\(H(x + y) = H(x' + y')\)
is intractable.Suppose you give Megan a hash function that is binding. Then she cannot find
  (in reasonable time) values \(r_{j}\) to match country \(C_{j}\)
such that\(H(r_{0} + C_{0}) = H(r_{1} + C_{1})  = H(r_{2} + C_{2})  = \ldots\).and so she can't wait for the winner \(C_{j}\) to be announced before announcing
her secret \(r_{j}\).In summary, we can use a hash function that is hiding and binding to
play the role of a trusted third party in a commitment.Puzzle FriendlyThe concept ofpuzzle friendlyis related tohiding.
Let \(r\) be a value picked from a spread-out distribution. Let
\(H\) map arbitrary length strings to \(n\)-bit strings.
Consider the following problem:
    Given \(H\), \(r\), and an \(n\)-bit value \(y\), compute any \(x\) such that
\(H(r+x) = y\).In this problem, as opposed to thehidingproblem, we are given \(r\) and not \(x\),The hash function \(H\) is said to bepuzzle friendlyexactly when any algorithm to solve this problem is about as slow
as a brute-force algorithm which checks \(H(r+x) = y\) for random values
of \(x\).
The number of steps taken by any
algorithm that solves this problem is not significantly lower than
\(2^{n}\).Now, let's look at the following related problem.  Given \(H\), \(r\),
and a set \(Y\) of \(n\)-bit strings, compute any \(x\) such that
\(H(r+x) \in Y\).  If \(Y\) consists of a single element \(y\) then
this problem is the same as that in the previous paragraph.  If \(Y\)
is a set of all \(n\)-bit strings then this problem is trivial because
any \(x\) solves the problem.  The probability that a random value
hashes to an element of \(Y\) is proportional to the cardinality of
\(Y\).  The cardinality of \(Y\) controls the expected time to
solve the puzzle.The hash function \(H\) ispuzzle friendlywhen
given \(H\), \(r\), and a set \(Y\) of \(n\)-bit values, the time
required to compute any \(x\) such that \(H(r+x) \in Y\) is
not significantly lower than \(2^{n} / |Y|\), where \(|Y|\) is the
cardinality of \(Y\).A Cryptographic Hash FunctionA cryptographic hash function is one that is collision resistant,
hiding and puzzle-friendly.Hashing Inputs of Arbitrary LengthLet \(f\) be a function that operates on input strings
of fixed length and produces output strings of fixed length. Let the
input and output strings of \(f\) have lengths \(M + N\) and \(M\),
respectively. We look at functions where \(N > 0\), and since the
output is smaller than the input, \(f\) is called acompression
function.We can use function \(f\) to define a function \(g\) whose inputs are strings of arbitrary
lengths and whose outputs are strings of length \(M\).
Example code for \(g\) is given below whereInitialValueis a given constant string of length \(M\).def g(y):
    output = InitialValue
    // pad y so that it's length is a multiple of N
    if len(y)%N > 0:  y = y + "0"*(N - len(y)%N)
    // partition y into blocks of size N
    blocks = [y[i: i+N] for i in range(0, len(y), N)]
    // Apply function f to the concatenation of the
    // previous output (length M) with each block
    // (length N) to get the next output (length M).
    for block in blocks: output = f(output+block)
    return outputHash PointersA hash pointer to an item \(D\) of data is a pair \((ptr, H(D))\)
where \(ptr\) is a pointer that 
points to \(D\), and \(H\) is a cryptographic hash function.Any data structure with pointers can be converted into a
data structure with hash pointers: merely replace a pointer \(ptr\) to
\(D\) by \((ptr, H(D))\).Tamper-Evident Data StructuresSingle BlockA simple example of a tamper-evident structure is a single block of
data D which is pointed to by a hash pointer consisting of a regular
pointer and a hash H(D).Fig.1: Hash Pointer points to a Tamper-Evident Block
    of DataAssume that a malicious agent cannot modifyboththe hash
pointer and the data that it points to. If an 
agent
    changes D to D' then the tampering can be discovered because the
    hash pointer won't match the data that it is pointing to:
    \(H(D') \neq H(D)\).Tamper-Evident Linked ListLet's look at linear linked list to which elements can be appended but
not deleted. The \(i\)-th element appended to the list points to the
\((i-1)\)-th element. 
Let's replace the pointers in the list by hashed
pointers.Fig.2: Hash Pointer points to a Tamper-Evident List
    of DataThe \(j\)-th element of the list, \(j > 0\), consists of data,
\(D_{j}\), and a hash pointer that points to the \(j - 1\)-th
  element of the list. The hash pointer in the \(j\)-th element
consists of a regular pointer, \(ptr_{j}\), which points to the \(j - 1\)-th
  element of the list, and a hashed value, \(HA_{j}\), which is a cryptographic hash of theentire\(j-1\)th element consisting  of \(D_{j-1}\), \(ptr_{j-1}\)
  and \(HA_{j-1}\).
The \(0\)-th element is called thegenesiselement and has
default values. The list is accessed by a hash pointer that points to
the last element of the list; let this pointer be \(ptr_{n}, HA_{n}\).Assume that agents cannot modify \(ptr_{n}, HA_{n}\). Then,
can any agent determine whether the list has been tampered with?Suppose the agent modifies \(D_{j}\), \(ptr_{j}\) or
\(HA_{j}\) for any \(j < n\). Any agent can detect this tampering because the
hash value \(HA_{j+1}\) will no longer match the \(j\)-th element of
the list. If
the malicious agent also modifies \(HA_{j+1}\) then 
hash value \(HA_{j+2}\) will no longer match the \(j+1\)-th element.By induction on \(j\), any agent can detect tampering with the list
provided malicious agents do not also modify the hash pointer to the
last element of the last.Tamper-Evident Acyclic Graphs and Merkle
TreesThe idea described in the previous paragraph to convert linear linked
lists can be used to convert directed acyclic graphs, in which nodes are
connected by pointers, into tamper-evident graphs. A specific case of
a directed acyclic graph is a rooted tree.AMerkle treeis a special case of a binary balanced tree in which
data items are stored only in the leaves.
Nodes that are not leaves 
contain only hash pointers to nodes in the next level down.
To prove that an element at the leaf is a member of the tree we need only the
\(log_{2}(n)\) hash pointers on the path from the root to that
leaf. By contrast, to prove that an element is a member of a linear
list we need to inspect \(O(n)\) elements, on average.Keys and Signed MessagesYou can create a random public-key, private-key pairby calling a function on your computer.With high probability, nobody else has this
specific pair of keys. Each individual's private key is a secret held
by that individual. Public keys are accessible by everybody.Sending messages securelyKeys are used to send messages securely.
Kamala sends a secure message to Joe by encrypting the message with
Joe's public key; Joe decrypts the message using Joe's private key.
An agent cannot decrypt the encrypted message without Joe's private key.Signing messagesSuppose Kamala needs to send a signed message to Joe while
ensuring that nobody can forge her signature. She encrypts the messageMwith her private key to get an encrypted messageM',
and sends the pair(M, M')securely to Joe, i.e., she encrypts(M, M')with Joe's public key, and sends the resulting
encrypted message to Joe.  When Joe receives the message, Joe decrypts
it using his private key to get(M, M'). Then Joe decryptsM'using Kamala's public key to get the decrypted messageM''. IfM'' = Mthen Joe knows that Kamala sentMbecause only an agent with Kamala's private signature could
have sent that message.Cryptocurrency Managed by a Trusted
AgentLet's start with a digital currency managed by a trusted agent that we will
call a bank. Later, we will
look at a consensus algorithm --- very different from Paxos and Byzantine
Generals --- which will allow cryptocurrencies without trusted
agent.The bank maintains a tamper-evident linear listLof transactions that we call a
tamper-evidentledger. 
Any agent can get a copy of the ledger.
This tamper-evident ledger is the foundation of the currency.A transaction is one of
two types:createorpay.
In a pay transaction,payersgive coins that they possess topayees. An agent can be both a 
payer and payee of the same transaction.
In acreatetransaction the bank creates coins that it gives to
agents --- the payees of the transaction; the bank acts as the payer.For this system to be trusted the bank must follow some protocol that determines
how and when the bank creates coins. We won't discuss these protocols.Apaytransaction is signed by all payers of the transaction.
Acreatetransaction is signed by the bank.
We discussed digital signatures and keys earlier.Each element of the tamper-evident ledger has:a unique id;the type of the transaction, either create or pay;list of payers: only for pay transactions --- a list
  indicating the agents who pay coins into the transaction and the
  amounts that they put in;array of value-payee pairs: for both create and pay transactions --- an
  array of pairs(value, payee public key), where each
  pair in the array indicates that coins of the specified value are
  given to the payee with the specified public key.Example of a create transactionAn example of acreatetransaction is:(3146, create,  [(2.1, 7xxxx...), (3.2, 8xxxx)]).The id of this transaction is 3146, the type of the transaction is
create, and the array of value-payee pairs is[(2.1, 7xxxx...), (3.2, 8xxx)]In this transaction the bank creates a coin of value 2.1 and gives it
to the agent with public key 7xxxx..., and the bank also
creates a coin of value 3.2 and gives it
to the agent with public key 8xxxx...The pair:(transaction id, index into array of value-payee pairs)uniquely identifies a (value, payee) tuple.For example (3146, 0) --- 
transaction id 3146, and array index 0 --- identifies value-payee[0] of
transaction 3146 which is specified by the 2-tuple: (2.1, 7xxxx...). So, the
transaction id and index, (3146, 0), tells everybody that the agent
with public key 7xxxx received 2.1 units of coin in transaction 3146.When this
transaction is in a tamper-evident ledger, every agent from that point
onwards knows that agent 7xxxx received 2.1 coins. Any modifications of this
record can be detected.Likewise, (3146, 1) --- 
transaction id 3146, and array index 1 --- identifies value-payee[1] which
is the 2-tuple (3.2, 8xxxx...).Pay transactionCoins are transferred from payers to payees in a pay transaction.Coins flowing into a pay transaction from payersThe payers are identified by a list of 2-tuples, where each 2-tuple is(transaction id, index into array of value-payee pairs)wheretransaction idis the id of the transaction in the
tamper-evident ledger. As we said earlier, this pair uniquely identifies an agent and a
value that this agent acquired in this transaction.
For example the pair --- transaction id, index --- such as (3146, 0)
identifies the 2-tuple (2.1, 7xxxx...); this 2-tuple asserts
that the agent
with public key 7xxxx received 2.1 units of coin in 
transaction 3146. Theentire amountspecified in the 2-tuple (2.1 in our
example) is value that flows into thispaytransaction from the agent
with public key 7xxxx.Suppose the payers in a pay transaction are identified by the list:[(3146, 0), (7359, 3)]and suppose pair 3 in transaction 7359 is (3.2, 8xxxx). Then the total
amount of coins flowing into this pay transaction is 2.1 + 3.2, and
this amount is disbursed to payees.Fig.3: Coins flowing into and out of a pay transaction.Coins flowing out of a pay transaction to payeesThe outflow of coins is specified by an array of value-payee pairs,
exactly as in a create transaction.
The transaction clears all coins: the total inflow from payers is
equal to the total outflow to payees in a transaction.The system may provide incentives, such as payment of coins, to
managers (e.g. banks) of cryptocurrencies. In this case, one of the
payees is the bank itself.Managing amounts spent in a transactionA transaction-id, index pair --- such as (3146, 0)
identifies a 2-tuple such as (2.1, 7xxxx...); this 2-tuple asserts
that the agent
with public key 7xxxx received 2.1 units of coin in 
transaction 3146. Theentire amountspecified in the 2-tuple (2.1 in our
example) is value that flows into the transaction. What should this
agent do if it
wants to put in more than 2.1 coins into the transaction? Or less than
2.1 coins?To put in more value, the bank identifies other transaction-id, index
pairs in which this agent received coins. For example, say that (4539, 2)
identifies a 2-tuple (3.2, 7xxxx), and assume that the payers in this transaction
are specified by the pairs (3146, 0) and (4539, 2). The pair (3146, 0)
asserts that agent (7xxxx) received 2.1 coins and the pair (4539, 2)
asserts that the same agent received 3.2 coins. So the total amount of 
coins flowing into this
transaction from this agent (7xxxx) includes 2.1 + 3.2.To put in less value, the agent acts as both payer and payee; the net
value that this agent pays out to other agents is the difference
between the amount that this agent puts in and takes out. For example,
if agent with public key 7xxxx wants to put in 1.9 coins into this
transaction its payer information can be given by the transaction-id,
index pair (3146, 0) which asserts that the agent received 2.1 coins
and the same agent is a payee that withdraws 0.2 coins.Preventing Double SpendingHow does the system prevent an agent from using the
same coin twice?For example, the transaction id, index pair
(3146, 0) identifies the 2-tuple (2.1, 7xxxx...); this tuple
asserts that the agent
with public key 7xxxx received 2.1 units of coin in 
transaction 3146. Why can't the agent with public key 7xxxx use the
2.1 coins that it received to buy items from Amazon and later use
the same 2.1 coins to buy more items from Walmart?The bank checks that the agent hasn't already spent the coin that it
is putting into a transaction.Before permitting the transaction that double-spends the 2.1 coins
with Walmart, the bank inspects the tamper-evident ledger
for all transactions after transaction 3146 and before the Walmart
transaction to ensure that the agent (7xxxx) hasn't already spent the
2.1 coins that it got in transaction 3146. The transaction that spends
the coins at Amazon will show up in the ledger, and so the transaction
with Walmart will not be allowed.Every agent that has the bank's hash pointer to the end of the
tamper-evident ledger can inspect the ledger to check that
double-spending hasn't occurred.The bank signs a valid transaction and appends it to the
tamper-evident ledger. All agents can see the bank's signature and
verify that nobody (not even the bank) has tampered with the
tamper-evident ledger.Example of a pay transactionAn example of the specification of a pay transaction is:(9431, pay,
  [(3146, 0), (4731, 2)],
  [(0.7, 7xxxx...), (4.6, 9xxxx...)]
).The id of this transaction is 9431; the type of the transaction is
pay; the payers into the transaction are identified by the pairs of
(transaction-id, index): (3146, 0), and (4731, 2); and the
payee array is [(0.7, 7xxxx...), (4.6, 9xxxx...)].Transaction validityThe bank appends a transaction toLif and only if the
transaction is valid. The bank checks for validity by carrying out the
following steps:The bank verifies that the payers into the transaction signed the
  transaction.The bank checks that the total value of coins paid out from the
  transaction does not
  exceed the total value paid in to the transaction. (If the value paid
  in exceeds the value paid out then the bank takes the difference as
  a transaction fee. More about fees later.)The bank verifies that the payers' claims to have received coins in
  previous transactions is genuine. For example,
  if the agent with
  public key 7xxxx... claims to have received coins worth 2.1 in the
  transaction with id 10, and payee array index 0, then the bank
  verifies this claim by that transaction.The bank ensures that coins paid into the transaction haven't
  already been spent.Optimizations: Blocks and Block ChainVerifying large numbers of small transactions requires more
computation than verifying small numbers of blocks of many transactions.
Ablock chainis a tamper-evident ledger in which
each element of the ledger is a block consisting of many transactions.
A block of transactions can be aggregated into a single large
transaction by aggregating all the payers and payees of the smaller
transactions.The amount of computation decreases as the number of transactions in a
block increases. The time required to fill a block with transactions
is larger when the number of transactions to fill the block increases.Checking the Trusted AgentConsider a system in which the trusted agent broadcasts its current
copy of the tamper-evident ledger to all agents.
Every agent can inspect its copy of the tamper-evident ledger to determine whether the
ledger has been tampered with. So, every agent can validate its trust
in the trusted agent; however, this validation
suffers from a crucial problem: Agents may only
have copies ofold, staleversions of the ledger.
By the time that an agent receives a copy of the ledger, the trusted
agent may have added more transactions to the ledger.Fig.4: Old Copy is a Prefix of the Block ChainAn old copy of the ledger can differ from the current copy in only one
way: the current copy may have transactions appended to the end
of the old copy. So all agents can validatepastbehavior of the
trusted agent. An agent cannot, however, treat its copy of the ledger
as the master copy because the agent may not have the transactions
added most recently to the ledger.In the next module we will see how the Bitcoin algorithm addresses
this problem.Advantages of this cryptocurrencyAny agent can get a copy of the tamper-evident ledger and verify that all
transactions in the ledger are valid. Any agent can verify that
the only way in which the ledger is modified is that elements are
append to its tail; all that the agent needs to do is to check that
the pointer to the tail is modified only by appending
elements. Because the ledger is tamper-evident, an agent can check
that the ledger doesn't change while the hash pointer to the end
of the ledger doesn't change.The bank can't forge a transaction because all payers sign the
transaction. Agents can remain anonymous because an agent's only
public information is the agent's public key, and an agent can create
multiple public keys. Every agent can verify the correctness of every
transaction.Disadvantages of this cryptocurrencyUsers may not trust the bank. Transactions are not private because the
bank has a record of all transactions. And the bank is a single point
of failure.Next we'll look at Bitcoin's algorithms for
implementing a cryptocurrency without trusted agents.ReviewWhat iscollision resistance? Why is it relevant to
  cryptocurrencies?Consider the example ofhidingin which Megan hides her
  predictions for the winner of the World Cup. Suppose you could
  choose two different random number generators for creating the
  secret \(r\); in one case the bits of \(r\) are uncorrelated, and in
  the other they are highly correlated. Which generator would you use
  and why?What isbinding? Why is it relevant to hiding information?What ispuzzle friendly?What is atamper-evident data structure? How would you
  implement a tamper-evident linear list? A tamper-evident tree?Describe the algorithm for cryptocurrency with a trusted agent to
  someone who knows absolutely nothing about cryptocurrency.How could the algorithm for cryptocurrency be used for a collection
  of distributed agents to keep track of a sequence of events, other
  than buying/selling currency?K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology



Figure Link: https://kmchandy.github.io/Crypto/CryptoCurrencyIntroduction/Slide1.jpg

Caption: Fig.1: Hash Pointer points to a Tamper-Evident Block
    of Data


By the pigeon-hole principle, we will find a collision in at most
    \(2^{n} + 1 = 17\) iterations.
    This  brute-force algorithm uses space \(2^{n}\) and finds a
    collision in at most  \(2^{n} + 1\) steps.
From theBirthday Paradoxa collision will be found with high probability
in \(2^{n/2}\) iterations though
the worse-case time is \(2^{n}+1\). If \(n = 256\) then a collision
will be found with high probability in \(2^{128}\) iterations; 
however, executing \(2^{128}\) steps is still intractable.For \(H\) to be
    collision resistant the output of \(H\) 
    must be \(n\)-bits for large \(n\). For example \(n = 256\) in the
    SHA-256 hash function.Commitment using HashesYou bet that your soccer friend, Megan, cannot predict the winner
    of the 2022 World Cup. Megan puts the name of the predicted
    winner, \(W\), in an envelope and
    gives it to a trusted third party. After the World Cup is over,
    the third party reveals Megan's prediction, and at that point you
    can find out whether Megan's prediction was accurate.The trusted third party provides two services:Hiding: You can't find out Megan's prediction until the third party reveals it.Binding: Megan can't change her prediction after giving it to the third
  party.Can we use a hash function instead of a trusted third party?HidingLet's try the following idea. Megan commits to \(W\) in the following
way. She announces a hash function, \(H\),
and the hash, \(y\), where \(y = H(W)\). You know \(H\) and \(y\). 
After the World Cup is over, she reveals her prediction, \(W\). At
this point you can verify that \(y = H(W)\).Does the hash function provide the services of the third party?
Can you discover Megan's prediction before she reveals it?It's easy. There are only 32 teams playing. Compute \(H(x)\) where
    \(x\) runs over each of the 32 teams. One of those teams has to be
\(W\). You can discover her prediction in at most 210 steps.Let's try another algorithm.  Megan selects a secret value \(r\) which
she keeps to herself. Instead
of giving you \(H(W)\), she gives you \(y\) where \(y = H(r + W)\) and
where \(+\) indicates concatenation of strings. Can
you discover \(W\) from \(H\) and \(y\) without knowing \(r\)?A brute-force solution is to try every combination of \(r\) and
\(W\). If \(r\) is obtained from a distribution that is spread out,
then finding \(W\) without knowing \(r\)  take so
much time that it is practically impossible.Hiding: Given \(H, y\), where \(y = H(r + W)\) and \(r\) is a secret,
discovering \(W\) is intractable.BindingDoes the hash function \(H\) and the secret \(r\) provide both
services of the trusted third party? Is Megan bound to her prediction
or can she change her "prediction" after knowing the winner of the
World Cup?Suppose Megan has values \(r\) and \(r'\) such that H(r + 'Brazil') =
H(r' + 'Italy'). After the World Cup is over, she can announce that
her secret is \(r\) if Brazil wins, and announce that it is \(r'\) if
Italy wins.A hash function \(H\) isbindingif finding pairs \((x,y)\) and \((x',y')\)
where \(y \neq y'\) such that:
\(H(x + y) = H(x' + y')\)
is intractable.Suppose you give Megan a hash function that is binding. Then she cannot find
  (in reasonable time) values \(r_{j}\) to match country \(C_{j}\)
such that\(H(r_{0} + C_{0}) = H(r_{1} + C_{1})  = H(r_{2} + C_{2})  = \ldots\).and so she can't wait for the winner \(C_{j}\) to be announced before announcing
her secret \(r_{j}\).In summary, we can use a hash function that is hiding and binding to
play the role of a trusted third party in a commitment.Puzzle FriendlyThe concept ofpuzzle friendlyis related tohiding.
Let \(r\) be a value picked from a spread-out distribution. Let
\(H\) map arbitrary length strings to \(n\)-bit strings.
Consider the following problem:
    Given \(H\), \(r\), and an \(n\)-bit value \(y\), compute any \(x\) such that
\(H(r+x) = y\).In this problem, as opposed to thehidingproblem, we are given \(r\) and not \(x\),The hash function \(H\) is said to bepuzzle friendlyexactly when any algorithm to solve this problem is about as slow
as a brute-force algorithm which checks \(H(r+x) = y\) for random values
of \(x\).
The number of steps taken by any
algorithm that solves this problem is not significantly lower than
\(2^{n}\).Now, let's look at the following related problem.  Given \(H\), \(r\),
and a set \(Y\) of \(n\)-bit strings, compute any \(x\) such that
\(H(r+x) \in Y\).  If \(Y\) consists of a single element \(y\) then
this problem is the same as that in the previous paragraph.  If \(Y\)
is a set of all \(n\)-bit strings then this problem is trivial because
any \(x\) solves the problem.  The probability that a random value
hashes to an element of \(Y\) is proportional to the cardinality of
\(Y\).  The cardinality of \(Y\) controls the expected time to
solve the puzzle.The hash function \(H\) ispuzzle friendlywhen
given \(H\), \(r\), and a set \(Y\) of \(n\)-bit values, the time
required to compute any \(x\) such that \(H(r+x) \in Y\) is
not significantly lower than \(2^{n} / |Y|\), where \(|Y|\) is the
cardinality of \(Y\).A Cryptographic Hash FunctionA cryptographic hash function is one that is collision resistant,
hiding and puzzle-friendly.Hashing Inputs of Arbitrary LengthLet \(f\) be a function that operates on input strings
of fixed length and produces output strings of fixed length. Let the
input and output strings of \(f\) have lengths \(M + N\) and \(M\),
respectively. We look at functions where \(N > 0\), and since the
output is smaller than the input, \(f\) is called acompression
function.We can use function \(f\) to define a function \(g\) whose inputs are strings of arbitrary
lengths and whose outputs are strings of length \(M\).
Example code for \(g\) is given below whereInitialValueis a given constant string of length \(M\).def g(y):
    output = InitialValue
    // pad y so that it's length is a multiple of N
    if len(y)%N > 0:  y = y + "0"*(N - len(y)%N)
    // partition y into blocks of size N
    blocks = [y[i: i+N] for i in range(0, len(y), N)]
    // Apply function f to the concatenation of the
    // previous output (length M) with each block
    // (length N) to get the next output (length M).
    for block in blocks: output = f(output+block)
    return outputHash PointersA hash pointer to an item \(D\) of data is a pair \((ptr, H(D))\)
where \(ptr\) is a pointer that 
points to \(D\), and \(H\) is a cryptographic hash function.Any data structure with pointers can be converted into a
data structure with hash pointers: merely replace a pointer \(ptr\) to
\(D\) by \((ptr, H(D))\).Tamper-Evident Data StructuresSingle BlockA simple example of a tamper-evident structure is a single block of
data D which is pointed to by a hash pointer consisting of a regular
pointer and a hash H(D).Fig.1: Hash Pointer points to a Tamper-Evident Block
    of DataAssume that a malicious agent cannot modifyboththe hash
pointer and the data that it points to. If an 
agent
    changes D to D' then the tampering can be discovered because the
    hash pointer won't match the data that it is pointing to:
    \(H(D') \neq H(D)\).Tamper-Evident Linked ListLet's look at linear linked list to which elements can be appended but
not deleted. The \(i\)-th element appended to the list points to the
\((i-1)\)-th element. 
Let's replace the pointers in the list by hashed
pointers.Fig.2: Hash Pointer points to a Tamper-Evident List
    of DataThe \(j\)-th element of the list, \(j > 0\), consists of data,
\(D_{j}\), and a hash pointer that points to the \(j - 1\)-th
  element of the list. The hash pointer in the \(j\)-th element
consists of a regular pointer, \(ptr_{j}\), which points to the \(j - 1\)-th
  element of the list, and a hashed value, \(HA_{j}\), which is a cryptographic hash of theentire\(j-1\)th element consisting  of \(D_{j-1}\), \(ptr_{j-1}\)
  and \(HA_{j-1}\).
The \(0\)-th element is called thegenesiselement and has
default values. The list is accessed by a hash pointer that points to
the last element of the list; let this pointer be \(ptr_{n}, HA_{n}\).Assume that agents cannot modify \(ptr_{n}, HA_{n}\). Then,
can any agent determine whether the list has been tampered with?Suppose the agent modifies \(D_{j}\), \(ptr_{j}\) or
\(HA_{j}\) for any \(j < n\). Any agent can detect this tampering because the
hash value \(HA_{j+1}\) will no longer match the \(j\)-th element of
the list. If
the malicious agent also modifies \(HA_{j+1}\) then 
hash value \(HA_{j+2}\) will no longer match the \(j+1\)-th element.By induction on \(j\), any agent can detect tampering with the list
provided malicious agents do not also modify the hash pointer to the
last element of the last.Tamper-Evident Acyclic Graphs and Merkle
TreesThe idea described in the previous paragraph to convert linear linked
lists can be used to convert directed acyclic graphs, in which nodes are
connected by pointers, into tamper-evident graphs. A specific case of
a directed acyclic graph is a rooted tree.AMerkle treeis a special case of a binary balanced tree in which
data items are stored only in the leaves.
Nodes that are not leaves 
contain only hash pointers to nodes in the next level down.
To prove that an element at the leaf is a member of the tree we need only the
\(log_{2}(n)\) hash pointers on the path from the root to that
leaf. By contrast, to prove that an element is a member of a linear
list we need to inspect \(O(n)\) elements, on average.Keys and Signed MessagesYou can create a random public-key, private-key pairby calling a function on your computer.With high probability, nobody else has this
specific pair of keys. Each individual's private key is a secret held
by that individual. Public keys are accessible by everybody.Sending messages securelyKeys are used to send messages securely.
Kamala sends a secure message to Joe by encrypting the message with
Joe's public key; Joe decrypts the message using Joe's private key.
An agent cannot decrypt the encrypted message without Joe's private key.Signing messagesSuppose Kamala needs to send a signed message to Joe while
ensuring that nobody can forge her signature. She encrypts the messageMwith her private key to get an encrypted messageM',
and sends the pair(M, M')securely to Joe, i.e., she encrypts(M, M')with Joe's public key, and sends the resulting
encrypted message to Joe.  When Joe receives the message, Joe decrypts
it using his private key to get(M, M'). Then Joe decryptsM'using Kamala's public key to get the decrypted messageM''. IfM'' = Mthen Joe knows that Kamala sentMbecause only an agent with Kamala's private signature could
have sent that message.Cryptocurrency Managed by a Trusted
AgentLet's start with a digital currency managed by a trusted agent that we will
call a bank. Later, we will
look at a consensus algorithm --- very different from Paxos and Byzantine
Generals --- which will allow cryptocurrencies without trusted
agent.The bank maintains a tamper-evident linear listLof transactions that we call a
tamper-evidentledger. 
Any agent can get a copy of the ledger.
This tamper-evident ledger is the foundation of the currency.A transaction is one of
two types:createorpay.
In a pay transaction,payersgive coins that they possess topayees. An agent can be both a 
payer and payee of the same transaction.
In acreatetransaction the bank creates coins that it gives to
agents --- the payees of the transaction; the bank acts as the payer.For this system to be trusted the bank must follow some protocol that determines
how and when the bank creates coins. We won't discuss these protocols.Apaytransaction is signed by all payers of the transaction.
Acreatetransaction is signed by the bank.
We discussed digital signatures and keys earlier.Each element of the tamper-evident ledger has:a unique id;the type of the transaction, either create or pay;list of payers: only for pay transactions --- a list
  indicating the agents who pay coins into the transaction and the
  amounts that they put in;array of value-payee pairs: for both create and pay transactions --- an
  array of pairs(value, payee public key), where each
  pair in the array indicates that coins of the specified value are
  given to the payee with the specified public key.Example of a create transactionAn example of acreatetransaction is:(3146, create,  [(2.1, 7xxxx...), (3.2, 8xxxx)]).The id of this transaction is 3146, the type of the transaction is
create, and the array of value-payee pairs is[(2.1, 7xxxx...), (3.2, 8xxx)]In this transaction the bank creates a coin of value 2.1 and gives it
to the agent with public key 7xxxx..., and the bank also
creates a coin of value 3.2 and gives it
to the agent with public key 8xxxx...The pair:(transaction id, index into array of value-payee pairs)uniquely identifies a (value, payee) tuple.For example (3146, 0) --- 
transaction id 3146, and array index 0 --- identifies value-payee[0] of
transaction 3146 which is specified by the 2-tuple: (2.1, 7xxxx...). So, the
transaction id and index, (3146, 0), tells everybody that the agent
with public key 7xxxx received 2.1 units of coin in transaction 3146.When this
transaction is in a tamper-evident ledger, every agent from that point
onwards knows that agent 7xxxx received 2.1 coins. Any modifications of this
record can be detected.Likewise, (3146, 1) --- 
transaction id 3146, and array index 1 --- identifies value-payee[1] which
is the 2-tuple (3.2, 8xxxx...).Pay transactionCoins are transferred from payers to payees in a pay transaction.Coins flowing into a pay transaction from payersThe payers are identified by a list of 2-tuples, where each 2-tuple is(transaction id, index into array of value-payee pairs)wheretransaction idis the id of the transaction in the
tamper-evident ledger. As we said earlier, this pair uniquely identifies an agent and a
value that this agent acquired in this transaction.
For example the pair --- transaction id, index --- such as (3146, 0)
identifies the 2-tuple (2.1, 7xxxx...); this 2-tuple asserts
that the agent
with public key 7xxxx received 2.1 units of coin in 
transaction 3146. Theentire amountspecified in the 2-tuple (2.1 in our
example) is value that flows into thispaytransaction from the agent
with public key 7xxxx.Suppose the payers in a pay transaction are identified by the list:[(3146, 0), (7359, 3)]and suppose pair 3 in transaction 7359 is (3.2, 8xxxx). Then the total
amount of coins flowing into this pay transaction is 2.1 + 3.2, and
this amount is disbursed to payees.Fig.3: Coins flowing into and out of a pay transaction.Coins flowing out of a pay transaction to payeesThe outflow of coins is specified by an array of value-payee pairs,
exactly as in a create transaction.
The transaction clears all coins: the total inflow from payers is
equal to the total outflow to payees in a transaction.The system may provide incentives, such as payment of coins, to
managers (e.g. banks) of cryptocurrencies. In this case, one of the
payees is the bank itself.Managing amounts spent in a transactionA transaction-id, index pair --- such as (3146, 0)
identifies a 2-tuple such as (2.1, 7xxxx...); this 2-tuple asserts
that the agent
with public key 7xxxx received 2.1 units of coin in 
transaction 3146. Theentire amountspecified in the 2-tuple (2.1 in our
example) is value that flows into the transaction. What should this
agent do if it
wants to put in more than 2.1 coins into the transaction? Or less than
2.1 coins?To put in more value, the bank identifies other transaction-id, index
pairs in which this agent received coins. For example, say that (4539, 2)
identifies a 2-tuple (3.2, 7xxxx), and assume that the payers in this transaction
are specified by the pairs (3146, 0) and (4539, 2). The pair (3146, 0)
asserts that agent (7xxxx) received 2.1 coins and the pair (4539, 2)
asserts that the same agent received 3.2 coins. So the total amount of 
coins flowing into this
transaction from this agent (7xxxx) includes 2.1 + 3.2.To put in less value, the agent acts as both payer and payee; the net
value that this agent pays out to other agents is the difference
between the amount that this agent puts in and takes out. For example,
if agent with public key 7xxxx wants to put in 1.9 coins into this
transaction its payer information can be given by the transaction-id,
index pair (3146, 0) which asserts that the agent received 2.1 coins
and the same agent is a payee that withdraws 0.2 coins.Preventing Double SpendingHow does the system prevent an agent from using the
same coin twice?For example, the transaction id, index pair
(3146, 0) identifies the 2-tuple (2.1, 7xxxx...); this tuple
asserts that the agent
with public key 7xxxx received 2.1 units of coin in 
transaction 3146. Why can't the agent with public key 7xxxx use the
2.1 coins that it received to buy items from Amazon and later use
the same 2.1 coins to buy more items from Walmart?The bank checks that the agent hasn't already spent the coin that it
is putting into a transaction.Before permitting the transaction that double-spends the 2.1 coins
with Walmart, the bank inspects the tamper-evident ledger
for all transactions after transaction 3146 and before the Walmart
transaction to ensure that the agent (7xxxx) hasn't already spent the
2.1 coins that it got in transaction 3146. The transaction that spends
the coins at Amazon will show up in the ledger, and so the transaction
with Walmart will not be allowed.Every agent that has the bank's hash pointer to the end of the
tamper-evident ledger can inspect the ledger to check that
double-spending hasn't occurred.The bank signs a valid transaction and appends it to the
tamper-evident ledger. All agents can see the bank's signature and
verify that nobody (not even the bank) has tampered with the
tamper-evident ledger.Example of a pay transactionAn example of the specification of a pay transaction is:(9431, pay,
  [(3146, 0), (4731, 2)],
  [(0.7, 7xxxx...), (4.6, 9xxxx...)]
).The id of this transaction is 9431; the type of the transaction is
pay; the payers into the transaction are identified by the pairs of
(transaction-id, index): (3146, 0), and (4731, 2); and the
payee array is [(0.7, 7xxxx...), (4.6, 9xxxx...)].Transaction validityThe bank appends a transaction toLif and only if the
transaction is valid. The bank checks for validity by carrying out the
following steps:The bank verifies that the payers into the transaction signed the
  transaction.The bank checks that the total value of coins paid out from the
  transaction does not
  exceed the total value paid in to the transaction. (If the value paid
  in exceeds the value paid out then the bank takes the difference as
  a transaction fee. More about fees later.)The bank verifies that the payers' claims to have received coins in
  previous transactions is genuine. For example,
  if the agent with
  public key 7xxxx... claims to have received coins worth 2.1 in the
  transaction with id 10, and payee array index 0, then the bank
  verifies this claim by that transaction.The bank ensures that coins paid into the transaction haven't
  already been spent.Optimizations: Blocks and Block ChainVerifying large numbers of small transactions requires more
computation than verifying small numbers of blocks of many transactions.
Ablock chainis a tamper-evident ledger in which
each element of the ledger is a block consisting of many transactions.
A block of transactions can be aggregated into a single large
transaction by aggregating all the payers and payees of the smaller
transactions.The amount of computation decreases as the number of transactions in a
block increases. The time required to fill a block with transactions
is larger when the number of transactions to fill the block increases.Checking the Trusted AgentConsider a system in which the trusted agent broadcasts its current
copy of the tamper-evident ledger to all agents.
Every agent can inspect its copy of the tamper-evident ledger to determine whether the
ledger has been tampered with. So, every agent can validate its trust
in the trusted agent; however, this validation
suffers from a crucial problem: Agents may only
have copies ofold, staleversions of the ledger.
By the time that an agent receives a copy of the ledger, the trusted
agent may have added more transactions to the ledger.Fig.4: Old Copy is a Prefix of the Block ChainAn old copy of the ledger can differ from the current copy in only one
way: the current copy may have transactions appended to the end
of the old copy. So all agents can validatepastbehavior of the
trusted agent. An agent cannot, however, treat its copy of the ledger
as the master copy because the agent may not have the transactions
added most recently to the ledger.In the next module we will see how the Bitcoin algorithm addresses
this problem.Advantages of this cryptocurrencyAny agent can get a copy of the tamper-evident ledger and verify that all
transactions in the ledger are valid. Any agent can verify that
the only way in which the ledger is modified is that elements are
append to its tail; all that the agent needs to do is to check that
the pointer to the tail is modified only by appending
elements. Because the ledger is tamper-evident, an agent can check
that the ledger doesn't change while the hash pointer to the end
of the ledger doesn't change.The bank can't forge a transaction because all payers sign the
transaction. Agents can remain anonymous because an agent's only
public information is the agent's public key, and an agent can create
multiple public keys. Every agent can verify the correctness of every
transaction.Disadvantages of this cryptocurrencyUsers may not trust the bank. Transactions are not private because the
bank has a record of all transactions. And the bank is a single point
of failure.Next we'll look at Bitcoin's algorithms for
implementing a cryptocurrency without trusted agents.ReviewWhat iscollision resistance? Why is it relevant to
  cryptocurrencies?Consider the example ofhidingin which Megan hides her
  predictions for the winner of the World Cup. Suppose you could
  choose two different random number generators for creating the
  secret \(r\); in one case the bits of \(r\) are uncorrelated, and in
  the other they are highly correlated. Which generator would you use
  and why?What isbinding? Why is it relevant to hiding information?What ispuzzle friendly?What is atamper-evident data structure? How would you
  implement a tamper-evident linear list? A tamper-evident tree?Describe the algorithm for cryptocurrency with a trusted agent to
  someone who knows absolutely nothing about cryptocurrency.How could the algorithm for cryptocurrency be used for a collection
  of distributed agents to keep track of a sequence of events, other
  than buying/selling currency?K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology



Figure Link: https://kmchandy.github.io/Crypto/CryptoCurrencyIntroduction/Slide1.jpg

Caption: Fig.1: Hash Pointer points to a Tamper-Evident Block
    of Data


For \(H\) to be
    collision resistant the output of \(H\) 
    must be \(n\)-bits for large \(n\). For example \(n = 256\) in the
    SHA-256 hash function.Commitment using HashesYou bet that your soccer friend, Megan, cannot predict the winner
    of the 2022 World Cup. Megan puts the name of the predicted
    winner, \(W\), in an envelope and
    gives it to a trusted third party. After the World Cup is over,
    the third party reveals Megan's prediction, and at that point you
    can find out whether Megan's prediction was accurate.The trusted third party provides two services:Hiding: You can't find out Megan's prediction until the third party reveals it.Binding: Megan can't change her prediction after giving it to the third
  party.Can we use a hash function instead of a trusted third party?HidingLet's try the following idea. Megan commits to \(W\) in the following
way. She announces a hash function, \(H\),
and the hash, \(y\), where \(y = H(W)\). You know \(H\) and \(y\). 
After the World Cup is over, she reveals her prediction, \(W\). At
this point you can verify that \(y = H(W)\).Does the hash function provide the services of the third party?
Can you discover Megan's prediction before she reveals it?It's easy. There are only 32 teams playing. Compute \(H(x)\) where
    \(x\) runs over each of the 32 teams. One of those teams has to be
\(W\). You can discover her prediction in at most 210 steps.Let's try another algorithm.  Megan selects a secret value \(r\) which
she keeps to herself. Instead
of giving you \(H(W)\), she gives you \(y\) where \(y = H(r + W)\) and
where \(+\) indicates concatenation of strings. Can
you discover \(W\) from \(H\) and \(y\) without knowing \(r\)?A brute-force solution is to try every combination of \(r\) and
\(W\). If \(r\) is obtained from a distribution that is spread out,
then finding \(W\) without knowing \(r\)  take so
much time that it is practically impossible.Hiding: Given \(H, y\), where \(y = H(r + W)\) and \(r\) is a secret,
discovering \(W\) is intractable.BindingDoes the hash function \(H\) and the secret \(r\) provide both
services of the trusted third party? Is Megan bound to her prediction
or can she change her "prediction" after knowing the winner of the
World Cup?Suppose Megan has values \(r\) and \(r'\) such that H(r + 'Brazil') =
H(r' + 'Italy'). After the World Cup is over, she can announce that
her secret is \(r\) if Brazil wins, and announce that it is \(r'\) if
Italy wins.A hash function \(H\) isbindingif finding pairs \((x,y)\) and \((x',y')\)
where \(y \neq y'\) such that:
\(H(x + y) = H(x' + y')\)
is intractable.Suppose you give Megan a hash function that is binding. Then she cannot find
  (in reasonable time) values \(r_{j}\) to match country \(C_{j}\)
such that\(H(r_{0} + C_{0}) = H(r_{1} + C_{1})  = H(r_{2} + C_{2})  = \ldots\).and so she can't wait for the winner \(C_{j}\) to be announced before announcing
her secret \(r_{j}\).In summary, we can use a hash function that is hiding and binding to
play the role of a trusted third party in a commitment.Puzzle FriendlyThe concept ofpuzzle friendlyis related tohiding.
Let \(r\) be a value picked from a spread-out distribution. Let
\(H\) map arbitrary length strings to \(n\)-bit strings.
Consider the following problem:
    Given \(H\), \(r\), and an \(n\)-bit value \(y\), compute any \(x\) such that
\(H(r+x) = y\).In this problem, as opposed to thehidingproblem, we are given \(r\) and not \(x\),The hash function \(H\) is said to bepuzzle friendlyexactly when any algorithm to solve this problem is about as slow
as a brute-force algorithm which checks \(H(r+x) = y\) for random values
of \(x\).
The number of steps taken by any
algorithm that solves this problem is not significantly lower than
\(2^{n}\).Now, let's look at the following related problem.  Given \(H\), \(r\),
and a set \(Y\) of \(n\)-bit strings, compute any \(x\) such that
\(H(r+x) \in Y\).  If \(Y\) consists of a single element \(y\) then
this problem is the same as that in the previous paragraph.  If \(Y\)
is a set of all \(n\)-bit strings then this problem is trivial because
any \(x\) solves the problem.  The probability that a random value
hashes to an element of \(Y\) is proportional to the cardinality of
\(Y\).  The cardinality of \(Y\) controls the expected time to
solve the puzzle.The hash function \(H\) ispuzzle friendlywhen
given \(H\), \(r\), and a set \(Y\) of \(n\)-bit values, the time
required to compute any \(x\) such that \(H(r+x) \in Y\) is
not significantly lower than \(2^{n} / |Y|\), where \(|Y|\) is the
cardinality of \(Y\).A Cryptographic Hash FunctionA cryptographic hash function is one that is collision resistant,
hiding and puzzle-friendly.Hashing Inputs of Arbitrary LengthLet \(f\) be a function that operates on input strings
of fixed length and produces output strings of fixed length. Let the
input and output strings of \(f\) have lengths \(M + N\) and \(M\),
respectively. We look at functions where \(N > 0\), and since the
output is smaller than the input, \(f\) is called acompression
function.We can use function \(f\) to define a function \(g\) whose inputs are strings of arbitrary
lengths and whose outputs are strings of length \(M\).
Example code for \(g\) is given below whereInitialValueis a given constant string of length \(M\).def g(y):
    output = InitialValue
    // pad y so that it's length is a multiple of N
    if len(y)%N > 0:  y = y + "0"*(N - len(y)%N)
    // partition y into blocks of size N
    blocks = [y[i: i+N] for i in range(0, len(y), N)]
    // Apply function f to the concatenation of the
    // previous output (length M) with each block
    // (length N) to get the next output (length M).
    for block in blocks: output = f(output+block)
    return outputHash PointersA hash pointer to an item \(D\) of data is a pair \((ptr, H(D))\)
where \(ptr\) is a pointer that 
points to \(D\), and \(H\) is a cryptographic hash function.Any data structure with pointers can be converted into a
data structure with hash pointers: merely replace a pointer \(ptr\) to
\(D\) by \((ptr, H(D))\).Tamper-Evident Data StructuresSingle BlockA simple example of a tamper-evident structure is a single block of
data D which is pointed to by a hash pointer consisting of a regular
pointer and a hash H(D).Fig.1: Hash Pointer points to a Tamper-Evident Block
    of DataAssume that a malicious agent cannot modifyboththe hash
pointer and the data that it points to. If an 
agent
    changes D to D' then the tampering can be discovered because the
    hash pointer won't match the data that it is pointing to:
    \(H(D') \neq H(D)\).Tamper-Evident Linked ListLet's look at linear linked list to which elements can be appended but
not deleted. The \(i\)-th element appended to the list points to the
\((i-1)\)-th element. 
Let's replace the pointers in the list by hashed
pointers.Fig.2: Hash Pointer points to a Tamper-Evident List
    of DataThe \(j\)-th element of the list, \(j > 0\), consists of data,
\(D_{j}\), and a hash pointer that points to the \(j - 1\)-th
  element of the list. The hash pointer in the \(j\)-th element
consists of a regular pointer, \(ptr_{j}\), which points to the \(j - 1\)-th
  element of the list, and a hashed value, \(HA_{j}\), which is a cryptographic hash of theentire\(j-1\)th element consisting  of \(D_{j-1}\), \(ptr_{j-1}\)
  and \(HA_{j-1}\).
The \(0\)-th element is called thegenesiselement and has
default values. The list is accessed by a hash pointer that points to
the last element of the list; let this pointer be \(ptr_{n}, HA_{n}\).Assume that agents cannot modify \(ptr_{n}, HA_{n}\). Then,
can any agent determine whether the list has been tampered with?Suppose the agent modifies \(D_{j}\), \(ptr_{j}\) or
\(HA_{j}\) for any \(j < n\). Any agent can detect this tampering because the
hash value \(HA_{j+1}\) will no longer match the \(j\)-th element of
the list. If
the malicious agent also modifies \(HA_{j+1}\) then 
hash value \(HA_{j+2}\) will no longer match the \(j+1\)-th element.By induction on \(j\), any agent can detect tampering with the list
provided malicious agents do not also modify the hash pointer to the
last element of the last.Tamper-Evident Acyclic Graphs and Merkle
TreesThe idea described in the previous paragraph to convert linear linked
lists can be used to convert directed acyclic graphs, in which nodes are
connected by pointers, into tamper-evident graphs. A specific case of
a directed acyclic graph is a rooted tree.AMerkle treeis a special case of a binary balanced tree in which
data items are stored only in the leaves.
Nodes that are not leaves 
contain only hash pointers to nodes in the next level down.
To prove that an element at the leaf is a member of the tree we need only the
\(log_{2}(n)\) hash pointers on the path from the root to that
leaf. By contrast, to prove that an element is a member of a linear
list we need to inspect \(O(n)\) elements, on average.Keys and Signed MessagesYou can create a random public-key, private-key pairby calling a function on your computer.With high probability, nobody else has this
specific pair of keys. Each individual's private key is a secret held
by that individual. Public keys are accessible by everybody.Sending messages securelyKeys are used to send messages securely.
Kamala sends a secure message to Joe by encrypting the message with
Joe's public key; Joe decrypts the message using Joe's private key.
An agent cannot decrypt the encrypted message without Joe's private key.Signing messagesSuppose Kamala needs to send a signed message to Joe while
ensuring that nobody can forge her signature. She encrypts the messageMwith her private key to get an encrypted messageM',
and sends the pair(M, M')securely to Joe, i.e., she encrypts(M, M')with Joe's public key, and sends the resulting
encrypted message to Joe.  When Joe receives the message, Joe decrypts
it using his private key to get(M, M'). Then Joe decryptsM'using Kamala's public key to get the decrypted messageM''. IfM'' = Mthen Joe knows that Kamala sentMbecause only an agent with Kamala's private signature could
have sent that message.Cryptocurrency Managed by a Trusted
AgentLet's start with a digital currency managed by a trusted agent that we will
call a bank. Later, we will
look at a consensus algorithm --- very different from Paxos and Byzantine
Generals --- which will allow cryptocurrencies without trusted
agent.The bank maintains a tamper-evident linear listLof transactions that we call a
tamper-evidentledger. 
Any agent can get a copy of the ledger.
This tamper-evident ledger is the foundation of the currency.A transaction is one of
two types:createorpay.
In a pay transaction,payersgive coins that they possess topayees. An agent can be both a 
payer and payee of the same transaction.
In acreatetransaction the bank creates coins that it gives to
agents --- the payees of the transaction; the bank acts as the payer.For this system to be trusted the bank must follow some protocol that determines
how and when the bank creates coins. We won't discuss these protocols.Apaytransaction is signed by all payers of the transaction.
Acreatetransaction is signed by the bank.
We discussed digital signatures and keys earlier.Each element of the tamper-evident ledger has:a unique id;the type of the transaction, either create or pay;list of payers: only for pay transactions --- a list
  indicating the agents who pay coins into the transaction and the
  amounts that they put in;array of value-payee pairs: for both create and pay transactions --- an
  array of pairs(value, payee public key), where each
  pair in the array indicates that coins of the specified value are
  given to the payee with the specified public key.Example of a create transactionAn example of acreatetransaction is:(3146, create,  [(2.1, 7xxxx...), (3.2, 8xxxx)]).The id of this transaction is 3146, the type of the transaction is
create, and the array of value-payee pairs is[(2.1, 7xxxx...), (3.2, 8xxx)]In this transaction the bank creates a coin of value 2.1 and gives it
to the agent with public key 7xxxx..., and the bank also
creates a coin of value 3.2 and gives it
to the agent with public key 8xxxx...The pair:(transaction id, index into array of value-payee pairs)uniquely identifies a (value, payee) tuple.For example (3146, 0) --- 
transaction id 3146, and array index 0 --- identifies value-payee[0] of
transaction 3146 which is specified by the 2-tuple: (2.1, 7xxxx...). So, the
transaction id and index, (3146, 0), tells everybody that the agent
with public key 7xxxx received 2.1 units of coin in transaction 3146.When this
transaction is in a tamper-evident ledger, every agent from that point
onwards knows that agent 7xxxx received 2.1 coins. Any modifications of this
record can be detected.Likewise, (3146, 1) --- 
transaction id 3146, and array index 1 --- identifies value-payee[1] which
is the 2-tuple (3.2, 8xxxx...).Pay transactionCoins are transferred from payers to payees in a pay transaction.Coins flowing into a pay transaction from payersThe payers are identified by a list of 2-tuples, where each 2-tuple is(transaction id, index into array of value-payee pairs)wheretransaction idis the id of the transaction in the
tamper-evident ledger. As we said earlier, this pair uniquely identifies an agent and a
value that this agent acquired in this transaction.
For example the pair --- transaction id, index --- such as (3146, 0)
identifies the 2-tuple (2.1, 7xxxx...); this 2-tuple asserts
that the agent
with public key 7xxxx received 2.1 units of coin in 
transaction 3146. Theentire amountspecified in the 2-tuple (2.1 in our
example) is value that flows into thispaytransaction from the agent
with public key 7xxxx.Suppose the payers in a pay transaction are identified by the list:[(3146, 0), (7359, 3)]and suppose pair 3 in transaction 7359 is (3.2, 8xxxx). Then the total
amount of coins flowing into this pay transaction is 2.1 + 3.2, and
this amount is disbursed to payees.Fig.3: Coins flowing into and out of a pay transaction.Coins flowing out of a pay transaction to payeesThe outflow of coins is specified by an array of value-payee pairs,
exactly as in a create transaction.
The transaction clears all coins: the total inflow from payers is
equal to the total outflow to payees in a transaction.The system may provide incentives, such as payment of coins, to
managers (e.g. banks) of cryptocurrencies. In this case, one of the
payees is the bank itself.Managing amounts spent in a transactionA transaction-id, index pair --- such as (3146, 0)
identifies a 2-tuple such as (2.1, 7xxxx...); this 2-tuple asserts
that the agent
with public key 7xxxx received 2.1 units of coin in 
transaction 3146. Theentire amountspecified in the 2-tuple (2.1 in our
example) is value that flows into the transaction. What should this
agent do if it
wants to put in more than 2.1 coins into the transaction? Or less than
2.1 coins?To put in more value, the bank identifies other transaction-id, index
pairs in which this agent received coins. For example, say that (4539, 2)
identifies a 2-tuple (3.2, 7xxxx), and assume that the payers in this transaction
are specified by the pairs (3146, 0) and (4539, 2). The pair (3146, 0)
asserts that agent (7xxxx) received 2.1 coins and the pair (4539, 2)
asserts that the same agent received 3.2 coins. So the total amount of 
coins flowing into this
transaction from this agent (7xxxx) includes 2.1 + 3.2.To put in less value, the agent acts as both payer and payee; the net
value that this agent pays out to other agents is the difference
between the amount that this agent puts in and takes out. For example,
if agent with public key 7xxxx wants to put in 1.9 coins into this
transaction its payer information can be given by the transaction-id,
index pair (3146, 0) which asserts that the agent received 2.1 coins
and the same agent is a payee that withdraws 0.2 coins.Preventing Double SpendingHow does the system prevent an agent from using the
same coin twice?For example, the transaction id, index pair
(3146, 0) identifies the 2-tuple (2.1, 7xxxx...); this tuple
asserts that the agent
with public key 7xxxx received 2.1 units of coin in 
transaction 3146. Why can't the agent with public key 7xxxx use the
2.1 coins that it received to buy items from Amazon and later use
the same 2.1 coins to buy more items from Walmart?The bank checks that the agent hasn't already spent the coin that it
is putting into a transaction.Before permitting the transaction that double-spends the 2.1 coins
with Walmart, the bank inspects the tamper-evident ledger
for all transactions after transaction 3146 and before the Walmart
transaction to ensure that the agent (7xxxx) hasn't already spent the
2.1 coins that it got in transaction 3146. The transaction that spends
the coins at Amazon will show up in the ledger, and so the transaction
with Walmart will not be allowed.Every agent that has the bank's hash pointer to the end of the
tamper-evident ledger can inspect the ledger to check that
double-spending hasn't occurred.The bank signs a valid transaction and appends it to the
tamper-evident ledger. All agents can see the bank's signature and
verify that nobody (not even the bank) has tampered with the
tamper-evident ledger.Example of a pay transactionAn example of the specification of a pay transaction is:(9431, pay,
  [(3146, 0), (4731, 2)],
  [(0.7, 7xxxx...), (4.6, 9xxxx...)]
).The id of this transaction is 9431; the type of the transaction is
pay; the payers into the transaction are identified by the pairs of
(transaction-id, index): (3146, 0), and (4731, 2); and the
payee array is [(0.7, 7xxxx...), (4.6, 9xxxx...)].Transaction validityThe bank appends a transaction toLif and only if the
transaction is valid. The bank checks for validity by carrying out the
following steps:The bank verifies that the payers into the transaction signed the
  transaction.The bank checks that the total value of coins paid out from the
  transaction does not
  exceed the total value paid in to the transaction. (If the value paid
  in exceeds the value paid out then the bank takes the difference as
  a transaction fee. More about fees later.)The bank verifies that the payers' claims to have received coins in
  previous transactions is genuine. For example,
  if the agent with
  public key 7xxxx... claims to have received coins worth 2.1 in the
  transaction with id 10, and payee array index 0, then the bank
  verifies this claim by that transaction.The bank ensures that coins paid into the transaction haven't
  already been spent.Optimizations: Blocks and Block ChainVerifying large numbers of small transactions requires more
computation than verifying small numbers of blocks of many transactions.
Ablock chainis a tamper-evident ledger in which
each element of the ledger is a block consisting of many transactions.
A block of transactions can be aggregated into a single large
transaction by aggregating all the payers and payees of the smaller
transactions.The amount of computation decreases as the number of transactions in a
block increases. The time required to fill a block with transactions
is larger when the number of transactions to fill the block increases.Checking the Trusted AgentConsider a system in which the trusted agent broadcasts its current
copy of the tamper-evident ledger to all agents.
Every agent can inspect its copy of the tamper-evident ledger to determine whether the
ledger has been tampered with. So, every agent can validate its trust
in the trusted agent; however, this validation
suffers from a crucial problem: Agents may only
have copies ofold, staleversions of the ledger.
By the time that an agent receives a copy of the ledger, the trusted
agent may have added more transactions to the ledger.Fig.4: Old Copy is a Prefix of the Block ChainAn old copy of the ledger can differ from the current copy in only one
way: the current copy may have transactions appended to the end
of the old copy. So all agents can validatepastbehavior of the
trusted agent. An agent cannot, however, treat its copy of the ledger
as the master copy because the agent may not have the transactions
added most recently to the ledger.In the next module we will see how the Bitcoin algorithm addresses
this problem.Advantages of this cryptocurrencyAny agent can get a copy of the tamper-evident ledger and verify that all
transactions in the ledger are valid. Any agent can verify that
the only way in which the ledger is modified is that elements are
append to its tail; all that the agent needs to do is to check that
the pointer to the tail is modified only by appending
elements. Because the ledger is tamper-evident, an agent can check
that the ledger doesn't change while the hash pointer to the end
of the ledger doesn't change.The bank can't forge a transaction because all payers sign the
transaction. Agents can remain anonymous because an agent's only
public information is the agent's public key, and an agent can create
multiple public keys. Every agent can verify the correctness of every
transaction.Disadvantages of this cryptocurrencyUsers may not trust the bank. Transactions are not private because the
bank has a record of all transactions. And the bank is a single point
of failure.Next we'll look at Bitcoin's algorithms for
implementing a cryptocurrency without trusted agents.ReviewWhat iscollision resistance? Why is it relevant to
  cryptocurrencies?Consider the example ofhidingin which Megan hides her
  predictions for the winner of the World Cup. Suppose you could
  choose two different random number generators for creating the
  secret \(r\); in one case the bits of \(r\) are uncorrelated, and in
  the other they are highly correlated. Which generator would you use
  and why?What isbinding? Why is it relevant to hiding information?What ispuzzle friendly?What is atamper-evident data structure? How would you
  implement a tamper-evident linear list? A tamper-evident tree?Describe the algorithm for cryptocurrency with a trusted agent to
  someone who knows absolutely nothing about cryptocurrency.How could the algorithm for cryptocurrency be used for a collection
  of distributed agents to keep track of a sequence of events, other
  than buying/selling currency?K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology



Figure Link: https://kmchandy.github.io/Crypto/CryptoCurrencyIntroduction/Slide1.jpg

Caption: Fig.1: Hash Pointer points to a Tamper-Evident Block
    of Data


The trusted third party provides two services:Hiding: You can't find out Megan's prediction until the third party reveals it.Binding: Megan can't change her prediction after giving it to the third
  party.Can we use a hash function instead of a trusted third party?HidingLet's try the following idea. Megan commits to \(W\) in the following
way. She announces a hash function, \(H\),
and the hash, \(y\), where \(y = H(W)\). You know \(H\) and \(y\). 
After the World Cup is over, she reveals her prediction, \(W\). At
this point you can verify that \(y = H(W)\).Does the hash function provide the services of the third party?
Can you discover Megan's prediction before she reveals it?It's easy. There are only 32 teams playing. Compute \(H(x)\) where
    \(x\) runs over each of the 32 teams. One of those teams has to be
\(W\). You can discover her prediction in at most 210 steps.Let's try another algorithm.  Megan selects a secret value \(r\) which
she keeps to herself. Instead
of giving you \(H(W)\), she gives you \(y\) where \(y = H(r + W)\) and
where \(+\) indicates concatenation of strings. Can
you discover \(W\) from \(H\) and \(y\) without knowing \(r\)?A brute-force solution is to try every combination of \(r\) and
\(W\). If \(r\) is obtained from a distribution that is spread out,
then finding \(W\) without knowing \(r\)  take so
much time that it is practically impossible.Hiding: Given \(H, y\), where \(y = H(r + W)\) and \(r\) is a secret,
discovering \(W\) is intractable.BindingDoes the hash function \(H\) and the secret \(r\) provide both
services of the trusted third party? Is Megan bound to her prediction
or can she change her "prediction" after knowing the winner of the
World Cup?Suppose Megan has values \(r\) and \(r'\) such that H(r + 'Brazil') =
H(r' + 'Italy'). After the World Cup is over, she can announce that
her secret is \(r\) if Brazil wins, and announce that it is \(r'\) if
Italy wins.A hash function \(H\) isbindingif finding pairs \((x,y)\) and \((x',y')\)
where \(y \neq y'\) such that:
\(H(x + y) = H(x' + y')\)
is intractable.Suppose you give Megan a hash function that is binding. Then she cannot find
  (in reasonable time) values \(r_{j}\) to match country \(C_{j}\)
such that\(H(r_{0} + C_{0}) = H(r_{1} + C_{1})  = H(r_{2} + C_{2})  = \ldots\).and so she can't wait for the winner \(C_{j}\) to be announced before announcing
her secret \(r_{j}\).In summary, we can use a hash function that is hiding and binding to
play the role of a trusted third party in a commitment.Puzzle FriendlyThe concept ofpuzzle friendlyis related tohiding.
Let \(r\) be a value picked from a spread-out distribution. Let
\(H\) map arbitrary length strings to \(n\)-bit strings.
Consider the following problem:
    Given \(H\), \(r\), and an \(n\)-bit value \(y\), compute any \(x\) such that
\(H(r+x) = y\).In this problem, as opposed to thehidingproblem, we are given \(r\) and not \(x\),The hash function \(H\) is said to bepuzzle friendlyexactly when any algorithm to solve this problem is about as slow
as a brute-force algorithm which checks \(H(r+x) = y\) for random values
of \(x\).
The number of steps taken by any
algorithm that solves this problem is not significantly lower than
\(2^{n}\).Now, let's look at the following related problem.  Given \(H\), \(r\),
and a set \(Y\) of \(n\)-bit strings, compute any \(x\) such that
\(H(r+x) \in Y\).  If \(Y\) consists of a single element \(y\) then
this problem is the same as that in the previous paragraph.  If \(Y\)
is a set of all \(n\)-bit strings then this problem is trivial because
any \(x\) solves the problem.  The probability that a random value
hashes to an element of \(Y\) is proportional to the cardinality of
\(Y\).  The cardinality of \(Y\) controls the expected time to
solve the puzzle.The hash function \(H\) ispuzzle friendlywhen
given \(H\), \(r\), and a set \(Y\) of \(n\)-bit values, the time
required to compute any \(x\) such that \(H(r+x) \in Y\) is
not significantly lower than \(2^{n} / |Y|\), where \(|Y|\) is the
cardinality of \(Y\).A Cryptographic Hash FunctionA cryptographic hash function is one that is collision resistant,
hiding and puzzle-friendly.Hashing Inputs of Arbitrary LengthLet \(f\) be a function that operates on input strings
of fixed length and produces output strings of fixed length. Let the
input and output strings of \(f\) have lengths \(M + N\) and \(M\),
respectively. We look at functions where \(N > 0\), and since the
output is smaller than the input, \(f\) is called acompression
function.We can use function \(f\) to define a function \(g\) whose inputs are strings of arbitrary
lengths and whose outputs are strings of length \(M\).
Example code for \(g\) is given below whereInitialValueis a given constant string of length \(M\).def g(y):
    output = InitialValue
    // pad y so that it's length is a multiple of N
    if len(y)%N > 0:  y = y + "0"*(N - len(y)%N)
    // partition y into blocks of size N
    blocks = [y[i: i+N] for i in range(0, len(y), N)]
    // Apply function f to the concatenation of the
    // previous output (length M) with each block
    // (length N) to get the next output (length M).
    for block in blocks: output = f(output+block)
    return outputHash PointersA hash pointer to an item \(D\) of data is a pair \((ptr, H(D))\)
where \(ptr\) is a pointer that 
points to \(D\), and \(H\) is a cryptographic hash function.Any data structure with pointers can be converted into a
data structure with hash pointers: merely replace a pointer \(ptr\) to
\(D\) by \((ptr, H(D))\).Tamper-Evident Data StructuresSingle BlockA simple example of a tamper-evident structure is a single block of
data D which is pointed to by a hash pointer consisting of a regular
pointer and a hash H(D).Fig.1: Hash Pointer points to a Tamper-Evident Block
    of DataAssume that a malicious agent cannot modifyboththe hash
pointer and the data that it points to. If an 
agent
    changes D to D' then the tampering can be discovered because the
    hash pointer won't match the data that it is pointing to:
    \(H(D') \neq H(D)\).Tamper-Evident Linked ListLet's look at linear linked list to which elements can be appended but
not deleted. The \(i\)-th element appended to the list points to the
\((i-1)\)-th element. 
Let's replace the pointers in the list by hashed
pointers.Fig.2: Hash Pointer points to a Tamper-Evident List
    of DataThe \(j\)-th element of the list, \(j > 0\), consists of data,
\(D_{j}\), and a hash pointer that points to the \(j - 1\)-th
  element of the list. The hash pointer in the \(j\)-th element
consists of a regular pointer, \(ptr_{j}\), which points to the \(j - 1\)-th
  element of the list, and a hashed value, \(HA_{j}\), which is a cryptographic hash of theentire\(j-1\)th element consisting  of \(D_{j-1}\), \(ptr_{j-1}\)
  and \(HA_{j-1}\).
The \(0\)-th element is called thegenesiselement and has
default values. The list is accessed by a hash pointer that points to
the last element of the list; let this pointer be \(ptr_{n}, HA_{n}\).Assume that agents cannot modify \(ptr_{n}, HA_{n}\). Then,
can any agent determine whether the list has been tampered with?Suppose the agent modifies \(D_{j}\), \(ptr_{j}\) or
\(HA_{j}\) for any \(j < n\). Any agent can detect this tampering because the
hash value \(HA_{j+1}\) will no longer match the \(j\)-th element of
the list. If
the malicious agent also modifies \(HA_{j+1}\) then 
hash value \(HA_{j+2}\) will no longer match the \(j+1\)-th element.By induction on \(j\), any agent can detect tampering with the list
provided malicious agents do not also modify the hash pointer to the
last element of the last.Tamper-Evident Acyclic Graphs and Merkle
TreesThe idea described in the previous paragraph to convert linear linked
lists can be used to convert directed acyclic graphs, in which nodes are
connected by pointers, into tamper-evident graphs. A specific case of
a directed acyclic graph is a rooted tree.AMerkle treeis a special case of a binary balanced tree in which
data items are stored only in the leaves.
Nodes that are not leaves 
contain only hash pointers to nodes in the next level down.
To prove that an element at the leaf is a member of the tree we need only the
\(log_{2}(n)\) hash pointers on the path from the root to that
leaf. By contrast, to prove that an element is a member of a linear
list we need to inspect \(O(n)\) elements, on average.Keys and Signed MessagesYou can create a random public-key, private-key pairby calling a function on your computer.With high probability, nobody else has this
specific pair of keys. Each individual's private key is a secret held
by that individual. Public keys are accessible by everybody.Sending messages securelyKeys are used to send messages securely.
Kamala sends a secure message to Joe by encrypting the message with
Joe's public key; Joe decrypts the message using Joe's private key.
An agent cannot decrypt the encrypted message without Joe's private key.Signing messagesSuppose Kamala needs to send a signed message to Joe while
ensuring that nobody can forge her signature. She encrypts the messageMwith her private key to get an encrypted messageM',
and sends the pair(M, M')securely to Joe, i.e., she encrypts(M, M')with Joe's public key, and sends the resulting
encrypted message to Joe.  When Joe receives the message, Joe decrypts
it using his private key to get(M, M'). Then Joe decryptsM'using Kamala's public key to get the decrypted messageM''. IfM'' = Mthen Joe knows that Kamala sentMbecause only an agent with Kamala's private signature could
have sent that message.Cryptocurrency Managed by a Trusted
AgentLet's start with a digital currency managed by a trusted agent that we will
call a bank. Later, we will
look at a consensus algorithm --- very different from Paxos and Byzantine
Generals --- which will allow cryptocurrencies without trusted
agent.The bank maintains a tamper-evident linear listLof transactions that we call a
tamper-evidentledger. 
Any agent can get a copy of the ledger.
This tamper-evident ledger is the foundation of the currency.A transaction is one of
two types:createorpay.
In a pay transaction,payersgive coins that they possess topayees. An agent can be both a 
payer and payee of the same transaction.
In acreatetransaction the bank creates coins that it gives to
agents --- the payees of the transaction; the bank acts as the payer.For this system to be trusted the bank must follow some protocol that determines
how and when the bank creates coins. We won't discuss these protocols.Apaytransaction is signed by all payers of the transaction.
Acreatetransaction is signed by the bank.
We discussed digital signatures and keys earlier.Each element of the tamper-evident ledger has:a unique id;the type of the transaction, either create or pay;list of payers: only for pay transactions --- a list
  indicating the agents who pay coins into the transaction and the
  amounts that they put in;array of value-payee pairs: for both create and pay transactions --- an
  array of pairs(value, payee public key), where each
  pair in the array indicates that coins of the specified value are
  given to the payee with the specified public key.Example of a create transactionAn example of acreatetransaction is:(3146, create,  [(2.1, 7xxxx...), (3.2, 8xxxx)]).The id of this transaction is 3146, the type of the transaction is
create, and the array of value-payee pairs is[(2.1, 7xxxx...), (3.2, 8xxx)]In this transaction the bank creates a coin of value 2.1 and gives it
to the agent with public key 7xxxx..., and the bank also
creates a coin of value 3.2 and gives it
to the agent with public key 8xxxx...The pair:(transaction id, index into array of value-payee pairs)uniquely identifies a (value, payee) tuple.For example (3146, 0) --- 
transaction id 3146, and array index 0 --- identifies value-payee[0] of
transaction 3146 which is specified by the 2-tuple: (2.1, 7xxxx...). So, the
transaction id and index, (3146, 0), tells everybody that the agent
with public key 7xxxx received 2.1 units of coin in transaction 3146.When this
transaction is in a tamper-evident ledger, every agent from that point
onwards knows that agent 7xxxx received 2.1 coins. Any modifications of this
record can be detected.Likewise, (3146, 1) --- 
transaction id 3146, and array index 1 --- identifies value-payee[1] which
is the 2-tuple (3.2, 8xxxx...).Pay transactionCoins are transferred from payers to payees in a pay transaction.Coins flowing into a pay transaction from payersThe payers are identified by a list of 2-tuples, where each 2-tuple is(transaction id, index into array of value-payee pairs)wheretransaction idis the id of the transaction in the
tamper-evident ledger. As we said earlier, this pair uniquely identifies an agent and a
value that this agent acquired in this transaction.
For example the pair --- transaction id, index --- such as (3146, 0)
identifies the 2-tuple (2.1, 7xxxx...); this 2-tuple asserts
that the agent
with public key 7xxxx received 2.1 units of coin in 
transaction 3146. Theentire amountspecified in the 2-tuple (2.1 in our
example) is value that flows into thispaytransaction from the agent
with public key 7xxxx.Suppose the payers in a pay transaction are identified by the list:[(3146, 0), (7359, 3)]and suppose pair 3 in transaction 7359 is (3.2, 8xxxx). Then the total
amount of coins flowing into this pay transaction is 2.1 + 3.2, and
this amount is disbursed to payees.Fig.3: Coins flowing into and out of a pay transaction.Coins flowing out of a pay transaction to payeesThe outflow of coins is specified by an array of value-payee pairs,
exactly as in a create transaction.
The transaction clears all coins: the total inflow from payers is
equal to the total outflow to payees in a transaction.The system may provide incentives, such as payment of coins, to
managers (e.g. banks) of cryptocurrencies. In this case, one of the
payees is the bank itself.Managing amounts spent in a transactionA transaction-id, index pair --- such as (3146, 0)
identifies a 2-tuple such as (2.1, 7xxxx...); this 2-tuple asserts
that the agent
with public key 7xxxx received 2.1 units of coin in 
transaction 3146. Theentire amountspecified in the 2-tuple (2.1 in our
example) is value that flows into the transaction. What should this
agent do if it
wants to put in more than 2.1 coins into the transaction? Or less than
2.1 coins?To put in more value, the bank identifies other transaction-id, index
pairs in which this agent received coins. For example, say that (4539, 2)
identifies a 2-tuple (3.2, 7xxxx), and assume that the payers in this transaction
are specified by the pairs (3146, 0) and (4539, 2). The pair (3146, 0)
asserts that agent (7xxxx) received 2.1 coins and the pair (4539, 2)
asserts that the same agent received 3.2 coins. So the total amount of 
coins flowing into this
transaction from this agent (7xxxx) includes 2.1 + 3.2.To put in less value, the agent acts as both payer and payee; the net
value that this agent pays out to other agents is the difference
between the amount that this agent puts in and takes out. For example,
if agent with public key 7xxxx wants to put in 1.9 coins into this
transaction its payer information can be given by the transaction-id,
index pair (3146, 0) which asserts that the agent received 2.1 coins
and the same agent is a payee that withdraws 0.2 coins.Preventing Double SpendingHow does the system prevent an agent from using the
same coin twice?For example, the transaction id, index pair
(3146, 0) identifies the 2-tuple (2.1, 7xxxx...); this tuple
asserts that the agent
with public key 7xxxx received 2.1 units of coin in 
transaction 3146. Why can't the agent with public key 7xxxx use the
2.1 coins that it received to buy items from Amazon and later use
the same 2.1 coins to buy more items from Walmart?The bank checks that the agent hasn't already spent the coin that it
is putting into a transaction.Before permitting the transaction that double-spends the 2.1 coins
with Walmart, the bank inspects the tamper-evident ledger
for all transactions after transaction 3146 and before the Walmart
transaction to ensure that the agent (7xxxx) hasn't already spent the
2.1 coins that it got in transaction 3146. The transaction that spends
the coins at Amazon will show up in the ledger, and so the transaction
with Walmart will not be allowed.Every agent that has the bank's hash pointer to the end of the
tamper-evident ledger can inspect the ledger to check that
double-spending hasn't occurred.The bank signs a valid transaction and appends it to the
tamper-evident ledger. All agents can see the bank's signature and
verify that nobody (not even the bank) has tampered with the
tamper-evident ledger.Example of a pay transactionAn example of the specification of a pay transaction is:(9431, pay,
  [(3146, 0), (4731, 2)],
  [(0.7, 7xxxx...), (4.6, 9xxxx...)]
).The id of this transaction is 9431; the type of the transaction is
pay; the payers into the transaction are identified by the pairs of
(transaction-id, index): (3146, 0), and (4731, 2); and the
payee array is [(0.7, 7xxxx...), (4.6, 9xxxx...)].Transaction validityThe bank appends a transaction toLif and only if the
transaction is valid. The bank checks for validity by carrying out the
following steps:The bank verifies that the payers into the transaction signed the
  transaction.The bank checks that the total value of coins paid out from the
  transaction does not
  exceed the total value paid in to the transaction. (If the value paid
  in exceeds the value paid out then the bank takes the difference as
  a transaction fee. More about fees later.)The bank verifies that the payers' claims to have received coins in
  previous transactions is genuine. For example,
  if the agent with
  public key 7xxxx... claims to have received coins worth 2.1 in the
  transaction with id 10, and payee array index 0, then the bank
  verifies this claim by that transaction.The bank ensures that coins paid into the transaction haven't
  already been spent.Optimizations: Blocks and Block ChainVerifying large numbers of small transactions requires more
computation than verifying small numbers of blocks of many transactions.
Ablock chainis a tamper-evident ledger in which
each element of the ledger is a block consisting of many transactions.
A block of transactions can be aggregated into a single large
transaction by aggregating all the payers and payees of the smaller
transactions.The amount of computation decreases as the number of transactions in a
block increases. The time required to fill a block with transactions
is larger when the number of transactions to fill the block increases.Checking the Trusted AgentConsider a system in which the trusted agent broadcasts its current
copy of the tamper-evident ledger to all agents.
Every agent can inspect its copy of the tamper-evident ledger to determine whether the
ledger has been tampered with. So, every agent can validate its trust
in the trusted agent; however, this validation
suffers from a crucial problem: Agents may only
have copies ofold, staleversions of the ledger.
By the time that an agent receives a copy of the ledger, the trusted
agent may have added more transactions to the ledger.Fig.4: Old Copy is a Prefix of the Block ChainAn old copy of the ledger can differ from the current copy in only one
way: the current copy may have transactions appended to the end
of the old copy. So all agents can validatepastbehavior of the
trusted agent. An agent cannot, however, treat its copy of the ledger
as the master copy because the agent may not have the transactions
added most recently to the ledger.In the next module we will see how the Bitcoin algorithm addresses
this problem.Advantages of this cryptocurrencyAny agent can get a copy of the tamper-evident ledger and verify that all
transactions in the ledger are valid. Any agent can verify that
the only way in which the ledger is modified is that elements are
append to its tail; all that the agent needs to do is to check that
the pointer to the tail is modified only by appending
elements. Because the ledger is tamper-evident, an agent can check
that the ledger doesn't change while the hash pointer to the end
of the ledger doesn't change.The bank can't forge a transaction because all payers sign the
transaction. Agents can remain anonymous because an agent's only
public information is the agent's public key, and an agent can create
multiple public keys. Every agent can verify the correctness of every
transaction.Disadvantages of this cryptocurrencyUsers may not trust the bank. Transactions are not private because the
bank has a record of all transactions. And the bank is a single point
of failure.Next we'll look at Bitcoin's algorithms for
implementing a cryptocurrency without trusted agents.ReviewWhat iscollision resistance? Why is it relevant to
  cryptocurrencies?Consider the example ofhidingin which Megan hides her
  predictions for the winner of the World Cup. Suppose you could
  choose two different random number generators for creating the
  secret \(r\); in one case the bits of \(r\) are uncorrelated, and in
  the other they are highly correlated. Which generator would you use
  and why?What isbinding? Why is it relevant to hiding information?What ispuzzle friendly?What is atamper-evident data structure? How would you
  implement a tamper-evident linear list? A tamper-evident tree?Describe the algorithm for cryptocurrency with a trusted agent to
  someone who knows absolutely nothing about cryptocurrency.How could the algorithm for cryptocurrency be used for a collection
  of distributed agents to keep track of a sequence of events, other
  than buying/selling currency?K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology



Figure Link: https://kmchandy.github.io/Crypto/CryptoCurrencyIntroduction/Slide1.jpg

Caption: Fig.1: Hash Pointer points to a Tamper-Evident Block
    of Data


Can we use a hash function instead of a trusted third party?HidingLet's try the following idea. Megan commits to \(W\) in the following
way. She announces a hash function, \(H\),
and the hash, \(y\), where \(y = H(W)\). You know \(H\) and \(y\). 
After the World Cup is over, she reveals her prediction, \(W\). At
this point you can verify that \(y = H(W)\).Does the hash function provide the services of the third party?
Can you discover Megan's prediction before she reveals it?It's easy. There are only 32 teams playing. Compute \(H(x)\) where
    \(x\) runs over each of the 32 teams. One of those teams has to be
\(W\). You can discover her prediction in at most 210 steps.Let's try another algorithm.  Megan selects a secret value \(r\) which
she keeps to herself. Instead
of giving you \(H(W)\), she gives you \(y\) where \(y = H(r + W)\) and
where \(+\) indicates concatenation of strings. Can
you discover \(W\) from \(H\) and \(y\) without knowing \(r\)?A brute-force solution is to try every combination of \(r\) and
\(W\). If \(r\) is obtained from a distribution that is spread out,
then finding \(W\) without knowing \(r\)  take so
much time that it is practically impossible.Hiding: Given \(H, y\), where \(y = H(r + W)\) and \(r\) is a secret,
discovering \(W\) is intractable.BindingDoes the hash function \(H\) and the secret \(r\) provide both
services of the trusted third party? Is Megan bound to her prediction
or can she change her "prediction" after knowing the winner of the
World Cup?Suppose Megan has values \(r\) and \(r'\) such that H(r + 'Brazil') =
H(r' + 'Italy'). After the World Cup is over, she can announce that
her secret is \(r\) if Brazil wins, and announce that it is \(r'\) if
Italy wins.A hash function \(H\) isbindingif finding pairs \((x,y)\) and \((x',y')\)
where \(y \neq y'\) such that:
\(H(x + y) = H(x' + y')\)
is intractable.Suppose you give Megan a hash function that is binding. Then she cannot find
  (in reasonable time) values \(r_{j}\) to match country \(C_{j}\)
such that\(H(r_{0} + C_{0}) = H(r_{1} + C_{1})  = H(r_{2} + C_{2})  = \ldots\).and so she can't wait for the winner \(C_{j}\) to be announced before announcing
her secret \(r_{j}\).In summary, we can use a hash function that is hiding and binding to
play the role of a trusted third party in a commitment.Puzzle FriendlyThe concept ofpuzzle friendlyis related tohiding.
Let \(r\) be a value picked from a spread-out distribution. Let
\(H\) map arbitrary length strings to \(n\)-bit strings.
Consider the following problem:
    Given \(H\), \(r\), and an \(n\)-bit value \(y\), compute any \(x\) such that
\(H(r+x) = y\).In this problem, as opposed to thehidingproblem, we are given \(r\) and not \(x\),The hash function \(H\) is said to bepuzzle friendlyexactly when any algorithm to solve this problem is about as slow
as a brute-force algorithm which checks \(H(r+x) = y\) for random values
of \(x\).
The number of steps taken by any
algorithm that solves this problem is not significantly lower than
\(2^{n}\).Now, let's look at the following related problem.  Given \(H\), \(r\),
and a set \(Y\) of \(n\)-bit strings, compute any \(x\) such that
\(H(r+x) \in Y\).  If \(Y\) consists of a single element \(y\) then
this problem is the same as that in the previous paragraph.  If \(Y\)
is a set of all \(n\)-bit strings then this problem is trivial because
any \(x\) solves the problem.  The probability that a random value
hashes to an element of \(Y\) is proportional to the cardinality of
\(Y\).  The cardinality of \(Y\) controls the expected time to
solve the puzzle.The hash function \(H\) ispuzzle friendlywhen
given \(H\), \(r\), and a set \(Y\) of \(n\)-bit values, the time
required to compute any \(x\) such that \(H(r+x) \in Y\) is
not significantly lower than \(2^{n} / |Y|\), where \(|Y|\) is the
cardinality of \(Y\).A Cryptographic Hash FunctionA cryptographic hash function is one that is collision resistant,
hiding and puzzle-friendly.Hashing Inputs of Arbitrary LengthLet \(f\) be a function that operates on input strings
of fixed length and produces output strings of fixed length. Let the
input and output strings of \(f\) have lengths \(M + N\) and \(M\),
respectively. We look at functions where \(N > 0\), and since the
output is smaller than the input, \(f\) is called acompression
function.We can use function \(f\) to define a function \(g\) whose inputs are strings of arbitrary
lengths and whose outputs are strings of length \(M\).
Example code for \(g\) is given below whereInitialValueis a given constant string of length \(M\).def g(y):
    output = InitialValue
    // pad y so that it's length is a multiple of N
    if len(y)%N > 0:  y = y + "0"*(N - len(y)%N)
    // partition y into blocks of size N
    blocks = [y[i: i+N] for i in range(0, len(y), N)]
    // Apply function f to the concatenation of the
    // previous output (length M) with each block
    // (length N) to get the next output (length M).
    for block in blocks: output = f(output+block)
    return outputHash PointersA hash pointer to an item \(D\) of data is a pair \((ptr, H(D))\)
where \(ptr\) is a pointer that 
points to \(D\), and \(H\) is a cryptographic hash function.Any data structure with pointers can be converted into a
data structure with hash pointers: merely replace a pointer \(ptr\) to
\(D\) by \((ptr, H(D))\).Tamper-Evident Data StructuresSingle BlockA simple example of a tamper-evident structure is a single block of
data D which is pointed to by a hash pointer consisting of a regular
pointer and a hash H(D).Fig.1: Hash Pointer points to a Tamper-Evident Block
    of DataAssume that a malicious agent cannot modifyboththe hash
pointer and the data that it points to. If an 
agent
    changes D to D' then the tampering can be discovered because the
    hash pointer won't match the data that it is pointing to:
    \(H(D') \neq H(D)\).Tamper-Evident Linked ListLet's look at linear linked list to which elements can be appended but
not deleted. The \(i\)-th element appended to the list points to the
\((i-1)\)-th element. 
Let's replace the pointers in the list by hashed
pointers.Fig.2: Hash Pointer points to a Tamper-Evident List
    of DataThe \(j\)-th element of the list, \(j > 0\), consists of data,
\(D_{j}\), and a hash pointer that points to the \(j - 1\)-th
  element of the list. The hash pointer in the \(j\)-th element
consists of a regular pointer, \(ptr_{j}\), which points to the \(j - 1\)-th
  element of the list, and a hashed value, \(HA_{j}\), which is a cryptographic hash of theentire\(j-1\)th element consisting  of \(D_{j-1}\), \(ptr_{j-1}\)
  and \(HA_{j-1}\).
The \(0\)-th element is called thegenesiselement and has
default values. The list is accessed by a hash pointer that points to
the last element of the list; let this pointer be \(ptr_{n}, HA_{n}\).Assume that agents cannot modify \(ptr_{n}, HA_{n}\). Then,
can any agent determine whether the list has been tampered with?Suppose the agent modifies \(D_{j}\), \(ptr_{j}\) or
\(HA_{j}\) for any \(j < n\). Any agent can detect this tampering because the
hash value \(HA_{j+1}\) will no longer match the \(j\)-th element of
the list. If
the malicious agent also modifies \(HA_{j+1}\) then 
hash value \(HA_{j+2}\) will no longer match the \(j+1\)-th element.By induction on \(j\), any agent can detect tampering with the list
provided malicious agents do not also modify the hash pointer to the
last element of the last.Tamper-Evident Acyclic Graphs and Merkle
TreesThe idea described in the previous paragraph to convert linear linked
lists can be used to convert directed acyclic graphs, in which nodes are
connected by pointers, into tamper-evident graphs. A specific case of
a directed acyclic graph is a rooted tree.AMerkle treeis a special case of a binary balanced tree in which
data items are stored only in the leaves.
Nodes that are not leaves 
contain only hash pointers to nodes in the next level down.
To prove that an element at the leaf is a member of the tree we need only the
\(log_{2}(n)\) hash pointers on the path from the root to that
leaf. By contrast, to prove that an element is a member of a linear
list we need to inspect \(O(n)\) elements, on average.Keys and Signed MessagesYou can create a random public-key, private-key pairby calling a function on your computer.With high probability, nobody else has this
specific pair of keys. Each individual's private key is a secret held
by that individual. Public keys are accessible by everybody.Sending messages securelyKeys are used to send messages securely.
Kamala sends a secure message to Joe by encrypting the message with
Joe's public key; Joe decrypts the message using Joe's private key.
An agent cannot decrypt the encrypted message without Joe's private key.Signing messagesSuppose Kamala needs to send a signed message to Joe while
ensuring that nobody can forge her signature. She encrypts the messageMwith her private key to get an encrypted messageM',
and sends the pair(M, M')securely to Joe, i.e., she encrypts(M, M')with Joe's public key, and sends the resulting
encrypted message to Joe.  When Joe receives the message, Joe decrypts
it using his private key to get(M, M'). Then Joe decryptsM'using Kamala's public key to get the decrypted messageM''. IfM'' = Mthen Joe knows that Kamala sentMbecause only an agent with Kamala's private signature could
have sent that message.Cryptocurrency Managed by a Trusted
AgentLet's start with a digital currency managed by a trusted agent that we will
call a bank. Later, we will
look at a consensus algorithm --- very different from Paxos and Byzantine
Generals --- which will allow cryptocurrencies without trusted
agent.The bank maintains a tamper-evident linear listLof transactions that we call a
tamper-evidentledger. 
Any agent can get a copy of the ledger.
This tamper-evident ledger is the foundation of the currency.A transaction is one of
two types:createorpay.
In a pay transaction,payersgive coins that they possess topayees. An agent can be both a 
payer and payee of the same transaction.
In acreatetransaction the bank creates coins that it gives to
agents --- the payees of the transaction; the bank acts as the payer.For this system to be trusted the bank must follow some protocol that determines
how and when the bank creates coins. We won't discuss these protocols.Apaytransaction is signed by all payers of the transaction.
Acreatetransaction is signed by the bank.
We discussed digital signatures and keys earlier.Each element of the tamper-evident ledger has:a unique id;the type of the transaction, either create or pay;list of payers: only for pay transactions --- a list
  indicating the agents who pay coins into the transaction and the
  amounts that they put in;array of value-payee pairs: for both create and pay transactions --- an
  array of pairs(value, payee public key), where each
  pair in the array indicates that coins of the specified value are
  given to the payee with the specified public key.Example of a create transactionAn example of acreatetransaction is:(3146, create,  [(2.1, 7xxxx...), (3.2, 8xxxx)]).The id of this transaction is 3146, the type of the transaction is
create, and the array of value-payee pairs is[(2.1, 7xxxx...), (3.2, 8xxx)]In this transaction the bank creates a coin of value 2.1 and gives it
to the agent with public key 7xxxx..., and the bank also
creates a coin of value 3.2 and gives it
to the agent with public key 8xxxx...The pair:(transaction id, index into array of value-payee pairs)uniquely identifies a (value, payee) tuple.For example (3146, 0) --- 
transaction id 3146, and array index 0 --- identifies value-payee[0] of
transaction 3146 which is specified by the 2-tuple: (2.1, 7xxxx...). So, the
transaction id and index, (3146, 0), tells everybody that the agent
with public key 7xxxx received 2.1 units of coin in transaction 3146.When this
transaction is in a tamper-evident ledger, every agent from that point
onwards knows that agent 7xxxx received 2.1 coins. Any modifications of this
record can be detected.Likewise, (3146, 1) --- 
transaction id 3146, and array index 1 --- identifies value-payee[1] which
is the 2-tuple (3.2, 8xxxx...).Pay transactionCoins are transferred from payers to payees in a pay transaction.Coins flowing into a pay transaction from payersThe payers are identified by a list of 2-tuples, where each 2-tuple is(transaction id, index into array of value-payee pairs)wheretransaction idis the id of the transaction in the
tamper-evident ledger. As we said earlier, this pair uniquely identifies an agent and a
value that this agent acquired in this transaction.
For example the pair --- transaction id, index --- such as (3146, 0)
identifies the 2-tuple (2.1, 7xxxx...); this 2-tuple asserts
that the agent
with public key 7xxxx received 2.1 units of coin in 
transaction 3146. Theentire amountspecified in the 2-tuple (2.1 in our
example) is value that flows into thispaytransaction from the agent
with public key 7xxxx.Suppose the payers in a pay transaction are identified by the list:[(3146, 0), (7359, 3)]and suppose pair 3 in transaction 7359 is (3.2, 8xxxx). Then the total
amount of coins flowing into this pay transaction is 2.1 + 3.2, and
this amount is disbursed to payees.Fig.3: Coins flowing into and out of a pay transaction.Coins flowing out of a pay transaction to payeesThe outflow of coins is specified by an array of value-payee pairs,
exactly as in a create transaction.
The transaction clears all coins: the total inflow from payers is
equal to the total outflow to payees in a transaction.The system may provide incentives, such as payment of coins, to
managers (e.g. banks) of cryptocurrencies. In this case, one of the
payees is the bank itself.Managing amounts spent in a transactionA transaction-id, index pair --- such as (3146, 0)
identifies a 2-tuple such as (2.1, 7xxxx...); this 2-tuple asserts
that the agent
with public key 7xxxx received 2.1 units of coin in 
transaction 3146. Theentire amountspecified in the 2-tuple (2.1 in our
example) is value that flows into the transaction. What should this
agent do if it
wants to put in more than 2.1 coins into the transaction? Or less than
2.1 coins?To put in more value, the bank identifies other transaction-id, index
pairs in which this agent received coins. For example, say that (4539, 2)
identifies a 2-tuple (3.2, 7xxxx), and assume that the payers in this transaction
are specified by the pairs (3146, 0) and (4539, 2). The pair (3146, 0)
asserts that agent (7xxxx) received 2.1 coins and the pair (4539, 2)
asserts that the same agent received 3.2 coins. So the total amount of 
coins flowing into this
transaction from this agent (7xxxx) includes 2.1 + 3.2.To put in less value, the agent acts as both payer and payee; the net
value that this agent pays out to other agents is the difference
between the amount that this agent puts in and takes out. For example,
if agent with public key 7xxxx wants to put in 1.9 coins into this
transaction its payer information can be given by the transaction-id,
index pair (3146, 0) which asserts that the agent received 2.1 coins
and the same agent is a payee that withdraws 0.2 coins.Preventing Double SpendingHow does the system prevent an agent from using the
same coin twice?For example, the transaction id, index pair
(3146, 0) identifies the 2-tuple (2.1, 7xxxx...); this tuple
asserts that the agent
with public key 7xxxx received 2.1 units of coin in 
transaction 3146. Why can't the agent with public key 7xxxx use the
2.1 coins that it received to buy items from Amazon and later use
the same 2.1 coins to buy more items from Walmart?The bank checks that the agent hasn't already spent the coin that it
is putting into a transaction.Before permitting the transaction that double-spends the 2.1 coins
with Walmart, the bank inspects the tamper-evident ledger
for all transactions after transaction 3146 and before the Walmart
transaction to ensure that the agent (7xxxx) hasn't already spent the
2.1 coins that it got in transaction 3146. The transaction that spends
the coins at Amazon will show up in the ledger, and so the transaction
with Walmart will not be allowed.Every agent that has the bank's hash pointer to the end of the
tamper-evident ledger can inspect the ledger to check that
double-spending hasn't occurred.The bank signs a valid transaction and appends it to the
tamper-evident ledger. All agents can see the bank's signature and
verify that nobody (not even the bank) has tampered with the
tamper-evident ledger.Example of a pay transactionAn example of the specification of a pay transaction is:(9431, pay,
  [(3146, 0), (4731, 2)],
  [(0.7, 7xxxx...), (4.6, 9xxxx...)]
).The id of this transaction is 9431; the type of the transaction is
pay; the payers into the transaction are identified by the pairs of
(transaction-id, index): (3146, 0), and (4731, 2); and the
payee array is [(0.7, 7xxxx...), (4.6, 9xxxx...)].Transaction validityThe bank appends a transaction toLif and only if the
transaction is valid. The bank checks for validity by carrying out the
following steps:The bank verifies that the payers into the transaction signed the
  transaction.The bank checks that the total value of coins paid out from the
  transaction does not
  exceed the total value paid in to the transaction. (If the value paid
  in exceeds the value paid out then the bank takes the difference as
  a transaction fee. More about fees later.)The bank verifies that the payers' claims to have received coins in
  previous transactions is genuine. For example,
  if the agent with
  public key 7xxxx... claims to have received coins worth 2.1 in the
  transaction with id 10, and payee array index 0, then the bank
  verifies this claim by that transaction.The bank ensures that coins paid into the transaction haven't
  already been spent.Optimizations: Blocks and Block ChainVerifying large numbers of small transactions requires more
computation than verifying small numbers of blocks of many transactions.
Ablock chainis a tamper-evident ledger in which
each element of the ledger is a block consisting of many transactions.
A block of transactions can be aggregated into a single large
transaction by aggregating all the payers and payees of the smaller
transactions.The amount of computation decreases as the number of transactions in a
block increases. The time required to fill a block with transactions
is larger when the number of transactions to fill the block increases.Checking the Trusted AgentConsider a system in which the trusted agent broadcasts its current
copy of the tamper-evident ledger to all agents.
Every agent can inspect its copy of the tamper-evident ledger to determine whether the
ledger has been tampered with. So, every agent can validate its trust
in the trusted agent; however, this validation
suffers from a crucial problem: Agents may only
have copies ofold, staleversions of the ledger.
By the time that an agent receives a copy of the ledger, the trusted
agent may have added more transactions to the ledger.Fig.4: Old Copy is a Prefix of the Block ChainAn old copy of the ledger can differ from the current copy in only one
way: the current copy may have transactions appended to the end
of the old copy. So all agents can validatepastbehavior of the
trusted agent. An agent cannot, however, treat its copy of the ledger
as the master copy because the agent may not have the transactions
added most recently to the ledger.In the next module we will see how the Bitcoin algorithm addresses
this problem.Advantages of this cryptocurrencyAny agent can get a copy of the tamper-evident ledger and verify that all
transactions in the ledger are valid. Any agent can verify that
the only way in which the ledger is modified is that elements are
append to its tail; all that the agent needs to do is to check that
the pointer to the tail is modified only by appending
elements. Because the ledger is tamper-evident, an agent can check
that the ledger doesn't change while the hash pointer to the end
of the ledger doesn't change.The bank can't forge a transaction because all payers sign the
transaction. Agents can remain anonymous because an agent's only
public information is the agent's public key, and an agent can create
multiple public keys. Every agent can verify the correctness of every
transaction.Disadvantages of this cryptocurrencyUsers may not trust the bank. Transactions are not private because the
bank has a record of all transactions. And the bank is a single point
of failure.Next we'll look at Bitcoin's algorithms for
implementing a cryptocurrency without trusted agents.ReviewWhat iscollision resistance? Why is it relevant to
  cryptocurrencies?Consider the example ofhidingin which Megan hides her
  predictions for the winner of the World Cup. Suppose you could
  choose two different random number generators for creating the
  secret \(r\); in one case the bits of \(r\) are uncorrelated, and in
  the other they are highly correlated. Which generator would you use
  and why?What isbinding? Why is it relevant to hiding information?What ispuzzle friendly?What is atamper-evident data structure? How would you
  implement a tamper-evident linear list? A tamper-evident tree?Describe the algorithm for cryptocurrency with a trusted agent to
  someone who knows absolutely nothing about cryptocurrency.How could the algorithm for cryptocurrency be used for a collection
  of distributed agents to keep track of a sequence of events, other
  than buying/selling currency?K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology



Figure Link: https://kmchandy.github.io/Crypto/CryptoCurrencyIntroduction/Slide1.jpg

Caption: Fig.1: Hash Pointer points to a Tamper-Evident Block
    of Data


Does the hash function provide the services of the third party?
Can you discover Megan's prediction before she reveals it?It's easy. There are only 32 teams playing. Compute \(H(x)\) where
    \(x\) runs over each of the 32 teams. One of those teams has to be
\(W\). You can discover her prediction in at most 210 steps.Let's try another algorithm.  Megan selects a secret value \(r\) which
she keeps to herself. Instead
of giving you \(H(W)\), she gives you \(y\) where \(y = H(r + W)\) and
where \(+\) indicates concatenation of strings. Can
you discover \(W\) from \(H\) and \(y\) without knowing \(r\)?A brute-force solution is to try every combination of \(r\) and
\(W\). If \(r\) is obtained from a distribution that is spread out,
then finding \(W\) without knowing \(r\)  take so
much time that it is practically impossible.Hiding: Given \(H, y\), where \(y = H(r + W)\) and \(r\) is a secret,
discovering \(W\) is intractable.BindingDoes the hash function \(H\) and the secret \(r\) provide both
services of the trusted third party? Is Megan bound to her prediction
or can she change her "prediction" after knowing the winner of the
World Cup?Suppose Megan has values \(r\) and \(r'\) such that H(r + 'Brazil') =
H(r' + 'Italy'). After the World Cup is over, she can announce that
her secret is \(r\) if Brazil wins, and announce that it is \(r'\) if
Italy wins.A hash function \(H\) isbindingif finding pairs \((x,y)\) and \((x',y')\)
where \(y \neq y'\) such that:
\(H(x + y) = H(x' + y')\)
is intractable.Suppose you give Megan a hash function that is binding. Then she cannot find
  (in reasonable time) values \(r_{j}\) to match country \(C_{j}\)
such that\(H(r_{0} + C_{0}) = H(r_{1} + C_{1})  = H(r_{2} + C_{2})  = \ldots\).and so she can't wait for the winner \(C_{j}\) to be announced before announcing
her secret \(r_{j}\).In summary, we can use a hash function that is hiding and binding to
play the role of a trusted third party in a commitment.Puzzle FriendlyThe concept ofpuzzle friendlyis related tohiding.
Let \(r\) be a value picked from a spread-out distribution. Let
\(H\) map arbitrary length strings to \(n\)-bit strings.
Consider the following problem:
    Given \(H\), \(r\), and an \(n\)-bit value \(y\), compute any \(x\) such that
\(H(r+x) = y\).In this problem, as opposed to thehidingproblem, we are given \(r\) and not \(x\),The hash function \(H\) is said to bepuzzle friendlyexactly when any algorithm to solve this problem is about as slow
as a brute-force algorithm which checks \(H(r+x) = y\) for random values
of \(x\).
The number of steps taken by any
algorithm that solves this problem is not significantly lower than
\(2^{n}\).Now, let's look at the following related problem.  Given \(H\), \(r\),
and a set \(Y\) of \(n\)-bit strings, compute any \(x\) such that
\(H(r+x) \in Y\).  If \(Y\) consists of a single element \(y\) then
this problem is the same as that in the previous paragraph.  If \(Y\)
is a set of all \(n\)-bit strings then this problem is trivial because
any \(x\) solves the problem.  The probability that a random value
hashes to an element of \(Y\) is proportional to the cardinality of
\(Y\).  The cardinality of \(Y\) controls the expected time to
solve the puzzle.The hash function \(H\) ispuzzle friendlywhen
given \(H\), \(r\), and a set \(Y\) of \(n\)-bit values, the time
required to compute any \(x\) such that \(H(r+x) \in Y\) is
not significantly lower than \(2^{n} / |Y|\), where \(|Y|\) is the
cardinality of \(Y\).A Cryptographic Hash FunctionA cryptographic hash function is one that is collision resistant,
hiding and puzzle-friendly.Hashing Inputs of Arbitrary LengthLet \(f\) be a function that operates on input strings
of fixed length and produces output strings of fixed length. Let the
input and output strings of \(f\) have lengths \(M + N\) and \(M\),
respectively. We look at functions where \(N > 0\), and since the
output is smaller than the input, \(f\) is called acompression
function.We can use function \(f\) to define a function \(g\) whose inputs are strings of arbitrary
lengths and whose outputs are strings of length \(M\).
Example code for \(g\) is given below whereInitialValueis a given constant string of length \(M\).def g(y):
    output = InitialValue
    // pad y so that it's length is a multiple of N
    if len(y)%N > 0:  y = y + "0"*(N - len(y)%N)
    // partition y into blocks of size N
    blocks = [y[i: i+N] for i in range(0, len(y), N)]
    // Apply function f to the concatenation of the
    // previous output (length M) with each block
    // (length N) to get the next output (length M).
    for block in blocks: output = f(output+block)
    return outputHash PointersA hash pointer to an item \(D\) of data is a pair \((ptr, H(D))\)
where \(ptr\) is a pointer that 
points to \(D\), and \(H\) is a cryptographic hash function.Any data structure with pointers can be converted into a
data structure with hash pointers: merely replace a pointer \(ptr\) to
\(D\) by \((ptr, H(D))\).Tamper-Evident Data StructuresSingle BlockA simple example of a tamper-evident structure is a single block of
data D which is pointed to by a hash pointer consisting of a regular
pointer and a hash H(D).Fig.1: Hash Pointer points to a Tamper-Evident Block
    of DataAssume that a malicious agent cannot modifyboththe hash
pointer and the data that it points to. If an 
agent
    changes D to D' then the tampering can be discovered because the
    hash pointer won't match the data that it is pointing to:
    \(H(D') \neq H(D)\).Tamper-Evident Linked ListLet's look at linear linked list to which elements can be appended but
not deleted. The \(i\)-th element appended to the list points to the
\((i-1)\)-th element. 
Let's replace the pointers in the list by hashed
pointers.Fig.2: Hash Pointer points to a Tamper-Evident List
    of DataThe \(j\)-th element of the list, \(j > 0\), consists of data,
\(D_{j}\), and a hash pointer that points to the \(j - 1\)-th
  element of the list. The hash pointer in the \(j\)-th element
consists of a regular pointer, \(ptr_{j}\), which points to the \(j - 1\)-th
  element of the list, and a hashed value, \(HA_{j}\), which is a cryptographic hash of theentire\(j-1\)th element consisting  of \(D_{j-1}\), \(ptr_{j-1}\)
  and \(HA_{j-1}\).
The \(0\)-th element is called thegenesiselement and has
default values. The list is accessed by a hash pointer that points to
the last element of the list; let this pointer be \(ptr_{n}, HA_{n}\).Assume that agents cannot modify \(ptr_{n}, HA_{n}\). Then,
can any agent determine whether the list has been tampered with?Suppose the agent modifies \(D_{j}\), \(ptr_{j}\) or
\(HA_{j}\) for any \(j < n\). Any agent can detect this tampering because the
hash value \(HA_{j+1}\) will no longer match the \(j\)-th element of
the list. If
the malicious agent also modifies \(HA_{j+1}\) then 
hash value \(HA_{j+2}\) will no longer match the \(j+1\)-th element.By induction on \(j\), any agent can detect tampering with the list
provided malicious agents do not also modify the hash pointer to the
last element of the last.Tamper-Evident Acyclic Graphs and Merkle
TreesThe idea described in the previous paragraph to convert linear linked
lists can be used to convert directed acyclic graphs, in which nodes are
connected by pointers, into tamper-evident graphs. A specific case of
a directed acyclic graph is a rooted tree.AMerkle treeis a special case of a binary balanced tree in which
data items are stored only in the leaves.
Nodes that are not leaves 
contain only hash pointers to nodes in the next level down.
To prove that an element at the leaf is a member of the tree we need only the
\(log_{2}(n)\) hash pointers on the path from the root to that
leaf. By contrast, to prove that an element is a member of a linear
list we need to inspect \(O(n)\) elements, on average.Keys and Signed MessagesYou can create a random public-key, private-key pairby calling a function on your computer.With high probability, nobody else has this
specific pair of keys. Each individual's private key is a secret held
by that individual. Public keys are accessible by everybody.Sending messages securelyKeys are used to send messages securely.
Kamala sends a secure message to Joe by encrypting the message with
Joe's public key; Joe decrypts the message using Joe's private key.
An agent cannot decrypt the encrypted message without Joe's private key.Signing messagesSuppose Kamala needs to send a signed message to Joe while
ensuring that nobody can forge her signature. She encrypts the messageMwith her private key to get an encrypted messageM',
and sends the pair(M, M')securely to Joe, i.e., she encrypts(M, M')with Joe's public key, and sends the resulting
encrypted message to Joe.  When Joe receives the message, Joe decrypts
it using his private key to get(M, M'). Then Joe decryptsM'using Kamala's public key to get the decrypted messageM''. IfM'' = Mthen Joe knows that Kamala sentMbecause only an agent with Kamala's private signature could
have sent that message.Cryptocurrency Managed by a Trusted
AgentLet's start with a digital currency managed by a trusted agent that we will
call a bank. Later, we will
look at a consensus algorithm --- very different from Paxos and Byzantine
Generals --- which will allow cryptocurrencies without trusted
agent.The bank maintains a tamper-evident linear listLof transactions that we call a
tamper-evidentledger. 
Any agent can get a copy of the ledger.
This tamper-evident ledger is the foundation of the currency.A transaction is one of
two types:createorpay.
In a pay transaction,payersgive coins that they possess topayees. An agent can be both a 
payer and payee of the same transaction.
In acreatetransaction the bank creates coins that it gives to
agents --- the payees of the transaction; the bank acts as the payer.For this system to be trusted the bank must follow some protocol that determines
how and when the bank creates coins. We won't discuss these protocols.Apaytransaction is signed by all payers of the transaction.
Acreatetransaction is signed by the bank.
We discussed digital signatures and keys earlier.Each element of the tamper-evident ledger has:a unique id;the type of the transaction, either create or pay;list of payers: only for pay transactions --- a list
  indicating the agents who pay coins into the transaction and the
  amounts that they put in;array of value-payee pairs: for both create and pay transactions --- an
  array of pairs(value, payee public key), where each
  pair in the array indicates that coins of the specified value are
  given to the payee with the specified public key.Example of a create transactionAn example of acreatetransaction is:(3146, create,  [(2.1, 7xxxx...), (3.2, 8xxxx)]).The id of this transaction is 3146, the type of the transaction is
create, and the array of value-payee pairs is[(2.1, 7xxxx...), (3.2, 8xxx)]In this transaction the bank creates a coin of value 2.1 and gives it
to the agent with public key 7xxxx..., and the bank also
creates a coin of value 3.2 and gives it
to the agent with public key 8xxxx...The pair:(transaction id, index into array of value-payee pairs)uniquely identifies a (value, payee) tuple.For example (3146, 0) --- 
transaction id 3146, and array index 0 --- identifies value-payee[0] of
transaction 3146 which is specified by the 2-tuple: (2.1, 7xxxx...). So, the
transaction id and index, (3146, 0), tells everybody that the agent
with public key 7xxxx received 2.1 units of coin in transaction 3146.When this
transaction is in a tamper-evident ledger, every agent from that point
onwards knows that agent 7xxxx received 2.1 coins. Any modifications of this
record can be detected.Likewise, (3146, 1) --- 
transaction id 3146, and array index 1 --- identifies value-payee[1] which
is the 2-tuple (3.2, 8xxxx...).Pay transactionCoins are transferred from payers to payees in a pay transaction.Coins flowing into a pay transaction from payersThe payers are identified by a list of 2-tuples, where each 2-tuple is(transaction id, index into array of value-payee pairs)wheretransaction idis the id of the transaction in the
tamper-evident ledger. As we said earlier, this pair uniquely identifies an agent and a
value that this agent acquired in this transaction.
For example the pair --- transaction id, index --- such as (3146, 0)
identifies the 2-tuple (2.1, 7xxxx...); this 2-tuple asserts
that the agent
with public key 7xxxx received 2.1 units of coin in 
transaction 3146. Theentire amountspecified in the 2-tuple (2.1 in our
example) is value that flows into thispaytransaction from the agent
with public key 7xxxx.Suppose the payers in a pay transaction are identified by the list:[(3146, 0), (7359, 3)]and suppose pair 3 in transaction 7359 is (3.2, 8xxxx). Then the total
amount of coins flowing into this pay transaction is 2.1 + 3.2, and
this amount is disbursed to payees.Fig.3: Coins flowing into and out of a pay transaction.Coins flowing out of a pay transaction to payeesThe outflow of coins is specified by an array of value-payee pairs,
exactly as in a create transaction.
The transaction clears all coins: the total inflow from payers is
equal to the total outflow to payees in a transaction.The system may provide incentives, such as payment of coins, to
managers (e.g. banks) of cryptocurrencies. In this case, one of the
payees is the bank itself.Managing amounts spent in a transactionA transaction-id, index pair --- such as (3146, 0)
identifies a 2-tuple such as (2.1, 7xxxx...); this 2-tuple asserts
that the agent
with public key 7xxxx received 2.1 units of coin in 
transaction 3146. Theentire amountspecified in the 2-tuple (2.1 in our
example) is value that flows into the transaction. What should this
agent do if it
wants to put in more than 2.1 coins into the transaction? Or less than
2.1 coins?To put in more value, the bank identifies other transaction-id, index
pairs in which this agent received coins. For example, say that (4539, 2)
identifies a 2-tuple (3.2, 7xxxx), and assume that the payers in this transaction
are specified by the pairs (3146, 0) and (4539, 2). The pair (3146, 0)
asserts that agent (7xxxx) received 2.1 coins and the pair (4539, 2)
asserts that the same agent received 3.2 coins. So the total amount of 
coins flowing into this
transaction from this agent (7xxxx) includes 2.1 + 3.2.To put in less value, the agent acts as both payer and payee; the net
value that this agent pays out to other agents is the difference
between the amount that this agent puts in and takes out. For example,
if agent with public key 7xxxx wants to put in 1.9 coins into this
transaction its payer information can be given by the transaction-id,
index pair (3146, 0) which asserts that the agent received 2.1 coins
and the same agent is a payee that withdraws 0.2 coins.Preventing Double SpendingHow does the system prevent an agent from using the
same coin twice?For example, the transaction id, index pair
(3146, 0) identifies the 2-tuple (2.1, 7xxxx...); this tuple
asserts that the agent
with public key 7xxxx received 2.1 units of coin in 
transaction 3146. Why can't the agent with public key 7xxxx use the
2.1 coins that it received to buy items from Amazon and later use
the same 2.1 coins to buy more items from Walmart?The bank checks that the agent hasn't already spent the coin that it
is putting into a transaction.Before permitting the transaction that double-spends the 2.1 coins
with Walmart, the bank inspects the tamper-evident ledger
for all transactions after transaction 3146 and before the Walmart
transaction to ensure that the agent (7xxxx) hasn't already spent the
2.1 coins that it got in transaction 3146. The transaction that spends
the coins at Amazon will show up in the ledger, and so the transaction
with Walmart will not be allowed.Every agent that has the bank's hash pointer to the end of the
tamper-evident ledger can inspect the ledger to check that
double-spending hasn't occurred.The bank signs a valid transaction and appends it to the
tamper-evident ledger. All agents can see the bank's signature and
verify that nobody (not even the bank) has tampered with the
tamper-evident ledger.Example of a pay transactionAn example of the specification of a pay transaction is:(9431, pay,
  [(3146, 0), (4731, 2)],
  [(0.7, 7xxxx...), (4.6, 9xxxx...)]
).The id of this transaction is 9431; the type of the transaction is
pay; the payers into the transaction are identified by the pairs of
(transaction-id, index): (3146, 0), and (4731, 2); and the
payee array is [(0.7, 7xxxx...), (4.6, 9xxxx...)].Transaction validityThe bank appends a transaction toLif and only if the
transaction is valid. The bank checks for validity by carrying out the
following steps:The bank verifies that the payers into the transaction signed the
  transaction.The bank checks that the total value of coins paid out from the
  transaction does not
  exceed the total value paid in to the transaction. (If the value paid
  in exceeds the value paid out then the bank takes the difference as
  a transaction fee. More about fees later.)The bank verifies that the payers' claims to have received coins in
  previous transactions is genuine. For example,
  if the agent with
  public key 7xxxx... claims to have received coins worth 2.1 in the
  transaction with id 10, and payee array index 0, then the bank
  verifies this claim by that transaction.The bank ensures that coins paid into the transaction haven't
  already been spent.Optimizations: Blocks and Block ChainVerifying large numbers of small transactions requires more
computation than verifying small numbers of blocks of many transactions.
Ablock chainis a tamper-evident ledger in which
each element of the ledger is a block consisting of many transactions.
A block of transactions can be aggregated into a single large
transaction by aggregating all the payers and payees of the smaller
transactions.The amount of computation decreases as the number of transactions in a
block increases. The time required to fill a block with transactions
is larger when the number of transactions to fill the block increases.Checking the Trusted AgentConsider a system in which the trusted agent broadcasts its current
copy of the tamper-evident ledger to all agents.
Every agent can inspect its copy of the tamper-evident ledger to determine whether the
ledger has been tampered with. So, every agent can validate its trust
in the trusted agent; however, this validation
suffers from a crucial problem: Agents may only
have copies ofold, staleversions of the ledger.
By the time that an agent receives a copy of the ledger, the trusted
agent may have added more transactions to the ledger.Fig.4: Old Copy is a Prefix of the Block ChainAn old copy of the ledger can differ from the current copy in only one
way: the current copy may have transactions appended to the end
of the old copy. So all agents can validatepastbehavior of the
trusted agent. An agent cannot, however, treat its copy of the ledger
as the master copy because the agent may not have the transactions
added most recently to the ledger.In the next module we will see how the Bitcoin algorithm addresses
this problem.Advantages of this cryptocurrencyAny agent can get a copy of the tamper-evident ledger and verify that all
transactions in the ledger are valid. Any agent can verify that
the only way in which the ledger is modified is that elements are
append to its tail; all that the agent needs to do is to check that
the pointer to the tail is modified only by appending
elements. Because the ledger is tamper-evident, an agent can check
that the ledger doesn't change while the hash pointer to the end
of the ledger doesn't change.The bank can't forge a transaction because all payers sign the
transaction. Agents can remain anonymous because an agent's only
public information is the agent's public key, and an agent can create
multiple public keys. Every agent can verify the correctness of every
transaction.Disadvantages of this cryptocurrencyUsers may not trust the bank. Transactions are not private because the
bank has a record of all transactions. And the bank is a single point
of failure.Next we'll look at Bitcoin's algorithms for
implementing a cryptocurrency without trusted agents.ReviewWhat iscollision resistance? Why is it relevant to
  cryptocurrencies?Consider the example ofhidingin which Megan hides her
  predictions for the winner of the World Cup. Suppose you could
  choose two different random number generators for creating the
  secret \(r\); in one case the bits of \(r\) are uncorrelated, and in
  the other they are highly correlated. Which generator would you use
  and why?What isbinding? Why is it relevant to hiding information?What ispuzzle friendly?What is atamper-evident data structure? How would you
  implement a tamper-evident linear list? A tamper-evident tree?Describe the algorithm for cryptocurrency with a trusted agent to
  someone who knows absolutely nothing about cryptocurrency.How could the algorithm for cryptocurrency be used for a collection
  of distributed agents to keep track of a sequence of events, other
  than buying/selling currency?K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology



Figure Link: https://kmchandy.github.io/Crypto/CryptoCurrencyIntroduction/Slide1.jpg

Caption: Fig.1: Hash Pointer points to a Tamper-Evident Block
    of Data


It's easy. There are only 32 teams playing. Compute \(H(x)\) where
    \(x\) runs over each of the 32 teams. One of those teams has to be
\(W\). You can discover her prediction in at most 210 steps.Let's try another algorithm.  Megan selects a secret value \(r\) which
she keeps to herself. Instead
of giving you \(H(W)\), she gives you \(y\) where \(y = H(r + W)\) and
where \(+\) indicates concatenation of strings. Can
you discover \(W\) from \(H\) and \(y\) without knowing \(r\)?A brute-force solution is to try every combination of \(r\) and
\(W\). If \(r\) is obtained from a distribution that is spread out,
then finding \(W\) without knowing \(r\)  take so
much time that it is practically impossible.Hiding: Given \(H, y\), where \(y = H(r + W)\) and \(r\) is a secret,
discovering \(W\) is intractable.BindingDoes the hash function \(H\) and the secret \(r\) provide both
services of the trusted third party? Is Megan bound to her prediction
or can she change her "prediction" after knowing the winner of the
World Cup?Suppose Megan has values \(r\) and \(r'\) such that H(r + 'Brazil') =
H(r' + 'Italy'). After the World Cup is over, she can announce that
her secret is \(r\) if Brazil wins, and announce that it is \(r'\) if
Italy wins.A hash function \(H\) isbindingif finding pairs \((x,y)\) and \((x',y')\)
where \(y \neq y'\) such that:
\(H(x + y) = H(x' + y')\)
is intractable.Suppose you give Megan a hash function that is binding. Then she cannot find
  (in reasonable time) values \(r_{j}\) to match country \(C_{j}\)
such that\(H(r_{0} + C_{0}) = H(r_{1} + C_{1})  = H(r_{2} + C_{2})  = \ldots\).and so she can't wait for the winner \(C_{j}\) to be announced before announcing
her secret \(r_{j}\).In summary, we can use a hash function that is hiding and binding to
play the role of a trusted third party in a commitment.Puzzle FriendlyThe concept ofpuzzle friendlyis related tohiding.
Let \(r\) be a value picked from a spread-out distribution. Let
\(H\) map arbitrary length strings to \(n\)-bit strings.
Consider the following problem:
    Given \(H\), \(r\), and an \(n\)-bit value \(y\), compute any \(x\) such that
\(H(r+x) = y\).In this problem, as opposed to thehidingproblem, we are given \(r\) and not \(x\),The hash function \(H\) is said to bepuzzle friendlyexactly when any algorithm to solve this problem is about as slow
as a brute-force algorithm which checks \(H(r+x) = y\) for random values
of \(x\).
The number of steps taken by any
algorithm that solves this problem is not significantly lower than
\(2^{n}\).Now, let's look at the following related problem.  Given \(H\), \(r\),
and a set \(Y\) of \(n\)-bit strings, compute any \(x\) such that
\(H(r+x) \in Y\).  If \(Y\) consists of a single element \(y\) then
this problem is the same as that in the previous paragraph.  If \(Y\)
is a set of all \(n\)-bit strings then this problem is trivial because
any \(x\) solves the problem.  The probability that a random value
hashes to an element of \(Y\) is proportional to the cardinality of
\(Y\).  The cardinality of \(Y\) controls the expected time to
solve the puzzle.The hash function \(H\) ispuzzle friendlywhen
given \(H\), \(r\), and a set \(Y\) of \(n\)-bit values, the time
required to compute any \(x\) such that \(H(r+x) \in Y\) is
not significantly lower than \(2^{n} / |Y|\), where \(|Y|\) is the
cardinality of \(Y\).A Cryptographic Hash FunctionA cryptographic hash function is one that is collision resistant,
hiding and puzzle-friendly.Hashing Inputs of Arbitrary LengthLet \(f\) be a function that operates on input strings
of fixed length and produces output strings of fixed length. Let the
input and output strings of \(f\) have lengths \(M + N\) and \(M\),
respectively. We look at functions where \(N > 0\), and since the
output is smaller than the input, \(f\) is called acompression
function.We can use function \(f\) to define a function \(g\) whose inputs are strings of arbitrary
lengths and whose outputs are strings of length \(M\).
Example code for \(g\) is given below whereInitialValueis a given constant string of length \(M\).def g(y):
    output = InitialValue
    // pad y so that it's length is a multiple of N
    if len(y)%N > 0:  y = y + "0"*(N - len(y)%N)
    // partition y into blocks of size N
    blocks = [y[i: i+N] for i in range(0, len(y), N)]
    // Apply function f to the concatenation of the
    // previous output (length M) with each block
    // (length N) to get the next output (length M).
    for block in blocks: output = f(output+block)
    return outputHash PointersA hash pointer to an item \(D\) of data is a pair \((ptr, H(D))\)
where \(ptr\) is a pointer that 
points to \(D\), and \(H\) is a cryptographic hash function.Any data structure with pointers can be converted into a
data structure with hash pointers: merely replace a pointer \(ptr\) to
\(D\) by \((ptr, H(D))\).Tamper-Evident Data StructuresSingle BlockA simple example of a tamper-evident structure is a single block of
data D which is pointed to by a hash pointer consisting of a regular
pointer and a hash H(D).Fig.1: Hash Pointer points to a Tamper-Evident Block
    of DataAssume that a malicious agent cannot modifyboththe hash
pointer and the data that it points to. If an 
agent
    changes D to D' then the tampering can be discovered because the
    hash pointer won't match the data that it is pointing to:
    \(H(D') \neq H(D)\).Tamper-Evident Linked ListLet's look at linear linked list to which elements can be appended but
not deleted. The \(i\)-th element appended to the list points to the
\((i-1)\)-th element. 
Let's replace the pointers in the list by hashed
pointers.Fig.2: Hash Pointer points to a Tamper-Evident List
    of DataThe \(j\)-th element of the list, \(j > 0\), consists of data,
\(D_{j}\), and a hash pointer that points to the \(j - 1\)-th
  element of the list. The hash pointer in the \(j\)-th element
consists of a regular pointer, \(ptr_{j}\), which points to the \(j - 1\)-th
  element of the list, and a hashed value, \(HA_{j}\), which is a cryptographic hash of theentire\(j-1\)th element consisting  of \(D_{j-1}\), \(ptr_{j-1}\)
  and \(HA_{j-1}\).
The \(0\)-th element is called thegenesiselement and has
default values. The list is accessed by a hash pointer that points to
the last element of the list; let this pointer be \(ptr_{n}, HA_{n}\).Assume that agents cannot modify \(ptr_{n}, HA_{n}\). Then,
can any agent determine whether the list has been tampered with?Suppose the agent modifies \(D_{j}\), \(ptr_{j}\) or
\(HA_{j}\) for any \(j < n\). Any agent can detect this tampering because the
hash value \(HA_{j+1}\) will no longer match the \(j\)-th element of
the list. If
the malicious agent also modifies \(HA_{j+1}\) then 
hash value \(HA_{j+2}\) will no longer match the \(j+1\)-th element.By induction on \(j\), any agent can detect tampering with the list
provided malicious agents do not also modify the hash pointer to the
last element of the last.Tamper-Evident Acyclic Graphs and Merkle
TreesThe idea described in the previous paragraph to convert linear linked
lists can be used to convert directed acyclic graphs, in which nodes are
connected by pointers, into tamper-evident graphs. A specific case of
a directed acyclic graph is a rooted tree.AMerkle treeis a special case of a binary balanced tree in which
data items are stored only in the leaves.
Nodes that are not leaves 
contain only hash pointers to nodes in the next level down.
To prove that an element at the leaf is a member of the tree we need only the
\(log_{2}(n)\) hash pointers on the path from the root to that
leaf. By contrast, to prove that an element is a member of a linear
list we need to inspect \(O(n)\) elements, on average.Keys and Signed MessagesYou can create a random public-key, private-key pairby calling a function on your computer.With high probability, nobody else has this
specific pair of keys. Each individual's private key is a secret held
by that individual. Public keys are accessible by everybody.Sending messages securelyKeys are used to send messages securely.
Kamala sends a secure message to Joe by encrypting the message with
Joe's public key; Joe decrypts the message using Joe's private key.
An agent cannot decrypt the encrypted message without Joe's private key.Signing messagesSuppose Kamala needs to send a signed message to Joe while
ensuring that nobody can forge her signature. She encrypts the messageMwith her private key to get an encrypted messageM',
and sends the pair(M, M')securely to Joe, i.e., she encrypts(M, M')with Joe's public key, and sends the resulting
encrypted message to Joe.  When Joe receives the message, Joe decrypts
it using his private key to get(M, M'). Then Joe decryptsM'using Kamala's public key to get the decrypted messageM''. IfM'' = Mthen Joe knows that Kamala sentMbecause only an agent with Kamala's private signature could
have sent that message.Cryptocurrency Managed by a Trusted
AgentLet's start with a digital currency managed by a trusted agent that we will
call a bank. Later, we will
look at a consensus algorithm --- very different from Paxos and Byzantine
Generals --- which will allow cryptocurrencies without trusted
agent.The bank maintains a tamper-evident linear listLof transactions that we call a
tamper-evidentledger. 
Any agent can get a copy of the ledger.
This tamper-evident ledger is the foundation of the currency.A transaction is one of
two types:createorpay.
In a pay transaction,payersgive coins that they possess topayees. An agent can be both a 
payer and payee of the same transaction.
In acreatetransaction the bank creates coins that it gives to
agents --- the payees of the transaction; the bank acts as the payer.For this system to be trusted the bank must follow some protocol that determines
how and when the bank creates coins. We won't discuss these protocols.Apaytransaction is signed by all payers of the transaction.
Acreatetransaction is signed by the bank.
We discussed digital signatures and keys earlier.Each element of the tamper-evident ledger has:a unique id;the type of the transaction, either create or pay;list of payers: only for pay transactions --- a list
  indicating the agents who pay coins into the transaction and the
  amounts that they put in;array of value-payee pairs: for both create and pay transactions --- an
  array of pairs(value, payee public key), where each
  pair in the array indicates that coins of the specified value are
  given to the payee with the specified public key.Example of a create transactionAn example of acreatetransaction is:(3146, create,  [(2.1, 7xxxx...), (3.2, 8xxxx)]).The id of this transaction is 3146, the type of the transaction is
create, and the array of value-payee pairs is[(2.1, 7xxxx...), (3.2, 8xxx)]In this transaction the bank creates a coin of value 2.1 and gives it
to the agent with public key 7xxxx..., and the bank also
creates a coin of value 3.2 and gives it
to the agent with public key 8xxxx...The pair:(transaction id, index into array of value-payee pairs)uniquely identifies a (value, payee) tuple.For example (3146, 0) --- 
transaction id 3146, and array index 0 --- identifies value-payee[0] of
transaction 3146 which is specified by the 2-tuple: (2.1, 7xxxx...). So, the
transaction id and index, (3146, 0), tells everybody that the agent
with public key 7xxxx received 2.1 units of coin in transaction 3146.When this
transaction is in a tamper-evident ledger, every agent from that point
onwards knows that agent 7xxxx received 2.1 coins. Any modifications of this
record can be detected.Likewise, (3146, 1) --- 
transaction id 3146, and array index 1 --- identifies value-payee[1] which
is the 2-tuple (3.2, 8xxxx...).Pay transactionCoins are transferred from payers to payees in a pay transaction.Coins flowing into a pay transaction from payersThe payers are identified by a list of 2-tuples, where each 2-tuple is(transaction id, index into array of value-payee pairs)wheretransaction idis the id of the transaction in the
tamper-evident ledger. As we said earlier, this pair uniquely identifies an agent and a
value that this agent acquired in this transaction.
For example the pair --- transaction id, index --- such as (3146, 0)
identifies the 2-tuple (2.1, 7xxxx...); this 2-tuple asserts
that the agent
with public key 7xxxx received 2.1 units of coin in 
transaction 3146. Theentire amountspecified in the 2-tuple (2.1 in our
example) is value that flows into thispaytransaction from the agent
with public key 7xxxx.Suppose the payers in a pay transaction are identified by the list:[(3146, 0), (7359, 3)]and suppose pair 3 in transaction 7359 is (3.2, 8xxxx). Then the total
amount of coins flowing into this pay transaction is 2.1 + 3.2, and
this amount is disbursed to payees.Fig.3: Coins flowing into and out of a pay transaction.Coins flowing out of a pay transaction to payeesThe outflow of coins is specified by an array of value-payee pairs,
exactly as in a create transaction.
The transaction clears all coins: the total inflow from payers is
equal to the total outflow to payees in a transaction.The system may provide incentives, such as payment of coins, to
managers (e.g. banks) of cryptocurrencies. In this case, one of the
payees is the bank itself.Managing amounts spent in a transactionA transaction-id, index pair --- such as (3146, 0)
identifies a 2-tuple such as (2.1, 7xxxx...); this 2-tuple asserts
that the agent
with public key 7xxxx received 2.1 units of coin in 
transaction 3146. Theentire amountspecified in the 2-tuple (2.1 in our
example) is value that flows into the transaction. What should this
agent do if it
wants to put in more than 2.1 coins into the transaction? Or less than
2.1 coins?To put in more value, the bank identifies other transaction-id, index
pairs in which this agent received coins. For example, say that (4539, 2)
identifies a 2-tuple (3.2, 7xxxx), and assume that the payers in this transaction
are specified by the pairs (3146, 0) and (4539, 2). The pair (3146, 0)
asserts that agent (7xxxx) received 2.1 coins and the pair (4539, 2)
asserts that the same agent received 3.2 coins. So the total amount of 
coins flowing into this
transaction from this agent (7xxxx) includes 2.1 + 3.2.To put in less value, the agent acts as both payer and payee; the net
value that this agent pays out to other agents is the difference
between the amount that this agent puts in and takes out. For example,
if agent with public key 7xxxx wants to put in 1.9 coins into this
transaction its payer information can be given by the transaction-id,
index pair (3146, 0) which asserts that the agent received 2.1 coins
and the same agent is a payee that withdraws 0.2 coins.Preventing Double SpendingHow does the system prevent an agent from using the
same coin twice?For example, the transaction id, index pair
(3146, 0) identifies the 2-tuple (2.1, 7xxxx...); this tuple
asserts that the agent
with public key 7xxxx received 2.1 units of coin in 
transaction 3146. Why can't the agent with public key 7xxxx use the
2.1 coins that it received to buy items from Amazon and later use
the same 2.1 coins to buy more items from Walmart?The bank checks that the agent hasn't already spent the coin that it
is putting into a transaction.Before permitting the transaction that double-spends the 2.1 coins
with Walmart, the bank inspects the tamper-evident ledger
for all transactions after transaction 3146 and before the Walmart
transaction to ensure that the agent (7xxxx) hasn't already spent the
2.1 coins that it got in transaction 3146. The transaction that spends
the coins at Amazon will show up in the ledger, and so the transaction
with Walmart will not be allowed.Every agent that has the bank's hash pointer to the end of the
tamper-evident ledger can inspect the ledger to check that
double-spending hasn't occurred.The bank signs a valid transaction and appends it to the
tamper-evident ledger. All agents can see the bank's signature and
verify that nobody (not even the bank) has tampered with the
tamper-evident ledger.Example of a pay transactionAn example of the specification of a pay transaction is:(9431, pay,
  [(3146, 0), (4731, 2)],
  [(0.7, 7xxxx...), (4.6, 9xxxx...)]
).The id of this transaction is 9431; the type of the transaction is
pay; the payers into the transaction are identified by the pairs of
(transaction-id, index): (3146, 0), and (4731, 2); and the
payee array is [(0.7, 7xxxx...), (4.6, 9xxxx...)].Transaction validityThe bank appends a transaction toLif and only if the
transaction is valid. The bank checks for validity by carrying out the
following steps:The bank verifies that the payers into the transaction signed the
  transaction.The bank checks that the total value of coins paid out from the
  transaction does not
  exceed the total value paid in to the transaction. (If the value paid
  in exceeds the value paid out then the bank takes the difference as
  a transaction fee. More about fees later.)The bank verifies that the payers' claims to have received coins in
  previous transactions is genuine. For example,
  if the agent with
  public key 7xxxx... claims to have received coins worth 2.1 in the
  transaction with id 10, and payee array index 0, then the bank
  verifies this claim by that transaction.The bank ensures that coins paid into the transaction haven't
  already been spent.Optimizations: Blocks and Block ChainVerifying large numbers of small transactions requires more
computation than verifying small numbers of blocks of many transactions.
Ablock chainis a tamper-evident ledger in which
each element of the ledger is a block consisting of many transactions.
A block of transactions can be aggregated into a single large
transaction by aggregating all the payers and payees of the smaller
transactions.The amount of computation decreases as the number of transactions in a
block increases. The time required to fill a block with transactions
is larger when the number of transactions to fill the block increases.Checking the Trusted AgentConsider a system in which the trusted agent broadcasts its current
copy of the tamper-evident ledger to all agents.
Every agent can inspect its copy of the tamper-evident ledger to determine whether the
ledger has been tampered with. So, every agent can validate its trust
in the trusted agent; however, this validation
suffers from a crucial problem: Agents may only
have copies ofold, staleversions of the ledger.
By the time that an agent receives a copy of the ledger, the trusted
agent may have added more transactions to the ledger.Fig.4: Old Copy is a Prefix of the Block ChainAn old copy of the ledger can differ from the current copy in only one
way: the current copy may have transactions appended to the end
of the old copy. So all agents can validatepastbehavior of the
trusted agent. An agent cannot, however, treat its copy of the ledger
as the master copy because the agent may not have the transactions
added most recently to the ledger.In the next module we will see how the Bitcoin algorithm addresses
this problem.Advantages of this cryptocurrencyAny agent can get a copy of the tamper-evident ledger and verify that all
transactions in the ledger are valid. Any agent can verify that
the only way in which the ledger is modified is that elements are
append to its tail; all that the agent needs to do is to check that
the pointer to the tail is modified only by appending
elements. Because the ledger is tamper-evident, an agent can check
that the ledger doesn't change while the hash pointer to the end
of the ledger doesn't change.The bank can't forge a transaction because all payers sign the
transaction. Agents can remain anonymous because an agent's only
public information is the agent's public key, and an agent can create
multiple public keys. Every agent can verify the correctness of every
transaction.Disadvantages of this cryptocurrencyUsers may not trust the bank. Transactions are not private because the
bank has a record of all transactions. And the bank is a single point
of failure.Next we'll look at Bitcoin's algorithms for
implementing a cryptocurrency without trusted agents.ReviewWhat iscollision resistance? Why is it relevant to
  cryptocurrencies?Consider the example ofhidingin which Megan hides her
  predictions for the winner of the World Cup. Suppose you could
  choose two different random number generators for creating the
  secret \(r\); in one case the bits of \(r\) are uncorrelated, and in
  the other they are highly correlated. Which generator would you use
  and why?What isbinding? Why is it relevant to hiding information?What ispuzzle friendly?What is atamper-evident data structure? How would you
  implement a tamper-evident linear list? A tamper-evident tree?Describe the algorithm for cryptocurrency with a trusted agent to
  someone who knows absolutely nothing about cryptocurrency.How could the algorithm for cryptocurrency be used for a collection
  of distributed agents to keep track of a sequence of events, other
  than buying/selling currency?K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology



Figure Link: https://kmchandy.github.io/Crypto/CryptoCurrencyIntroduction/Slide1.jpg

Caption: Fig.1: Hash Pointer points to a Tamper-Evident Block
    of Data


Let's try another algorithm.  Megan selects a secret value \(r\) which
she keeps to herself. Instead
of giving you \(H(W)\), she gives you \(y\) where \(y = H(r + W)\) and
where \(+\) indicates concatenation of strings. Can
you discover \(W\) from \(H\) and \(y\) without knowing \(r\)?A brute-force solution is to try every combination of \(r\) and
\(W\). If \(r\) is obtained from a distribution that is spread out,
then finding \(W\) without knowing \(r\)  take so
much time that it is practically impossible.Hiding: Given \(H, y\), where \(y = H(r + W)\) and \(r\) is a secret,
discovering \(W\) is intractable.BindingDoes the hash function \(H\) and the secret \(r\) provide both
services of the trusted third party? Is Megan bound to her prediction
or can she change her "prediction" after knowing the winner of the
World Cup?Suppose Megan has values \(r\) and \(r'\) such that H(r + 'Brazil') =
H(r' + 'Italy'). After the World Cup is over, she can announce that
her secret is \(r\) if Brazil wins, and announce that it is \(r'\) if
Italy wins.A hash function \(H\) isbindingif finding pairs \((x,y)\) and \((x',y')\)
where \(y \neq y'\) such that:
\(H(x + y) = H(x' + y')\)
is intractable.Suppose you give Megan a hash function that is binding. Then she cannot find
  (in reasonable time) values \(r_{j}\) to match country \(C_{j}\)
such that\(H(r_{0} + C_{0}) = H(r_{1} + C_{1})  = H(r_{2} + C_{2})  = \ldots\).and so she can't wait for the winner \(C_{j}\) to be announced before announcing
her secret \(r_{j}\).In summary, we can use a hash function that is hiding and binding to
play the role of a trusted third party in a commitment.Puzzle FriendlyThe concept ofpuzzle friendlyis related tohiding.
Let \(r\) be a value picked from a spread-out distribution. Let
\(H\) map arbitrary length strings to \(n\)-bit strings.
Consider the following problem:
    Given \(H\), \(r\), and an \(n\)-bit value \(y\), compute any \(x\) such that
\(H(r+x) = y\).In this problem, as opposed to thehidingproblem, we are given \(r\) and not \(x\),The hash function \(H\) is said to bepuzzle friendlyexactly when any algorithm to solve this problem is about as slow
as a brute-force algorithm which checks \(H(r+x) = y\) for random values
of \(x\).
The number of steps taken by any
algorithm that solves this problem is not significantly lower than
\(2^{n}\).Now, let's look at the following related problem.  Given \(H\), \(r\),
and a set \(Y\) of \(n\)-bit strings, compute any \(x\) such that
\(H(r+x) \in Y\).  If \(Y\) consists of a single element \(y\) then
this problem is the same as that in the previous paragraph.  If \(Y\)
is a set of all \(n\)-bit strings then this problem is trivial because
any \(x\) solves the problem.  The probability that a random value
hashes to an element of \(Y\) is proportional to the cardinality of
\(Y\).  The cardinality of \(Y\) controls the expected time to
solve the puzzle.The hash function \(H\) ispuzzle friendlywhen
given \(H\), \(r\), and a set \(Y\) of \(n\)-bit values, the time
required to compute any \(x\) such that \(H(r+x) \in Y\) is
not significantly lower than \(2^{n} / |Y|\), where \(|Y|\) is the
cardinality of \(Y\).A Cryptographic Hash FunctionA cryptographic hash function is one that is collision resistant,
hiding and puzzle-friendly.Hashing Inputs of Arbitrary LengthLet \(f\) be a function that operates on input strings
of fixed length and produces output strings of fixed length. Let the
input and output strings of \(f\) have lengths \(M + N\) and \(M\),
respectively. We look at functions where \(N > 0\), and since the
output is smaller than the input, \(f\) is called acompression
function.We can use function \(f\) to define a function \(g\) whose inputs are strings of arbitrary
lengths and whose outputs are strings of length \(M\).
Example code for \(g\) is given below whereInitialValueis a given constant string of length \(M\).def g(y):
    output = InitialValue
    // pad y so that it's length is a multiple of N
    if len(y)%N > 0:  y = y + "0"*(N - len(y)%N)
    // partition y into blocks of size N
    blocks = [y[i: i+N] for i in range(0, len(y), N)]
    // Apply function f to the concatenation of the
    // previous output (length M) with each block
    // (length N) to get the next output (length M).
    for block in blocks: output = f(output+block)
    return outputHash PointersA hash pointer to an item \(D\) of data is a pair \((ptr, H(D))\)
where \(ptr\) is a pointer that 
points to \(D\), and \(H\) is a cryptographic hash function.Any data structure with pointers can be converted into a
data structure with hash pointers: merely replace a pointer \(ptr\) to
\(D\) by \((ptr, H(D))\).Tamper-Evident Data StructuresSingle BlockA simple example of a tamper-evident structure is a single block of
data D which is pointed to by a hash pointer consisting of a regular
pointer and a hash H(D).Fig.1: Hash Pointer points to a Tamper-Evident Block
    of DataAssume that a malicious agent cannot modifyboththe hash
pointer and the data that it points to. If an 
agent
    changes D to D' then the tampering can be discovered because the
    hash pointer won't match the data that it is pointing to:
    \(H(D') \neq H(D)\).Tamper-Evident Linked ListLet's look at linear linked list to which elements can be appended but
not deleted. The \(i\)-th element appended to the list points to the
\((i-1)\)-th element. 
Let's replace the pointers in the list by hashed
pointers.Fig.2: Hash Pointer points to a Tamper-Evident List
    of DataThe \(j\)-th element of the list, \(j > 0\), consists of data,
\(D_{j}\), and a hash pointer that points to the \(j - 1\)-th
  element of the list. The hash pointer in the \(j\)-th element
consists of a regular pointer, \(ptr_{j}\), which points to the \(j - 1\)-th
  element of the list, and a hashed value, \(HA_{j}\), which is a cryptographic hash of theentire\(j-1\)th element consisting  of \(D_{j-1}\), \(ptr_{j-1}\)
  and \(HA_{j-1}\).
The \(0\)-th element is called thegenesiselement and has
default values. The list is accessed by a hash pointer that points to
the last element of the list; let this pointer be \(ptr_{n}, HA_{n}\).Assume that agents cannot modify \(ptr_{n}, HA_{n}\). Then,
can any agent determine whether the list has been tampered with?Suppose the agent modifies \(D_{j}\), \(ptr_{j}\) or
\(HA_{j}\) for any \(j < n\). Any agent can detect this tampering because the
hash value \(HA_{j+1}\) will no longer match the \(j\)-th element of
the list. If
the malicious agent also modifies \(HA_{j+1}\) then 
hash value \(HA_{j+2}\) will no longer match the \(j+1\)-th element.By induction on \(j\), any agent can detect tampering with the list
provided malicious agents do not also modify the hash pointer to the
last element of the last.Tamper-Evident Acyclic Graphs and Merkle
TreesThe idea described in the previous paragraph to convert linear linked
lists can be used to convert directed acyclic graphs, in which nodes are
connected by pointers, into tamper-evident graphs. A specific case of
a directed acyclic graph is a rooted tree.AMerkle treeis a special case of a binary balanced tree in which
data items are stored only in the leaves.
Nodes that are not leaves 
contain only hash pointers to nodes in the next level down.
To prove that an element at the leaf is a member of the tree we need only the
\(log_{2}(n)\) hash pointers on the path from the root to that
leaf. By contrast, to prove that an element is a member of a linear
list we need to inspect \(O(n)\) elements, on average.Keys and Signed MessagesYou can create a random public-key, private-key pairby calling a function on your computer.With high probability, nobody else has this
specific pair of keys. Each individual's private key is a secret held
by that individual. Public keys are accessible by everybody.Sending messages securelyKeys are used to send messages securely.
Kamala sends a secure message to Joe by encrypting the message with
Joe's public key; Joe decrypts the message using Joe's private key.
An agent cannot decrypt the encrypted message without Joe's private key.Signing messagesSuppose Kamala needs to send a signed message to Joe while
ensuring that nobody can forge her signature. She encrypts the messageMwith her private key to get an encrypted messageM',
and sends the pair(M, M')securely to Joe, i.e., she encrypts(M, M')with Joe's public key, and sends the resulting
encrypted message to Joe.  When Joe receives the message, Joe decrypts
it using his private key to get(M, M'). Then Joe decryptsM'using Kamala's public key to get the decrypted messageM''. IfM'' = Mthen Joe knows that Kamala sentMbecause only an agent with Kamala's private signature could
have sent that message.Cryptocurrency Managed by a Trusted
AgentLet's start with a digital currency managed by a trusted agent that we will
call a bank. Later, we will
look at a consensus algorithm --- very different from Paxos and Byzantine
Generals --- which will allow cryptocurrencies without trusted
agent.The bank maintains a tamper-evident linear listLof transactions that we call a
tamper-evidentledger. 
Any agent can get a copy of the ledger.
This tamper-evident ledger is the foundation of the currency.A transaction is one of
two types:createorpay.
In a pay transaction,payersgive coins that they possess topayees. An agent can be both a 
payer and payee of the same transaction.
In acreatetransaction the bank creates coins that it gives to
agents --- the payees of the transaction; the bank acts as the payer.For this system to be trusted the bank must follow some protocol that determines
how and when the bank creates coins. We won't discuss these protocols.Apaytransaction is signed by all payers of the transaction.
Acreatetransaction is signed by the bank.
We discussed digital signatures and keys earlier.Each element of the tamper-evident ledger has:a unique id;the type of the transaction, either create or pay;list of payers: only for pay transactions --- a list
  indicating the agents who pay coins into the transaction and the
  amounts that they put in;array of value-payee pairs: for both create and pay transactions --- an
  array of pairs(value, payee public key), where each
  pair in the array indicates that coins of the specified value are
  given to the payee with the specified public key.Example of a create transactionAn example of acreatetransaction is:(3146, create,  [(2.1, 7xxxx...), (3.2, 8xxxx)]).The id of this transaction is 3146, the type of the transaction is
create, and the array of value-payee pairs is[(2.1, 7xxxx...), (3.2, 8xxx)]In this transaction the bank creates a coin of value 2.1 and gives it
to the agent with public key 7xxxx..., and the bank also
creates a coin of value 3.2 and gives it
to the agent with public key 8xxxx...The pair:(transaction id, index into array of value-payee pairs)uniquely identifies a (value, payee) tuple.For example (3146, 0) --- 
transaction id 3146, and array index 0 --- identifies value-payee[0] of
transaction 3146 which is specified by the 2-tuple: (2.1, 7xxxx...). So, the
transaction id and index, (3146, 0), tells everybody that the agent
with public key 7xxxx received 2.1 units of coin in transaction 3146.When this
transaction is in a tamper-evident ledger, every agent from that point
onwards knows that agent 7xxxx received 2.1 coins. Any modifications of this
record can be detected.Likewise, (3146, 1) --- 
transaction id 3146, and array index 1 --- identifies value-payee[1] which
is the 2-tuple (3.2, 8xxxx...).Pay transactionCoins are transferred from payers to payees in a pay transaction.Coins flowing into a pay transaction from payersThe payers are identified by a list of 2-tuples, where each 2-tuple is(transaction id, index into array of value-payee pairs)wheretransaction idis the id of the transaction in the
tamper-evident ledger. As we said earlier, this pair uniquely identifies an agent and a
value that this agent acquired in this transaction.
For example the pair --- transaction id, index --- such as (3146, 0)
identifies the 2-tuple (2.1, 7xxxx...); this 2-tuple asserts
that the agent
with public key 7xxxx received 2.1 units of coin in 
transaction 3146. Theentire amountspecified in the 2-tuple (2.1 in our
example) is value that flows into thispaytransaction from the agent
with public key 7xxxx.Suppose the payers in a pay transaction are identified by the list:[(3146, 0), (7359, 3)]and suppose pair 3 in transaction 7359 is (3.2, 8xxxx). Then the total
amount of coins flowing into this pay transaction is 2.1 + 3.2, and
this amount is disbursed to payees.Fig.3: Coins flowing into and out of a pay transaction.Coins flowing out of a pay transaction to payeesThe outflow of coins is specified by an array of value-payee pairs,
exactly as in a create transaction.
The transaction clears all coins: the total inflow from payers is
equal to the total outflow to payees in a transaction.The system may provide incentives, such as payment of coins, to
managers (e.g. banks) of cryptocurrencies. In this case, one of the
payees is the bank itself.Managing amounts spent in a transactionA transaction-id, index pair --- such as (3146, 0)
identifies a 2-tuple such as (2.1, 7xxxx...); this 2-tuple asserts
that the agent
with public key 7xxxx received 2.1 units of coin in 
transaction 3146. Theentire amountspecified in the 2-tuple (2.1 in our
example) is value that flows into the transaction. What should this
agent do if it
wants to put in more than 2.1 coins into the transaction? Or less than
2.1 coins?To put in more value, the bank identifies other transaction-id, index
pairs in which this agent received coins. For example, say that (4539, 2)
identifies a 2-tuple (3.2, 7xxxx), and assume that the payers in this transaction
are specified by the pairs (3146, 0) and (4539, 2). The pair (3146, 0)
asserts that agent (7xxxx) received 2.1 coins and the pair (4539, 2)
asserts that the same agent received 3.2 coins. So the total amount of 
coins flowing into this
transaction from this agent (7xxxx) includes 2.1 + 3.2.To put in less value, the agent acts as both payer and payee; the net
value that this agent pays out to other agents is the difference
between the amount that this agent puts in and takes out. For example,
if agent with public key 7xxxx wants to put in 1.9 coins into this
transaction its payer information can be given by the transaction-id,
index pair (3146, 0) which asserts that the agent received 2.1 coins
and the same agent is a payee that withdraws 0.2 coins.Preventing Double SpendingHow does the system prevent an agent from using the
same coin twice?For example, the transaction id, index pair
(3146, 0) identifies the 2-tuple (2.1, 7xxxx...); this tuple
asserts that the agent
with public key 7xxxx received 2.1 units of coin in 
transaction 3146. Why can't the agent with public key 7xxxx use the
2.1 coins that it received to buy items from Amazon and later use
the same 2.1 coins to buy more items from Walmart?The bank checks that the agent hasn't already spent the coin that it
is putting into a transaction.Before permitting the transaction that double-spends the 2.1 coins
with Walmart, the bank inspects the tamper-evident ledger
for all transactions after transaction 3146 and before the Walmart
transaction to ensure that the agent (7xxxx) hasn't already spent the
2.1 coins that it got in transaction 3146. The transaction that spends
the coins at Amazon will show up in the ledger, and so the transaction
with Walmart will not be allowed.Every agent that has the bank's hash pointer to the end of the
tamper-evident ledger can inspect the ledger to check that
double-spending hasn't occurred.The bank signs a valid transaction and appends it to the
tamper-evident ledger. All agents can see the bank's signature and
verify that nobody (not even the bank) has tampered with the
tamper-evident ledger.Example of a pay transactionAn example of the specification of a pay transaction is:(9431, pay,
  [(3146, 0), (4731, 2)],
  [(0.7, 7xxxx...), (4.6, 9xxxx...)]
).The id of this transaction is 9431; the type of the transaction is
pay; the payers into the transaction are identified by the pairs of
(transaction-id, index): (3146, 0), and (4731, 2); and the
payee array is [(0.7, 7xxxx...), (4.6, 9xxxx...)].Transaction validityThe bank appends a transaction toLif and only if the
transaction is valid. The bank checks for validity by carrying out the
following steps:The bank verifies that the payers into the transaction signed the
  transaction.The bank checks that the total value of coins paid out from the
  transaction does not
  exceed the total value paid in to the transaction. (If the value paid
  in exceeds the value paid out then the bank takes the difference as
  a transaction fee. More about fees later.)The bank verifies that the payers' claims to have received coins in
  previous transactions is genuine. For example,
  if the agent with
  public key 7xxxx... claims to have received coins worth 2.1 in the
  transaction with id 10, and payee array index 0, then the bank
  verifies this claim by that transaction.The bank ensures that coins paid into the transaction haven't
  already been spent.Optimizations: Blocks and Block ChainVerifying large numbers of small transactions requires more
computation than verifying small numbers of blocks of many transactions.
Ablock chainis a tamper-evident ledger in which
each element of the ledger is a block consisting of many transactions.
A block of transactions can be aggregated into a single large
transaction by aggregating all the payers and payees of the smaller
transactions.The amount of computation decreases as the number of transactions in a
block increases. The time required to fill a block with transactions
is larger when the number of transactions to fill the block increases.Checking the Trusted AgentConsider a system in which the trusted agent broadcasts its current
copy of the tamper-evident ledger to all agents.
Every agent can inspect its copy of the tamper-evident ledger to determine whether the
ledger has been tampered with. So, every agent can validate its trust
in the trusted agent; however, this validation
suffers from a crucial problem: Agents may only
have copies ofold, staleversions of the ledger.
By the time that an agent receives a copy of the ledger, the trusted
agent may have added more transactions to the ledger.Fig.4: Old Copy is a Prefix of the Block ChainAn old copy of the ledger can differ from the current copy in only one
way: the current copy may have transactions appended to the end
of the old copy. So all agents can validatepastbehavior of the
trusted agent. An agent cannot, however, treat its copy of the ledger
as the master copy because the agent may not have the transactions
added most recently to the ledger.In the next module we will see how the Bitcoin algorithm addresses
this problem.Advantages of this cryptocurrencyAny agent can get a copy of the tamper-evident ledger and verify that all
transactions in the ledger are valid. Any agent can verify that
the only way in which the ledger is modified is that elements are
append to its tail; all that the agent needs to do is to check that
the pointer to the tail is modified only by appending
elements. Because the ledger is tamper-evident, an agent can check
that the ledger doesn't change while the hash pointer to the end
of the ledger doesn't change.The bank can't forge a transaction because all payers sign the
transaction. Agents can remain anonymous because an agent's only
public information is the agent's public key, and an agent can create
multiple public keys. Every agent can verify the correctness of every
transaction.Disadvantages of this cryptocurrencyUsers may not trust the bank. Transactions are not private because the
bank has a record of all transactions. And the bank is a single point
of failure.Next we'll look at Bitcoin's algorithms for
implementing a cryptocurrency without trusted agents.ReviewWhat iscollision resistance? Why is it relevant to
  cryptocurrencies?Consider the example ofhidingin which Megan hides her
  predictions for the winner of the World Cup. Suppose you could
  choose two different random number generators for creating the
  secret \(r\); in one case the bits of \(r\) are uncorrelated, and in
  the other they are highly correlated. Which generator would you use
  and why?What isbinding? Why is it relevant to hiding information?What ispuzzle friendly?What is atamper-evident data structure? How would you
  implement a tamper-evident linear list? A tamper-evident tree?Describe the algorithm for cryptocurrency with a trusted agent to
  someone who knows absolutely nothing about cryptocurrency.How could the algorithm for cryptocurrency be used for a collection
  of distributed agents to keep track of a sequence of events, other
  than buying/selling currency?K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology



Figure Link: https://kmchandy.github.io/Crypto/CryptoCurrencyIntroduction/Slide1.jpg

Caption: Fig.1: Hash Pointer points to a Tamper-Evident Block
    of Data


A brute-force solution is to try every combination of \(r\) and
\(W\). If \(r\) is obtained from a distribution that is spread out,
then finding \(W\) without knowing \(r\)  take so
much time that it is practically impossible.Hiding: Given \(H, y\), where \(y = H(r + W)\) and \(r\) is a secret,
discovering \(W\) is intractable.BindingDoes the hash function \(H\) and the secret \(r\) provide both
services of the trusted third party? Is Megan bound to her prediction
or can she change her "prediction" after knowing the winner of the
World Cup?Suppose Megan has values \(r\) and \(r'\) such that H(r + 'Brazil') =
H(r' + 'Italy'). After the World Cup is over, she can announce that
her secret is \(r\) if Brazil wins, and announce that it is \(r'\) if
Italy wins.A hash function \(H\) isbindingif finding pairs \((x,y)\) and \((x',y')\)
where \(y \neq y'\) such that:
\(H(x + y) = H(x' + y')\)
is intractable.Suppose you give Megan a hash function that is binding. Then she cannot find
  (in reasonable time) values \(r_{j}\) to match country \(C_{j}\)
such that\(H(r_{0} + C_{0}) = H(r_{1} + C_{1})  = H(r_{2} + C_{2})  = \ldots\).and so she can't wait for the winner \(C_{j}\) to be announced before announcing
her secret \(r_{j}\).In summary, we can use a hash function that is hiding and binding to
play the role of a trusted third party in a commitment.Puzzle FriendlyThe concept ofpuzzle friendlyis related tohiding.
Let \(r\) be a value picked from a spread-out distribution. Let
\(H\) map arbitrary length strings to \(n\)-bit strings.
Consider the following problem:
    Given \(H\), \(r\), and an \(n\)-bit value \(y\), compute any \(x\) such that
\(H(r+x) = y\).In this problem, as opposed to thehidingproblem, we are given \(r\) and not \(x\),The hash function \(H\) is said to bepuzzle friendlyexactly when any algorithm to solve this problem is about as slow
as a brute-force algorithm which checks \(H(r+x) = y\) for random values
of \(x\).
The number of steps taken by any
algorithm that solves this problem is not significantly lower than
\(2^{n}\).Now, let's look at the following related problem.  Given \(H\), \(r\),
and a set \(Y\) of \(n\)-bit strings, compute any \(x\) such that
\(H(r+x) \in Y\).  If \(Y\) consists of a single element \(y\) then
this problem is the same as that in the previous paragraph.  If \(Y\)
is a set of all \(n\)-bit strings then this problem is trivial because
any \(x\) solves the problem.  The probability that a random value
hashes to an element of \(Y\) is proportional to the cardinality of
\(Y\).  The cardinality of \(Y\) controls the expected time to
solve the puzzle.The hash function \(H\) ispuzzle friendlywhen
given \(H\), \(r\), and a set \(Y\) of \(n\)-bit values, the time
required to compute any \(x\) such that \(H(r+x) \in Y\) is
not significantly lower than \(2^{n} / |Y|\), where \(|Y|\) is the
cardinality of \(Y\).A Cryptographic Hash FunctionA cryptographic hash function is one that is collision resistant,
hiding and puzzle-friendly.Hashing Inputs of Arbitrary LengthLet \(f\) be a function that operates on input strings
of fixed length and produces output strings of fixed length. Let the
input and output strings of \(f\) have lengths \(M + N\) and \(M\),
respectively. We look at functions where \(N > 0\), and since the
output is smaller than the input, \(f\) is called acompression
function.We can use function \(f\) to define a function \(g\) whose inputs are strings of arbitrary
lengths and whose outputs are strings of length \(M\).
Example code for \(g\) is given below whereInitialValueis a given constant string of length \(M\).def g(y):
    output = InitialValue
    // pad y so that it's length is a multiple of N
    if len(y)%N > 0:  y = y + "0"*(N - len(y)%N)
    // partition y into blocks of size N
    blocks = [y[i: i+N] for i in range(0, len(y), N)]
    // Apply function f to the concatenation of the
    // previous output (length M) with each block
    // (length N) to get the next output (length M).
    for block in blocks: output = f(output+block)
    return outputHash PointersA hash pointer to an item \(D\) of data is a pair \((ptr, H(D))\)
where \(ptr\) is a pointer that 
points to \(D\), and \(H\) is a cryptographic hash function.Any data structure with pointers can be converted into a
data structure with hash pointers: merely replace a pointer \(ptr\) to
\(D\) by \((ptr, H(D))\).Tamper-Evident Data StructuresSingle BlockA simple example of a tamper-evident structure is a single block of
data D which is pointed to by a hash pointer consisting of a regular
pointer and a hash H(D).Fig.1: Hash Pointer points to a Tamper-Evident Block
    of DataAssume that a malicious agent cannot modifyboththe hash
pointer and the data that it points to. If an 
agent
    changes D to D' then the tampering can be discovered because the
    hash pointer won't match the data that it is pointing to:
    \(H(D') \neq H(D)\).Tamper-Evident Linked ListLet's look at linear linked list to which elements can be appended but
not deleted. The \(i\)-th element appended to the list points to the
\((i-1)\)-th element. 
Let's replace the pointers in the list by hashed
pointers.Fig.2: Hash Pointer points to a Tamper-Evident List
    of DataThe \(j\)-th element of the list, \(j > 0\), consists of data,
\(D_{j}\), and a hash pointer that points to the \(j - 1\)-th
  element of the list. The hash pointer in the \(j\)-th element
consists of a regular pointer, \(ptr_{j}\), which points to the \(j - 1\)-th
  element of the list, and a hashed value, \(HA_{j}\), which is a cryptographic hash of theentire\(j-1\)th element consisting  of \(D_{j-1}\), \(ptr_{j-1}\)
  and \(HA_{j-1}\).
The \(0\)-th element is called thegenesiselement and has
default values. The list is accessed by a hash pointer that points to
the last element of the list; let this pointer be \(ptr_{n}, HA_{n}\).Assume that agents cannot modify \(ptr_{n}, HA_{n}\). Then,
can any agent determine whether the list has been tampered with?Suppose the agent modifies \(D_{j}\), \(ptr_{j}\) or
\(HA_{j}\) for any \(j < n\). Any agent can detect this tampering because the
hash value \(HA_{j+1}\) will no longer match the \(j\)-th element of
the list. If
the malicious agent also modifies \(HA_{j+1}\) then 
hash value \(HA_{j+2}\) will no longer match the \(j+1\)-th element.By induction on \(j\), any agent can detect tampering with the list
provided malicious agents do not also modify the hash pointer to the
last element of the last.Tamper-Evident Acyclic Graphs and Merkle
TreesThe idea described in the previous paragraph to convert linear linked
lists can be used to convert directed acyclic graphs, in which nodes are
connected by pointers, into tamper-evident graphs. A specific case of
a directed acyclic graph is a rooted tree.AMerkle treeis a special case of a binary balanced tree in which
data items are stored only in the leaves.
Nodes that are not leaves 
contain only hash pointers to nodes in the next level down.
To prove that an element at the leaf is a member of the tree we need only the
\(log_{2}(n)\) hash pointers on the path from the root to that
leaf. By contrast, to prove that an element is a member of a linear
list we need to inspect \(O(n)\) elements, on average.Keys and Signed MessagesYou can create a random public-key, private-key pairby calling a function on your computer.With high probability, nobody else has this
specific pair of keys. Each individual's private key is a secret held
by that individual. Public keys are accessible by everybody.Sending messages securelyKeys are used to send messages securely.
Kamala sends a secure message to Joe by encrypting the message with
Joe's public key; Joe decrypts the message using Joe's private key.
An agent cannot decrypt the encrypted message without Joe's private key.Signing messagesSuppose Kamala needs to send a signed message to Joe while
ensuring that nobody can forge her signature. She encrypts the messageMwith her private key to get an encrypted messageM',
and sends the pair(M, M')securely to Joe, i.e., she encrypts(M, M')with Joe's public key, and sends the resulting
encrypted message to Joe.  When Joe receives the message, Joe decrypts
it using his private key to get(M, M'). Then Joe decryptsM'using Kamala's public key to get the decrypted messageM''. IfM'' = Mthen Joe knows that Kamala sentMbecause only an agent with Kamala's private signature could
have sent that message.Cryptocurrency Managed by a Trusted
AgentLet's start with a digital currency managed by a trusted agent that we will
call a bank. Later, we will
look at a consensus algorithm --- very different from Paxos and Byzantine
Generals --- which will allow cryptocurrencies without trusted
agent.The bank maintains a tamper-evident linear listLof transactions that we call a
tamper-evidentledger. 
Any agent can get a copy of the ledger.
This tamper-evident ledger is the foundation of the currency.A transaction is one of
two types:createorpay.
In a pay transaction,payersgive coins that they possess topayees. An agent can be both a 
payer and payee of the same transaction.
In acreatetransaction the bank creates coins that it gives to
agents --- the payees of the transaction; the bank acts as the payer.For this system to be trusted the bank must follow some protocol that determines
how and when the bank creates coins. We won't discuss these protocols.Apaytransaction is signed by all payers of the transaction.
Acreatetransaction is signed by the bank.
We discussed digital signatures and keys earlier.Each element of the tamper-evident ledger has:a unique id;the type of the transaction, either create or pay;list of payers: only for pay transactions --- a list
  indicating the agents who pay coins into the transaction and the
  amounts that they put in;array of value-payee pairs: for both create and pay transactions --- an
  array of pairs(value, payee public key), where each
  pair in the array indicates that coins of the specified value are
  given to the payee with the specified public key.Example of a create transactionAn example of acreatetransaction is:(3146, create,  [(2.1, 7xxxx...), (3.2, 8xxxx)]).The id of this transaction is 3146, the type of the transaction is
create, and the array of value-payee pairs is[(2.1, 7xxxx...), (3.2, 8xxx)]In this transaction the bank creates a coin of value 2.1 and gives it
to the agent with public key 7xxxx..., and the bank also
creates a coin of value 3.2 and gives it
to the agent with public key 8xxxx...The pair:(transaction id, index into array of value-payee pairs)uniquely identifies a (value, payee) tuple.For example (3146, 0) --- 
transaction id 3146, and array index 0 --- identifies value-payee[0] of
transaction 3146 which is specified by the 2-tuple: (2.1, 7xxxx...). So, the
transaction id and index, (3146, 0), tells everybody that the agent
with public key 7xxxx received 2.1 units of coin in transaction 3146.When this
transaction is in a tamper-evident ledger, every agent from that point
onwards knows that agent 7xxxx received 2.1 coins. Any modifications of this
record can be detected.Likewise, (3146, 1) --- 
transaction id 3146, and array index 1 --- identifies value-payee[1] which
is the 2-tuple (3.2, 8xxxx...).Pay transactionCoins are transferred from payers to payees in a pay transaction.Coins flowing into a pay transaction from payersThe payers are identified by a list of 2-tuples, where each 2-tuple is(transaction id, index into array of value-payee pairs)wheretransaction idis the id of the transaction in the
tamper-evident ledger. As we said earlier, this pair uniquely identifies an agent and a
value that this agent acquired in this transaction.
For example the pair --- transaction id, index --- such as (3146, 0)
identifies the 2-tuple (2.1, 7xxxx...); this 2-tuple asserts
that the agent
with public key 7xxxx received 2.1 units of coin in 
transaction 3146. Theentire amountspecified in the 2-tuple (2.1 in our
example) is value that flows into thispaytransaction from the agent
with public key 7xxxx.Suppose the payers in a pay transaction are identified by the list:[(3146, 0), (7359, 3)]and suppose pair 3 in transaction 7359 is (3.2, 8xxxx). Then the total
amount of coins flowing into this pay transaction is 2.1 + 3.2, and
this amount is disbursed to payees.Fig.3: Coins flowing into and out of a pay transaction.Coins flowing out of a pay transaction to payeesThe outflow of coins is specified by an array of value-payee pairs,
exactly as in a create transaction.
The transaction clears all coins: the total inflow from payers is
equal to the total outflow to payees in a transaction.The system may provide incentives, such as payment of coins, to
managers (e.g. banks) of cryptocurrencies. In this case, one of the
payees is the bank itself.Managing amounts spent in a transactionA transaction-id, index pair --- such as (3146, 0)
identifies a 2-tuple such as (2.1, 7xxxx...); this 2-tuple asserts
that the agent
with public key 7xxxx received 2.1 units of coin in 
transaction 3146. Theentire amountspecified in the 2-tuple (2.1 in our
example) is value that flows into the transaction. What should this
agent do if it
wants to put in more than 2.1 coins into the transaction? Or less than
2.1 coins?To put in more value, the bank identifies other transaction-id, index
pairs in which this agent received coins. For example, say that (4539, 2)
identifies a 2-tuple (3.2, 7xxxx), and assume that the payers in this transaction
are specified by the pairs (3146, 0) and (4539, 2). The pair (3146, 0)
asserts that agent (7xxxx) received 2.1 coins and the pair (4539, 2)
asserts that the same agent received 3.2 coins. So the total amount of 
coins flowing into this
transaction from this agent (7xxxx) includes 2.1 + 3.2.To put in less value, the agent acts as both payer and payee; the net
value that this agent pays out to other agents is the difference
between the amount that this agent puts in and takes out. For example,
if agent with public key 7xxxx wants to put in 1.9 coins into this
transaction its payer information can be given by the transaction-id,
index pair (3146, 0) which asserts that the agent received 2.1 coins
and the same agent is a payee that withdraws 0.2 coins.Preventing Double SpendingHow does the system prevent an agent from using the
same coin twice?For example, the transaction id, index pair
(3146, 0) identifies the 2-tuple (2.1, 7xxxx...); this tuple
asserts that the agent
with public key 7xxxx received 2.1 units of coin in 
transaction 3146. Why can't the agent with public key 7xxxx use the
2.1 coins that it received to buy items from Amazon and later use
the same 2.1 coins to buy more items from Walmart?The bank checks that the agent hasn't already spent the coin that it
is putting into a transaction.Before permitting the transaction that double-spends the 2.1 coins
with Walmart, the bank inspects the tamper-evident ledger
for all transactions after transaction 3146 and before the Walmart
transaction to ensure that the agent (7xxxx) hasn't already spent the
2.1 coins that it got in transaction 3146. The transaction that spends
the coins at Amazon will show up in the ledger, and so the transaction
with Walmart will not be allowed.Every agent that has the bank's hash pointer to the end of the
tamper-evident ledger can inspect the ledger to check that
double-spending hasn't occurred.The bank signs a valid transaction and appends it to the
tamper-evident ledger. All agents can see the bank's signature and
verify that nobody (not even the bank) has tampered with the
tamper-evident ledger.Example of a pay transactionAn example of the specification of a pay transaction is:(9431, pay,
  [(3146, 0), (4731, 2)],
  [(0.7, 7xxxx...), (4.6, 9xxxx...)]
).The id of this transaction is 9431; the type of the transaction is
pay; the payers into the transaction are identified by the pairs of
(transaction-id, index): (3146, 0), and (4731, 2); and the
payee array is [(0.7, 7xxxx...), (4.6, 9xxxx...)].Transaction validityThe bank appends a transaction toLif and only if the
transaction is valid. The bank checks for validity by carrying out the
following steps:The bank verifies that the payers into the transaction signed the
  transaction.The bank checks that the total value of coins paid out from the
  transaction does not
  exceed the total value paid in to the transaction. (If the value paid
  in exceeds the value paid out then the bank takes the difference as
  a transaction fee. More about fees later.)The bank verifies that the payers' claims to have received coins in
  previous transactions is genuine. For example,
  if the agent with
  public key 7xxxx... claims to have received coins worth 2.1 in the
  transaction with id 10, and payee array index 0, then the bank
  verifies this claim by that transaction.The bank ensures that coins paid into the transaction haven't
  already been spent.Optimizations: Blocks and Block ChainVerifying large numbers of small transactions requires more
computation than verifying small numbers of blocks of many transactions.
Ablock chainis a tamper-evident ledger in which
each element of the ledger is a block consisting of many transactions.
A block of transactions can be aggregated into a single large
transaction by aggregating all the payers and payees of the smaller
transactions.The amount of computation decreases as the number of transactions in a
block increases. The time required to fill a block with transactions
is larger when the number of transactions to fill the block increases.Checking the Trusted AgentConsider a system in which the trusted agent broadcasts its current
copy of the tamper-evident ledger to all agents.
Every agent can inspect its copy of the tamper-evident ledger to determine whether the
ledger has been tampered with. So, every agent can validate its trust
in the trusted agent; however, this validation
suffers from a crucial problem: Agents may only
have copies ofold, staleversions of the ledger.
By the time that an agent receives a copy of the ledger, the trusted
agent may have added more transactions to the ledger.Fig.4: Old Copy is a Prefix of the Block ChainAn old copy of the ledger can differ from the current copy in only one
way: the current copy may have transactions appended to the end
of the old copy. So all agents can validatepastbehavior of the
trusted agent. An agent cannot, however, treat its copy of the ledger
as the master copy because the agent may not have the transactions
added most recently to the ledger.In the next module we will see how the Bitcoin algorithm addresses
this problem.Advantages of this cryptocurrencyAny agent can get a copy of the tamper-evident ledger and verify that all
transactions in the ledger are valid. Any agent can verify that
the only way in which the ledger is modified is that elements are
append to its tail; all that the agent needs to do is to check that
the pointer to the tail is modified only by appending
elements. Because the ledger is tamper-evident, an agent can check
that the ledger doesn't change while the hash pointer to the end
of the ledger doesn't change.The bank can't forge a transaction because all payers sign the
transaction. Agents can remain anonymous because an agent's only
public information is the agent's public key, and an agent can create
multiple public keys. Every agent can verify the correctness of every
transaction.Disadvantages of this cryptocurrencyUsers may not trust the bank. Transactions are not private because the
bank has a record of all transactions. And the bank is a single point
of failure.Next we'll look at Bitcoin's algorithms for
implementing a cryptocurrency without trusted agents.ReviewWhat iscollision resistance? Why is it relevant to
  cryptocurrencies?Consider the example ofhidingin which Megan hides her
  predictions for the winner of the World Cup. Suppose you could
  choose two different random number generators for creating the
  secret \(r\); in one case the bits of \(r\) are uncorrelated, and in
  the other they are highly correlated. Which generator would you use
  and why?What isbinding? Why is it relevant to hiding information?What ispuzzle friendly?What is atamper-evident data structure? How would you
  implement a tamper-evident linear list? A tamper-evident tree?Describe the algorithm for cryptocurrency with a trusted agent to
  someone who knows absolutely nothing about cryptocurrency.How could the algorithm for cryptocurrency be used for a collection
  of distributed agents to keep track of a sequence of events, other
  than buying/selling currency?K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology



Figure Link: https://kmchandy.github.io/Crypto/CryptoCurrencyIntroduction/Slide1.jpg

Caption: Fig.1: Hash Pointer points to a Tamper-Evident Block
    of Data


Does the hash function \(H\) and the secret \(r\) provide both
services of the trusted third party? Is Megan bound to her prediction
or can she change her "prediction" after knowing the winner of the
World Cup?Suppose Megan has values \(r\) and \(r'\) such that H(r + 'Brazil') =
H(r' + 'Italy'). After the World Cup is over, she can announce that
her secret is \(r\) if Brazil wins, and announce that it is \(r'\) if
Italy wins.A hash function \(H\) isbindingif finding pairs \((x,y)\) and \((x',y')\)
where \(y \neq y'\) such that:
\(H(x + y) = H(x' + y')\)
is intractable.Suppose you give Megan a hash function that is binding. Then she cannot find
  (in reasonable time) values \(r_{j}\) to match country \(C_{j}\)
such that\(H(r_{0} + C_{0}) = H(r_{1} + C_{1})  = H(r_{2} + C_{2})  = \ldots\).and so she can't wait for the winner \(C_{j}\) to be announced before announcing
her secret \(r_{j}\).In summary, we can use a hash function that is hiding and binding to
play the role of a trusted third party in a commitment.Puzzle FriendlyThe concept ofpuzzle friendlyis related tohiding.
Let \(r\) be a value picked from a spread-out distribution. Let
\(H\) map arbitrary length strings to \(n\)-bit strings.
Consider the following problem:
    Given \(H\), \(r\), and an \(n\)-bit value \(y\), compute any \(x\) such that
\(H(r+x) = y\).In this problem, as opposed to thehidingproblem, we are given \(r\) and not \(x\),The hash function \(H\) is said to bepuzzle friendlyexactly when any algorithm to solve this problem is about as slow
as a brute-force algorithm which checks \(H(r+x) = y\) for random values
of \(x\).
The number of steps taken by any
algorithm that solves this problem is not significantly lower than
\(2^{n}\).Now, let's look at the following related problem.  Given \(H\), \(r\),
and a set \(Y\) of \(n\)-bit strings, compute any \(x\) such that
\(H(r+x) \in Y\).  If \(Y\) consists of a single element \(y\) then
this problem is the same as that in the previous paragraph.  If \(Y\)
is a set of all \(n\)-bit strings then this problem is trivial because
any \(x\) solves the problem.  The probability that a random value
hashes to an element of \(Y\) is proportional to the cardinality of
\(Y\).  The cardinality of \(Y\) controls the expected time to
solve the puzzle.The hash function \(H\) ispuzzle friendlywhen
given \(H\), \(r\), and a set \(Y\) of \(n\)-bit values, the time
required to compute any \(x\) such that \(H(r+x) \in Y\) is
not significantly lower than \(2^{n} / |Y|\), where \(|Y|\) is the
cardinality of \(Y\).A Cryptographic Hash FunctionA cryptographic hash function is one that is collision resistant,
hiding and puzzle-friendly.Hashing Inputs of Arbitrary LengthLet \(f\) be a function that operates on input strings
of fixed length and produces output strings of fixed length. Let the
input and output strings of \(f\) have lengths \(M + N\) and \(M\),
respectively. We look at functions where \(N > 0\), and since the
output is smaller than the input, \(f\) is called acompression
function.We can use function \(f\) to define a function \(g\) whose inputs are strings of arbitrary
lengths and whose outputs are strings of length \(M\).
Example code for \(g\) is given below whereInitialValueis a given constant string of length \(M\).def g(y):
    output = InitialValue
    // pad y so that it's length is a multiple of N
    if len(y)%N > 0:  y = y + "0"*(N - len(y)%N)
    // partition y into blocks of size N
    blocks = [y[i: i+N] for i in range(0, len(y), N)]
    // Apply function f to the concatenation of the
    // previous output (length M) with each block
    // (length N) to get the next output (length M).
    for block in blocks: output = f(output+block)
    return outputHash PointersA hash pointer to an item \(D\) of data is a pair \((ptr, H(D))\)
where \(ptr\) is a pointer that 
points to \(D\), and \(H\) is a cryptographic hash function.Any data structure with pointers can be converted into a
data structure with hash pointers: merely replace a pointer \(ptr\) to
\(D\) by \((ptr, H(D))\).Tamper-Evident Data StructuresSingle BlockA simple example of a tamper-evident structure is a single block of
data D which is pointed to by a hash pointer consisting of a regular
pointer and a hash H(D).Fig.1: Hash Pointer points to a Tamper-Evident Block
    of DataAssume that a malicious agent cannot modifyboththe hash
pointer and the data that it points to. If an 
agent
    changes D to D' then the tampering can be discovered because the
    hash pointer won't match the data that it is pointing to:
    \(H(D') \neq H(D)\).Tamper-Evident Linked ListLet's look at linear linked list to which elements can be appended but
not deleted. The \(i\)-th element appended to the list points to the
\((i-1)\)-th element. 
Let's replace the pointers in the list by hashed
pointers.Fig.2: Hash Pointer points to a Tamper-Evident List
    of DataThe \(j\)-th element of the list, \(j > 0\), consists of data,
\(D_{j}\), and a hash pointer that points to the \(j - 1\)-th
  element of the list. The hash pointer in the \(j\)-th element
consists of a regular pointer, \(ptr_{j}\), which points to the \(j - 1\)-th
  element of the list, and a hashed value, \(HA_{j}\), which is a cryptographic hash of theentire\(j-1\)th element consisting  of \(D_{j-1}\), \(ptr_{j-1}\)
  and \(HA_{j-1}\).
The \(0\)-th element is called thegenesiselement and has
default values. The list is accessed by a hash pointer that points to
the last element of the list; let this pointer be \(ptr_{n}, HA_{n}\).Assume that agents cannot modify \(ptr_{n}, HA_{n}\). Then,
can any agent determine whether the list has been tampered with?Suppose the agent modifies \(D_{j}\), \(ptr_{j}\) or
\(HA_{j}\) for any \(j < n\). Any agent can detect this tampering because the
hash value \(HA_{j+1}\) will no longer match the \(j\)-th element of
the list. If
the malicious agent also modifies \(HA_{j+1}\) then 
hash value \(HA_{j+2}\) will no longer match the \(j+1\)-th element.By induction on \(j\), any agent can detect tampering with the list
provided malicious agents do not also modify the hash pointer to the
last element of the last.Tamper-Evident Acyclic Graphs and Merkle
TreesThe idea described in the previous paragraph to convert linear linked
lists can be used to convert directed acyclic graphs, in which nodes are
connected by pointers, into tamper-evident graphs. A specific case of
a directed acyclic graph is a rooted tree.AMerkle treeis a special case of a binary balanced tree in which
data items are stored only in the leaves.
Nodes that are not leaves 
contain only hash pointers to nodes in the next level down.
To prove that an element at the leaf is a member of the tree we need only the
\(log_{2}(n)\) hash pointers on the path from the root to that
leaf. By contrast, to prove that an element is a member of a linear
list we need to inspect \(O(n)\) elements, on average.Keys and Signed MessagesYou can create a random public-key, private-key pairby calling a function on your computer.With high probability, nobody else has this
specific pair of keys. Each individual's private key is a secret held
by that individual. Public keys are accessible by everybody.Sending messages securelyKeys are used to send messages securely.
Kamala sends a secure message to Joe by encrypting the message with
Joe's public key; Joe decrypts the message using Joe's private key.
An agent cannot decrypt the encrypted message without Joe's private key.Signing messagesSuppose Kamala needs to send a signed message to Joe while
ensuring that nobody can forge her signature. She encrypts the messageMwith her private key to get an encrypted messageM',
and sends the pair(M, M')securely to Joe, i.e., she encrypts(M, M')with Joe's public key, and sends the resulting
encrypted message to Joe.  When Joe receives the message, Joe decrypts
it using his private key to get(M, M'). Then Joe decryptsM'using Kamala's public key to get the decrypted messageM''. IfM'' = Mthen Joe knows that Kamala sentMbecause only an agent with Kamala's private signature could
have sent that message.Cryptocurrency Managed by a Trusted
AgentLet's start with a digital currency managed by a trusted agent that we will
call a bank. Later, we will
look at a consensus algorithm --- very different from Paxos and Byzantine
Generals --- which will allow cryptocurrencies without trusted
agent.The bank maintains a tamper-evident linear listLof transactions that we call a
tamper-evidentledger. 
Any agent can get a copy of the ledger.
This tamper-evident ledger is the foundation of the currency.A transaction is one of
two types:createorpay.
In a pay transaction,payersgive coins that they possess topayees. An agent can be both a 
payer and payee of the same transaction.
In acreatetransaction the bank creates coins that it gives to
agents --- the payees of the transaction; the bank acts as the payer.For this system to be trusted the bank must follow some protocol that determines
how and when the bank creates coins. We won't discuss these protocols.Apaytransaction is signed by all payers of the transaction.
Acreatetransaction is signed by the bank.
We discussed digital signatures and keys earlier.Each element of the tamper-evident ledger has:a unique id;the type of the transaction, either create or pay;list of payers: only for pay transactions --- a list
  indicating the agents who pay coins into the transaction and the
  amounts that they put in;array of value-payee pairs: for both create and pay transactions --- an
  array of pairs(value, payee public key), where each
  pair in the array indicates that coins of the specified value are
  given to the payee with the specified public key.Example of a create transactionAn example of acreatetransaction is:(3146, create,  [(2.1, 7xxxx...), (3.2, 8xxxx)]).The id of this transaction is 3146, the type of the transaction is
create, and the array of value-payee pairs is[(2.1, 7xxxx...), (3.2, 8xxx)]In this transaction the bank creates a coin of value 2.1 and gives it
to the agent with public key 7xxxx..., and the bank also
creates a coin of value 3.2 and gives it
to the agent with public key 8xxxx...The pair:(transaction id, index into array of value-payee pairs)uniquely identifies a (value, payee) tuple.For example (3146, 0) --- 
transaction id 3146, and array index 0 --- identifies value-payee[0] of
transaction 3146 which is specified by the 2-tuple: (2.1, 7xxxx...). So, the
transaction id and index, (3146, 0), tells everybody that the agent
with public key 7xxxx received 2.1 units of coin in transaction 3146.When this
transaction is in a tamper-evident ledger, every agent from that point
onwards knows that agent 7xxxx received 2.1 coins. Any modifications of this
record can be detected.Likewise, (3146, 1) --- 
transaction id 3146, and array index 1 --- identifies value-payee[1] which
is the 2-tuple (3.2, 8xxxx...).Pay transactionCoins are transferred from payers to payees in a pay transaction.Coins flowing into a pay transaction from payersThe payers are identified by a list of 2-tuples, where each 2-tuple is(transaction id, index into array of value-payee pairs)wheretransaction idis the id of the transaction in the
tamper-evident ledger. As we said earlier, this pair uniquely identifies an agent and a
value that this agent acquired in this transaction.
For example the pair --- transaction id, index --- such as (3146, 0)
identifies the 2-tuple (2.1, 7xxxx...); this 2-tuple asserts
that the agent
with public key 7xxxx received 2.1 units of coin in 
transaction 3146. Theentire amountspecified in the 2-tuple (2.1 in our
example) is value that flows into thispaytransaction from the agent
with public key 7xxxx.Suppose the payers in a pay transaction are identified by the list:[(3146, 0), (7359, 3)]and suppose pair 3 in transaction 7359 is (3.2, 8xxxx). Then the total
amount of coins flowing into this pay transaction is 2.1 + 3.2, and
this amount is disbursed to payees.Fig.3: Coins flowing into and out of a pay transaction.Coins flowing out of a pay transaction to payeesThe outflow of coins is specified by an array of value-payee pairs,
exactly as in a create transaction.
The transaction clears all coins: the total inflow from payers is
equal to the total outflow to payees in a transaction.The system may provide incentives, such as payment of coins, to
managers (e.g. banks) of cryptocurrencies. In this case, one of the
payees is the bank itself.Managing amounts spent in a transactionA transaction-id, index pair --- such as (3146, 0)
identifies a 2-tuple such as (2.1, 7xxxx...); this 2-tuple asserts
that the agent
with public key 7xxxx received 2.1 units of coin in 
transaction 3146. Theentire amountspecified in the 2-tuple (2.1 in our
example) is value that flows into the transaction. What should this
agent do if it
wants to put in more than 2.1 coins into the transaction? Or less than
2.1 coins?To put in more value, the bank identifies other transaction-id, index
pairs in which this agent received coins. For example, say that (4539, 2)
identifies a 2-tuple (3.2, 7xxxx), and assume that the payers in this transaction
are specified by the pairs (3146, 0) and (4539, 2). The pair (3146, 0)
asserts that agent (7xxxx) received 2.1 coins and the pair (4539, 2)
asserts that the same agent received 3.2 coins. So the total amount of 
coins flowing into this
transaction from this agent (7xxxx) includes 2.1 + 3.2.To put in less value, the agent acts as both payer and payee; the net
value that this agent pays out to other agents is the difference
between the amount that this agent puts in and takes out. For example,
if agent with public key 7xxxx wants to put in 1.9 coins into this
transaction its payer information can be given by the transaction-id,
index pair (3146, 0) which asserts that the agent received 2.1 coins
and the same agent is a payee that withdraws 0.2 coins.Preventing Double SpendingHow does the system prevent an agent from using the
same coin twice?For example, the transaction id, index pair
(3146, 0) identifies the 2-tuple (2.1, 7xxxx...); this tuple
asserts that the agent
with public key 7xxxx received 2.1 units of coin in 
transaction 3146. Why can't the agent with public key 7xxxx use the
2.1 coins that it received to buy items from Amazon and later use
the same 2.1 coins to buy more items from Walmart?The bank checks that the agent hasn't already spent the coin that it
is putting into a transaction.Before permitting the transaction that double-spends the 2.1 coins
with Walmart, the bank inspects the tamper-evident ledger
for all transactions after transaction 3146 and before the Walmart
transaction to ensure that the agent (7xxxx) hasn't already spent the
2.1 coins that it got in transaction 3146. The transaction that spends
the coins at Amazon will show up in the ledger, and so the transaction
with Walmart will not be allowed.Every agent that has the bank's hash pointer to the end of the
tamper-evident ledger can inspect the ledger to check that
double-spending hasn't occurred.The bank signs a valid transaction and appends it to the
tamper-evident ledger. All agents can see the bank's signature and
verify that nobody (not even the bank) has tampered with the
tamper-evident ledger.Example of a pay transactionAn example of the specification of a pay transaction is:(9431, pay,
  [(3146, 0), (4731, 2)],
  [(0.7, 7xxxx...), (4.6, 9xxxx...)]
).The id of this transaction is 9431; the type of the transaction is
pay; the payers into the transaction are identified by the pairs of
(transaction-id, index): (3146, 0), and (4731, 2); and the
payee array is [(0.7, 7xxxx...), (4.6, 9xxxx...)].Transaction validityThe bank appends a transaction toLif and only if the
transaction is valid. The bank checks for validity by carrying out the
following steps:The bank verifies that the payers into the transaction signed the
  transaction.The bank checks that the total value of coins paid out from the
  transaction does not
  exceed the total value paid in to the transaction. (If the value paid
  in exceeds the value paid out then the bank takes the difference as
  a transaction fee. More about fees later.)The bank verifies that the payers' claims to have received coins in
  previous transactions is genuine. For example,
  if the agent with
  public key 7xxxx... claims to have received coins worth 2.1 in the
  transaction with id 10, and payee array index 0, then the bank
  verifies this claim by that transaction.The bank ensures that coins paid into the transaction haven't
  already been spent.Optimizations: Blocks and Block ChainVerifying large numbers of small transactions requires more
computation than verifying small numbers of blocks of many transactions.
Ablock chainis a tamper-evident ledger in which
each element of the ledger is a block consisting of many transactions.
A block of transactions can be aggregated into a single large
transaction by aggregating all the payers and payees of the smaller
transactions.The amount of computation decreases as the number of transactions in a
block increases. The time required to fill a block with transactions
is larger when the number of transactions to fill the block increases.Checking the Trusted AgentConsider a system in which the trusted agent broadcasts its current
copy of the tamper-evident ledger to all agents.
Every agent can inspect its copy of the tamper-evident ledger to determine whether the
ledger has been tampered with. So, every agent can validate its trust
in the trusted agent; however, this validation
suffers from a crucial problem: Agents may only
have copies ofold, staleversions of the ledger.
By the time that an agent receives a copy of the ledger, the trusted
agent may have added more transactions to the ledger.Fig.4: Old Copy is a Prefix of the Block ChainAn old copy of the ledger can differ from the current copy in only one
way: the current copy may have transactions appended to the end
of the old copy. So all agents can validatepastbehavior of the
trusted agent. An agent cannot, however, treat its copy of the ledger
as the master copy because the agent may not have the transactions
added most recently to the ledger.In the next module we will see how the Bitcoin algorithm addresses
this problem.Advantages of this cryptocurrencyAny agent can get a copy of the tamper-evident ledger and verify that all
transactions in the ledger are valid. Any agent can verify that
the only way in which the ledger is modified is that elements are
append to its tail; all that the agent needs to do is to check that
the pointer to the tail is modified only by appending
elements. Because the ledger is tamper-evident, an agent can check
that the ledger doesn't change while the hash pointer to the end
of the ledger doesn't change.The bank can't forge a transaction because all payers sign the
transaction. Agents can remain anonymous because an agent's only
public information is the agent's public key, and an agent can create
multiple public keys. Every agent can verify the correctness of every
transaction.Disadvantages of this cryptocurrencyUsers may not trust the bank. Transactions are not private because the
bank has a record of all transactions. And the bank is a single point
of failure.Next we'll look at Bitcoin's algorithms for
implementing a cryptocurrency without trusted agents.ReviewWhat iscollision resistance? Why is it relevant to
  cryptocurrencies?Consider the example ofhidingin which Megan hides her
  predictions for the winner of the World Cup. Suppose you could
  choose two different random number generators for creating the
  secret \(r\); in one case the bits of \(r\) are uncorrelated, and in
  the other they are highly correlated. Which generator would you use
  and why?What isbinding? Why is it relevant to hiding information?What ispuzzle friendly?What is atamper-evident data structure? How would you
  implement a tamper-evident linear list? A tamper-evident tree?Describe the algorithm for cryptocurrency with a trusted agent to
  someone who knows absolutely nothing about cryptocurrency.How could the algorithm for cryptocurrency be used for a collection
  of distributed agents to keep track of a sequence of events, other
  than buying/selling currency?K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology



Figure Link: https://kmchandy.github.io/Crypto/CryptoCurrencyIntroduction/Slide1.jpg

Caption: Fig.1: Hash Pointer points to a Tamper-Evident Block
    of Data


Suppose Megan has values \(r\) and \(r'\) such that H(r + 'Brazil') =
H(r' + 'Italy'). After the World Cup is over, she can announce that
her secret is \(r\) if Brazil wins, and announce that it is \(r'\) if
Italy wins.A hash function \(H\) isbindingif finding pairs \((x,y)\) and \((x',y')\)
where \(y \neq y'\) such that:
\(H(x + y) = H(x' + y')\)
is intractable.Suppose you give Megan a hash function that is binding. Then she cannot find
  (in reasonable time) values \(r_{j}\) to match country \(C_{j}\)
such that\(H(r_{0} + C_{0}) = H(r_{1} + C_{1})  = H(r_{2} + C_{2})  = \ldots\).and so she can't wait for the winner \(C_{j}\) to be announced before announcing
her secret \(r_{j}\).In summary, we can use a hash function that is hiding and binding to
play the role of a trusted third party in a commitment.Puzzle FriendlyThe concept ofpuzzle friendlyis related tohiding.
Let \(r\) be a value picked from a spread-out distribution. Let
\(H\) map arbitrary length strings to \(n\)-bit strings.
Consider the following problem:
    Given \(H\), \(r\), and an \(n\)-bit value \(y\), compute any \(x\) such that
\(H(r+x) = y\).In this problem, as opposed to thehidingproblem, we are given \(r\) and not \(x\),The hash function \(H\) is said to bepuzzle friendlyexactly when any algorithm to solve this problem is about as slow
as a brute-force algorithm which checks \(H(r+x) = y\) for random values
of \(x\).
The number of steps taken by any
algorithm that solves this problem is not significantly lower than
\(2^{n}\).Now, let's look at the following related problem.  Given \(H\), \(r\),
and a set \(Y\) of \(n\)-bit strings, compute any \(x\) such that
\(H(r+x) \in Y\).  If \(Y\) consists of a single element \(y\) then
this problem is the same as that in the previous paragraph.  If \(Y\)
is a set of all \(n\)-bit strings then this problem is trivial because
any \(x\) solves the problem.  The probability that a random value
hashes to an element of \(Y\) is proportional to the cardinality of
\(Y\).  The cardinality of \(Y\) controls the expected time to
solve the puzzle.The hash function \(H\) ispuzzle friendlywhen
given \(H\), \(r\), and a set \(Y\) of \(n\)-bit values, the time
required to compute any \(x\) such that \(H(r+x) \in Y\) is
not significantly lower than \(2^{n} / |Y|\), where \(|Y|\) is the
cardinality of \(Y\).A Cryptographic Hash FunctionA cryptographic hash function is one that is collision resistant,
hiding and puzzle-friendly.Hashing Inputs of Arbitrary LengthLet \(f\) be a function that operates on input strings
of fixed length and produces output strings of fixed length. Let the
input and output strings of \(f\) have lengths \(M + N\) and \(M\),
respectively. We look at functions where \(N > 0\), and since the
output is smaller than the input, \(f\) is called acompression
function.We can use function \(f\) to define a function \(g\) whose inputs are strings of arbitrary
lengths and whose outputs are strings of length \(M\).
Example code for \(g\) is given below whereInitialValueis a given constant string of length \(M\).def g(y):
    output = InitialValue
    // pad y so that it's length is a multiple of N
    if len(y)%N > 0:  y = y + "0"*(N - len(y)%N)
    // partition y into blocks of size N
    blocks = [y[i: i+N] for i in range(0, len(y), N)]
    // Apply function f to the concatenation of the
    // previous output (length M) with each block
    // (length N) to get the next output (length M).
    for block in blocks: output = f(output+block)
    return outputHash PointersA hash pointer to an item \(D\) of data is a pair \((ptr, H(D))\)
where \(ptr\) is a pointer that 
points to \(D\), and \(H\) is a cryptographic hash function.Any data structure with pointers can be converted into a
data structure with hash pointers: merely replace a pointer \(ptr\) to
\(D\) by \((ptr, H(D))\).Tamper-Evident Data StructuresSingle BlockA simple example of a tamper-evident structure is a single block of
data D which is pointed to by a hash pointer consisting of a regular
pointer and a hash H(D).Fig.1: Hash Pointer points to a Tamper-Evident Block
    of DataAssume that a malicious agent cannot modifyboththe hash
pointer and the data that it points to. If an 
agent
    changes D to D' then the tampering can be discovered because the
    hash pointer won't match the data that it is pointing to:
    \(H(D') \neq H(D)\).Tamper-Evident Linked ListLet's look at linear linked list to which elements can be appended but
not deleted. The \(i\)-th element appended to the list points to the
\((i-1)\)-th element. 
Let's replace the pointers in the list by hashed
pointers.Fig.2: Hash Pointer points to a Tamper-Evident List
    of DataThe \(j\)-th element of the list, \(j > 0\), consists of data,
\(D_{j}\), and a hash pointer that points to the \(j - 1\)-th
  element of the list. The hash pointer in the \(j\)-th element
consists of a regular pointer, \(ptr_{j}\), which points to the \(j - 1\)-th
  element of the list, and a hashed value, \(HA_{j}\), which is a cryptographic hash of theentire\(j-1\)th element consisting  of \(D_{j-1}\), \(ptr_{j-1}\)
  and \(HA_{j-1}\).
The \(0\)-th element is called thegenesiselement and has
default values. The list is accessed by a hash pointer that points to
the last element of the list; let this pointer be \(ptr_{n}, HA_{n}\).Assume that agents cannot modify \(ptr_{n}, HA_{n}\). Then,
can any agent determine whether the list has been tampered with?Suppose the agent modifies \(D_{j}\), \(ptr_{j}\) or
\(HA_{j}\) for any \(j < n\). Any agent can detect this tampering because the
hash value \(HA_{j+1}\) will no longer match the \(j\)-th element of
the list. If
the malicious agent also modifies \(HA_{j+1}\) then 
hash value \(HA_{j+2}\) will no longer match the \(j+1\)-th element.By induction on \(j\), any agent can detect tampering with the list
provided malicious agents do not also modify the hash pointer to the
last element of the last.Tamper-Evident Acyclic Graphs and Merkle
TreesThe idea described in the previous paragraph to convert linear linked
lists can be used to convert directed acyclic graphs, in which nodes are
connected by pointers, into tamper-evident graphs. A specific case of
a directed acyclic graph is a rooted tree.AMerkle treeis a special case of a binary balanced tree in which
data items are stored only in the leaves.
Nodes that are not leaves 
contain only hash pointers to nodes in the next level down.
To prove that an element at the leaf is a member of the tree we need only the
\(log_{2}(n)\) hash pointers on the path from the root to that
leaf. By contrast, to prove that an element is a member of a linear
list we need to inspect \(O(n)\) elements, on average.Keys and Signed MessagesYou can create a random public-key, private-key pairby calling a function on your computer.With high probability, nobody else has this
specific pair of keys. Each individual's private key is a secret held
by that individual. Public keys are accessible by everybody.Sending messages securelyKeys are used to send messages securely.
Kamala sends a secure message to Joe by encrypting the message with
Joe's public key; Joe decrypts the message using Joe's private key.
An agent cannot decrypt the encrypted message without Joe's private key.Signing messagesSuppose Kamala needs to send a signed message to Joe while
ensuring that nobody can forge her signature. She encrypts the messageMwith her private key to get an encrypted messageM',
and sends the pair(M, M')securely to Joe, i.e., she encrypts(M, M')with Joe's public key, and sends the resulting
encrypted message to Joe.  When Joe receives the message, Joe decrypts
it using his private key to get(M, M'). Then Joe decryptsM'using Kamala's public key to get the decrypted messageM''. IfM'' = Mthen Joe knows that Kamala sentMbecause only an agent with Kamala's private signature could
have sent that message.Cryptocurrency Managed by a Trusted
AgentLet's start with a digital currency managed by a trusted agent that we will
call a bank. Later, we will
look at a consensus algorithm --- very different from Paxos and Byzantine
Generals --- which will allow cryptocurrencies without trusted
agent.The bank maintains a tamper-evident linear listLof transactions that we call a
tamper-evidentledger. 
Any agent can get a copy of the ledger.
This tamper-evident ledger is the foundation of the currency.A transaction is one of
two types:createorpay.
In a pay transaction,payersgive coins that they possess topayees. An agent can be both a 
payer and payee of the same transaction.
In acreatetransaction the bank creates coins that it gives to
agents --- the payees of the transaction; the bank acts as the payer.For this system to be trusted the bank must follow some protocol that determines
how and when the bank creates coins. We won't discuss these protocols.Apaytransaction is signed by all payers of the transaction.
Acreatetransaction is signed by the bank.
We discussed digital signatures and keys earlier.Each element of the tamper-evident ledger has:a unique id;the type of the transaction, either create or pay;list of payers: only for pay transactions --- a list
  indicating the agents who pay coins into the transaction and the
  amounts that they put in;array of value-payee pairs: for both create and pay transactions --- an
  array of pairs(value, payee public key), where each
  pair in the array indicates that coins of the specified value are
  given to the payee with the specified public key.Example of a create transactionAn example of acreatetransaction is:(3146, create,  [(2.1, 7xxxx...), (3.2, 8xxxx)]).The id of this transaction is 3146, the type of the transaction is
create, and the array of value-payee pairs is[(2.1, 7xxxx...), (3.2, 8xxx)]In this transaction the bank creates a coin of value 2.1 and gives it
to the agent with public key 7xxxx..., and the bank also
creates a coin of value 3.2 and gives it
to the agent with public key 8xxxx...The pair:(transaction id, index into array of value-payee pairs)uniquely identifies a (value, payee) tuple.For example (3146, 0) --- 
transaction id 3146, and array index 0 --- identifies value-payee[0] of
transaction 3146 which is specified by the 2-tuple: (2.1, 7xxxx...). So, the
transaction id and index, (3146, 0), tells everybody that the agent
with public key 7xxxx received 2.1 units of coin in transaction 3146.When this
transaction is in a tamper-evident ledger, every agent from that point
onwards knows that agent 7xxxx received 2.1 coins. Any modifications of this
record can be detected.Likewise, (3146, 1) --- 
transaction id 3146, and array index 1 --- identifies value-payee[1] which
is the 2-tuple (3.2, 8xxxx...).Pay transactionCoins are transferred from payers to payees in a pay transaction.Coins flowing into a pay transaction from payersThe payers are identified by a list of 2-tuples, where each 2-tuple is(transaction id, index into array of value-payee pairs)wheretransaction idis the id of the transaction in the
tamper-evident ledger. As we said earlier, this pair uniquely identifies an agent and a
value that this agent acquired in this transaction.
For example the pair --- transaction id, index --- such as (3146, 0)
identifies the 2-tuple (2.1, 7xxxx...); this 2-tuple asserts
that the agent
with public key 7xxxx received 2.1 units of coin in 
transaction 3146. Theentire amountspecified in the 2-tuple (2.1 in our
example) is value that flows into thispaytransaction from the agent
with public key 7xxxx.Suppose the payers in a pay transaction are identified by the list:[(3146, 0), (7359, 3)]and suppose pair 3 in transaction 7359 is (3.2, 8xxxx). Then the total
amount of coins flowing into this pay transaction is 2.1 + 3.2, and
this amount is disbursed to payees.Fig.3: Coins flowing into and out of a pay transaction.Coins flowing out of a pay transaction to payeesThe outflow of coins is specified by an array of value-payee pairs,
exactly as in a create transaction.
The transaction clears all coins: the total inflow from payers is
equal to the total outflow to payees in a transaction.The system may provide incentives, such as payment of coins, to
managers (e.g. banks) of cryptocurrencies. In this case, one of the
payees is the bank itself.Managing amounts spent in a transactionA transaction-id, index pair --- such as (3146, 0)
identifies a 2-tuple such as (2.1, 7xxxx...); this 2-tuple asserts
that the agent
with public key 7xxxx received 2.1 units of coin in 
transaction 3146. Theentire amountspecified in the 2-tuple (2.1 in our
example) is value that flows into the transaction. What should this
agent do if it
wants to put in more than 2.1 coins into the transaction? Or less than
2.1 coins?To put in more value, the bank identifies other transaction-id, index
pairs in which this agent received coins. For example, say that (4539, 2)
identifies a 2-tuple (3.2, 7xxxx), and assume that the payers in this transaction
are specified by the pairs (3146, 0) and (4539, 2). The pair (3146, 0)
asserts that agent (7xxxx) received 2.1 coins and the pair (4539, 2)
asserts that the same agent received 3.2 coins. So the total amount of 
coins flowing into this
transaction from this agent (7xxxx) includes 2.1 + 3.2.To put in less value, the agent acts as both payer and payee; the net
value that this agent pays out to other agents is the difference
between the amount that this agent puts in and takes out. For example,
if agent with public key 7xxxx wants to put in 1.9 coins into this
transaction its payer information can be given by the transaction-id,
index pair (3146, 0) which asserts that the agent received 2.1 coins
and the same agent is a payee that withdraws 0.2 coins.Preventing Double SpendingHow does the system prevent an agent from using the
same coin twice?For example, the transaction id, index pair
(3146, 0) identifies the 2-tuple (2.1, 7xxxx...); this tuple
asserts that the agent
with public key 7xxxx received 2.1 units of coin in 
transaction 3146. Why can't the agent with public key 7xxxx use the
2.1 coins that it received to buy items from Amazon and later use
the same 2.1 coins to buy more items from Walmart?The bank checks that the agent hasn't already spent the coin that it
is putting into a transaction.Before permitting the transaction that double-spends the 2.1 coins
with Walmart, the bank inspects the tamper-evident ledger
for all transactions after transaction 3146 and before the Walmart
transaction to ensure that the agent (7xxxx) hasn't already spent the
2.1 coins that it got in transaction 3146. The transaction that spends
the coins at Amazon will show up in the ledger, and so the transaction
with Walmart will not be allowed.Every agent that has the bank's hash pointer to the end of the
tamper-evident ledger can inspect the ledger to check that
double-spending hasn't occurred.The bank signs a valid transaction and appends it to the
tamper-evident ledger. All agents can see the bank's signature and
verify that nobody (not even the bank) has tampered with the
tamper-evident ledger.Example of a pay transactionAn example of the specification of a pay transaction is:(9431, pay,
  [(3146, 0), (4731, 2)],
  [(0.7, 7xxxx...), (4.6, 9xxxx...)]
).The id of this transaction is 9431; the type of the transaction is
pay; the payers into the transaction are identified by the pairs of
(transaction-id, index): (3146, 0), and (4731, 2); and the
payee array is [(0.7, 7xxxx...), (4.6, 9xxxx...)].Transaction validityThe bank appends a transaction toLif and only if the
transaction is valid. The bank checks for validity by carrying out the
following steps:The bank verifies that the payers into the transaction signed the
  transaction.The bank checks that the total value of coins paid out from the
  transaction does not
  exceed the total value paid in to the transaction. (If the value paid
  in exceeds the value paid out then the bank takes the difference as
  a transaction fee. More about fees later.)The bank verifies that the payers' claims to have received coins in
  previous transactions is genuine. For example,
  if the agent with
  public key 7xxxx... claims to have received coins worth 2.1 in the
  transaction with id 10, and payee array index 0, then the bank
  verifies this claim by that transaction.The bank ensures that coins paid into the transaction haven't
  already been spent.Optimizations: Blocks and Block ChainVerifying large numbers of small transactions requires more
computation than verifying small numbers of blocks of many transactions.
Ablock chainis a tamper-evident ledger in which
each element of the ledger is a block consisting of many transactions.
A block of transactions can be aggregated into a single large
transaction by aggregating all the payers and payees of the smaller
transactions.The amount of computation decreases as the number of transactions in a
block increases. The time required to fill a block with transactions
is larger when the number of transactions to fill the block increases.Checking the Trusted AgentConsider a system in which the trusted agent broadcasts its current
copy of the tamper-evident ledger to all agents.
Every agent can inspect its copy of the tamper-evident ledger to determine whether the
ledger has been tampered with. So, every agent can validate its trust
in the trusted agent; however, this validation
suffers from a crucial problem: Agents may only
have copies ofold, staleversions of the ledger.
By the time that an agent receives a copy of the ledger, the trusted
agent may have added more transactions to the ledger.Fig.4: Old Copy is a Prefix of the Block ChainAn old copy of the ledger can differ from the current copy in only one
way: the current copy may have transactions appended to the end
of the old copy. So all agents can validatepastbehavior of the
trusted agent. An agent cannot, however, treat its copy of the ledger
as the master copy because the agent may not have the transactions
added most recently to the ledger.In the next module we will see how the Bitcoin algorithm addresses
this problem.Advantages of this cryptocurrencyAny agent can get a copy of the tamper-evident ledger and verify that all
transactions in the ledger are valid. Any agent can verify that
the only way in which the ledger is modified is that elements are
append to its tail; all that the agent needs to do is to check that
the pointer to the tail is modified only by appending
elements. Because the ledger is tamper-evident, an agent can check
that the ledger doesn't change while the hash pointer to the end
of the ledger doesn't change.The bank can't forge a transaction because all payers sign the
transaction. Agents can remain anonymous because an agent's only
public information is the agent's public key, and an agent can create
multiple public keys. Every agent can verify the correctness of every
transaction.Disadvantages of this cryptocurrencyUsers may not trust the bank. Transactions are not private because the
bank has a record of all transactions. And the bank is a single point
of failure.Next we'll look at Bitcoin's algorithms for
implementing a cryptocurrency without trusted agents.ReviewWhat iscollision resistance? Why is it relevant to
  cryptocurrencies?Consider the example ofhidingin which Megan hides her
  predictions for the winner of the World Cup. Suppose you could
  choose two different random number generators for creating the
  secret \(r\); in one case the bits of \(r\) are uncorrelated, and in
  the other they are highly correlated. Which generator would you use
  and why?What isbinding? Why is it relevant to hiding information?What ispuzzle friendly?What is atamper-evident data structure? How would you
  implement a tamper-evident linear list? A tamper-evident tree?Describe the algorithm for cryptocurrency with a trusted agent to
  someone who knows absolutely nothing about cryptocurrency.How could the algorithm for cryptocurrency be used for a collection
  of distributed agents to keep track of a sequence of events, other
  than buying/selling currency?K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology



Figure Link: https://kmchandy.github.io/Crypto/CryptoCurrencyIntroduction/Slide1.jpg

Caption: Fig.1: Hash Pointer points to a Tamper-Evident Block
    of Data


Suppose you give Megan a hash function that is binding. Then she cannot find
  (in reasonable time) values \(r_{j}\) to match country \(C_{j}\)
such that\(H(r_{0} + C_{0}) = H(r_{1} + C_{1})  = H(r_{2} + C_{2})  = \ldots\).and so she can't wait for the winner \(C_{j}\) to be announced before announcing
her secret \(r_{j}\).In summary, we can use a hash function that is hiding and binding to
play the role of a trusted third party in a commitment.Puzzle FriendlyThe concept ofpuzzle friendlyis related tohiding.
Let \(r\) be a value picked from a spread-out distribution. Let
\(H\) map arbitrary length strings to \(n\)-bit strings.
Consider the following problem:
    Given \(H\), \(r\), and an \(n\)-bit value \(y\), compute any \(x\) such that
\(H(r+x) = y\).In this problem, as opposed to thehidingproblem, we are given \(r\) and not \(x\),The hash function \(H\) is said to bepuzzle friendlyexactly when any algorithm to solve this problem is about as slow
as a brute-force algorithm which checks \(H(r+x) = y\) for random values
of \(x\).
The number of steps taken by any
algorithm that solves this problem is not significantly lower than
\(2^{n}\).Now, let's look at the following related problem.  Given \(H\), \(r\),
and a set \(Y\) of \(n\)-bit strings, compute any \(x\) such that
\(H(r+x) \in Y\).  If \(Y\) consists of a single element \(y\) then
this problem is the same as that in the previous paragraph.  If \(Y\)
is a set of all \(n\)-bit strings then this problem is trivial because
any \(x\) solves the problem.  The probability that a random value
hashes to an element of \(Y\) is proportional to the cardinality of
\(Y\).  The cardinality of \(Y\) controls the expected time to
solve the puzzle.The hash function \(H\) ispuzzle friendlywhen
given \(H\), \(r\), and a set \(Y\) of \(n\)-bit values, the time
required to compute any \(x\) such that \(H(r+x) \in Y\) is
not significantly lower than \(2^{n} / |Y|\), where \(|Y|\) is the
cardinality of \(Y\).A Cryptographic Hash FunctionA cryptographic hash function is one that is collision resistant,
hiding and puzzle-friendly.Hashing Inputs of Arbitrary LengthLet \(f\) be a function that operates on input strings
of fixed length and produces output strings of fixed length. Let the
input and output strings of \(f\) have lengths \(M + N\) and \(M\),
respectively. We look at functions where \(N > 0\), and since the
output is smaller than the input, \(f\) is called acompression
function.We can use function \(f\) to define a function \(g\) whose inputs are strings of arbitrary
lengths and whose outputs are strings of length \(M\).
Example code for \(g\) is given below whereInitialValueis a given constant string of length \(M\).def g(y):
    output = InitialValue
    // pad y so that it's length is a multiple of N
    if len(y)%N > 0:  y = y + "0"*(N - len(y)%N)
    // partition y into blocks of size N
    blocks = [y[i: i+N] for i in range(0, len(y), N)]
    // Apply function f to the concatenation of the
    // previous output (length M) with each block
    // (length N) to get the next output (length M).
    for block in blocks: output = f(output+block)
    return outputHash PointersA hash pointer to an item \(D\) of data is a pair \((ptr, H(D))\)
where \(ptr\) is a pointer that 
points to \(D\), and \(H\) is a cryptographic hash function.Any data structure with pointers can be converted into a
data structure with hash pointers: merely replace a pointer \(ptr\) to
\(D\) by \((ptr, H(D))\).Tamper-Evident Data StructuresSingle BlockA simple example of a tamper-evident structure is a single block of
data D which is pointed to by a hash pointer consisting of a regular
pointer and a hash H(D).Fig.1: Hash Pointer points to a Tamper-Evident Block
    of DataAssume that a malicious agent cannot modifyboththe hash
pointer and the data that it points to. If an 
agent
    changes D to D' then the tampering can be discovered because the
    hash pointer won't match the data that it is pointing to:
    \(H(D') \neq H(D)\).Tamper-Evident Linked ListLet's look at linear linked list to which elements can be appended but
not deleted. The \(i\)-th element appended to the list points to the
\((i-1)\)-th element. 
Let's replace the pointers in the list by hashed
pointers.Fig.2: Hash Pointer points to a Tamper-Evident List
    of DataThe \(j\)-th element of the list, \(j > 0\), consists of data,
\(D_{j}\), and a hash pointer that points to the \(j - 1\)-th
  element of the list. The hash pointer in the \(j\)-th element
consists of a regular pointer, \(ptr_{j}\), which points to the \(j - 1\)-th
  element of the list, and a hashed value, \(HA_{j}\), which is a cryptographic hash of theentire\(j-1\)th element consisting  of \(D_{j-1}\), \(ptr_{j-1}\)
  and \(HA_{j-1}\).
The \(0\)-th element is called thegenesiselement and has
default values. The list is accessed by a hash pointer that points to
the last element of the list; let this pointer be \(ptr_{n}, HA_{n}\).Assume that agents cannot modify \(ptr_{n}, HA_{n}\). Then,
can any agent determine whether the list has been tampered with?Suppose the agent modifies \(D_{j}\), \(ptr_{j}\) or
\(HA_{j}\) for any \(j < n\). Any agent can detect this tampering because the
hash value \(HA_{j+1}\) will no longer match the \(j\)-th element of
the list. If
the malicious agent also modifies \(HA_{j+1}\) then 
hash value \(HA_{j+2}\) will no longer match the \(j+1\)-th element.By induction on \(j\), any agent can detect tampering with the list
provided malicious agents do not also modify the hash pointer to the
last element of the last.Tamper-Evident Acyclic Graphs and Merkle
TreesThe idea described in the previous paragraph to convert linear linked
lists can be used to convert directed acyclic graphs, in which nodes are
connected by pointers, into tamper-evident graphs. A specific case of
a directed acyclic graph is a rooted tree.AMerkle treeis a special case of a binary balanced tree in which
data items are stored only in the leaves.
Nodes that are not leaves 
contain only hash pointers to nodes in the next level down.
To prove that an element at the leaf is a member of the tree we need only the
\(log_{2}(n)\) hash pointers on the path from the root to that
leaf. By contrast, to prove that an element is a member of a linear
list we need to inspect \(O(n)\) elements, on average.Keys and Signed MessagesYou can create a random public-key, private-key pairby calling a function on your computer.With high probability, nobody else has this
specific pair of keys. Each individual's private key is a secret held
by that individual. Public keys are accessible by everybody.Sending messages securelyKeys are used to send messages securely.
Kamala sends a secure message to Joe by encrypting the message with
Joe's public key; Joe decrypts the message using Joe's private key.
An agent cannot decrypt the encrypted message without Joe's private key.Signing messagesSuppose Kamala needs to send a signed message to Joe while
ensuring that nobody can forge her signature. She encrypts the messageMwith her private key to get an encrypted messageM',
and sends the pair(M, M')securely to Joe, i.e., she encrypts(M, M')with Joe's public key, and sends the resulting
encrypted message to Joe.  When Joe receives the message, Joe decrypts
it using his private key to get(M, M'). Then Joe decryptsM'using Kamala's public key to get the decrypted messageM''. IfM'' = Mthen Joe knows that Kamala sentMbecause only an agent with Kamala's private signature could
have sent that message.Cryptocurrency Managed by a Trusted
AgentLet's start with a digital currency managed by a trusted agent that we will
call a bank. Later, we will
look at a consensus algorithm --- very different from Paxos and Byzantine
Generals --- which will allow cryptocurrencies without trusted
agent.The bank maintains a tamper-evident linear listLof transactions that we call a
tamper-evidentledger. 
Any agent can get a copy of the ledger.
This tamper-evident ledger is the foundation of the currency.A transaction is one of
two types:createorpay.
In a pay transaction,payersgive coins that they possess topayees. An agent can be both a 
payer and payee of the same transaction.
In acreatetransaction the bank creates coins that it gives to
agents --- the payees of the transaction; the bank acts as the payer.For this system to be trusted the bank must follow some protocol that determines
how and when the bank creates coins. We won't discuss these protocols.Apaytransaction is signed by all payers of the transaction.
Acreatetransaction is signed by the bank.
We discussed digital signatures and keys earlier.Each element of the tamper-evident ledger has:a unique id;the type of the transaction, either create or pay;list of payers: only for pay transactions --- a list
  indicating the agents who pay coins into the transaction and the
  amounts that they put in;array of value-payee pairs: for both create and pay transactions --- an
  array of pairs(value, payee public key), where each
  pair in the array indicates that coins of the specified value are
  given to the payee with the specified public key.Example of a create transactionAn example of acreatetransaction is:(3146, create,  [(2.1, 7xxxx...), (3.2, 8xxxx)]).The id of this transaction is 3146, the type of the transaction is
create, and the array of value-payee pairs is[(2.1, 7xxxx...), (3.2, 8xxx)]In this transaction the bank creates a coin of value 2.1 and gives it
to the agent with public key 7xxxx..., and the bank also
creates a coin of value 3.2 and gives it
to the agent with public key 8xxxx...The pair:(transaction id, index into array of value-payee pairs)uniquely identifies a (value, payee) tuple.For example (3146, 0) --- 
transaction id 3146, and array index 0 --- identifies value-payee[0] of
transaction 3146 which is specified by the 2-tuple: (2.1, 7xxxx...). So, the
transaction id and index, (3146, 0), tells everybody that the agent
with public key 7xxxx received 2.1 units of coin in transaction 3146.When this
transaction is in a tamper-evident ledger, every agent from that point
onwards knows that agent 7xxxx received 2.1 coins. Any modifications of this
record can be detected.Likewise, (3146, 1) --- 
transaction id 3146, and array index 1 --- identifies value-payee[1] which
is the 2-tuple (3.2, 8xxxx...).Pay transactionCoins are transferred from payers to payees in a pay transaction.Coins flowing into a pay transaction from payersThe payers are identified by a list of 2-tuples, where each 2-tuple is(transaction id, index into array of value-payee pairs)wheretransaction idis the id of the transaction in the
tamper-evident ledger. As we said earlier, this pair uniquely identifies an agent and a
value that this agent acquired in this transaction.
For example the pair --- transaction id, index --- such as (3146, 0)
identifies the 2-tuple (2.1, 7xxxx...); this 2-tuple asserts
that the agent
with public key 7xxxx received 2.1 units of coin in 
transaction 3146. Theentire amountspecified in the 2-tuple (2.1 in our
example) is value that flows into thispaytransaction from the agent
with public key 7xxxx.Suppose the payers in a pay transaction are identified by the list:[(3146, 0), (7359, 3)]and suppose pair 3 in transaction 7359 is (3.2, 8xxxx). Then the total
amount of coins flowing into this pay transaction is 2.1 + 3.2, and
this amount is disbursed to payees.Fig.3: Coins flowing into and out of a pay transaction.Coins flowing out of a pay transaction to payeesThe outflow of coins is specified by an array of value-payee pairs,
exactly as in a create transaction.
The transaction clears all coins: the total inflow from payers is
equal to the total outflow to payees in a transaction.The system may provide incentives, such as payment of coins, to
managers (e.g. banks) of cryptocurrencies. In this case, one of the
payees is the bank itself.Managing amounts spent in a transactionA transaction-id, index pair --- such as (3146, 0)
identifies a 2-tuple such as (2.1, 7xxxx...); this 2-tuple asserts
that the agent
with public key 7xxxx received 2.1 units of coin in 
transaction 3146. Theentire amountspecified in the 2-tuple (2.1 in our
example) is value that flows into the transaction. What should this
agent do if it
wants to put in more than 2.1 coins into the transaction? Or less than
2.1 coins?To put in more value, the bank identifies other transaction-id, index
pairs in which this agent received coins. For example, say that (4539, 2)
identifies a 2-tuple (3.2, 7xxxx), and assume that the payers in this transaction
are specified by the pairs (3146, 0) and (4539, 2). The pair (3146, 0)
asserts that agent (7xxxx) received 2.1 coins and the pair (4539, 2)
asserts that the same agent received 3.2 coins. So the total amount of 
coins flowing into this
transaction from this agent (7xxxx) includes 2.1 + 3.2.To put in less value, the agent acts as both payer and payee; the net
value that this agent pays out to other agents is the difference
between the amount that this agent puts in and takes out. For example,
if agent with public key 7xxxx wants to put in 1.9 coins into this
transaction its payer information can be given by the transaction-id,
index pair (3146, 0) which asserts that the agent received 2.1 coins
and the same agent is a payee that withdraws 0.2 coins.Preventing Double SpendingHow does the system prevent an agent from using the
same coin twice?For example, the transaction id, index pair
(3146, 0) identifies the 2-tuple (2.1, 7xxxx...); this tuple
asserts that the agent
with public key 7xxxx received 2.1 units of coin in 
transaction 3146. Why can't the agent with public key 7xxxx use the
2.1 coins that it received to buy items from Amazon and later use
the same 2.1 coins to buy more items from Walmart?The bank checks that the agent hasn't already spent the coin that it
is putting into a transaction.Before permitting the transaction that double-spends the 2.1 coins
with Walmart, the bank inspects the tamper-evident ledger
for all transactions after transaction 3146 and before the Walmart
transaction to ensure that the agent (7xxxx) hasn't already spent the
2.1 coins that it got in transaction 3146. The transaction that spends
the coins at Amazon will show up in the ledger, and so the transaction
with Walmart will not be allowed.Every agent that has the bank's hash pointer to the end of the
tamper-evident ledger can inspect the ledger to check that
double-spending hasn't occurred.The bank signs a valid transaction and appends it to the
tamper-evident ledger. All agents can see the bank's signature and
verify that nobody (not even the bank) has tampered with the
tamper-evident ledger.Example of a pay transactionAn example of the specification of a pay transaction is:(9431, pay,
  [(3146, 0), (4731, 2)],
  [(0.7, 7xxxx...), (4.6, 9xxxx...)]
).The id of this transaction is 9431; the type of the transaction is
pay; the payers into the transaction are identified by the pairs of
(transaction-id, index): (3146, 0), and (4731, 2); and the
payee array is [(0.7, 7xxxx...), (4.6, 9xxxx...)].Transaction validityThe bank appends a transaction toLif and only if the
transaction is valid. The bank checks for validity by carrying out the
following steps:The bank verifies that the payers into the transaction signed the
  transaction.The bank checks that the total value of coins paid out from the
  transaction does not
  exceed the total value paid in to the transaction. (If the value paid
  in exceeds the value paid out then the bank takes the difference as
  a transaction fee. More about fees later.)The bank verifies that the payers' claims to have received coins in
  previous transactions is genuine. For example,
  if the agent with
  public key 7xxxx... claims to have received coins worth 2.1 in the
  transaction with id 10, and payee array index 0, then the bank
  verifies this claim by that transaction.The bank ensures that coins paid into the transaction haven't
  already been spent.Optimizations: Blocks and Block ChainVerifying large numbers of small transactions requires more
computation than verifying small numbers of blocks of many transactions.
Ablock chainis a tamper-evident ledger in which
each element of the ledger is a block consisting of many transactions.
A block of transactions can be aggregated into a single large
transaction by aggregating all the payers and payees of the smaller
transactions.The amount of computation decreases as the number of transactions in a
block increases. The time required to fill a block with transactions
is larger when the number of transactions to fill the block increases.Checking the Trusted AgentConsider a system in which the trusted agent broadcasts its current
copy of the tamper-evident ledger to all agents.
Every agent can inspect its copy of the tamper-evident ledger to determine whether the
ledger has been tampered with. So, every agent can validate its trust
in the trusted agent; however, this validation
suffers from a crucial problem: Agents may only
have copies ofold, staleversions of the ledger.
By the time that an agent receives a copy of the ledger, the trusted
agent may have added more transactions to the ledger.Fig.4: Old Copy is a Prefix of the Block ChainAn old copy of the ledger can differ from the current copy in only one
way: the current copy may have transactions appended to the end
of the old copy. So all agents can validatepastbehavior of the
trusted agent. An agent cannot, however, treat its copy of the ledger
as the master copy because the agent may not have the transactions
added most recently to the ledger.In the next module we will see how the Bitcoin algorithm addresses
this problem.Advantages of this cryptocurrencyAny agent can get a copy of the tamper-evident ledger and verify that all
transactions in the ledger are valid. Any agent can verify that
the only way in which the ledger is modified is that elements are
append to its tail; all that the agent needs to do is to check that
the pointer to the tail is modified only by appending
elements. Because the ledger is tamper-evident, an agent can check
that the ledger doesn't change while the hash pointer to the end
of the ledger doesn't change.The bank can't forge a transaction because all payers sign the
transaction. Agents can remain anonymous because an agent's only
public information is the agent's public key, and an agent can create
multiple public keys. Every agent can verify the correctness of every
transaction.Disadvantages of this cryptocurrencyUsers may not trust the bank. Transactions are not private because the
bank has a record of all transactions. And the bank is a single point
of failure.Next we'll look at Bitcoin's algorithms for
implementing a cryptocurrency without trusted agents.ReviewWhat iscollision resistance? Why is it relevant to
  cryptocurrencies?Consider the example ofhidingin which Megan hides her
  predictions for the winner of the World Cup. Suppose you could
  choose two different random number generators for creating the
  secret \(r\); in one case the bits of \(r\) are uncorrelated, and in
  the other they are highly correlated. Which generator would you use
  and why?What isbinding? Why is it relevant to hiding information?What ispuzzle friendly?What is atamper-evident data structure? How would you
  implement a tamper-evident linear list? A tamper-evident tree?Describe the algorithm for cryptocurrency with a trusted agent to
  someone who knows absolutely nothing about cryptocurrency.How could the algorithm for cryptocurrency be used for a collection
  of distributed agents to keep track of a sequence of events, other
  than buying/selling currency?K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology



Figure Link: https://kmchandy.github.io/Crypto/CryptoCurrencyIntroduction/Slide1.jpg

Caption: Fig.1: Hash Pointer points to a Tamper-Evident Block
    of Data


\(H(r_{0} + C_{0}) = H(r_{1} + C_{1})  = H(r_{2} + C_{2})  = \ldots\).and so she can't wait for the winner \(C_{j}\) to be announced before announcing
her secret \(r_{j}\).In summary, we can use a hash function that is hiding and binding to
play the role of a trusted third party in a commitment.Puzzle FriendlyThe concept ofpuzzle friendlyis related tohiding.
Let \(r\) be a value picked from a spread-out distribution. Let
\(H\) map arbitrary length strings to \(n\)-bit strings.
Consider the following problem:
    Given \(H\), \(r\), and an \(n\)-bit value \(y\), compute any \(x\) such that
\(H(r+x) = y\).In this problem, as opposed to thehidingproblem, we are given \(r\) and not \(x\),The hash function \(H\) is said to bepuzzle friendlyexactly when any algorithm to solve this problem is about as slow
as a brute-force algorithm which checks \(H(r+x) = y\) for random values
of \(x\).
The number of steps taken by any
algorithm that solves this problem is not significantly lower than
\(2^{n}\).Now, let's look at the following related problem.  Given \(H\), \(r\),
and a set \(Y\) of \(n\)-bit strings, compute any \(x\) such that
\(H(r+x) \in Y\).  If \(Y\) consists of a single element \(y\) then
this problem is the same as that in the previous paragraph.  If \(Y\)
is a set of all \(n\)-bit strings then this problem is trivial because
any \(x\) solves the problem.  The probability that a random value
hashes to an element of \(Y\) is proportional to the cardinality of
\(Y\).  The cardinality of \(Y\) controls the expected time to
solve the puzzle.The hash function \(H\) ispuzzle friendlywhen
given \(H\), \(r\), and a set \(Y\) of \(n\)-bit values, the time
required to compute any \(x\) such that \(H(r+x) \in Y\) is
not significantly lower than \(2^{n} / |Y|\), where \(|Y|\) is the
cardinality of \(Y\).A Cryptographic Hash FunctionA cryptographic hash function is one that is collision resistant,
hiding and puzzle-friendly.Hashing Inputs of Arbitrary LengthLet \(f\) be a function that operates on input strings
of fixed length and produces output strings of fixed length. Let the
input and output strings of \(f\) have lengths \(M + N\) and \(M\),
respectively. We look at functions where \(N > 0\), and since the
output is smaller than the input, \(f\) is called acompression
function.We can use function \(f\) to define a function \(g\) whose inputs are strings of arbitrary
lengths and whose outputs are strings of length \(M\).
Example code for \(g\) is given below whereInitialValueis a given constant string of length \(M\).def g(y):
    output = InitialValue
    // pad y so that it's length is a multiple of N
    if len(y)%N > 0:  y = y + "0"*(N - len(y)%N)
    // partition y into blocks of size N
    blocks = [y[i: i+N] for i in range(0, len(y), N)]
    // Apply function f to the concatenation of the
    // previous output (length M) with each block
    // (length N) to get the next output (length M).
    for block in blocks: output = f(output+block)
    return outputHash PointersA hash pointer to an item \(D\) of data is a pair \((ptr, H(D))\)
where \(ptr\) is a pointer that 
points to \(D\), and \(H\) is a cryptographic hash function.Any data structure with pointers can be converted into a
data structure with hash pointers: merely replace a pointer \(ptr\) to
\(D\) by \((ptr, H(D))\).Tamper-Evident Data StructuresSingle BlockA simple example of a tamper-evident structure is a single block of
data D which is pointed to by a hash pointer consisting of a regular
pointer and a hash H(D).Fig.1: Hash Pointer points to a Tamper-Evident Block
    of DataAssume that a malicious agent cannot modifyboththe hash
pointer and the data that it points to. If an 
agent
    changes D to D' then the tampering can be discovered because the
    hash pointer won't match the data that it is pointing to:
    \(H(D') \neq H(D)\).Tamper-Evident Linked ListLet's look at linear linked list to which elements can be appended but
not deleted. The \(i\)-th element appended to the list points to the
\((i-1)\)-th element. 
Let's replace the pointers in the list by hashed
pointers.Fig.2: Hash Pointer points to a Tamper-Evident List
    of DataThe \(j\)-th element of the list, \(j > 0\), consists of data,
\(D_{j}\), and a hash pointer that points to the \(j - 1\)-th
  element of the list. The hash pointer in the \(j\)-th element
consists of a regular pointer, \(ptr_{j}\), which points to the \(j - 1\)-th
  element of the list, and a hashed value, \(HA_{j}\), which is a cryptographic hash of theentire\(j-1\)th element consisting  of \(D_{j-1}\), \(ptr_{j-1}\)
  and \(HA_{j-1}\).
The \(0\)-th element is called thegenesiselement and has
default values. The list is accessed by a hash pointer that points to
the last element of the list; let this pointer be \(ptr_{n}, HA_{n}\).Assume that agents cannot modify \(ptr_{n}, HA_{n}\). Then,
can any agent determine whether the list has been tampered with?Suppose the agent modifies \(D_{j}\), \(ptr_{j}\) or
\(HA_{j}\) for any \(j < n\). Any agent can detect this tampering because the
hash value \(HA_{j+1}\) will no longer match the \(j\)-th element of
the list. If
the malicious agent also modifies \(HA_{j+1}\) then 
hash value \(HA_{j+2}\) will no longer match the \(j+1\)-th element.By induction on \(j\), any agent can detect tampering with the list
provided malicious agents do not also modify the hash pointer to the
last element of the last.Tamper-Evident Acyclic Graphs and Merkle
TreesThe idea described in the previous paragraph to convert linear linked
lists can be used to convert directed acyclic graphs, in which nodes are
connected by pointers, into tamper-evident graphs. A specific case of
a directed acyclic graph is a rooted tree.AMerkle treeis a special case of a binary balanced tree in which
data items are stored only in the leaves.
Nodes that are not leaves 
contain only hash pointers to nodes in the next level down.
To prove that an element at the leaf is a member of the tree we need only the
\(log_{2}(n)\) hash pointers on the path from the root to that
leaf. By contrast, to prove that an element is a member of a linear
list we need to inspect \(O(n)\) elements, on average.Keys and Signed MessagesYou can create a random public-key, private-key pairby calling a function on your computer.With high probability, nobody else has this
specific pair of keys. Each individual's private key is a secret held
by that individual. Public keys are accessible by everybody.Sending messages securelyKeys are used to send messages securely.
Kamala sends a secure message to Joe by encrypting the message with
Joe's public key; Joe decrypts the message using Joe's private key.
An agent cannot decrypt the encrypted message without Joe's private key.Signing messagesSuppose Kamala needs to send a signed message to Joe while
ensuring that nobody can forge her signature. She encrypts the messageMwith her private key to get an encrypted messageM',
and sends the pair(M, M')securely to Joe, i.e., she encrypts(M, M')with Joe's public key, and sends the resulting
encrypted message to Joe.  When Joe receives the message, Joe decrypts
it using his private key to get(M, M'). Then Joe decryptsM'using Kamala's public key to get the decrypted messageM''. IfM'' = Mthen Joe knows that Kamala sentMbecause only an agent with Kamala's private signature could
have sent that message.Cryptocurrency Managed by a Trusted
AgentLet's start with a digital currency managed by a trusted agent that we will
call a bank. Later, we will
look at a consensus algorithm --- very different from Paxos and Byzantine
Generals --- which will allow cryptocurrencies without trusted
agent.The bank maintains a tamper-evident linear listLof transactions that we call a
tamper-evidentledger. 
Any agent can get a copy of the ledger.
This tamper-evident ledger is the foundation of the currency.A transaction is one of
two types:createorpay.
In a pay transaction,payersgive coins that they possess topayees. An agent can be both a 
payer and payee of the same transaction.
In acreatetransaction the bank creates coins that it gives to
agents --- the payees of the transaction; the bank acts as the payer.For this system to be trusted the bank must follow some protocol that determines
how and when the bank creates coins. We won't discuss these protocols.Apaytransaction is signed by all payers of the transaction.
Acreatetransaction is signed by the bank.
We discussed digital signatures and keys earlier.Each element of the tamper-evident ledger has:a unique id;the type of the transaction, either create or pay;list of payers: only for pay transactions --- a list
  indicating the agents who pay coins into the transaction and the
  amounts that they put in;array of value-payee pairs: for both create and pay transactions --- an
  array of pairs(value, payee public key), where each
  pair in the array indicates that coins of the specified value are
  given to the payee with the specified public key.Example of a create transactionAn example of acreatetransaction is:(3146, create,  [(2.1, 7xxxx...), (3.2, 8xxxx)]).The id of this transaction is 3146, the type of the transaction is
create, and the array of value-payee pairs is[(2.1, 7xxxx...), (3.2, 8xxx)]In this transaction the bank creates a coin of value 2.1 and gives it
to the agent with public key 7xxxx..., and the bank also
creates a coin of value 3.2 and gives it
to the agent with public key 8xxxx...The pair:(transaction id, index into array of value-payee pairs)uniquely identifies a (value, payee) tuple.For example (3146, 0) --- 
transaction id 3146, and array index 0 --- identifies value-payee[0] of
transaction 3146 which is specified by the 2-tuple: (2.1, 7xxxx...). So, the
transaction id and index, (3146, 0), tells everybody that the agent
with public key 7xxxx received 2.1 units of coin in transaction 3146.When this
transaction is in a tamper-evident ledger, every agent from that point
onwards knows that agent 7xxxx received 2.1 coins. Any modifications of this
record can be detected.Likewise, (3146, 1) --- 
transaction id 3146, and array index 1 --- identifies value-payee[1] which
is the 2-tuple (3.2, 8xxxx...).Pay transactionCoins are transferred from payers to payees in a pay transaction.Coins flowing into a pay transaction from payersThe payers are identified by a list of 2-tuples, where each 2-tuple is(transaction id, index into array of value-payee pairs)wheretransaction idis the id of the transaction in the
tamper-evident ledger. As we said earlier, this pair uniquely identifies an agent and a
value that this agent acquired in this transaction.
For example the pair --- transaction id, index --- such as (3146, 0)
identifies the 2-tuple (2.1, 7xxxx...); this 2-tuple asserts
that the agent
with public key 7xxxx received 2.1 units of coin in 
transaction 3146. Theentire amountspecified in the 2-tuple (2.1 in our
example) is value that flows into thispaytransaction from the agent
with public key 7xxxx.Suppose the payers in a pay transaction are identified by the list:[(3146, 0), (7359, 3)]and suppose pair 3 in transaction 7359 is (3.2, 8xxxx). Then the total
amount of coins flowing into this pay transaction is 2.1 + 3.2, and
this amount is disbursed to payees.Fig.3: Coins flowing into and out of a pay transaction.Coins flowing out of a pay transaction to payeesThe outflow of coins is specified by an array of value-payee pairs,
exactly as in a create transaction.
The transaction clears all coins: the total inflow from payers is
equal to the total outflow to payees in a transaction.The system may provide incentives, such as payment of coins, to
managers (e.g. banks) of cryptocurrencies. In this case, one of the
payees is the bank itself.Managing amounts spent in a transactionA transaction-id, index pair --- such as (3146, 0)
identifies a 2-tuple such as (2.1, 7xxxx...); this 2-tuple asserts
that the agent
with public key 7xxxx received 2.1 units of coin in 
transaction 3146. Theentire amountspecified in the 2-tuple (2.1 in our
example) is value that flows into the transaction. What should this
agent do if it
wants to put in more than 2.1 coins into the transaction? Or less than
2.1 coins?To put in more value, the bank identifies other transaction-id, index
pairs in which this agent received coins. For example, say that (4539, 2)
identifies a 2-tuple (3.2, 7xxxx), and assume that the payers in this transaction
are specified by the pairs (3146, 0) and (4539, 2). The pair (3146, 0)
asserts that agent (7xxxx) received 2.1 coins and the pair (4539, 2)
asserts that the same agent received 3.2 coins. So the total amount of 
coins flowing into this
transaction from this agent (7xxxx) includes 2.1 + 3.2.To put in less value, the agent acts as both payer and payee; the net
value that this agent pays out to other agents is the difference
between the amount that this agent puts in and takes out. For example,
if agent with public key 7xxxx wants to put in 1.9 coins into this
transaction its payer information can be given by the transaction-id,
index pair (3146, 0) which asserts that the agent received 2.1 coins
and the same agent is a payee that withdraws 0.2 coins.Preventing Double SpendingHow does the system prevent an agent from using the
same coin twice?For example, the transaction id, index pair
(3146, 0) identifies the 2-tuple (2.1, 7xxxx...); this tuple
asserts that the agent
with public key 7xxxx received 2.1 units of coin in 
transaction 3146. Why can't the agent with public key 7xxxx use the
2.1 coins that it received to buy items from Amazon and later use
the same 2.1 coins to buy more items from Walmart?The bank checks that the agent hasn't already spent the coin that it
is putting into a transaction.Before permitting the transaction that double-spends the 2.1 coins
with Walmart, the bank inspects the tamper-evident ledger
for all transactions after transaction 3146 and before the Walmart
transaction to ensure that the agent (7xxxx) hasn't already spent the
2.1 coins that it got in transaction 3146. The transaction that spends
the coins at Amazon will show up in the ledger, and so the transaction
with Walmart will not be allowed.Every agent that has the bank's hash pointer to the end of the
tamper-evident ledger can inspect the ledger to check that
double-spending hasn't occurred.The bank signs a valid transaction and appends it to the
tamper-evident ledger. All agents can see the bank's signature and
verify that nobody (not even the bank) has tampered with the
tamper-evident ledger.Example of a pay transactionAn example of the specification of a pay transaction is:(9431, pay,
  [(3146, 0), (4731, 2)],
  [(0.7, 7xxxx...), (4.6, 9xxxx...)]
).The id of this transaction is 9431; the type of the transaction is
pay; the payers into the transaction are identified by the pairs of
(transaction-id, index): (3146, 0), and (4731, 2); and the
payee array is [(0.7, 7xxxx...), (4.6, 9xxxx...)].Transaction validityThe bank appends a transaction toLif and only if the
transaction is valid. The bank checks for validity by carrying out the
following steps:The bank verifies that the payers into the transaction signed the
  transaction.The bank checks that the total value of coins paid out from the
  transaction does not
  exceed the total value paid in to the transaction. (If the value paid
  in exceeds the value paid out then the bank takes the difference as
  a transaction fee. More about fees later.)The bank verifies that the payers' claims to have received coins in
  previous transactions is genuine. For example,
  if the agent with
  public key 7xxxx... claims to have received coins worth 2.1 in the
  transaction with id 10, and payee array index 0, then the bank
  verifies this claim by that transaction.The bank ensures that coins paid into the transaction haven't
  already been spent.Optimizations: Blocks and Block ChainVerifying large numbers of small transactions requires more
computation than verifying small numbers of blocks of many transactions.
Ablock chainis a tamper-evident ledger in which
each element of the ledger is a block consisting of many transactions.
A block of transactions can be aggregated into a single large
transaction by aggregating all the payers and payees of the smaller
transactions.The amount of computation decreases as the number of transactions in a
block increases. The time required to fill a block with transactions
is larger when the number of transactions to fill the block increases.Checking the Trusted AgentConsider a system in which the trusted agent broadcasts its current
copy of the tamper-evident ledger to all agents.
Every agent can inspect its copy of the tamper-evident ledger to determine whether the
ledger has been tampered with. So, every agent can validate its trust
in the trusted agent; however, this validation
suffers from a crucial problem: Agents may only
have copies ofold, staleversions of the ledger.
By the time that an agent receives a copy of the ledger, the trusted
agent may have added more transactions to the ledger.Fig.4: Old Copy is a Prefix of the Block ChainAn old copy of the ledger can differ from the current copy in only one
way: the current copy may have transactions appended to the end
of the old copy. So all agents can validatepastbehavior of the
trusted agent. An agent cannot, however, treat its copy of the ledger
as the master copy because the agent may not have the transactions
added most recently to the ledger.In the next module we will see how the Bitcoin algorithm addresses
this problem.Advantages of this cryptocurrencyAny agent can get a copy of the tamper-evident ledger and verify that all
transactions in the ledger are valid. Any agent can verify that
the only way in which the ledger is modified is that elements are
append to its tail; all that the agent needs to do is to check that
the pointer to the tail is modified only by appending
elements. Because the ledger is tamper-evident, an agent can check
that the ledger doesn't change while the hash pointer to the end
of the ledger doesn't change.The bank can't forge a transaction because all payers sign the
transaction. Agents can remain anonymous because an agent's only
public information is the agent's public key, and an agent can create
multiple public keys. Every agent can verify the correctness of every
transaction.Disadvantages of this cryptocurrencyUsers may not trust the bank. Transactions are not private because the
bank has a record of all transactions. And the bank is a single point
of failure.Next we'll look at Bitcoin's algorithms for
implementing a cryptocurrency without trusted agents.ReviewWhat iscollision resistance? Why is it relevant to
  cryptocurrencies?Consider the example ofhidingin which Megan hides her
  predictions for the winner of the World Cup. Suppose you could
  choose two different random number generators for creating the
  secret \(r\); in one case the bits of \(r\) are uncorrelated, and in
  the other they are highly correlated. Which generator would you use
  and why?What isbinding? Why is it relevant to hiding information?What ispuzzle friendly?What is atamper-evident data structure? How would you
  implement a tamper-evident linear list? A tamper-evident tree?Describe the algorithm for cryptocurrency with a trusted agent to
  someone who knows absolutely nothing about cryptocurrency.How could the algorithm for cryptocurrency be used for a collection
  of distributed agents to keep track of a sequence of events, other
  than buying/selling currency?K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology



Figure Link: https://kmchandy.github.io/Crypto/CryptoCurrencyIntroduction/Slide1.jpg

Caption: Fig.1: Hash Pointer points to a Tamper-Evident Block
    of Data


and so she can't wait for the winner \(C_{j}\) to be announced before announcing
her secret \(r_{j}\).In summary, we can use a hash function that is hiding and binding to
play the role of a trusted third party in a commitment.Puzzle FriendlyThe concept ofpuzzle friendlyis related tohiding.
Let \(r\) be a value picked from a spread-out distribution. Let
\(H\) map arbitrary length strings to \(n\)-bit strings.
Consider the following problem:
    Given \(H\), \(r\), and an \(n\)-bit value \(y\), compute any \(x\) such that
\(H(r+x) = y\).In this problem, as opposed to thehidingproblem, we are given \(r\) and not \(x\),The hash function \(H\) is said to bepuzzle friendlyexactly when any algorithm to solve this problem is about as slow
as a brute-force algorithm which checks \(H(r+x) = y\) for random values
of \(x\).
The number of steps taken by any
algorithm that solves this problem is not significantly lower than
\(2^{n}\).Now, let's look at the following related problem.  Given \(H\), \(r\),
and a set \(Y\) of \(n\)-bit strings, compute any \(x\) such that
\(H(r+x) \in Y\).  If \(Y\) consists of a single element \(y\) then
this problem is the same as that in the previous paragraph.  If \(Y\)
is a set of all \(n\)-bit strings then this problem is trivial because
any \(x\) solves the problem.  The probability that a random value
hashes to an element of \(Y\) is proportional to the cardinality of
\(Y\).  The cardinality of \(Y\) controls the expected time to
solve the puzzle.The hash function \(H\) ispuzzle friendlywhen
given \(H\), \(r\), and a set \(Y\) of \(n\)-bit values, the time
required to compute any \(x\) such that \(H(r+x) \in Y\) is
not significantly lower than \(2^{n} / |Y|\), where \(|Y|\) is the
cardinality of \(Y\).A Cryptographic Hash FunctionA cryptographic hash function is one that is collision resistant,
hiding and puzzle-friendly.Hashing Inputs of Arbitrary LengthLet \(f\) be a function that operates on input strings
of fixed length and produces output strings of fixed length. Let the
input and output strings of \(f\) have lengths \(M + N\) and \(M\),
respectively. We look at functions where \(N > 0\), and since the
output is smaller than the input, \(f\) is called acompression
function.We can use function \(f\) to define a function \(g\) whose inputs are strings of arbitrary
lengths and whose outputs are strings of length \(M\).
Example code for \(g\) is given below whereInitialValueis a given constant string of length \(M\).def g(y):
    output = InitialValue
    // pad y so that it's length is a multiple of N
    if len(y)%N > 0:  y = y + "0"*(N - len(y)%N)
    // partition y into blocks of size N
    blocks = [y[i: i+N] for i in range(0, len(y), N)]
    // Apply function f to the concatenation of the
    // previous output (length M) with each block
    // (length N) to get the next output (length M).
    for block in blocks: output = f(output+block)
    return outputHash PointersA hash pointer to an item \(D\) of data is a pair \((ptr, H(D))\)
where \(ptr\) is a pointer that 
points to \(D\), and \(H\) is a cryptographic hash function.Any data structure with pointers can be converted into a
data structure with hash pointers: merely replace a pointer \(ptr\) to
\(D\) by \((ptr, H(D))\).Tamper-Evident Data StructuresSingle BlockA simple example of a tamper-evident structure is a single block of
data D which is pointed to by a hash pointer consisting of a regular
pointer and a hash H(D).Fig.1: Hash Pointer points to a Tamper-Evident Block
    of DataAssume that a malicious agent cannot modifyboththe hash
pointer and the data that it points to. If an 
agent
    changes D to D' then the tampering can be discovered because the
    hash pointer won't match the data that it is pointing to:
    \(H(D') \neq H(D)\).Tamper-Evident Linked ListLet's look at linear linked list to which elements can be appended but
not deleted. The \(i\)-th element appended to the list points to the
\((i-1)\)-th element. 
Let's replace the pointers in the list by hashed
pointers.Fig.2: Hash Pointer points to a Tamper-Evident List
    of DataThe \(j\)-th element of the list, \(j > 0\), consists of data,
\(D_{j}\), and a hash pointer that points to the \(j - 1\)-th
  element of the list. The hash pointer in the \(j\)-th element
consists of a regular pointer, \(ptr_{j}\), which points to the \(j - 1\)-th
  element of the list, and a hashed value, \(HA_{j}\), which is a cryptographic hash of theentire\(j-1\)th element consisting  of \(D_{j-1}\), \(ptr_{j-1}\)
  and \(HA_{j-1}\).
The \(0\)-th element is called thegenesiselement and has
default values. The list is accessed by a hash pointer that points to
the last element of the list; let this pointer be \(ptr_{n}, HA_{n}\).Assume that agents cannot modify \(ptr_{n}, HA_{n}\). Then,
can any agent determine whether the list has been tampered with?Suppose the agent modifies \(D_{j}\), \(ptr_{j}\) or
\(HA_{j}\) for any \(j < n\). Any agent can detect this tampering because the
hash value \(HA_{j+1}\) will no longer match the \(j\)-th element of
the list. If
the malicious agent also modifies \(HA_{j+1}\) then 
hash value \(HA_{j+2}\) will no longer match the \(j+1\)-th element.By induction on \(j\), any agent can detect tampering with the list
provided malicious agents do not also modify the hash pointer to the
last element of the last.Tamper-Evident Acyclic Graphs and Merkle
TreesThe idea described in the previous paragraph to convert linear linked
lists can be used to convert directed acyclic graphs, in which nodes are
connected by pointers, into tamper-evident graphs. A specific case of
a directed acyclic graph is a rooted tree.AMerkle treeis a special case of a binary balanced tree in which
data items are stored only in the leaves.
Nodes that are not leaves 
contain only hash pointers to nodes in the next level down.
To prove that an element at the leaf is a member of the tree we need only the
\(log_{2}(n)\) hash pointers on the path from the root to that
leaf. By contrast, to prove that an element is a member of a linear
list we need to inspect \(O(n)\) elements, on average.Keys and Signed MessagesYou can create a random public-key, private-key pairby calling a function on your computer.With high probability, nobody else has this
specific pair of keys. Each individual's private key is a secret held
by that individual. Public keys are accessible by everybody.Sending messages securelyKeys are used to send messages securely.
Kamala sends a secure message to Joe by encrypting the message with
Joe's public key; Joe decrypts the message using Joe's private key.
An agent cannot decrypt the encrypted message without Joe's private key.Signing messagesSuppose Kamala needs to send a signed message to Joe while
ensuring that nobody can forge her signature. She encrypts the messageMwith her private key to get an encrypted messageM',
and sends the pair(M, M')securely to Joe, i.e., she encrypts(M, M')with Joe's public key, and sends the resulting
encrypted message to Joe.  When Joe receives the message, Joe decrypts
it using his private key to get(M, M'). Then Joe decryptsM'using Kamala's public key to get the decrypted messageM''. IfM'' = Mthen Joe knows that Kamala sentMbecause only an agent with Kamala's private signature could
have sent that message.Cryptocurrency Managed by a Trusted
AgentLet's start with a digital currency managed by a trusted agent that we will
call a bank. Later, we will
look at a consensus algorithm --- very different from Paxos and Byzantine
Generals --- which will allow cryptocurrencies without trusted
agent.The bank maintains a tamper-evident linear listLof transactions that we call a
tamper-evidentledger. 
Any agent can get a copy of the ledger.
This tamper-evident ledger is the foundation of the currency.A transaction is one of
two types:createorpay.
In a pay transaction,payersgive coins that they possess topayees. An agent can be both a 
payer and payee of the same transaction.
In acreatetransaction the bank creates coins that it gives to
agents --- the payees of the transaction; the bank acts as the payer.For this system to be trusted the bank must follow some protocol that determines
how and when the bank creates coins. We won't discuss these protocols.Apaytransaction is signed by all payers of the transaction.
Acreatetransaction is signed by the bank.
We discussed digital signatures and keys earlier.Each element of the tamper-evident ledger has:a unique id;the type of the transaction, either create or pay;list of payers: only for pay transactions --- a list
  indicating the agents who pay coins into the transaction and the
  amounts that they put in;array of value-payee pairs: for both create and pay transactions --- an
  array of pairs(value, payee public key), where each
  pair in the array indicates that coins of the specified value are
  given to the payee with the specified public key.Example of a create transactionAn example of acreatetransaction is:(3146, create,  [(2.1, 7xxxx...), (3.2, 8xxxx)]).The id of this transaction is 3146, the type of the transaction is
create, and the array of value-payee pairs is[(2.1, 7xxxx...), (3.2, 8xxx)]In this transaction the bank creates a coin of value 2.1 and gives it
to the agent with public key 7xxxx..., and the bank also
creates a coin of value 3.2 and gives it
to the agent with public key 8xxxx...The pair:(transaction id, index into array of value-payee pairs)uniquely identifies a (value, payee) tuple.For example (3146, 0) --- 
transaction id 3146, and array index 0 --- identifies value-payee[0] of
transaction 3146 which is specified by the 2-tuple: (2.1, 7xxxx...). So, the
transaction id and index, (3146, 0), tells everybody that the agent
with public key 7xxxx received 2.1 units of coin in transaction 3146.When this
transaction is in a tamper-evident ledger, every agent from that point
onwards knows that agent 7xxxx received 2.1 coins. Any modifications of this
record can be detected.Likewise, (3146, 1) --- 
transaction id 3146, and array index 1 --- identifies value-payee[1] which
is the 2-tuple (3.2, 8xxxx...).Pay transactionCoins are transferred from payers to payees in a pay transaction.Coins flowing into a pay transaction from payersThe payers are identified by a list of 2-tuples, where each 2-tuple is(transaction id, index into array of value-payee pairs)wheretransaction idis the id of the transaction in the
tamper-evident ledger. As we said earlier, this pair uniquely identifies an agent and a
value that this agent acquired in this transaction.
For example the pair --- transaction id, index --- such as (3146, 0)
identifies the 2-tuple (2.1, 7xxxx...); this 2-tuple asserts
that the agent
with public key 7xxxx received 2.1 units of coin in 
transaction 3146. Theentire amountspecified in the 2-tuple (2.1 in our
example) is value that flows into thispaytransaction from the agent
with public key 7xxxx.Suppose the payers in a pay transaction are identified by the list:[(3146, 0), (7359, 3)]and suppose pair 3 in transaction 7359 is (3.2, 8xxxx). Then the total
amount of coins flowing into this pay transaction is 2.1 + 3.2, and
this amount is disbursed to payees.Fig.3: Coins flowing into and out of a pay transaction.Coins flowing out of a pay transaction to payeesThe outflow of coins is specified by an array of value-payee pairs,
exactly as in a create transaction.
The transaction clears all coins: the total inflow from payers is
equal to the total outflow to payees in a transaction.The system may provide incentives, such as payment of coins, to
managers (e.g. banks) of cryptocurrencies. In this case, one of the
payees is the bank itself.Managing amounts spent in a transactionA transaction-id, index pair --- such as (3146, 0)
identifies a 2-tuple such as (2.1, 7xxxx...); this 2-tuple asserts
that the agent
with public key 7xxxx received 2.1 units of coin in 
transaction 3146. Theentire amountspecified in the 2-tuple (2.1 in our
example) is value that flows into the transaction. What should this
agent do if it
wants to put in more than 2.1 coins into the transaction? Or less than
2.1 coins?To put in more value, the bank identifies other transaction-id, index
pairs in which this agent received coins. For example, say that (4539, 2)
identifies a 2-tuple (3.2, 7xxxx), and assume that the payers in this transaction
are specified by the pairs (3146, 0) and (4539, 2). The pair (3146, 0)
asserts that agent (7xxxx) received 2.1 coins and the pair (4539, 2)
asserts that the same agent received 3.2 coins. So the total amount of 
coins flowing into this
transaction from this agent (7xxxx) includes 2.1 + 3.2.To put in less value, the agent acts as both payer and payee; the net
value that this agent pays out to other agents is the difference
between the amount that this agent puts in and takes out. For example,
if agent with public key 7xxxx wants to put in 1.9 coins into this
transaction its payer information can be given by the transaction-id,
index pair (3146, 0) which asserts that the agent received 2.1 coins
and the same agent is a payee that withdraws 0.2 coins.Preventing Double SpendingHow does the system prevent an agent from using the
same coin twice?For example, the transaction id, index pair
(3146, 0) identifies the 2-tuple (2.1, 7xxxx...); this tuple
asserts that the agent
with public key 7xxxx received 2.1 units of coin in 
transaction 3146. Why can't the agent with public key 7xxxx use the
2.1 coins that it received to buy items from Amazon and later use
the same 2.1 coins to buy more items from Walmart?The bank checks that the agent hasn't already spent the coin that it
is putting into a transaction.Before permitting the transaction that double-spends the 2.1 coins
with Walmart, the bank inspects the tamper-evident ledger
for all transactions after transaction 3146 and before the Walmart
transaction to ensure that the agent (7xxxx) hasn't already spent the
2.1 coins that it got in transaction 3146. The transaction that spends
the coins at Amazon will show up in the ledger, and so the transaction
with Walmart will not be allowed.Every agent that has the bank's hash pointer to the end of the
tamper-evident ledger can inspect the ledger to check that
double-spending hasn't occurred.The bank signs a valid transaction and appends it to the
tamper-evident ledger. All agents can see the bank's signature and
verify that nobody (not even the bank) has tampered with the
tamper-evident ledger.Example of a pay transactionAn example of the specification of a pay transaction is:(9431, pay,
  [(3146, 0), (4731, 2)],
  [(0.7, 7xxxx...), (4.6, 9xxxx...)]
).The id of this transaction is 9431; the type of the transaction is
pay; the payers into the transaction are identified by the pairs of
(transaction-id, index): (3146, 0), and (4731, 2); and the
payee array is [(0.7, 7xxxx...), (4.6, 9xxxx...)].Transaction validityThe bank appends a transaction toLif and only if the
transaction is valid. The bank checks for validity by carrying out the
following steps:The bank verifies that the payers into the transaction signed the
  transaction.The bank checks that the total value of coins paid out from the
  transaction does not
  exceed the total value paid in to the transaction. (If the value paid
  in exceeds the value paid out then the bank takes the difference as
  a transaction fee. More about fees later.)The bank verifies that the payers' claims to have received coins in
  previous transactions is genuine. For example,
  if the agent with
  public key 7xxxx... claims to have received coins worth 2.1 in the
  transaction with id 10, and payee array index 0, then the bank
  verifies this claim by that transaction.The bank ensures that coins paid into the transaction haven't
  already been spent.Optimizations: Blocks and Block ChainVerifying large numbers of small transactions requires more
computation than verifying small numbers of blocks of many transactions.
Ablock chainis a tamper-evident ledger in which
each element of the ledger is a block consisting of many transactions.
A block of transactions can be aggregated into a single large
transaction by aggregating all the payers and payees of the smaller
transactions.The amount of computation decreases as the number of transactions in a
block increases. The time required to fill a block with transactions
is larger when the number of transactions to fill the block increases.Checking the Trusted AgentConsider a system in which the trusted agent broadcasts its current
copy of the tamper-evident ledger to all agents.
Every agent can inspect its copy of the tamper-evident ledger to determine whether the
ledger has been tampered with. So, every agent can validate its trust
in the trusted agent; however, this validation
suffers from a crucial problem: Agents may only
have copies ofold, staleversions of the ledger.
By the time that an agent receives a copy of the ledger, the trusted
agent may have added more transactions to the ledger.Fig.4: Old Copy is a Prefix of the Block ChainAn old copy of the ledger can differ from the current copy in only one
way: the current copy may have transactions appended to the end
of the old copy. So all agents can validatepastbehavior of the
trusted agent. An agent cannot, however, treat its copy of the ledger
as the master copy because the agent may not have the transactions
added most recently to the ledger.In the next module we will see how the Bitcoin algorithm addresses
this problem.Advantages of this cryptocurrencyAny agent can get a copy of the tamper-evident ledger and verify that all
transactions in the ledger are valid. Any agent can verify that
the only way in which the ledger is modified is that elements are
append to its tail; all that the agent needs to do is to check that
the pointer to the tail is modified only by appending
elements. Because the ledger is tamper-evident, an agent can check
that the ledger doesn't change while the hash pointer to the end
of the ledger doesn't change.The bank can't forge a transaction because all payers sign the
transaction. Agents can remain anonymous because an agent's only
public information is the agent's public key, and an agent can create
multiple public keys. Every agent can verify the correctness of every
transaction.Disadvantages of this cryptocurrencyUsers may not trust the bank. Transactions are not private because the
bank has a record of all transactions. And the bank is a single point
of failure.Next we'll look at Bitcoin's algorithms for
implementing a cryptocurrency without trusted agents.ReviewWhat iscollision resistance? Why is it relevant to
  cryptocurrencies?Consider the example ofhidingin which Megan hides her
  predictions for the winner of the World Cup. Suppose you could
  choose two different random number generators for creating the
  secret \(r\); in one case the bits of \(r\) are uncorrelated, and in
  the other they are highly correlated. Which generator would you use
  and why?What isbinding? Why is it relevant to hiding information?What ispuzzle friendly?What is atamper-evident data structure? How would you
  implement a tamper-evident linear list? A tamper-evident tree?Describe the algorithm for cryptocurrency with a trusted agent to
  someone who knows absolutely nothing about cryptocurrency.How could the algorithm for cryptocurrency be used for a collection
  of distributed agents to keep track of a sequence of events, other
  than buying/selling currency?K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology



Figure Link: https://kmchandy.github.io/Crypto/CryptoCurrencyIntroduction/Slide1.jpg

Caption: Fig.1: Hash Pointer points to a Tamper-Evident Block
    of Data


In summary, we can use a hash function that is hiding and binding to
play the role of a trusted third party in a commitment.Puzzle FriendlyThe concept ofpuzzle friendlyis related tohiding.
Let \(r\) be a value picked from a spread-out distribution. Let
\(H\) map arbitrary length strings to \(n\)-bit strings.
Consider the following problem:
    Given \(H\), \(r\), and an \(n\)-bit value \(y\), compute any \(x\) such that
\(H(r+x) = y\).In this problem, as opposed to thehidingproblem, we are given \(r\) and not \(x\),The hash function \(H\) is said to bepuzzle friendlyexactly when any algorithm to solve this problem is about as slow
as a brute-force algorithm which checks \(H(r+x) = y\) for random values
of \(x\).
The number of steps taken by any
algorithm that solves this problem is not significantly lower than
\(2^{n}\).Now, let's look at the following related problem.  Given \(H\), \(r\),
and a set \(Y\) of \(n\)-bit strings, compute any \(x\) such that
\(H(r+x) \in Y\).  If \(Y\) consists of a single element \(y\) then
this problem is the same as that in the previous paragraph.  If \(Y\)
is a set of all \(n\)-bit strings then this problem is trivial because
any \(x\) solves the problem.  The probability that a random value
hashes to an element of \(Y\) is proportional to the cardinality of
\(Y\).  The cardinality of \(Y\) controls the expected time to
solve the puzzle.The hash function \(H\) ispuzzle friendlywhen
given \(H\), \(r\), and a set \(Y\) of \(n\)-bit values, the time
required to compute any \(x\) such that \(H(r+x) \in Y\) is
not significantly lower than \(2^{n} / |Y|\), where \(|Y|\) is the
cardinality of \(Y\).A Cryptographic Hash FunctionA cryptographic hash function is one that is collision resistant,
hiding and puzzle-friendly.Hashing Inputs of Arbitrary LengthLet \(f\) be a function that operates on input strings
of fixed length and produces output strings of fixed length. Let the
input and output strings of \(f\) have lengths \(M + N\) and \(M\),
respectively. We look at functions where \(N > 0\), and since the
output is smaller than the input, \(f\) is called acompression
function.We can use function \(f\) to define a function \(g\) whose inputs are strings of arbitrary
lengths and whose outputs are strings of length \(M\).
Example code for \(g\) is given below whereInitialValueis a given constant string of length \(M\).def g(y):
    output = InitialValue
    // pad y so that it's length is a multiple of N
    if len(y)%N > 0:  y = y + "0"*(N - len(y)%N)
    // partition y into blocks of size N
    blocks = [y[i: i+N] for i in range(0, len(y), N)]
    // Apply function f to the concatenation of the
    // previous output (length M) with each block
    // (length N) to get the next output (length M).
    for block in blocks: output = f(output+block)
    return outputHash PointersA hash pointer to an item \(D\) of data is a pair \((ptr, H(D))\)
where \(ptr\) is a pointer that 
points to \(D\), and \(H\) is a cryptographic hash function.Any data structure with pointers can be converted into a
data structure with hash pointers: merely replace a pointer \(ptr\) to
\(D\) by \((ptr, H(D))\).Tamper-Evident Data StructuresSingle BlockA simple example of a tamper-evident structure is a single block of
data D which is pointed to by a hash pointer consisting of a regular
pointer and a hash H(D).Fig.1: Hash Pointer points to a Tamper-Evident Block
    of DataAssume that a malicious agent cannot modifyboththe hash
pointer and the data that it points to. If an 
agent
    changes D to D' then the tampering can be discovered because the
    hash pointer won't match the data that it is pointing to:
    \(H(D') \neq H(D)\).Tamper-Evident Linked ListLet's look at linear linked list to which elements can be appended but
not deleted. The \(i\)-th element appended to the list points to the
\((i-1)\)-th element. 
Let's replace the pointers in the list by hashed
pointers.Fig.2: Hash Pointer points to a Tamper-Evident List
    of DataThe \(j\)-th element of the list, \(j > 0\), consists of data,
\(D_{j}\), and a hash pointer that points to the \(j - 1\)-th
  element of the list. The hash pointer in the \(j\)-th element
consists of a regular pointer, \(ptr_{j}\), which points to the \(j - 1\)-th
  element of the list, and a hashed value, \(HA_{j}\), which is a cryptographic hash of theentire\(j-1\)th element consisting  of \(D_{j-1}\), \(ptr_{j-1}\)
  and \(HA_{j-1}\).
The \(0\)-th element is called thegenesiselement and has
default values. The list is accessed by a hash pointer that points to
the last element of the list; let this pointer be \(ptr_{n}, HA_{n}\).Assume that agents cannot modify \(ptr_{n}, HA_{n}\). Then,
can any agent determine whether the list has been tampered with?Suppose the agent modifies \(D_{j}\), \(ptr_{j}\) or
\(HA_{j}\) for any \(j < n\). Any agent can detect this tampering because the
hash value \(HA_{j+1}\) will no longer match the \(j\)-th element of
the list. If
the malicious agent also modifies \(HA_{j+1}\) then 
hash value \(HA_{j+2}\) will no longer match the \(j+1\)-th element.By induction on \(j\), any agent can detect tampering with the list
provided malicious agents do not also modify the hash pointer to the
last element of the last.Tamper-Evident Acyclic Graphs and Merkle
TreesThe idea described in the previous paragraph to convert linear linked
lists can be used to convert directed acyclic graphs, in which nodes are
connected by pointers, into tamper-evident graphs. A specific case of
a directed acyclic graph is a rooted tree.AMerkle treeis a special case of a binary balanced tree in which
data items are stored only in the leaves.
Nodes that are not leaves 
contain only hash pointers to nodes in the next level down.
To prove that an element at the leaf is a member of the tree we need only the
\(log_{2}(n)\) hash pointers on the path from the root to that
leaf. By contrast, to prove that an element is a member of a linear
list we need to inspect \(O(n)\) elements, on average.Keys and Signed MessagesYou can create a random public-key, private-key pairby calling a function on your computer.With high probability, nobody else has this
specific pair of keys. Each individual's private key is a secret held
by that individual. Public keys are accessible by everybody.Sending messages securelyKeys are used to send messages securely.
Kamala sends a secure message to Joe by encrypting the message with
Joe's public key; Joe decrypts the message using Joe's private key.
An agent cannot decrypt the encrypted message without Joe's private key.Signing messagesSuppose Kamala needs to send a signed message to Joe while
ensuring that nobody can forge her signature. She encrypts the messageMwith her private key to get an encrypted messageM',
and sends the pair(M, M')securely to Joe, i.e., she encrypts(M, M')with Joe's public key, and sends the resulting
encrypted message to Joe.  When Joe receives the message, Joe decrypts
it using his private key to get(M, M'). Then Joe decryptsM'using Kamala's public key to get the decrypted messageM''. IfM'' = Mthen Joe knows that Kamala sentMbecause only an agent with Kamala's private signature could
have sent that message.Cryptocurrency Managed by a Trusted
AgentLet's start with a digital currency managed by a trusted agent that we will
call a bank. Later, we will
look at a consensus algorithm --- very different from Paxos and Byzantine
Generals --- which will allow cryptocurrencies without trusted
agent.The bank maintains a tamper-evident linear listLof transactions that we call a
tamper-evidentledger. 
Any agent can get a copy of the ledger.
This tamper-evident ledger is the foundation of the currency.A transaction is one of
two types:createorpay.
In a pay transaction,payersgive coins that they possess topayees. An agent can be both a 
payer and payee of the same transaction.
In acreatetransaction the bank creates coins that it gives to
agents --- the payees of the transaction; the bank acts as the payer.For this system to be trusted the bank must follow some protocol that determines
how and when the bank creates coins. We won't discuss these protocols.Apaytransaction is signed by all payers of the transaction.
Acreatetransaction is signed by the bank.
We discussed digital signatures and keys earlier.Each element of the tamper-evident ledger has:a unique id;the type of the transaction, either create or pay;list of payers: only for pay transactions --- a list
  indicating the agents who pay coins into the transaction and the
  amounts that they put in;array of value-payee pairs: for both create and pay transactions --- an
  array of pairs(value, payee public key), where each
  pair in the array indicates that coins of the specified value are
  given to the payee with the specified public key.Example of a create transactionAn example of acreatetransaction is:(3146, create,  [(2.1, 7xxxx...), (3.2, 8xxxx)]).The id of this transaction is 3146, the type of the transaction is
create, and the array of value-payee pairs is[(2.1, 7xxxx...), (3.2, 8xxx)]In this transaction the bank creates a coin of value 2.1 and gives it
to the agent with public key 7xxxx..., and the bank also
creates a coin of value 3.2 and gives it
to the agent with public key 8xxxx...The pair:(transaction id, index into array of value-payee pairs)uniquely identifies a (value, payee) tuple.For example (3146, 0) --- 
transaction id 3146, and array index 0 --- identifies value-payee[0] of
transaction 3146 which is specified by the 2-tuple: (2.1, 7xxxx...). So, the
transaction id and index, (3146, 0), tells everybody that the agent
with public key 7xxxx received 2.1 units of coin in transaction 3146.When this
transaction is in a tamper-evident ledger, every agent from that point
onwards knows that agent 7xxxx received 2.1 coins. Any modifications of this
record can be detected.Likewise, (3146, 1) --- 
transaction id 3146, and array index 1 --- identifies value-payee[1] which
is the 2-tuple (3.2, 8xxxx...).Pay transactionCoins are transferred from payers to payees in a pay transaction.Coins flowing into a pay transaction from payersThe payers are identified by a list of 2-tuples, where each 2-tuple is(transaction id, index into array of value-payee pairs)wheretransaction idis the id of the transaction in the
tamper-evident ledger. As we said earlier, this pair uniquely identifies an agent and a
value that this agent acquired in this transaction.
For example the pair --- transaction id, index --- such as (3146, 0)
identifies the 2-tuple (2.1, 7xxxx...); this 2-tuple asserts
that the agent
with public key 7xxxx received 2.1 units of coin in 
transaction 3146. Theentire amountspecified in the 2-tuple (2.1 in our
example) is value that flows into thispaytransaction from the agent
with public key 7xxxx.Suppose the payers in a pay transaction are identified by the list:[(3146, 0), (7359, 3)]and suppose pair 3 in transaction 7359 is (3.2, 8xxxx). Then the total
amount of coins flowing into this pay transaction is 2.1 + 3.2, and
this amount is disbursed to payees.Fig.3: Coins flowing into and out of a pay transaction.Coins flowing out of a pay transaction to payeesThe outflow of coins is specified by an array of value-payee pairs,
exactly as in a create transaction.
The transaction clears all coins: the total inflow from payers is
equal to the total outflow to payees in a transaction.The system may provide incentives, such as payment of coins, to
managers (e.g. banks) of cryptocurrencies. In this case, one of the
payees is the bank itself.Managing amounts spent in a transactionA transaction-id, index pair --- such as (3146, 0)
identifies a 2-tuple such as (2.1, 7xxxx...); this 2-tuple asserts
that the agent
with public key 7xxxx received 2.1 units of coin in 
transaction 3146. Theentire amountspecified in the 2-tuple (2.1 in our
example) is value that flows into the transaction. What should this
agent do if it
wants to put in more than 2.1 coins into the transaction? Or less than
2.1 coins?To put in more value, the bank identifies other transaction-id, index
pairs in which this agent received coins. For example, say that (4539, 2)
identifies a 2-tuple (3.2, 7xxxx), and assume that the payers in this transaction
are specified by the pairs (3146, 0) and (4539, 2). The pair (3146, 0)
asserts that agent (7xxxx) received 2.1 coins and the pair (4539, 2)
asserts that the same agent received 3.2 coins. So the total amount of 
coins flowing into this
transaction from this agent (7xxxx) includes 2.1 + 3.2.To put in less value, the agent acts as both payer and payee; the net
value that this agent pays out to other agents is the difference
between the amount that this agent puts in and takes out. For example,
if agent with public key 7xxxx wants to put in 1.9 coins into this
transaction its payer information can be given by the transaction-id,
index pair (3146, 0) which asserts that the agent received 2.1 coins
and the same agent is a payee that withdraws 0.2 coins.Preventing Double SpendingHow does the system prevent an agent from using the
same coin twice?For example, the transaction id, index pair
(3146, 0) identifies the 2-tuple (2.1, 7xxxx...); this tuple
asserts that the agent
with public key 7xxxx received 2.1 units of coin in 
transaction 3146. Why can't the agent with public key 7xxxx use the
2.1 coins that it received to buy items from Amazon and later use
the same 2.1 coins to buy more items from Walmart?The bank checks that the agent hasn't already spent the coin that it
is putting into a transaction.Before permitting the transaction that double-spends the 2.1 coins
with Walmart, the bank inspects the tamper-evident ledger
for all transactions after transaction 3146 and before the Walmart
transaction to ensure that the agent (7xxxx) hasn't already spent the
2.1 coins that it got in transaction 3146. The transaction that spends
the coins at Amazon will show up in the ledger, and so the transaction
with Walmart will not be allowed.Every agent that has the bank's hash pointer to the end of the
tamper-evident ledger can inspect the ledger to check that
double-spending hasn't occurred.The bank signs a valid transaction and appends it to the
tamper-evident ledger. All agents can see the bank's signature and
verify that nobody (not even the bank) has tampered with the
tamper-evident ledger.Example of a pay transactionAn example of the specification of a pay transaction is:(9431, pay,
  [(3146, 0), (4731, 2)],
  [(0.7, 7xxxx...), (4.6, 9xxxx...)]
).The id of this transaction is 9431; the type of the transaction is
pay; the payers into the transaction are identified by the pairs of
(transaction-id, index): (3146, 0), and (4731, 2); and the
payee array is [(0.7, 7xxxx...), (4.6, 9xxxx...)].Transaction validityThe bank appends a transaction toLif and only if the
transaction is valid. The bank checks for validity by carrying out the
following steps:The bank verifies that the payers into the transaction signed the
  transaction.The bank checks that the total value of coins paid out from the
  transaction does not
  exceed the total value paid in to the transaction. (If the value paid
  in exceeds the value paid out then the bank takes the difference as
  a transaction fee. More about fees later.)The bank verifies that the payers' claims to have received coins in
  previous transactions is genuine. For example,
  if the agent with
  public key 7xxxx... claims to have received coins worth 2.1 in the
  transaction with id 10, and payee array index 0, then the bank
  verifies this claim by that transaction.The bank ensures that coins paid into the transaction haven't
  already been spent.Optimizations: Blocks and Block ChainVerifying large numbers of small transactions requires more
computation than verifying small numbers of blocks of many transactions.
Ablock chainis a tamper-evident ledger in which
each element of the ledger is a block consisting of many transactions.
A block of transactions can be aggregated into a single large
transaction by aggregating all the payers and payees of the smaller
transactions.The amount of computation decreases as the number of transactions in a
block increases. The time required to fill a block with transactions
is larger when the number of transactions to fill the block increases.Checking the Trusted AgentConsider a system in which the trusted agent broadcasts its current
copy of the tamper-evident ledger to all agents.
Every agent can inspect its copy of the tamper-evident ledger to determine whether the
ledger has been tampered with. So, every agent can validate its trust
in the trusted agent; however, this validation
suffers from a crucial problem: Agents may only
have copies ofold, staleversions of the ledger.
By the time that an agent receives a copy of the ledger, the trusted
agent may have added more transactions to the ledger.Fig.4: Old Copy is a Prefix of the Block ChainAn old copy of the ledger can differ from the current copy in only one
way: the current copy may have transactions appended to the end
of the old copy. So all agents can validatepastbehavior of the
trusted agent. An agent cannot, however, treat its copy of the ledger
as the master copy because the agent may not have the transactions
added most recently to the ledger.In the next module we will see how the Bitcoin algorithm addresses
this problem.Advantages of this cryptocurrencyAny agent can get a copy of the tamper-evident ledger and verify that all
transactions in the ledger are valid. Any agent can verify that
the only way in which the ledger is modified is that elements are
append to its tail; all that the agent needs to do is to check that
the pointer to the tail is modified only by appending
elements. Because the ledger is tamper-evident, an agent can check
that the ledger doesn't change while the hash pointer to the end
of the ledger doesn't change.The bank can't forge a transaction because all payers sign the
transaction. Agents can remain anonymous because an agent's only
public information is the agent's public key, and an agent can create
multiple public keys. Every agent can verify the correctness of every
transaction.Disadvantages of this cryptocurrencyUsers may not trust the bank. Transactions are not private because the
bank has a record of all transactions. And the bank is a single point
of failure.Next we'll look at Bitcoin's algorithms for
implementing a cryptocurrency without trusted agents.ReviewWhat iscollision resistance? Why is it relevant to
  cryptocurrencies?Consider the example ofhidingin which Megan hides her
  predictions for the winner of the World Cup. Suppose you could
  choose two different random number generators for creating the
  secret \(r\); in one case the bits of \(r\) are uncorrelated, and in
  the other they are highly correlated. Which generator would you use
  and why?What isbinding? Why is it relevant to hiding information?What ispuzzle friendly?What is atamper-evident data structure? How would you
  implement a tamper-evident linear list? A tamper-evident tree?Describe the algorithm for cryptocurrency with a trusted agent to
  someone who knows absolutely nothing about cryptocurrency.How could the algorithm for cryptocurrency be used for a collection
  of distributed agents to keep track of a sequence of events, other
  than buying/selling currency?K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology



Figure Link: https://kmchandy.github.io/Crypto/CryptoCurrencyIntroduction/Slide1.jpg

Caption: Fig.1: Hash Pointer points to a Tamper-Evident Block
    of Data


In this problem, as opposed to thehidingproblem, we are given \(r\) and not \(x\),The hash function \(H\) is said to bepuzzle friendlyexactly when any algorithm to solve this problem is about as slow
as a brute-force algorithm which checks \(H(r+x) = y\) for random values
of \(x\).
The number of steps taken by any
algorithm that solves this problem is not significantly lower than
\(2^{n}\).Now, let's look at the following related problem.  Given \(H\), \(r\),
and a set \(Y\) of \(n\)-bit strings, compute any \(x\) such that
\(H(r+x) \in Y\).  If \(Y\) consists of a single element \(y\) then
this problem is the same as that in the previous paragraph.  If \(Y\)
is a set of all \(n\)-bit strings then this problem is trivial because
any \(x\) solves the problem.  The probability that a random value
hashes to an element of \(Y\) is proportional to the cardinality of
\(Y\).  The cardinality of \(Y\) controls the expected time to
solve the puzzle.The hash function \(H\) ispuzzle friendlywhen
given \(H\), \(r\), and a set \(Y\) of \(n\)-bit values, the time
required to compute any \(x\) such that \(H(r+x) \in Y\) is
not significantly lower than \(2^{n} / |Y|\), where \(|Y|\) is the
cardinality of \(Y\).A Cryptographic Hash FunctionA cryptographic hash function is one that is collision resistant,
hiding and puzzle-friendly.Hashing Inputs of Arbitrary LengthLet \(f\) be a function that operates on input strings
of fixed length and produces output strings of fixed length. Let the
input and output strings of \(f\) have lengths \(M + N\) and \(M\),
respectively. We look at functions where \(N > 0\), and since the
output is smaller than the input, \(f\) is called acompression
function.We can use function \(f\) to define a function \(g\) whose inputs are strings of arbitrary
lengths and whose outputs are strings of length \(M\).
Example code for \(g\) is given below whereInitialValueis a given constant string of length \(M\).def g(y):
    output = InitialValue
    // pad y so that it's length is a multiple of N
    if len(y)%N > 0:  y = y + "0"*(N - len(y)%N)
    // partition y into blocks of size N
    blocks = [y[i: i+N] for i in range(0, len(y), N)]
    // Apply function f to the concatenation of the
    // previous output (length M) with each block
    // (length N) to get the next output (length M).
    for block in blocks: output = f(output+block)
    return outputHash PointersA hash pointer to an item \(D\) of data is a pair \((ptr, H(D))\)
where \(ptr\) is a pointer that 
points to \(D\), and \(H\) is a cryptographic hash function.Any data structure with pointers can be converted into a
data structure with hash pointers: merely replace a pointer \(ptr\) to
\(D\) by \((ptr, H(D))\).Tamper-Evident Data StructuresSingle BlockA simple example of a tamper-evident structure is a single block of
data D which is pointed to by a hash pointer consisting of a regular
pointer and a hash H(D).Fig.1: Hash Pointer points to a Tamper-Evident Block
    of DataAssume that a malicious agent cannot modifyboththe hash
pointer and the data that it points to. If an 
agent
    changes D to D' then the tampering can be discovered because the
    hash pointer won't match the data that it is pointing to:
    \(H(D') \neq H(D)\).Tamper-Evident Linked ListLet's look at linear linked list to which elements can be appended but
not deleted. The \(i\)-th element appended to the list points to the
\((i-1)\)-th element. 
Let's replace the pointers in the list by hashed
pointers.Fig.2: Hash Pointer points to a Tamper-Evident List
    of DataThe \(j\)-th element of the list, \(j > 0\), consists of data,
\(D_{j}\), and a hash pointer that points to the \(j - 1\)-th
  element of the list. The hash pointer in the \(j\)-th element
consists of a regular pointer, \(ptr_{j}\), which points to the \(j - 1\)-th
  element of the list, and a hashed value, \(HA_{j}\), which is a cryptographic hash of theentire\(j-1\)th element consisting  of \(D_{j-1}\), \(ptr_{j-1}\)
  and \(HA_{j-1}\).
The \(0\)-th element is called thegenesiselement and has
default values. The list is accessed by a hash pointer that points to
the last element of the list; let this pointer be \(ptr_{n}, HA_{n}\).Assume that agents cannot modify \(ptr_{n}, HA_{n}\). Then,
can any agent determine whether the list has been tampered with?Suppose the agent modifies \(D_{j}\), \(ptr_{j}\) or
\(HA_{j}\) for any \(j < n\). Any agent can detect this tampering because the
hash value \(HA_{j+1}\) will no longer match the \(j\)-th element of
the list. If
the malicious agent also modifies \(HA_{j+1}\) then 
hash value \(HA_{j+2}\) will no longer match the \(j+1\)-th element.By induction on \(j\), any agent can detect tampering with the list
provided malicious agents do not also modify the hash pointer to the
last element of the last.Tamper-Evident Acyclic Graphs and Merkle
TreesThe idea described in the previous paragraph to convert linear linked
lists can be used to convert directed acyclic graphs, in which nodes are
connected by pointers, into tamper-evident graphs. A specific case of
a directed acyclic graph is a rooted tree.AMerkle treeis a special case of a binary balanced tree in which
data items are stored only in the leaves.
Nodes that are not leaves 
contain only hash pointers to nodes in the next level down.
To prove that an element at the leaf is a member of the tree we need only the
\(log_{2}(n)\) hash pointers on the path from the root to that
leaf. By contrast, to prove that an element is a member of a linear
list we need to inspect \(O(n)\) elements, on average.Keys and Signed MessagesYou can create a random public-key, private-key pairby calling a function on your computer.With high probability, nobody else has this
specific pair of keys. Each individual's private key is a secret held
by that individual. Public keys are accessible by everybody.Sending messages securelyKeys are used to send messages securely.
Kamala sends a secure message to Joe by encrypting the message with
Joe's public key; Joe decrypts the message using Joe's private key.
An agent cannot decrypt the encrypted message without Joe's private key.Signing messagesSuppose Kamala needs to send a signed message to Joe while
ensuring that nobody can forge her signature. She encrypts the messageMwith her private key to get an encrypted messageM',
and sends the pair(M, M')securely to Joe, i.e., she encrypts(M, M')with Joe's public key, and sends the resulting
encrypted message to Joe.  When Joe receives the message, Joe decrypts
it using his private key to get(M, M'). Then Joe decryptsM'using Kamala's public key to get the decrypted messageM''. IfM'' = Mthen Joe knows that Kamala sentMbecause only an agent with Kamala's private signature could
have sent that message.Cryptocurrency Managed by a Trusted
AgentLet's start with a digital currency managed by a trusted agent that we will
call a bank. Later, we will
look at a consensus algorithm --- very different from Paxos and Byzantine
Generals --- which will allow cryptocurrencies without trusted
agent.The bank maintains a tamper-evident linear listLof transactions that we call a
tamper-evidentledger. 
Any agent can get a copy of the ledger.
This tamper-evident ledger is the foundation of the currency.A transaction is one of
two types:createorpay.
In a pay transaction,payersgive coins that they possess topayees. An agent can be both a 
payer and payee of the same transaction.
In acreatetransaction the bank creates coins that it gives to
agents --- the payees of the transaction; the bank acts as the payer.For this system to be trusted the bank must follow some protocol that determines
how and when the bank creates coins. We won't discuss these protocols.Apaytransaction is signed by all payers of the transaction.
Acreatetransaction is signed by the bank.
We discussed digital signatures and keys earlier.Each element of the tamper-evident ledger has:a unique id;the type of the transaction, either create or pay;list of payers: only for pay transactions --- a list
  indicating the agents who pay coins into the transaction and the
  amounts that they put in;array of value-payee pairs: for both create and pay transactions --- an
  array of pairs(value, payee public key), where each
  pair in the array indicates that coins of the specified value are
  given to the payee with the specified public key.Example of a create transactionAn example of acreatetransaction is:(3146, create,  [(2.1, 7xxxx...), (3.2, 8xxxx)]).The id of this transaction is 3146, the type of the transaction is
create, and the array of value-payee pairs is[(2.1, 7xxxx...), (3.2, 8xxx)]In this transaction the bank creates a coin of value 2.1 and gives it
to the agent with public key 7xxxx..., and the bank also
creates a coin of value 3.2 and gives it
to the agent with public key 8xxxx...The pair:(transaction id, index into array of value-payee pairs)uniquely identifies a (value, payee) tuple.For example (3146, 0) --- 
transaction id 3146, and array index 0 --- identifies value-payee[0] of
transaction 3146 which is specified by the 2-tuple: (2.1, 7xxxx...). So, the
transaction id and index, (3146, 0), tells everybody that the agent
with public key 7xxxx received 2.1 units of coin in transaction 3146.When this
transaction is in a tamper-evident ledger, every agent from that point
onwards knows that agent 7xxxx received 2.1 coins. Any modifications of this
record can be detected.Likewise, (3146, 1) --- 
transaction id 3146, and array index 1 --- identifies value-payee[1] which
is the 2-tuple (3.2, 8xxxx...).Pay transactionCoins are transferred from payers to payees in a pay transaction.Coins flowing into a pay transaction from payersThe payers are identified by a list of 2-tuples, where each 2-tuple is(transaction id, index into array of value-payee pairs)wheretransaction idis the id of the transaction in the
tamper-evident ledger. As we said earlier, this pair uniquely identifies an agent and a
value that this agent acquired in this transaction.
For example the pair --- transaction id, index --- such as (3146, 0)
identifies the 2-tuple (2.1, 7xxxx...); this 2-tuple asserts
that the agent
with public key 7xxxx received 2.1 units of coin in 
transaction 3146. Theentire amountspecified in the 2-tuple (2.1 in our
example) is value that flows into thispaytransaction from the agent
with public key 7xxxx.Suppose the payers in a pay transaction are identified by the list:[(3146, 0), (7359, 3)]and suppose pair 3 in transaction 7359 is (3.2, 8xxxx). Then the total
amount of coins flowing into this pay transaction is 2.1 + 3.2, and
this amount is disbursed to payees.Fig.3: Coins flowing into and out of a pay transaction.Coins flowing out of a pay transaction to payeesThe outflow of coins is specified by an array of value-payee pairs,
exactly as in a create transaction.
The transaction clears all coins: the total inflow from payers is
equal to the total outflow to payees in a transaction.The system may provide incentives, such as payment of coins, to
managers (e.g. banks) of cryptocurrencies. In this case, one of the
payees is the bank itself.Managing amounts spent in a transactionA transaction-id, index pair --- such as (3146, 0)
identifies a 2-tuple such as (2.1, 7xxxx...); this 2-tuple asserts
that the agent
with public key 7xxxx received 2.1 units of coin in 
transaction 3146. Theentire amountspecified in the 2-tuple (2.1 in our
example) is value that flows into the transaction. What should this
agent do if it
wants to put in more than 2.1 coins into the transaction? Or less than
2.1 coins?To put in more value, the bank identifies other transaction-id, index
pairs in which this agent received coins. For example, say that (4539, 2)
identifies a 2-tuple (3.2, 7xxxx), and assume that the payers in this transaction
are specified by the pairs (3146, 0) and (4539, 2). The pair (3146, 0)
asserts that agent (7xxxx) received 2.1 coins and the pair (4539, 2)
asserts that the same agent received 3.2 coins. So the total amount of 
coins flowing into this
transaction from this agent (7xxxx) includes 2.1 + 3.2.To put in less value, the agent acts as both payer and payee; the net
value that this agent pays out to other agents is the difference
between the amount that this agent puts in and takes out. For example,
if agent with public key 7xxxx wants to put in 1.9 coins into this
transaction its payer information can be given by the transaction-id,
index pair (3146, 0) which asserts that the agent received 2.1 coins
and the same agent is a payee that withdraws 0.2 coins.Preventing Double SpendingHow does the system prevent an agent from using the
same coin twice?For example, the transaction id, index pair
(3146, 0) identifies the 2-tuple (2.1, 7xxxx...); this tuple
asserts that the agent
with public key 7xxxx received 2.1 units of coin in 
transaction 3146. Why can't the agent with public key 7xxxx use the
2.1 coins that it received to buy items from Amazon and later use
the same 2.1 coins to buy more items from Walmart?The bank checks that the agent hasn't already spent the coin that it
is putting into a transaction.Before permitting the transaction that double-spends the 2.1 coins
with Walmart, the bank inspects the tamper-evident ledger
for all transactions after transaction 3146 and before the Walmart
transaction to ensure that the agent (7xxxx) hasn't already spent the
2.1 coins that it got in transaction 3146. The transaction that spends
the coins at Amazon will show up in the ledger, and so the transaction
with Walmart will not be allowed.Every agent that has the bank's hash pointer to the end of the
tamper-evident ledger can inspect the ledger to check that
double-spending hasn't occurred.The bank signs a valid transaction and appends it to the
tamper-evident ledger. All agents can see the bank's signature and
verify that nobody (not even the bank) has tampered with the
tamper-evident ledger.Example of a pay transactionAn example of the specification of a pay transaction is:(9431, pay,
  [(3146, 0), (4731, 2)],
  [(0.7, 7xxxx...), (4.6, 9xxxx...)]
).The id of this transaction is 9431; the type of the transaction is
pay; the payers into the transaction are identified by the pairs of
(transaction-id, index): (3146, 0), and (4731, 2); and the
payee array is [(0.7, 7xxxx...), (4.6, 9xxxx...)].Transaction validityThe bank appends a transaction toLif and only if the
transaction is valid. The bank checks for validity by carrying out the
following steps:The bank verifies that the payers into the transaction signed the
  transaction.The bank checks that the total value of coins paid out from the
  transaction does not
  exceed the total value paid in to the transaction. (If the value paid
  in exceeds the value paid out then the bank takes the difference as
  a transaction fee. More about fees later.)The bank verifies that the payers' claims to have received coins in
  previous transactions is genuine. For example,
  if the agent with
  public key 7xxxx... claims to have received coins worth 2.1 in the
  transaction with id 10, and payee array index 0, then the bank
  verifies this claim by that transaction.The bank ensures that coins paid into the transaction haven't
  already been spent.Optimizations: Blocks and Block ChainVerifying large numbers of small transactions requires more
computation than verifying small numbers of blocks of many transactions.
Ablock chainis a tamper-evident ledger in which
each element of the ledger is a block consisting of many transactions.
A block of transactions can be aggregated into a single large
transaction by aggregating all the payers and payees of the smaller
transactions.The amount of computation decreases as the number of transactions in a
block increases. The time required to fill a block with transactions
is larger when the number of transactions to fill the block increases.Checking the Trusted AgentConsider a system in which the trusted agent broadcasts its current
copy of the tamper-evident ledger to all agents.
Every agent can inspect its copy of the tamper-evident ledger to determine whether the
ledger has been tampered with. So, every agent can validate its trust
in the trusted agent; however, this validation
suffers from a crucial problem: Agents may only
have copies ofold, staleversions of the ledger.
By the time that an agent receives a copy of the ledger, the trusted
agent may have added more transactions to the ledger.Fig.4: Old Copy is a Prefix of the Block ChainAn old copy of the ledger can differ from the current copy in only one
way: the current copy may have transactions appended to the end
of the old copy. So all agents can validatepastbehavior of the
trusted agent. An agent cannot, however, treat its copy of the ledger
as the master copy because the agent may not have the transactions
added most recently to the ledger.In the next module we will see how the Bitcoin algorithm addresses
this problem.Advantages of this cryptocurrencyAny agent can get a copy of the tamper-evident ledger and verify that all
transactions in the ledger are valid. Any agent can verify that
the only way in which the ledger is modified is that elements are
append to its tail; all that the agent needs to do is to check that
the pointer to the tail is modified only by appending
elements. Because the ledger is tamper-evident, an agent can check
that the ledger doesn't change while the hash pointer to the end
of the ledger doesn't change.The bank can't forge a transaction because all payers sign the
transaction. Agents can remain anonymous because an agent's only
public information is the agent's public key, and an agent can create
multiple public keys. Every agent can verify the correctness of every
transaction.Disadvantages of this cryptocurrencyUsers may not trust the bank. Transactions are not private because the
bank has a record of all transactions. And the bank is a single point
of failure.Next we'll look at Bitcoin's algorithms for
implementing a cryptocurrency without trusted agents.ReviewWhat iscollision resistance? Why is it relevant to
  cryptocurrencies?Consider the example ofhidingin which Megan hides her
  predictions for the winner of the World Cup. Suppose you could
  choose two different random number generators for creating the
  secret \(r\); in one case the bits of \(r\) are uncorrelated, and in
  the other they are highly correlated. Which generator would you use
  and why?What isbinding? Why is it relevant to hiding information?What ispuzzle friendly?What is atamper-evident data structure? How would you
  implement a tamper-evident linear list? A tamper-evident tree?Describe the algorithm for cryptocurrency with a trusted agent to
  someone who knows absolutely nothing about cryptocurrency.How could the algorithm for cryptocurrency be used for a collection
  of distributed agents to keep track of a sequence of events, other
  than buying/selling currency?K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology



Figure Link: https://kmchandy.github.io/Crypto/CryptoCurrencyIntroduction/Slide1.jpg

Caption: Fig.1: Hash Pointer points to a Tamper-Evident Block
    of Data


The hash function \(H\) is said to bepuzzle friendlyexactly when any algorithm to solve this problem is about as slow
as a brute-force algorithm which checks \(H(r+x) = y\) for random values
of \(x\).
The number of steps taken by any
algorithm that solves this problem is not significantly lower than
\(2^{n}\).Now, let's look at the following related problem.  Given \(H\), \(r\),
and a set \(Y\) of \(n\)-bit strings, compute any \(x\) such that
\(H(r+x) \in Y\).  If \(Y\) consists of a single element \(y\) then
this problem is the same as that in the previous paragraph.  If \(Y\)
is a set of all \(n\)-bit strings then this problem is trivial because
any \(x\) solves the problem.  The probability that a random value
hashes to an element of \(Y\) is proportional to the cardinality of
\(Y\).  The cardinality of \(Y\) controls the expected time to
solve the puzzle.The hash function \(H\) ispuzzle friendlywhen
given \(H\), \(r\), and a set \(Y\) of \(n\)-bit values, the time
required to compute any \(x\) such that \(H(r+x) \in Y\) is
not significantly lower than \(2^{n} / |Y|\), where \(|Y|\) is the
cardinality of \(Y\).A Cryptographic Hash FunctionA cryptographic hash function is one that is collision resistant,
hiding and puzzle-friendly.Hashing Inputs of Arbitrary LengthLet \(f\) be a function that operates on input strings
of fixed length and produces output strings of fixed length. Let the
input and output strings of \(f\) have lengths \(M + N\) and \(M\),
respectively. We look at functions where \(N > 0\), and since the
output is smaller than the input, \(f\) is called acompression
function.We can use function \(f\) to define a function \(g\) whose inputs are strings of arbitrary
lengths and whose outputs are strings of length \(M\).
Example code for \(g\) is given below whereInitialValueis a given constant string of length \(M\).def g(y):
    output = InitialValue
    // pad y so that it's length is a multiple of N
    if len(y)%N > 0:  y = y + "0"*(N - len(y)%N)
    // partition y into blocks of size N
    blocks = [y[i: i+N] for i in range(0, len(y), N)]
    // Apply function f to the concatenation of the
    // previous output (length M) with each block
    // (length N) to get the next output (length M).
    for block in blocks: output = f(output+block)
    return outputHash PointersA hash pointer to an item \(D\) of data is a pair \((ptr, H(D))\)
where \(ptr\) is a pointer that 
points to \(D\), and \(H\) is a cryptographic hash function.Any data structure with pointers can be converted into a
data structure with hash pointers: merely replace a pointer \(ptr\) to
\(D\) by \((ptr, H(D))\).Tamper-Evident Data StructuresSingle BlockA simple example of a tamper-evident structure is a single block of
data D which is pointed to by a hash pointer consisting of a regular
pointer and a hash H(D).Fig.1: Hash Pointer points to a Tamper-Evident Block
    of DataAssume that a malicious agent cannot modifyboththe hash
pointer and the data that it points to. If an 
agent
    changes D to D' then the tampering can be discovered because the
    hash pointer won't match the data that it is pointing to:
    \(H(D') \neq H(D)\).Tamper-Evident Linked ListLet's look at linear linked list to which elements can be appended but
not deleted. The \(i\)-th element appended to the list points to the
\((i-1)\)-th element. 
Let's replace the pointers in the list by hashed
pointers.Fig.2: Hash Pointer points to a Tamper-Evident List
    of DataThe \(j\)-th element of the list, \(j > 0\), consists of data,
\(D_{j}\), and a hash pointer that points to the \(j - 1\)-th
  element of the list. The hash pointer in the \(j\)-th element
consists of a regular pointer, \(ptr_{j}\), which points to the \(j - 1\)-th
  element of the list, and a hashed value, \(HA_{j}\), which is a cryptographic hash of theentire\(j-1\)th element consisting  of \(D_{j-1}\), \(ptr_{j-1}\)
  and \(HA_{j-1}\).
The \(0\)-th element is called thegenesiselement and has
default values. The list is accessed by a hash pointer that points to
the last element of the list; let this pointer be \(ptr_{n}, HA_{n}\).Assume that agents cannot modify \(ptr_{n}, HA_{n}\). Then,
can any agent determine whether the list has been tampered with?Suppose the agent modifies \(D_{j}\), \(ptr_{j}\) or
\(HA_{j}\) for any \(j < n\). Any agent can detect this tampering because the
hash value \(HA_{j+1}\) will no longer match the \(j\)-th element of
the list. If
the malicious agent also modifies \(HA_{j+1}\) then 
hash value \(HA_{j+2}\) will no longer match the \(j+1\)-th element.By induction on \(j\), any agent can detect tampering with the list
provided malicious agents do not also modify the hash pointer to the
last element of the last.Tamper-Evident Acyclic Graphs and Merkle
TreesThe idea described in the previous paragraph to convert linear linked
lists can be used to convert directed acyclic graphs, in which nodes are
connected by pointers, into tamper-evident graphs. A specific case of
a directed acyclic graph is a rooted tree.AMerkle treeis a special case of a binary balanced tree in which
data items are stored only in the leaves.
Nodes that are not leaves 
contain only hash pointers to nodes in the next level down.
To prove that an element at the leaf is a member of the tree we need only the
\(log_{2}(n)\) hash pointers on the path from the root to that
leaf. By contrast, to prove that an element is a member of a linear
list we need to inspect \(O(n)\) elements, on average.Keys and Signed MessagesYou can create a random public-key, private-key pairby calling a function on your computer.With high probability, nobody else has this
specific pair of keys. Each individual's private key is a secret held
by that individual. Public keys are accessible by everybody.Sending messages securelyKeys are used to send messages securely.
Kamala sends a secure message to Joe by encrypting the message with
Joe's public key; Joe decrypts the message using Joe's private key.
An agent cannot decrypt the encrypted message without Joe's private key.Signing messagesSuppose Kamala needs to send a signed message to Joe while
ensuring that nobody can forge her signature. She encrypts the messageMwith her private key to get an encrypted messageM',
and sends the pair(M, M')securely to Joe, i.e., she encrypts(M, M')with Joe's public key, and sends the resulting
encrypted message to Joe.  When Joe receives the message, Joe decrypts
it using his private key to get(M, M'). Then Joe decryptsM'using Kamala's public key to get the decrypted messageM''. IfM'' = Mthen Joe knows that Kamala sentMbecause only an agent with Kamala's private signature could
have sent that message.Cryptocurrency Managed by a Trusted
AgentLet's start with a digital currency managed by a trusted agent that we will
call a bank. Later, we will
look at a consensus algorithm --- very different from Paxos and Byzantine
Generals --- which will allow cryptocurrencies without trusted
agent.The bank maintains a tamper-evident linear listLof transactions that we call a
tamper-evidentledger. 
Any agent can get a copy of the ledger.
This tamper-evident ledger is the foundation of the currency.A transaction is one of
two types:createorpay.
In a pay transaction,payersgive coins that they possess topayees. An agent can be both a 
payer and payee of the same transaction.
In acreatetransaction the bank creates coins that it gives to
agents --- the payees of the transaction; the bank acts as the payer.For this system to be trusted the bank must follow some protocol that determines
how and when the bank creates coins. We won't discuss these protocols.Apaytransaction is signed by all payers of the transaction.
Acreatetransaction is signed by the bank.
We discussed digital signatures and keys earlier.Each element of the tamper-evident ledger has:a unique id;the type of the transaction, either create or pay;list of payers: only for pay transactions --- a list
  indicating the agents who pay coins into the transaction and the
  amounts that they put in;array of value-payee pairs: for both create and pay transactions --- an
  array of pairs(value, payee public key), where each
  pair in the array indicates that coins of the specified value are
  given to the payee with the specified public key.Example of a create transactionAn example of acreatetransaction is:(3146, create,  [(2.1, 7xxxx...), (3.2, 8xxxx)]).The id of this transaction is 3146, the type of the transaction is
create, and the array of value-payee pairs is[(2.1, 7xxxx...), (3.2, 8xxx)]In this transaction the bank creates a coin of value 2.1 and gives it
to the agent with public key 7xxxx..., and the bank also
creates a coin of value 3.2 and gives it
to the agent with public key 8xxxx...The pair:(transaction id, index into array of value-payee pairs)uniquely identifies a (value, payee) tuple.For example (3146, 0) --- 
transaction id 3146, and array index 0 --- identifies value-payee[0] of
transaction 3146 which is specified by the 2-tuple: (2.1, 7xxxx...). So, the
transaction id and index, (3146, 0), tells everybody that the agent
with public key 7xxxx received 2.1 units of coin in transaction 3146.When this
transaction is in a tamper-evident ledger, every agent from that point
onwards knows that agent 7xxxx received 2.1 coins. Any modifications of this
record can be detected.Likewise, (3146, 1) --- 
transaction id 3146, and array index 1 --- identifies value-payee[1] which
is the 2-tuple (3.2, 8xxxx...).Pay transactionCoins are transferred from payers to payees in a pay transaction.Coins flowing into a pay transaction from payersThe payers are identified by a list of 2-tuples, where each 2-tuple is(transaction id, index into array of value-payee pairs)wheretransaction idis the id of the transaction in the
tamper-evident ledger. As we said earlier, this pair uniquely identifies an agent and a
value that this agent acquired in this transaction.
For example the pair --- transaction id, index --- such as (3146, 0)
identifies the 2-tuple (2.1, 7xxxx...); this 2-tuple asserts
that the agent
with public key 7xxxx received 2.1 units of coin in 
transaction 3146. Theentire amountspecified in the 2-tuple (2.1 in our
example) is value that flows into thispaytransaction from the agent
with public key 7xxxx.Suppose the payers in a pay transaction are identified by the list:[(3146, 0), (7359, 3)]and suppose pair 3 in transaction 7359 is (3.2, 8xxxx). Then the total
amount of coins flowing into this pay transaction is 2.1 + 3.2, and
this amount is disbursed to payees.Fig.3: Coins flowing into and out of a pay transaction.Coins flowing out of a pay transaction to payeesThe outflow of coins is specified by an array of value-payee pairs,
exactly as in a create transaction.
The transaction clears all coins: the total inflow from payers is
equal to the total outflow to payees in a transaction.The system may provide incentives, such as payment of coins, to
managers (e.g. banks) of cryptocurrencies. In this case, one of the
payees is the bank itself.Managing amounts spent in a transactionA transaction-id, index pair --- such as (3146, 0)
identifies a 2-tuple such as (2.1, 7xxxx...); this 2-tuple asserts
that the agent
with public key 7xxxx received 2.1 units of coin in 
transaction 3146. Theentire amountspecified in the 2-tuple (2.1 in our
example) is value that flows into the transaction. What should this
agent do if it
wants to put in more than 2.1 coins into the transaction? Or less than
2.1 coins?To put in more value, the bank identifies other transaction-id, index
pairs in which this agent received coins. For example, say that (4539, 2)
identifies a 2-tuple (3.2, 7xxxx), and assume that the payers in this transaction
are specified by the pairs (3146, 0) and (4539, 2). The pair (3146, 0)
asserts that agent (7xxxx) received 2.1 coins and the pair (4539, 2)
asserts that the same agent received 3.2 coins. So the total amount of 
coins flowing into this
transaction from this agent (7xxxx) includes 2.1 + 3.2.To put in less value, the agent acts as both payer and payee; the net
value that this agent pays out to other agents is the difference
between the amount that this agent puts in and takes out. For example,
if agent with public key 7xxxx wants to put in 1.9 coins into this
transaction its payer information can be given by the transaction-id,
index pair (3146, 0) which asserts that the agent received 2.1 coins
and the same agent is a payee that withdraws 0.2 coins.Preventing Double SpendingHow does the system prevent an agent from using the
same coin twice?For example, the transaction id, index pair
(3146, 0) identifies the 2-tuple (2.1, 7xxxx...); this tuple
asserts that the agent
with public key 7xxxx received 2.1 units of coin in 
transaction 3146. Why can't the agent with public key 7xxxx use the
2.1 coins that it received to buy items from Amazon and later use
the same 2.1 coins to buy more items from Walmart?The bank checks that the agent hasn't already spent the coin that it
is putting into a transaction.Before permitting the transaction that double-spends the 2.1 coins
with Walmart, the bank inspects the tamper-evident ledger
for all transactions after transaction 3146 and before the Walmart
transaction to ensure that the agent (7xxxx) hasn't already spent the
2.1 coins that it got in transaction 3146. The transaction that spends
the coins at Amazon will show up in the ledger, and so the transaction
with Walmart will not be allowed.Every agent that has the bank's hash pointer to the end of the
tamper-evident ledger can inspect the ledger to check that
double-spending hasn't occurred.The bank signs a valid transaction and appends it to the
tamper-evident ledger. All agents can see the bank's signature and
verify that nobody (not even the bank) has tampered with the
tamper-evident ledger.Example of a pay transactionAn example of the specification of a pay transaction is:(9431, pay,
  [(3146, 0), (4731, 2)],
  [(0.7, 7xxxx...), (4.6, 9xxxx...)]
).The id of this transaction is 9431; the type of the transaction is
pay; the payers into the transaction are identified by the pairs of
(transaction-id, index): (3146, 0), and (4731, 2); and the
payee array is [(0.7, 7xxxx...), (4.6, 9xxxx...)].Transaction validityThe bank appends a transaction toLif and only if the
transaction is valid. The bank checks for validity by carrying out the
following steps:The bank verifies that the payers into the transaction signed the
  transaction.The bank checks that the total value of coins paid out from the
  transaction does not
  exceed the total value paid in to the transaction. (If the value paid
  in exceeds the value paid out then the bank takes the difference as
  a transaction fee. More about fees later.)The bank verifies that the payers' claims to have received coins in
  previous transactions is genuine. For example,
  if the agent with
  public key 7xxxx... claims to have received coins worth 2.1 in the
  transaction with id 10, and payee array index 0, then the bank
  verifies this claim by that transaction.The bank ensures that coins paid into the transaction haven't
  already been spent.Optimizations: Blocks and Block ChainVerifying large numbers of small transactions requires more
computation than verifying small numbers of blocks of many transactions.
Ablock chainis a tamper-evident ledger in which
each element of the ledger is a block consisting of many transactions.
A block of transactions can be aggregated into a single large
transaction by aggregating all the payers and payees of the smaller
transactions.The amount of computation decreases as the number of transactions in a
block increases. The time required to fill a block with transactions
is larger when the number of transactions to fill the block increases.Checking the Trusted AgentConsider a system in which the trusted agent broadcasts its current
copy of the tamper-evident ledger to all agents.
Every agent can inspect its copy of the tamper-evident ledger to determine whether the
ledger has been tampered with. So, every agent can validate its trust
in the trusted agent; however, this validation
suffers from a crucial problem: Agents may only
have copies ofold, staleversions of the ledger.
By the time that an agent receives a copy of the ledger, the trusted
agent may have added more transactions to the ledger.Fig.4: Old Copy is a Prefix of the Block ChainAn old copy of the ledger can differ from the current copy in only one
way: the current copy may have transactions appended to the end
of the old copy. So all agents can validatepastbehavior of the
trusted agent. An agent cannot, however, treat its copy of the ledger
as the master copy because the agent may not have the transactions
added most recently to the ledger.In the next module we will see how the Bitcoin algorithm addresses
this problem.Advantages of this cryptocurrencyAny agent can get a copy of the tamper-evident ledger and verify that all
transactions in the ledger are valid. Any agent can verify that
the only way in which the ledger is modified is that elements are
append to its tail; all that the agent needs to do is to check that
the pointer to the tail is modified only by appending
elements. Because the ledger is tamper-evident, an agent can check
that the ledger doesn't change while the hash pointer to the end
of the ledger doesn't change.The bank can't forge a transaction because all payers sign the
transaction. Agents can remain anonymous because an agent's only
public information is the agent's public key, and an agent can create
multiple public keys. Every agent can verify the correctness of every
transaction.Disadvantages of this cryptocurrencyUsers may not trust the bank. Transactions are not private because the
bank has a record of all transactions. And the bank is a single point
of failure.Next we'll look at Bitcoin's algorithms for
implementing a cryptocurrency without trusted agents.ReviewWhat iscollision resistance? Why is it relevant to
  cryptocurrencies?Consider the example ofhidingin which Megan hides her
  predictions for the winner of the World Cup. Suppose you could
  choose two different random number generators for creating the
  secret \(r\); in one case the bits of \(r\) are uncorrelated, and in
  the other they are highly correlated. Which generator would you use
  and why?What isbinding? Why is it relevant to hiding information?What ispuzzle friendly?What is atamper-evident data structure? How would you
  implement a tamper-evident linear list? A tamper-evident tree?Describe the algorithm for cryptocurrency with a trusted agent to
  someone who knows absolutely nothing about cryptocurrency.How could the algorithm for cryptocurrency be used for a collection
  of distributed agents to keep track of a sequence of events, other
  than buying/selling currency?K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology



Figure Link: https://kmchandy.github.io/Crypto/CryptoCurrencyIntroduction/Slide1.jpg

Caption: Fig.1: Hash Pointer points to a Tamper-Evident Block
    of Data


Now, let's look at the following related problem.  Given \(H\), \(r\),
and a set \(Y\) of \(n\)-bit strings, compute any \(x\) such that
\(H(r+x) \in Y\).  If \(Y\) consists of a single element \(y\) then
this problem is the same as that in the previous paragraph.  If \(Y\)
is a set of all \(n\)-bit strings then this problem is trivial because
any \(x\) solves the problem.  The probability that a random value
hashes to an element of \(Y\) is proportional to the cardinality of
\(Y\).  The cardinality of \(Y\) controls the expected time to
solve the puzzle.The hash function \(H\) ispuzzle friendlywhen
given \(H\), \(r\), and a set \(Y\) of \(n\)-bit values, the time
required to compute any \(x\) such that \(H(r+x) \in Y\) is
not significantly lower than \(2^{n} / |Y|\), where \(|Y|\) is the
cardinality of \(Y\).A Cryptographic Hash FunctionA cryptographic hash function is one that is collision resistant,
hiding and puzzle-friendly.Hashing Inputs of Arbitrary LengthLet \(f\) be a function that operates on input strings
of fixed length and produces output strings of fixed length. Let the
input and output strings of \(f\) have lengths \(M + N\) and \(M\),
respectively. We look at functions where \(N > 0\), and since the
output is smaller than the input, \(f\) is called acompression
function.We can use function \(f\) to define a function \(g\) whose inputs are strings of arbitrary
lengths and whose outputs are strings of length \(M\).
Example code for \(g\) is given below whereInitialValueis a given constant string of length \(M\).def g(y):
    output = InitialValue
    // pad y so that it's length is a multiple of N
    if len(y)%N > 0:  y = y + "0"*(N - len(y)%N)
    // partition y into blocks of size N
    blocks = [y[i: i+N] for i in range(0, len(y), N)]
    // Apply function f to the concatenation of the
    // previous output (length M) with each block
    // (length N) to get the next output (length M).
    for block in blocks: output = f(output+block)
    return outputHash PointersA hash pointer to an item \(D\) of data is a pair \((ptr, H(D))\)
where \(ptr\) is a pointer that 
points to \(D\), and \(H\) is a cryptographic hash function.Any data structure with pointers can be converted into a
data structure with hash pointers: merely replace a pointer \(ptr\) to
\(D\) by \((ptr, H(D))\).Tamper-Evident Data StructuresSingle BlockA simple example of a tamper-evident structure is a single block of
data D which is pointed to by a hash pointer consisting of a regular
pointer and a hash H(D).Fig.1: Hash Pointer points to a Tamper-Evident Block
    of DataAssume that a malicious agent cannot modifyboththe hash
pointer and the data that it points to. If an 
agent
    changes D to D' then the tampering can be discovered because the
    hash pointer won't match the data that it is pointing to:
    \(H(D') \neq H(D)\).Tamper-Evident Linked ListLet's look at linear linked list to which elements can be appended but
not deleted. The \(i\)-th element appended to the list points to the
\((i-1)\)-th element. 
Let's replace the pointers in the list by hashed
pointers.Fig.2: Hash Pointer points to a Tamper-Evident List
    of DataThe \(j\)-th element of the list, \(j > 0\), consists of data,
\(D_{j}\), and a hash pointer that points to the \(j - 1\)-th
  element of the list. The hash pointer in the \(j\)-th element
consists of a regular pointer, \(ptr_{j}\), which points to the \(j - 1\)-th
  element of the list, and a hashed value, \(HA_{j}\), which is a cryptographic hash of theentire\(j-1\)th element consisting  of \(D_{j-1}\), \(ptr_{j-1}\)
  and \(HA_{j-1}\).
The \(0\)-th element is called thegenesiselement and has
default values. The list is accessed by a hash pointer that points to
the last element of the list; let this pointer be \(ptr_{n}, HA_{n}\).Assume that agents cannot modify \(ptr_{n}, HA_{n}\). Then,
can any agent determine whether the list has been tampered with?Suppose the agent modifies \(D_{j}\), \(ptr_{j}\) or
\(HA_{j}\) for any \(j < n\). Any agent can detect this tampering because the
hash value \(HA_{j+1}\) will no longer match the \(j\)-th element of
the list. If
the malicious agent also modifies \(HA_{j+1}\) then 
hash value \(HA_{j+2}\) will no longer match the \(j+1\)-th element.By induction on \(j\), any agent can detect tampering with the list
provided malicious agents do not also modify the hash pointer to the
last element of the last.Tamper-Evident Acyclic Graphs and Merkle
TreesThe idea described in the previous paragraph to convert linear linked
lists can be used to convert directed acyclic graphs, in which nodes are
connected by pointers, into tamper-evident graphs. A specific case of
a directed acyclic graph is a rooted tree.AMerkle treeis a special case of a binary balanced tree in which
data items are stored only in the leaves.
Nodes that are not leaves 
contain only hash pointers to nodes in the next level down.
To prove that an element at the leaf is a member of the tree we need only the
\(log_{2}(n)\) hash pointers on the path from the root to that
leaf. By contrast, to prove that an element is a member of a linear
list we need to inspect \(O(n)\) elements, on average.Keys and Signed MessagesYou can create a random public-key, private-key pairby calling a function on your computer.With high probability, nobody else has this
specific pair of keys. Each individual's private key is a secret held
by that individual. Public keys are accessible by everybody.Sending messages securelyKeys are used to send messages securely.
Kamala sends a secure message to Joe by encrypting the message with
Joe's public key; Joe decrypts the message using Joe's private key.
An agent cannot decrypt the encrypted message without Joe's private key.Signing messagesSuppose Kamala needs to send a signed message to Joe while
ensuring that nobody can forge her signature. She encrypts the messageMwith her private key to get an encrypted messageM',
and sends the pair(M, M')securely to Joe, i.e., she encrypts(M, M')with Joe's public key, and sends the resulting
encrypted message to Joe.  When Joe receives the message, Joe decrypts
it using his private key to get(M, M'). Then Joe decryptsM'using Kamala's public key to get the decrypted messageM''. IfM'' = Mthen Joe knows that Kamala sentMbecause only an agent with Kamala's private signature could
have sent that message.Cryptocurrency Managed by a Trusted
AgentLet's start with a digital currency managed by a trusted agent that we will
call a bank. Later, we will
look at a consensus algorithm --- very different from Paxos and Byzantine
Generals --- which will allow cryptocurrencies without trusted
agent.The bank maintains a tamper-evident linear listLof transactions that we call a
tamper-evidentledger. 
Any agent can get a copy of the ledger.
This tamper-evident ledger is the foundation of the currency.A transaction is one of
two types:createorpay.
In a pay transaction,payersgive coins that they possess topayees. An agent can be both a 
payer and payee of the same transaction.
In acreatetransaction the bank creates coins that it gives to
agents --- the payees of the transaction; the bank acts as the payer.For this system to be trusted the bank must follow some protocol that determines
how and when the bank creates coins. We won't discuss these protocols.Apaytransaction is signed by all payers of the transaction.
Acreatetransaction is signed by the bank.
We discussed digital signatures and keys earlier.Each element of the tamper-evident ledger has:a unique id;the type of the transaction, either create or pay;list of payers: only for pay transactions --- a list
  indicating the agents who pay coins into the transaction and the
  amounts that they put in;array of value-payee pairs: for both create and pay transactions --- an
  array of pairs(value, payee public key), where each
  pair in the array indicates that coins of the specified value are
  given to the payee with the specified public key.Example of a create transactionAn example of acreatetransaction is:(3146, create,  [(2.1, 7xxxx...), (3.2, 8xxxx)]).The id of this transaction is 3146, the type of the transaction is
create, and the array of value-payee pairs is[(2.1, 7xxxx...), (3.2, 8xxx)]In this transaction the bank creates a coin of value 2.1 and gives it
to the agent with public key 7xxxx..., and the bank also
creates a coin of value 3.2 and gives it
to the agent with public key 8xxxx...The pair:(transaction id, index into array of value-payee pairs)uniquely identifies a (value, payee) tuple.For example (3146, 0) --- 
transaction id 3146, and array index 0 --- identifies value-payee[0] of
transaction 3146 which is specified by the 2-tuple: (2.1, 7xxxx...). So, the
transaction id and index, (3146, 0), tells everybody that the agent
with public key 7xxxx received 2.1 units of coin in transaction 3146.When this
transaction is in a tamper-evident ledger, every agent from that point
onwards knows that agent 7xxxx received 2.1 coins. Any modifications of this
record can be detected.Likewise, (3146, 1) --- 
transaction id 3146, and array index 1 --- identifies value-payee[1] which
is the 2-tuple (3.2, 8xxxx...).Pay transactionCoins are transferred from payers to payees in a pay transaction.Coins flowing into a pay transaction from payersThe payers are identified by a list of 2-tuples, where each 2-tuple is(transaction id, index into array of value-payee pairs)wheretransaction idis the id of the transaction in the
tamper-evident ledger. As we said earlier, this pair uniquely identifies an agent and a
value that this agent acquired in this transaction.
For example the pair --- transaction id, index --- such as (3146, 0)
identifies the 2-tuple (2.1, 7xxxx...); this 2-tuple asserts
that the agent
with public key 7xxxx received 2.1 units of coin in 
transaction 3146. Theentire amountspecified in the 2-tuple (2.1 in our
example) is value that flows into thispaytransaction from the agent
with public key 7xxxx.Suppose the payers in a pay transaction are identified by the list:[(3146, 0), (7359, 3)]and suppose pair 3 in transaction 7359 is (3.2, 8xxxx). Then the total
amount of coins flowing into this pay transaction is 2.1 + 3.2, and
this amount is disbursed to payees.Fig.3: Coins flowing into and out of a pay transaction.Coins flowing out of a pay transaction to payeesThe outflow of coins is specified by an array of value-payee pairs,
exactly as in a create transaction.
The transaction clears all coins: the total inflow from payers is
equal to the total outflow to payees in a transaction.The system may provide incentives, such as payment of coins, to
managers (e.g. banks) of cryptocurrencies. In this case, one of the
payees is the bank itself.Managing amounts spent in a transactionA transaction-id, index pair --- such as (3146, 0)
identifies a 2-tuple such as (2.1, 7xxxx...); this 2-tuple asserts
that the agent
with public key 7xxxx received 2.1 units of coin in 
transaction 3146. Theentire amountspecified in the 2-tuple (2.1 in our
example) is value that flows into the transaction. What should this
agent do if it
wants to put in more than 2.1 coins into the transaction? Or less than
2.1 coins?To put in more value, the bank identifies other transaction-id, index
pairs in which this agent received coins. For example, say that (4539, 2)
identifies a 2-tuple (3.2, 7xxxx), and assume that the payers in this transaction
are specified by the pairs (3146, 0) and (4539, 2). The pair (3146, 0)
asserts that agent (7xxxx) received 2.1 coins and the pair (4539, 2)
asserts that the same agent received 3.2 coins. So the total amount of 
coins flowing into this
transaction from this agent (7xxxx) includes 2.1 + 3.2.To put in less value, the agent acts as both payer and payee; the net
value that this agent pays out to other agents is the difference
between the amount that this agent puts in and takes out. For example,
if agent with public key 7xxxx wants to put in 1.9 coins into this
transaction its payer information can be given by the transaction-id,
index pair (3146, 0) which asserts that the agent received 2.1 coins
and the same agent is a payee that withdraws 0.2 coins.Preventing Double SpendingHow does the system prevent an agent from using the
same coin twice?For example, the transaction id, index pair
(3146, 0) identifies the 2-tuple (2.1, 7xxxx...); this tuple
asserts that the agent
with public key 7xxxx received 2.1 units of coin in 
transaction 3146. Why can't the agent with public key 7xxxx use the
2.1 coins that it received to buy items from Amazon and later use
the same 2.1 coins to buy more items from Walmart?The bank checks that the agent hasn't already spent the coin that it
is putting into a transaction.Before permitting the transaction that double-spends the 2.1 coins
with Walmart, the bank inspects the tamper-evident ledger
for all transactions after transaction 3146 and before the Walmart
transaction to ensure that the agent (7xxxx) hasn't already spent the
2.1 coins that it got in transaction 3146. The transaction that spends
the coins at Amazon will show up in the ledger, and so the transaction
with Walmart will not be allowed.Every agent that has the bank's hash pointer to the end of the
tamper-evident ledger can inspect the ledger to check that
double-spending hasn't occurred.The bank signs a valid transaction and appends it to the
tamper-evident ledger. All agents can see the bank's signature and
verify that nobody (not even the bank) has tampered with the
tamper-evident ledger.Example of a pay transactionAn example of the specification of a pay transaction is:(9431, pay,
  [(3146, 0), (4731, 2)],
  [(0.7, 7xxxx...), (4.6, 9xxxx...)]
).The id of this transaction is 9431; the type of the transaction is
pay; the payers into the transaction are identified by the pairs of
(transaction-id, index): (3146, 0), and (4731, 2); and the
payee array is [(0.7, 7xxxx...), (4.6, 9xxxx...)].Transaction validityThe bank appends a transaction toLif and only if the
transaction is valid. The bank checks for validity by carrying out the
following steps:The bank verifies that the payers into the transaction signed the
  transaction.The bank checks that the total value of coins paid out from the
  transaction does not
  exceed the total value paid in to the transaction. (If the value paid
  in exceeds the value paid out then the bank takes the difference as
  a transaction fee. More about fees later.)The bank verifies that the payers' claims to have received coins in
  previous transactions is genuine. For example,
  if the agent with
  public key 7xxxx... claims to have received coins worth 2.1 in the
  transaction with id 10, and payee array index 0, then the bank
  verifies this claim by that transaction.The bank ensures that coins paid into the transaction haven't
  already been spent.Optimizations: Blocks and Block ChainVerifying large numbers of small transactions requires more
computation than verifying small numbers of blocks of many transactions.
Ablock chainis a tamper-evident ledger in which
each element of the ledger is a block consisting of many transactions.
A block of transactions can be aggregated into a single large
transaction by aggregating all the payers and payees of the smaller
transactions.The amount of computation decreases as the number of transactions in a
block increases. The time required to fill a block with transactions
is larger when the number of transactions to fill the block increases.Checking the Trusted AgentConsider a system in which the trusted agent broadcasts its current
copy of the tamper-evident ledger to all agents.
Every agent can inspect its copy of the tamper-evident ledger to determine whether the
ledger has been tampered with. So, every agent can validate its trust
in the trusted agent; however, this validation
suffers from a crucial problem: Agents may only
have copies ofold, staleversions of the ledger.
By the time that an agent receives a copy of the ledger, the trusted
agent may have added more transactions to the ledger.Fig.4: Old Copy is a Prefix of the Block ChainAn old copy of the ledger can differ from the current copy in only one
way: the current copy may have transactions appended to the end
of the old copy. So all agents can validatepastbehavior of the
trusted agent. An agent cannot, however, treat its copy of the ledger
as the master copy because the agent may not have the transactions
added most recently to the ledger.In the next module we will see how the Bitcoin algorithm addresses
this problem.Advantages of this cryptocurrencyAny agent can get a copy of the tamper-evident ledger and verify that all
transactions in the ledger are valid. Any agent can verify that
the only way in which the ledger is modified is that elements are
append to its tail; all that the agent needs to do is to check that
the pointer to the tail is modified only by appending
elements. Because the ledger is tamper-evident, an agent can check
that the ledger doesn't change while the hash pointer to the end
of the ledger doesn't change.The bank can't forge a transaction because all payers sign the
transaction. Agents can remain anonymous because an agent's only
public information is the agent's public key, and an agent can create
multiple public keys. Every agent can verify the correctness of every
transaction.Disadvantages of this cryptocurrencyUsers may not trust the bank. Transactions are not private because the
bank has a record of all transactions. And the bank is a single point
of failure.Next we'll look at Bitcoin's algorithms for
implementing a cryptocurrency without trusted agents.ReviewWhat iscollision resistance? Why is it relevant to
  cryptocurrencies?Consider the example ofhidingin which Megan hides her
  predictions for the winner of the World Cup. Suppose you could
  choose two different random number generators for creating the
  secret \(r\); in one case the bits of \(r\) are uncorrelated, and in
  the other they are highly correlated. Which generator would you use
  and why?What isbinding? Why is it relevant to hiding information?What ispuzzle friendly?What is atamper-evident data structure? How would you
  implement a tamper-evident linear list? A tamper-evident tree?Describe the algorithm for cryptocurrency with a trusted agent to
  someone who knows absolutely nothing about cryptocurrency.How could the algorithm for cryptocurrency be used for a collection
  of distributed agents to keep track of a sequence of events, other
  than buying/selling currency?K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology



Figure Link: https://kmchandy.github.io/Crypto/CryptoCurrencyIntroduction/Slide1.jpg

Caption: Fig.1: Hash Pointer points to a Tamper-Evident Block
    of Data


We can use function \(f\) to define a function \(g\) whose inputs are strings of arbitrary
lengths and whose outputs are strings of length \(M\).
Example code for \(g\) is given below whereInitialValueis a given constant string of length \(M\).def g(y):
    output = InitialValue
    // pad y so that it's length is a multiple of N
    if len(y)%N > 0:  y = y + "0"*(N - len(y)%N)
    // partition y into blocks of size N
    blocks = [y[i: i+N] for i in range(0, len(y), N)]
    // Apply function f to the concatenation of the
    // previous output (length M) with each block
    // (length N) to get the next output (length M).
    for block in blocks: output = f(output+block)
    return outputHash PointersA hash pointer to an item \(D\) of data is a pair \((ptr, H(D))\)
where \(ptr\) is a pointer that 
points to \(D\), and \(H\) is a cryptographic hash function.Any data structure with pointers can be converted into a
data structure with hash pointers: merely replace a pointer \(ptr\) to
\(D\) by \((ptr, H(D))\).Tamper-Evident Data StructuresSingle BlockA simple example of a tamper-evident structure is a single block of
data D which is pointed to by a hash pointer consisting of a regular
pointer and a hash H(D).Fig.1: Hash Pointer points to a Tamper-Evident Block
    of DataAssume that a malicious agent cannot modifyboththe hash
pointer and the data that it points to. If an 
agent
    changes D to D' then the tampering can be discovered because the
    hash pointer won't match the data that it is pointing to:
    \(H(D') \neq H(D)\).Tamper-Evident Linked ListLet's look at linear linked list to which elements can be appended but
not deleted. The \(i\)-th element appended to the list points to the
\((i-1)\)-th element. 
Let's replace the pointers in the list by hashed
pointers.Fig.2: Hash Pointer points to a Tamper-Evident List
    of DataThe \(j\)-th element of the list, \(j > 0\), consists of data,
\(D_{j}\), and a hash pointer that points to the \(j - 1\)-th
  element of the list. The hash pointer in the \(j\)-th element
consists of a regular pointer, \(ptr_{j}\), which points to the \(j - 1\)-th
  element of the list, and a hashed value, \(HA_{j}\), which is a cryptographic hash of theentire\(j-1\)th element consisting  of \(D_{j-1}\), \(ptr_{j-1}\)
  and \(HA_{j-1}\).
The \(0\)-th element is called thegenesiselement and has
default values. The list is accessed by a hash pointer that points to
the last element of the list; let this pointer be \(ptr_{n}, HA_{n}\).Assume that agents cannot modify \(ptr_{n}, HA_{n}\). Then,
can any agent determine whether the list has been tampered with?Suppose the agent modifies \(D_{j}\), \(ptr_{j}\) or
\(HA_{j}\) for any \(j < n\). Any agent can detect this tampering because the
hash value \(HA_{j+1}\) will no longer match the \(j\)-th element of
the list. If
the malicious agent also modifies \(HA_{j+1}\) then 
hash value \(HA_{j+2}\) will no longer match the \(j+1\)-th element.By induction on \(j\), any agent can detect tampering with the list
provided malicious agents do not also modify the hash pointer to the
last element of the last.Tamper-Evident Acyclic Graphs and Merkle
TreesThe idea described in the previous paragraph to convert linear linked
lists can be used to convert directed acyclic graphs, in which nodes are
connected by pointers, into tamper-evident graphs. A specific case of
a directed acyclic graph is a rooted tree.AMerkle treeis a special case of a binary balanced tree in which
data items are stored only in the leaves.
Nodes that are not leaves 
contain only hash pointers to nodes in the next level down.
To prove that an element at the leaf is a member of the tree we need only the
\(log_{2}(n)\) hash pointers on the path from the root to that
leaf. By contrast, to prove that an element is a member of a linear
list we need to inspect \(O(n)\) elements, on average.Keys and Signed MessagesYou can create a random public-key, private-key pairby calling a function on your computer.With high probability, nobody else has this
specific pair of keys. Each individual's private key is a secret held
by that individual. Public keys are accessible by everybody.Sending messages securelyKeys are used to send messages securely.
Kamala sends a secure message to Joe by encrypting the message with
Joe's public key; Joe decrypts the message using Joe's private key.
An agent cannot decrypt the encrypted message without Joe's private key.Signing messagesSuppose Kamala needs to send a signed message to Joe while
ensuring that nobody can forge her signature. She encrypts the messageMwith her private key to get an encrypted messageM',
and sends the pair(M, M')securely to Joe, i.e., she encrypts(M, M')with Joe's public key, and sends the resulting
encrypted message to Joe.  When Joe receives the message, Joe decrypts
it using his private key to get(M, M'). Then Joe decryptsM'using Kamala's public key to get the decrypted messageM''. IfM'' = Mthen Joe knows that Kamala sentMbecause only an agent with Kamala's private signature could
have sent that message.Cryptocurrency Managed by a Trusted
AgentLet's start with a digital currency managed by a trusted agent that we will
call a bank. Later, we will
look at a consensus algorithm --- very different from Paxos and Byzantine
Generals --- which will allow cryptocurrencies without trusted
agent.The bank maintains a tamper-evident linear listLof transactions that we call a
tamper-evidentledger. 
Any agent can get a copy of the ledger.
This tamper-evident ledger is the foundation of the currency.A transaction is one of
two types:createorpay.
In a pay transaction,payersgive coins that they possess topayees. An agent can be both a 
payer and payee of the same transaction.
In acreatetransaction the bank creates coins that it gives to
agents --- the payees of the transaction; the bank acts as the payer.For this system to be trusted the bank must follow some protocol that determines
how and when the bank creates coins. We won't discuss these protocols.Apaytransaction is signed by all payers of the transaction.
Acreatetransaction is signed by the bank.
We discussed digital signatures and keys earlier.Each element of the tamper-evident ledger has:a unique id;the type of the transaction, either create or pay;list of payers: only for pay transactions --- a list
  indicating the agents who pay coins into the transaction and the
  amounts that they put in;array of value-payee pairs: for both create and pay transactions --- an
  array of pairs(value, payee public key), where each
  pair in the array indicates that coins of the specified value are
  given to the payee with the specified public key.Example of a create transactionAn example of acreatetransaction is:(3146, create,  [(2.1, 7xxxx...), (3.2, 8xxxx)]).The id of this transaction is 3146, the type of the transaction is
create, and the array of value-payee pairs is[(2.1, 7xxxx...), (3.2, 8xxx)]In this transaction the bank creates a coin of value 2.1 and gives it
to the agent with public key 7xxxx..., and the bank also
creates a coin of value 3.2 and gives it
to the agent with public key 8xxxx...The pair:(transaction id, index into array of value-payee pairs)uniquely identifies a (value, payee) tuple.For example (3146, 0) --- 
transaction id 3146, and array index 0 --- identifies value-payee[0] of
transaction 3146 which is specified by the 2-tuple: (2.1, 7xxxx...). So, the
transaction id and index, (3146, 0), tells everybody that the agent
with public key 7xxxx received 2.1 units of coin in transaction 3146.When this
transaction is in a tamper-evident ledger, every agent from that point
onwards knows that agent 7xxxx received 2.1 coins. Any modifications of this
record can be detected.Likewise, (3146, 1) --- 
transaction id 3146, and array index 1 --- identifies value-payee[1] which
is the 2-tuple (3.2, 8xxxx...).Pay transactionCoins are transferred from payers to payees in a pay transaction.Coins flowing into a pay transaction from payersThe payers are identified by a list of 2-tuples, where each 2-tuple is(transaction id, index into array of value-payee pairs)wheretransaction idis the id of the transaction in the
tamper-evident ledger. As we said earlier, this pair uniquely identifies an agent and a
value that this agent acquired in this transaction.
For example the pair --- transaction id, index --- such as (3146, 0)
identifies the 2-tuple (2.1, 7xxxx...); this 2-tuple asserts
that the agent
with public key 7xxxx received 2.1 units of coin in 
transaction 3146. Theentire amountspecified in the 2-tuple (2.1 in our
example) is value that flows into thispaytransaction from the agent
with public key 7xxxx.Suppose the payers in a pay transaction are identified by the list:[(3146, 0), (7359, 3)]and suppose pair 3 in transaction 7359 is (3.2, 8xxxx). Then the total
amount of coins flowing into this pay transaction is 2.1 + 3.2, and
this amount is disbursed to payees.Fig.3: Coins flowing into and out of a pay transaction.Coins flowing out of a pay transaction to payeesThe outflow of coins is specified by an array of value-payee pairs,
exactly as in a create transaction.
The transaction clears all coins: the total inflow from payers is
equal to the total outflow to payees in a transaction.The system may provide incentives, such as payment of coins, to
managers (e.g. banks) of cryptocurrencies. In this case, one of the
payees is the bank itself.Managing amounts spent in a transactionA transaction-id, index pair --- such as (3146, 0)
identifies a 2-tuple such as (2.1, 7xxxx...); this 2-tuple asserts
that the agent
with public key 7xxxx received 2.1 units of coin in 
transaction 3146. Theentire amountspecified in the 2-tuple (2.1 in our
example) is value that flows into the transaction. What should this
agent do if it
wants to put in more than 2.1 coins into the transaction? Or less than
2.1 coins?To put in more value, the bank identifies other transaction-id, index
pairs in which this agent received coins. For example, say that (4539, 2)
identifies a 2-tuple (3.2, 7xxxx), and assume that the payers in this transaction
are specified by the pairs (3146, 0) and (4539, 2). The pair (3146, 0)
asserts that agent (7xxxx) received 2.1 coins and the pair (4539, 2)
asserts that the same agent received 3.2 coins. So the total amount of 
coins flowing into this
transaction from this agent (7xxxx) includes 2.1 + 3.2.To put in less value, the agent acts as both payer and payee; the net
value that this agent pays out to other agents is the difference
between the amount that this agent puts in and takes out. For example,
if agent with public key 7xxxx wants to put in 1.9 coins into this
transaction its payer information can be given by the transaction-id,
index pair (3146, 0) which asserts that the agent received 2.1 coins
and the same agent is a payee that withdraws 0.2 coins.Preventing Double SpendingHow does the system prevent an agent from using the
same coin twice?For example, the transaction id, index pair
(3146, 0) identifies the 2-tuple (2.1, 7xxxx...); this tuple
asserts that the agent
with public key 7xxxx received 2.1 units of coin in 
transaction 3146. Why can't the agent with public key 7xxxx use the
2.1 coins that it received to buy items from Amazon and later use
the same 2.1 coins to buy more items from Walmart?The bank checks that the agent hasn't already spent the coin that it
is putting into a transaction.Before permitting the transaction that double-spends the 2.1 coins
with Walmart, the bank inspects the tamper-evident ledger
for all transactions after transaction 3146 and before the Walmart
transaction to ensure that the agent (7xxxx) hasn't already spent the
2.1 coins that it got in transaction 3146. The transaction that spends
the coins at Amazon will show up in the ledger, and so the transaction
with Walmart will not be allowed.Every agent that has the bank's hash pointer to the end of the
tamper-evident ledger can inspect the ledger to check that
double-spending hasn't occurred.The bank signs a valid transaction and appends it to the
tamper-evident ledger. All agents can see the bank's signature and
verify that nobody (not even the bank) has tampered with the
tamper-evident ledger.Example of a pay transactionAn example of the specification of a pay transaction is:(9431, pay,
  [(3146, 0), (4731, 2)],
  [(0.7, 7xxxx...), (4.6, 9xxxx...)]
).The id of this transaction is 9431; the type of the transaction is
pay; the payers into the transaction are identified by the pairs of
(transaction-id, index): (3146, 0), and (4731, 2); and the
payee array is [(0.7, 7xxxx...), (4.6, 9xxxx...)].Transaction validityThe bank appends a transaction toLif and only if the
transaction is valid. The bank checks for validity by carrying out the
following steps:The bank verifies that the payers into the transaction signed the
  transaction.The bank checks that the total value of coins paid out from the
  transaction does not
  exceed the total value paid in to the transaction. (If the value paid
  in exceeds the value paid out then the bank takes the difference as
  a transaction fee. More about fees later.)The bank verifies that the payers' claims to have received coins in
  previous transactions is genuine. For example,
  if the agent with
  public key 7xxxx... claims to have received coins worth 2.1 in the
  transaction with id 10, and payee array index 0, then the bank
  verifies this claim by that transaction.The bank ensures that coins paid into the transaction haven't
  already been spent.Optimizations: Blocks and Block ChainVerifying large numbers of small transactions requires more
computation than verifying small numbers of blocks of many transactions.
Ablock chainis a tamper-evident ledger in which
each element of the ledger is a block consisting of many transactions.
A block of transactions can be aggregated into a single large
transaction by aggregating all the payers and payees of the smaller
transactions.The amount of computation decreases as the number of transactions in a
block increases. The time required to fill a block with transactions
is larger when the number of transactions to fill the block increases.Checking the Trusted AgentConsider a system in which the trusted agent broadcasts its current
copy of the tamper-evident ledger to all agents.
Every agent can inspect its copy of the tamper-evident ledger to determine whether the
ledger has been tampered with. So, every agent can validate its trust
in the trusted agent; however, this validation
suffers from a crucial problem: Agents may only
have copies ofold, staleversions of the ledger.
By the time that an agent receives a copy of the ledger, the trusted
agent may have added more transactions to the ledger.Fig.4: Old Copy is a Prefix of the Block ChainAn old copy of the ledger can differ from the current copy in only one
way: the current copy may have transactions appended to the end
of the old copy. So all agents can validatepastbehavior of the
trusted agent. An agent cannot, however, treat its copy of the ledger
as the master copy because the agent may not have the transactions
added most recently to the ledger.In the next module we will see how the Bitcoin algorithm addresses
this problem.Advantages of this cryptocurrencyAny agent can get a copy of the tamper-evident ledger and verify that all
transactions in the ledger are valid. Any agent can verify that
the only way in which the ledger is modified is that elements are
append to its tail; all that the agent needs to do is to check that
the pointer to the tail is modified only by appending
elements. Because the ledger is tamper-evident, an agent can check
that the ledger doesn't change while the hash pointer to the end
of the ledger doesn't change.The bank can't forge a transaction because all payers sign the
transaction. Agents can remain anonymous because an agent's only
public information is the agent's public key, and an agent can create
multiple public keys. Every agent can verify the correctness of every
transaction.Disadvantages of this cryptocurrencyUsers may not trust the bank. Transactions are not private because the
bank has a record of all transactions. And the bank is a single point
of failure.Next we'll look at Bitcoin's algorithms for
implementing a cryptocurrency without trusted agents.ReviewWhat iscollision resistance? Why is it relevant to
  cryptocurrencies?Consider the example ofhidingin which Megan hides her
  predictions for the winner of the World Cup. Suppose you could
  choose two different random number generators for creating the
  secret \(r\); in one case the bits of \(r\) are uncorrelated, and in
  the other they are highly correlated. Which generator would you use
  and why?What isbinding? Why is it relevant to hiding information?What ispuzzle friendly?What is atamper-evident data structure? How would you
  implement a tamper-evident linear list? A tamper-evident tree?Describe the algorithm for cryptocurrency with a trusted agent to
  someone who knows absolutely nothing about cryptocurrency.How could the algorithm for cryptocurrency be used for a collection
  of distributed agents to keep track of a sequence of events, other
  than buying/selling currency?K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology



Figure Link: https://kmchandy.github.io/Crypto/CryptoCurrencyIntroduction/Slide1.jpg

Caption: Fig.1: Hash Pointer points to a Tamper-Evident Block
    of Data


Single Block



Figure Link: https://kmchandy.github.io/Crypto/CryptoCurrencyIntroduction/Slide1.jpg

Caption: Fig.1: Hash Pointer points to a Tamper-Evident Block
    of Data


The \(j\)-th element of the list, \(j > 0\), consists of data,
\(D_{j}\), and a hash pointer that points to the \(j - 1\)-th
  element of the list. The hash pointer in the \(j\)-th element
consists of a regular pointer, \(ptr_{j}\), which points to the \(j - 1\)-th
  element of the list, and a hashed value, \(HA_{j}\), which is a cryptographic hash of theentire\(j-1\)th element consisting  of \(D_{j-1}\), \(ptr_{j-1}\)
  and \(HA_{j-1}\).
The \(0\)-th element is called thegenesiselement and has
default values. The list is accessed by a hash pointer that points to
the last element of the list; let this pointer be \(ptr_{n}, HA_{n}\).Assume that agents cannot modify \(ptr_{n}, HA_{n}\). Then,
can any agent determine whether the list has been tampered with?Suppose the agent modifies \(D_{j}\), \(ptr_{j}\) or
\(HA_{j}\) for any \(j < n\). Any agent can detect this tampering because the
hash value \(HA_{j+1}\) will no longer match the \(j\)-th element of
the list. If
the malicious agent also modifies \(HA_{j+1}\) then 
hash value \(HA_{j+2}\) will no longer match the \(j+1\)-th element.By induction on \(j\), any agent can detect tampering with the list
provided malicious agents do not also modify the hash pointer to the
last element of the last.Tamper-Evident Acyclic Graphs and Merkle
TreesThe idea described in the previous paragraph to convert linear linked
lists can be used to convert directed acyclic graphs, in which nodes are
connected by pointers, into tamper-evident graphs. A specific case of
a directed acyclic graph is a rooted tree.AMerkle treeis a special case of a binary balanced tree in which
data items are stored only in the leaves.
Nodes that are not leaves 
contain only hash pointers to nodes in the next level down.
To prove that an element at the leaf is a member of the tree we need only the
\(log_{2}(n)\) hash pointers on the path from the root to that
leaf. By contrast, to prove that an element is a member of a linear
list we need to inspect \(O(n)\) elements, on average.Keys and Signed MessagesYou can create a random public-key, private-key pairby calling a function on your computer.With high probability, nobody else has this
specific pair of keys. Each individual's private key is a secret held
by that individual. Public keys are accessible by everybody.Sending messages securelyKeys are used to send messages securely.
Kamala sends a secure message to Joe by encrypting the message with
Joe's public key; Joe decrypts the message using Joe's private key.
An agent cannot decrypt the encrypted message without Joe's private key.Signing messagesSuppose Kamala needs to send a signed message to Joe while
ensuring that nobody can forge her signature. She encrypts the messageMwith her private key to get an encrypted messageM',
and sends the pair(M, M')securely to Joe, i.e., she encrypts(M, M')with Joe's public key, and sends the resulting
encrypted message to Joe.  When Joe receives the message, Joe decrypts
it using his private key to get(M, M'). Then Joe decryptsM'using Kamala's public key to get the decrypted messageM''. IfM'' = Mthen Joe knows that Kamala sentMbecause only an agent with Kamala's private signature could
have sent that message.Cryptocurrency Managed by a Trusted
AgentLet's start with a digital currency managed by a trusted agent that we will
call a bank. Later, we will
look at a consensus algorithm --- very different from Paxos and Byzantine
Generals --- which will allow cryptocurrencies without trusted
agent.The bank maintains a tamper-evident linear listLof transactions that we call a
tamper-evidentledger. 
Any agent can get a copy of the ledger.
This tamper-evident ledger is the foundation of the currency.A transaction is one of
two types:createorpay.
In a pay transaction,payersgive coins that they possess topayees. An agent can be both a 
payer and payee of the same transaction.
In acreatetransaction the bank creates coins that it gives to
agents --- the payees of the transaction; the bank acts as the payer.For this system to be trusted the bank must follow some protocol that determines
how and when the bank creates coins. We won't discuss these protocols.Apaytransaction is signed by all payers of the transaction.
Acreatetransaction is signed by the bank.
We discussed digital signatures and keys earlier.Each element of the tamper-evident ledger has:a unique id;the type of the transaction, either create or pay;list of payers: only for pay transactions --- a list
  indicating the agents who pay coins into the transaction and the
  amounts that they put in;array of value-payee pairs: for both create and pay transactions --- an
  array of pairs(value, payee public key), where each
  pair in the array indicates that coins of the specified value are
  given to the payee with the specified public key.Example of a create transactionAn example of acreatetransaction is:(3146, create,  [(2.1, 7xxxx...), (3.2, 8xxxx)]).The id of this transaction is 3146, the type of the transaction is
create, and the array of value-payee pairs is[(2.1, 7xxxx...), (3.2, 8xxx)]In this transaction the bank creates a coin of value 2.1 and gives it
to the agent with public key 7xxxx..., and the bank also
creates a coin of value 3.2 and gives it
to the agent with public key 8xxxx...The pair:(transaction id, index into array of value-payee pairs)uniquely identifies a (value, payee) tuple.For example (3146, 0) --- 
transaction id 3146, and array index 0 --- identifies value-payee[0] of
transaction 3146 which is specified by the 2-tuple: (2.1, 7xxxx...). So, the
transaction id and index, (3146, 0), tells everybody that the agent
with public key 7xxxx received 2.1 units of coin in transaction 3146.When this
transaction is in a tamper-evident ledger, every agent from that point
onwards knows that agent 7xxxx received 2.1 coins. Any modifications of this
record can be detected.Likewise, (3146, 1) --- 
transaction id 3146, and array index 1 --- identifies value-payee[1] which
is the 2-tuple (3.2, 8xxxx...).Pay transactionCoins are transferred from payers to payees in a pay transaction.Coins flowing into a pay transaction from payersThe payers are identified by a list of 2-tuples, where each 2-tuple is(transaction id, index into array of value-payee pairs)wheretransaction idis the id of the transaction in the
tamper-evident ledger. As we said earlier, this pair uniquely identifies an agent and a
value that this agent acquired in this transaction.
For example the pair --- transaction id, index --- such as (3146, 0)
identifies the 2-tuple (2.1, 7xxxx...); this 2-tuple asserts
that the agent
with public key 7xxxx received 2.1 units of coin in 
transaction 3146. Theentire amountspecified in the 2-tuple (2.1 in our
example) is value that flows into thispaytransaction from the agent
with public key 7xxxx.Suppose the payers in a pay transaction are identified by the list:[(3146, 0), (7359, 3)]and suppose pair 3 in transaction 7359 is (3.2, 8xxxx). Then the total
amount of coins flowing into this pay transaction is 2.1 + 3.2, and
this amount is disbursed to payees.Fig.3: Coins flowing into and out of a pay transaction.Coins flowing out of a pay transaction to payeesThe outflow of coins is specified by an array of value-payee pairs,
exactly as in a create transaction.
The transaction clears all coins: the total inflow from payers is
equal to the total outflow to payees in a transaction.The system may provide incentives, such as payment of coins, to
managers (e.g. banks) of cryptocurrencies. In this case, one of the
payees is the bank itself.Managing amounts spent in a transactionA transaction-id, index pair --- such as (3146, 0)
identifies a 2-tuple such as (2.1, 7xxxx...); this 2-tuple asserts
that the agent
with public key 7xxxx received 2.1 units of coin in 
transaction 3146. Theentire amountspecified in the 2-tuple (2.1 in our
example) is value that flows into the transaction. What should this
agent do if it
wants to put in more than 2.1 coins into the transaction? Or less than
2.1 coins?To put in more value, the bank identifies other transaction-id, index
pairs in which this agent received coins. For example, say that (4539, 2)
identifies a 2-tuple (3.2, 7xxxx), and assume that the payers in this transaction
are specified by the pairs (3146, 0) and (4539, 2). The pair (3146, 0)
asserts that agent (7xxxx) received 2.1 coins and the pair (4539, 2)
asserts that the same agent received 3.2 coins. So the total amount of 
coins flowing into this
transaction from this agent (7xxxx) includes 2.1 + 3.2.To put in less value, the agent acts as both payer and payee; the net
value that this agent pays out to other agents is the difference
between the amount that this agent puts in and takes out. For example,
if agent with public key 7xxxx wants to put in 1.9 coins into this
transaction its payer information can be given by the transaction-id,
index pair (3146, 0) which asserts that the agent received 2.1 coins
and the same agent is a payee that withdraws 0.2 coins.Preventing Double SpendingHow does the system prevent an agent from using the
same coin twice?For example, the transaction id, index pair
(3146, 0) identifies the 2-tuple (2.1, 7xxxx...); this tuple
asserts that the agent
with public key 7xxxx received 2.1 units of coin in 
transaction 3146. Why can't the agent with public key 7xxxx use the
2.1 coins that it received to buy items from Amazon and later use
the same 2.1 coins to buy more items from Walmart?The bank checks that the agent hasn't already spent the coin that it
is putting into a transaction.Before permitting the transaction that double-spends the 2.1 coins
with Walmart, the bank inspects the tamper-evident ledger
for all transactions after transaction 3146 and before the Walmart
transaction to ensure that the agent (7xxxx) hasn't already spent the
2.1 coins that it got in transaction 3146. The transaction that spends
the coins at Amazon will show up in the ledger, and so the transaction
with Walmart will not be allowed.Every agent that has the bank's hash pointer to the end of the
tamper-evident ledger can inspect the ledger to check that
double-spending hasn't occurred.The bank signs a valid transaction and appends it to the
tamper-evident ledger. All agents can see the bank's signature and
verify that nobody (not even the bank) has tampered with the
tamper-evident ledger.Example of a pay transactionAn example of the specification of a pay transaction is:(9431, pay,
  [(3146, 0), (4731, 2)],
  [(0.7, 7xxxx...), (4.6, 9xxxx...)]
).The id of this transaction is 9431; the type of the transaction is
pay; the payers into the transaction are identified by the pairs of
(transaction-id, index): (3146, 0), and (4731, 2); and the
payee array is [(0.7, 7xxxx...), (4.6, 9xxxx...)].Transaction validityThe bank appends a transaction toLif and only if the
transaction is valid. The bank checks for validity by carrying out the
following steps:The bank verifies that the payers into the transaction signed the
  transaction.The bank checks that the total value of coins paid out from the
  transaction does not
  exceed the total value paid in to the transaction. (If the value paid
  in exceeds the value paid out then the bank takes the difference as
  a transaction fee. More about fees later.)The bank verifies that the payers' claims to have received coins in
  previous transactions is genuine. For example,
  if the agent with
  public key 7xxxx... claims to have received coins worth 2.1 in the
  transaction with id 10, and payee array index 0, then the bank
  verifies this claim by that transaction.The bank ensures that coins paid into the transaction haven't
  already been spent.Optimizations: Blocks and Block ChainVerifying large numbers of small transactions requires more
computation than verifying small numbers of blocks of many transactions.
Ablock chainis a tamper-evident ledger in which
each element of the ledger is a block consisting of many transactions.
A block of transactions can be aggregated into a single large
transaction by aggregating all the payers and payees of the smaller
transactions.The amount of computation decreases as the number of transactions in a
block increases. The time required to fill a block with transactions
is larger when the number of transactions to fill the block increases.Checking the Trusted AgentConsider a system in which the trusted agent broadcasts its current
copy of the tamper-evident ledger to all agents.
Every agent can inspect its copy of the tamper-evident ledger to determine whether the
ledger has been tampered with. So, every agent can validate its trust
in the trusted agent; however, this validation
suffers from a crucial problem: Agents may only
have copies ofold, staleversions of the ledger.
By the time that an agent receives a copy of the ledger, the trusted
agent may have added more transactions to the ledger.Fig.4: Old Copy is a Prefix of the Block ChainAn old copy of the ledger can differ from the current copy in only one
way: the current copy may have transactions appended to the end
of the old copy. So all agents can validatepastbehavior of the
trusted agent. An agent cannot, however, treat its copy of the ledger
as the master copy because the agent may not have the transactions
added most recently to the ledger.In the next module we will see how the Bitcoin algorithm addresses
this problem.Advantages of this cryptocurrencyAny agent can get a copy of the tamper-evident ledger and verify that all
transactions in the ledger are valid. Any agent can verify that
the only way in which the ledger is modified is that elements are
append to its tail; all that the agent needs to do is to check that
the pointer to the tail is modified only by appending
elements. Because the ledger is tamper-evident, an agent can check
that the ledger doesn't change while the hash pointer to the end
of the ledger doesn't change.The bank can't forge a transaction because all payers sign the
transaction. Agents can remain anonymous because an agent's only
public information is the agent's public key, and an agent can create
multiple public keys. Every agent can verify the correctness of every
transaction.Disadvantages of this cryptocurrencyUsers may not trust the bank. Transactions are not private because the
bank has a record of all transactions. And the bank is a single point
of failure.Next we'll look at Bitcoin's algorithms for
implementing a cryptocurrency without trusted agents.ReviewWhat iscollision resistance? Why is it relevant to
  cryptocurrencies?Consider the example ofhidingin which Megan hides her
  predictions for the winner of the World Cup. Suppose you could
  choose two different random number generators for creating the
  secret \(r\); in one case the bits of \(r\) are uncorrelated, and in
  the other they are highly correlated. Which generator would you use
  and why?What isbinding? Why is it relevant to hiding information?What ispuzzle friendly?What is atamper-evident data structure? How would you
  implement a tamper-evident linear list? A tamper-evident tree?Describe the algorithm for cryptocurrency with a trusted agent to
  someone who knows absolutely nothing about cryptocurrency.How could the algorithm for cryptocurrency be used for a collection
  of distributed agents to keep track of a sequence of events, other
  than buying/selling currency?K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology



Figure Link: https://kmchandy.github.io/Crypto/CryptoCurrencyIntroduction/Slide3.jpg

Caption: Fig.3: Coins flowing into and out of a pay transaction.


Assume that agents cannot modify \(ptr_{n}, HA_{n}\). Then,
can any agent determine whether the list has been tampered with?Suppose the agent modifies \(D_{j}\), \(ptr_{j}\) or
\(HA_{j}\) for any \(j < n\). Any agent can detect this tampering because the
hash value \(HA_{j+1}\) will no longer match the \(j\)-th element of
the list. If
the malicious agent also modifies \(HA_{j+1}\) then 
hash value \(HA_{j+2}\) will no longer match the \(j+1\)-th element.By induction on \(j\), any agent can detect tampering with the list
provided malicious agents do not also modify the hash pointer to the
last element of the last.Tamper-Evident Acyclic Graphs and Merkle
TreesThe idea described in the previous paragraph to convert linear linked
lists can be used to convert directed acyclic graphs, in which nodes are
connected by pointers, into tamper-evident graphs. A specific case of
a directed acyclic graph is a rooted tree.AMerkle treeis a special case of a binary balanced tree in which
data items are stored only in the leaves.
Nodes that are not leaves 
contain only hash pointers to nodes in the next level down.
To prove that an element at the leaf is a member of the tree we need only the
\(log_{2}(n)\) hash pointers on the path from the root to that
leaf. By contrast, to prove that an element is a member of a linear
list we need to inspect \(O(n)\) elements, on average.Keys and Signed MessagesYou can create a random public-key, private-key pairby calling a function on your computer.With high probability, nobody else has this
specific pair of keys. Each individual's private key is a secret held
by that individual. Public keys are accessible by everybody.Sending messages securelyKeys are used to send messages securely.
Kamala sends a secure message to Joe by encrypting the message with
Joe's public key; Joe decrypts the message using Joe's private key.
An agent cannot decrypt the encrypted message without Joe's private key.Signing messagesSuppose Kamala needs to send a signed message to Joe while
ensuring that nobody can forge her signature. She encrypts the messageMwith her private key to get an encrypted messageM',
and sends the pair(M, M')securely to Joe, i.e., she encrypts(M, M')with Joe's public key, and sends the resulting
encrypted message to Joe.  When Joe receives the message, Joe decrypts
it using his private key to get(M, M'). Then Joe decryptsM'using Kamala's public key to get the decrypted messageM''. IfM'' = Mthen Joe knows that Kamala sentMbecause only an agent with Kamala's private signature could
have sent that message.Cryptocurrency Managed by a Trusted
AgentLet's start with a digital currency managed by a trusted agent that we will
call a bank. Later, we will
look at a consensus algorithm --- very different from Paxos and Byzantine
Generals --- which will allow cryptocurrencies without trusted
agent.The bank maintains a tamper-evident linear listLof transactions that we call a
tamper-evidentledger. 
Any agent can get a copy of the ledger.
This tamper-evident ledger is the foundation of the currency.A transaction is one of
two types:createorpay.
In a pay transaction,payersgive coins that they possess topayees. An agent can be both a 
payer and payee of the same transaction.
In acreatetransaction the bank creates coins that it gives to
agents --- the payees of the transaction; the bank acts as the payer.For this system to be trusted the bank must follow some protocol that determines
how and when the bank creates coins. We won't discuss these protocols.Apaytransaction is signed by all payers of the transaction.
Acreatetransaction is signed by the bank.
We discussed digital signatures and keys earlier.Each element of the tamper-evident ledger has:a unique id;the type of the transaction, either create or pay;list of payers: only for pay transactions --- a list
  indicating the agents who pay coins into the transaction and the
  amounts that they put in;array of value-payee pairs: for both create and pay transactions --- an
  array of pairs(value, payee public key), where each
  pair in the array indicates that coins of the specified value are
  given to the payee with the specified public key.Example of a create transactionAn example of acreatetransaction is:(3146, create,  [(2.1, 7xxxx...), (3.2, 8xxxx)]).The id of this transaction is 3146, the type of the transaction is
create, and the array of value-payee pairs is[(2.1, 7xxxx...), (3.2, 8xxx)]In this transaction the bank creates a coin of value 2.1 and gives it
to the agent with public key 7xxxx..., and the bank also
creates a coin of value 3.2 and gives it
to the agent with public key 8xxxx...The pair:(transaction id, index into array of value-payee pairs)uniquely identifies a (value, payee) tuple.For example (3146, 0) --- 
transaction id 3146, and array index 0 --- identifies value-payee[0] of
transaction 3146 which is specified by the 2-tuple: (2.1, 7xxxx...). So, the
transaction id and index, (3146, 0), tells everybody that the agent
with public key 7xxxx received 2.1 units of coin in transaction 3146.When this
transaction is in a tamper-evident ledger, every agent from that point
onwards knows that agent 7xxxx received 2.1 coins. Any modifications of this
record can be detected.Likewise, (3146, 1) --- 
transaction id 3146, and array index 1 --- identifies value-payee[1] which
is the 2-tuple (3.2, 8xxxx...).Pay transactionCoins are transferred from payers to payees in a pay transaction.Coins flowing into a pay transaction from payersThe payers are identified by a list of 2-tuples, where each 2-tuple is(transaction id, index into array of value-payee pairs)wheretransaction idis the id of the transaction in the
tamper-evident ledger. As we said earlier, this pair uniquely identifies an agent and a
value that this agent acquired in this transaction.
For example the pair --- transaction id, index --- such as (3146, 0)
identifies the 2-tuple (2.1, 7xxxx...); this 2-tuple asserts
that the agent
with public key 7xxxx received 2.1 units of coin in 
transaction 3146. Theentire amountspecified in the 2-tuple (2.1 in our
example) is value that flows into thispaytransaction from the agent
with public key 7xxxx.Suppose the payers in a pay transaction are identified by the list:[(3146, 0), (7359, 3)]and suppose pair 3 in transaction 7359 is (3.2, 8xxxx). Then the total
amount of coins flowing into this pay transaction is 2.1 + 3.2, and
this amount is disbursed to payees.Fig.3: Coins flowing into and out of a pay transaction.Coins flowing out of a pay transaction to payeesThe outflow of coins is specified by an array of value-payee pairs,
exactly as in a create transaction.
The transaction clears all coins: the total inflow from payers is
equal to the total outflow to payees in a transaction.The system may provide incentives, such as payment of coins, to
managers (e.g. banks) of cryptocurrencies. In this case, one of the
payees is the bank itself.Managing amounts spent in a transactionA transaction-id, index pair --- such as (3146, 0)
identifies a 2-tuple such as (2.1, 7xxxx...); this 2-tuple asserts
that the agent
with public key 7xxxx received 2.1 units of coin in 
transaction 3146. Theentire amountspecified in the 2-tuple (2.1 in our
example) is value that flows into the transaction. What should this
agent do if it
wants to put in more than 2.1 coins into the transaction? Or less than
2.1 coins?To put in more value, the bank identifies other transaction-id, index
pairs in which this agent received coins. For example, say that (4539, 2)
identifies a 2-tuple (3.2, 7xxxx), and assume that the payers in this transaction
are specified by the pairs (3146, 0) and (4539, 2). The pair (3146, 0)
asserts that agent (7xxxx) received 2.1 coins and the pair (4539, 2)
asserts that the same agent received 3.2 coins. So the total amount of 
coins flowing into this
transaction from this agent (7xxxx) includes 2.1 + 3.2.To put in less value, the agent acts as both payer and payee; the net
value that this agent pays out to other agents is the difference
between the amount that this agent puts in and takes out. For example,
if agent with public key 7xxxx wants to put in 1.9 coins into this
transaction its payer information can be given by the transaction-id,
index pair (3146, 0) which asserts that the agent received 2.1 coins
and the same agent is a payee that withdraws 0.2 coins.Preventing Double SpendingHow does the system prevent an agent from using the
same coin twice?For example, the transaction id, index pair
(3146, 0) identifies the 2-tuple (2.1, 7xxxx...); this tuple
asserts that the agent
with public key 7xxxx received 2.1 units of coin in 
transaction 3146. Why can't the agent with public key 7xxxx use the
2.1 coins that it received to buy items from Amazon and later use
the same 2.1 coins to buy more items from Walmart?The bank checks that the agent hasn't already spent the coin that it
is putting into a transaction.Before permitting the transaction that double-spends the 2.1 coins
with Walmart, the bank inspects the tamper-evident ledger
for all transactions after transaction 3146 and before the Walmart
transaction to ensure that the agent (7xxxx) hasn't already spent the
2.1 coins that it got in transaction 3146. The transaction that spends
the coins at Amazon will show up in the ledger, and so the transaction
with Walmart will not be allowed.Every agent that has the bank's hash pointer to the end of the
tamper-evident ledger can inspect the ledger to check that
double-spending hasn't occurred.The bank signs a valid transaction and appends it to the
tamper-evident ledger. All agents can see the bank's signature and
verify that nobody (not even the bank) has tampered with the
tamper-evident ledger.Example of a pay transactionAn example of the specification of a pay transaction is:(9431, pay,
  [(3146, 0), (4731, 2)],
  [(0.7, 7xxxx...), (4.6, 9xxxx...)]
).The id of this transaction is 9431; the type of the transaction is
pay; the payers into the transaction are identified by the pairs of
(transaction-id, index): (3146, 0), and (4731, 2); and the
payee array is [(0.7, 7xxxx...), (4.6, 9xxxx...)].Transaction validityThe bank appends a transaction toLif and only if the
transaction is valid. The bank checks for validity by carrying out the
following steps:The bank verifies that the payers into the transaction signed the
  transaction.The bank checks that the total value of coins paid out from the
  transaction does not
  exceed the total value paid in to the transaction. (If the value paid
  in exceeds the value paid out then the bank takes the difference as
  a transaction fee. More about fees later.)The bank verifies that the payers' claims to have received coins in
  previous transactions is genuine. For example,
  if the agent with
  public key 7xxxx... claims to have received coins worth 2.1 in the
  transaction with id 10, and payee array index 0, then the bank
  verifies this claim by that transaction.The bank ensures that coins paid into the transaction haven't
  already been spent.Optimizations: Blocks and Block ChainVerifying large numbers of small transactions requires more
computation than verifying small numbers of blocks of many transactions.
Ablock chainis a tamper-evident ledger in which
each element of the ledger is a block consisting of many transactions.
A block of transactions can be aggregated into a single large
transaction by aggregating all the payers and payees of the smaller
transactions.The amount of computation decreases as the number of transactions in a
block increases. The time required to fill a block with transactions
is larger when the number of transactions to fill the block increases.Checking the Trusted AgentConsider a system in which the trusted agent broadcasts its current
copy of the tamper-evident ledger to all agents.
Every agent can inspect its copy of the tamper-evident ledger to determine whether the
ledger has been tampered with. So, every agent can validate its trust
in the trusted agent; however, this validation
suffers from a crucial problem: Agents may only
have copies ofold, staleversions of the ledger.
By the time that an agent receives a copy of the ledger, the trusted
agent may have added more transactions to the ledger.Fig.4: Old Copy is a Prefix of the Block ChainAn old copy of the ledger can differ from the current copy in only one
way: the current copy may have transactions appended to the end
of the old copy. So all agents can validatepastbehavior of the
trusted agent. An agent cannot, however, treat its copy of the ledger
as the master copy because the agent may not have the transactions
added most recently to the ledger.In the next module we will see how the Bitcoin algorithm addresses
this problem.Advantages of this cryptocurrencyAny agent can get a copy of the tamper-evident ledger and verify that all
transactions in the ledger are valid. Any agent can verify that
the only way in which the ledger is modified is that elements are
append to its tail; all that the agent needs to do is to check that
the pointer to the tail is modified only by appending
elements. Because the ledger is tamper-evident, an agent can check
that the ledger doesn't change while the hash pointer to the end
of the ledger doesn't change.The bank can't forge a transaction because all payers sign the
transaction. Agents can remain anonymous because an agent's only
public information is the agent's public key, and an agent can create
multiple public keys. Every agent can verify the correctness of every
transaction.Disadvantages of this cryptocurrencyUsers may not trust the bank. Transactions are not private because the
bank has a record of all transactions. And the bank is a single point
of failure.Next we'll look at Bitcoin's algorithms for
implementing a cryptocurrency without trusted agents.ReviewWhat iscollision resistance? Why is it relevant to
  cryptocurrencies?Consider the example ofhidingin which Megan hides her
  predictions for the winner of the World Cup. Suppose you could
  choose two different random number generators for creating the
  secret \(r\); in one case the bits of \(r\) are uncorrelated, and in
  the other they are highly correlated. Which generator would you use
  and why?What isbinding? Why is it relevant to hiding information?What ispuzzle friendly?What is atamper-evident data structure? How would you
  implement a tamper-evident linear list? A tamper-evident tree?Describe the algorithm for cryptocurrency with a trusted agent to
  someone who knows absolutely nothing about cryptocurrency.How could the algorithm for cryptocurrency be used for a collection
  of distributed agents to keep track of a sequence of events, other
  than buying/selling currency?K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology



Figure Link: https://kmchandy.github.io/Crypto/CryptoCurrencyIntroduction/Slide3.jpg

Caption: Fig.3: Coins flowing into and out of a pay transaction.


Suppose the agent modifies \(D_{j}\), \(ptr_{j}\) or
\(HA_{j}\) for any \(j < n\). Any agent can detect this tampering because the
hash value \(HA_{j+1}\) will no longer match the \(j\)-th element of
the list. If
the malicious agent also modifies \(HA_{j+1}\) then 
hash value \(HA_{j+2}\) will no longer match the \(j+1\)-th element.By induction on \(j\), any agent can detect tampering with the list
provided malicious agents do not also modify the hash pointer to the
last element of the last.Tamper-Evident Acyclic Graphs and Merkle
TreesThe idea described in the previous paragraph to convert linear linked
lists can be used to convert directed acyclic graphs, in which nodes are
connected by pointers, into tamper-evident graphs. A specific case of
a directed acyclic graph is a rooted tree.AMerkle treeis a special case of a binary balanced tree in which
data items are stored only in the leaves.
Nodes that are not leaves 
contain only hash pointers to nodes in the next level down.
To prove that an element at the leaf is a member of the tree we need only the
\(log_{2}(n)\) hash pointers on the path from the root to that
leaf. By contrast, to prove that an element is a member of a linear
list we need to inspect \(O(n)\) elements, on average.Keys and Signed MessagesYou can create a random public-key, private-key pairby calling a function on your computer.With high probability, nobody else has this
specific pair of keys. Each individual's private key is a secret held
by that individual. Public keys are accessible by everybody.Sending messages securelyKeys are used to send messages securely.
Kamala sends a secure message to Joe by encrypting the message with
Joe's public key; Joe decrypts the message using Joe's private key.
An agent cannot decrypt the encrypted message without Joe's private key.Signing messagesSuppose Kamala needs to send a signed message to Joe while
ensuring that nobody can forge her signature. She encrypts the messageMwith her private key to get an encrypted messageM',
and sends the pair(M, M')securely to Joe, i.e., she encrypts(M, M')with Joe's public key, and sends the resulting
encrypted message to Joe.  When Joe receives the message, Joe decrypts
it using his private key to get(M, M'). Then Joe decryptsM'using Kamala's public key to get the decrypted messageM''. IfM'' = Mthen Joe knows that Kamala sentMbecause only an agent with Kamala's private signature could
have sent that message.Cryptocurrency Managed by a Trusted
AgentLet's start with a digital currency managed by a trusted agent that we will
call a bank. Later, we will
look at a consensus algorithm --- very different from Paxos and Byzantine
Generals --- which will allow cryptocurrencies without trusted
agent.The bank maintains a tamper-evident linear listLof transactions that we call a
tamper-evidentledger. 
Any agent can get a copy of the ledger.
This tamper-evident ledger is the foundation of the currency.A transaction is one of
two types:createorpay.
In a pay transaction,payersgive coins that they possess topayees. An agent can be both a 
payer and payee of the same transaction.
In acreatetransaction the bank creates coins that it gives to
agents --- the payees of the transaction; the bank acts as the payer.For this system to be trusted the bank must follow some protocol that determines
how and when the bank creates coins. We won't discuss these protocols.Apaytransaction is signed by all payers of the transaction.
Acreatetransaction is signed by the bank.
We discussed digital signatures and keys earlier.Each element of the tamper-evident ledger has:a unique id;the type of the transaction, either create or pay;list of payers: only for pay transactions --- a list
  indicating the agents who pay coins into the transaction and the
  amounts that they put in;array of value-payee pairs: for both create and pay transactions --- an
  array of pairs(value, payee public key), where each
  pair in the array indicates that coins of the specified value are
  given to the payee with the specified public key.Example of a create transactionAn example of acreatetransaction is:(3146, create,  [(2.1, 7xxxx...), (3.2, 8xxxx)]).The id of this transaction is 3146, the type of the transaction is
create, and the array of value-payee pairs is[(2.1, 7xxxx...), (3.2, 8xxx)]In this transaction the bank creates a coin of value 2.1 and gives it
to the agent with public key 7xxxx..., and the bank also
creates a coin of value 3.2 and gives it
to the agent with public key 8xxxx...The pair:(transaction id, index into array of value-payee pairs)uniquely identifies a (value, payee) tuple.For example (3146, 0) --- 
transaction id 3146, and array index 0 --- identifies value-payee[0] of
transaction 3146 which is specified by the 2-tuple: (2.1, 7xxxx...). So, the
transaction id and index, (3146, 0), tells everybody that the agent
with public key 7xxxx received 2.1 units of coin in transaction 3146.When this
transaction is in a tamper-evident ledger, every agent from that point
onwards knows that agent 7xxxx received 2.1 coins. Any modifications of this
record can be detected.Likewise, (3146, 1) --- 
transaction id 3146, and array index 1 --- identifies value-payee[1] which
is the 2-tuple (3.2, 8xxxx...).Pay transactionCoins are transferred from payers to payees in a pay transaction.Coins flowing into a pay transaction from payersThe payers are identified by a list of 2-tuples, where each 2-tuple is(transaction id, index into array of value-payee pairs)wheretransaction idis the id of the transaction in the
tamper-evident ledger. As we said earlier, this pair uniquely identifies an agent and a
value that this agent acquired in this transaction.
For example the pair --- transaction id, index --- such as (3146, 0)
identifies the 2-tuple (2.1, 7xxxx...); this 2-tuple asserts
that the agent
with public key 7xxxx received 2.1 units of coin in 
transaction 3146. Theentire amountspecified in the 2-tuple (2.1 in our
example) is value that flows into thispaytransaction from the agent
with public key 7xxxx.Suppose the payers in a pay transaction are identified by the list:[(3146, 0), (7359, 3)]and suppose pair 3 in transaction 7359 is (3.2, 8xxxx). Then the total
amount of coins flowing into this pay transaction is 2.1 + 3.2, and
this amount is disbursed to payees.Fig.3: Coins flowing into and out of a pay transaction.Coins flowing out of a pay transaction to payeesThe outflow of coins is specified by an array of value-payee pairs,
exactly as in a create transaction.
The transaction clears all coins: the total inflow from payers is
equal to the total outflow to payees in a transaction.The system may provide incentives, such as payment of coins, to
managers (e.g. banks) of cryptocurrencies. In this case, one of the
payees is the bank itself.Managing amounts spent in a transactionA transaction-id, index pair --- such as (3146, 0)
identifies a 2-tuple such as (2.1, 7xxxx...); this 2-tuple asserts
that the agent
with public key 7xxxx received 2.1 units of coin in 
transaction 3146. Theentire amountspecified in the 2-tuple (2.1 in our
example) is value that flows into the transaction. What should this
agent do if it
wants to put in more than 2.1 coins into the transaction? Or less than
2.1 coins?To put in more value, the bank identifies other transaction-id, index
pairs in which this agent received coins. For example, say that (4539, 2)
identifies a 2-tuple (3.2, 7xxxx), and assume that the payers in this transaction
are specified by the pairs (3146, 0) and (4539, 2). The pair (3146, 0)
asserts that agent (7xxxx) received 2.1 coins and the pair (4539, 2)
asserts that the same agent received 3.2 coins. So the total amount of 
coins flowing into this
transaction from this agent (7xxxx) includes 2.1 + 3.2.To put in less value, the agent acts as both payer and payee; the net
value that this agent pays out to other agents is the difference
between the amount that this agent puts in and takes out. For example,
if agent with public key 7xxxx wants to put in 1.9 coins into this
transaction its payer information can be given by the transaction-id,
index pair (3146, 0) which asserts that the agent received 2.1 coins
and the same agent is a payee that withdraws 0.2 coins.Preventing Double SpendingHow does the system prevent an agent from using the
same coin twice?For example, the transaction id, index pair
(3146, 0) identifies the 2-tuple (2.1, 7xxxx...); this tuple
asserts that the agent
with public key 7xxxx received 2.1 units of coin in 
transaction 3146. Why can't the agent with public key 7xxxx use the
2.1 coins that it received to buy items from Amazon and later use
the same 2.1 coins to buy more items from Walmart?The bank checks that the agent hasn't already spent the coin that it
is putting into a transaction.Before permitting the transaction that double-spends the 2.1 coins
with Walmart, the bank inspects the tamper-evident ledger
for all transactions after transaction 3146 and before the Walmart
transaction to ensure that the agent (7xxxx) hasn't already spent the
2.1 coins that it got in transaction 3146. The transaction that spends
the coins at Amazon will show up in the ledger, and so the transaction
with Walmart will not be allowed.Every agent that has the bank's hash pointer to the end of the
tamper-evident ledger can inspect the ledger to check that
double-spending hasn't occurred.The bank signs a valid transaction and appends it to the
tamper-evident ledger. All agents can see the bank's signature and
verify that nobody (not even the bank) has tampered with the
tamper-evident ledger.Example of a pay transactionAn example of the specification of a pay transaction is:(9431, pay,
  [(3146, 0), (4731, 2)],
  [(0.7, 7xxxx...), (4.6, 9xxxx...)]
).The id of this transaction is 9431; the type of the transaction is
pay; the payers into the transaction are identified by the pairs of
(transaction-id, index): (3146, 0), and (4731, 2); and the
payee array is [(0.7, 7xxxx...), (4.6, 9xxxx...)].Transaction validityThe bank appends a transaction toLif and only if the
transaction is valid. The bank checks for validity by carrying out the
following steps:The bank verifies that the payers into the transaction signed the
  transaction.The bank checks that the total value of coins paid out from the
  transaction does not
  exceed the total value paid in to the transaction. (If the value paid
  in exceeds the value paid out then the bank takes the difference as
  a transaction fee. More about fees later.)The bank verifies that the payers' claims to have received coins in
  previous transactions is genuine. For example,
  if the agent with
  public key 7xxxx... claims to have received coins worth 2.1 in the
  transaction with id 10, and payee array index 0, then the bank
  verifies this claim by that transaction.The bank ensures that coins paid into the transaction haven't
  already been spent.Optimizations: Blocks and Block ChainVerifying large numbers of small transactions requires more
computation than verifying small numbers of blocks of many transactions.
Ablock chainis a tamper-evident ledger in which
each element of the ledger is a block consisting of many transactions.
A block of transactions can be aggregated into a single large
transaction by aggregating all the payers and payees of the smaller
transactions.The amount of computation decreases as the number of transactions in a
block increases. The time required to fill a block with transactions
is larger when the number of transactions to fill the block increases.Checking the Trusted AgentConsider a system in which the trusted agent broadcasts its current
copy of the tamper-evident ledger to all agents.
Every agent can inspect its copy of the tamper-evident ledger to determine whether the
ledger has been tampered with. So, every agent can validate its trust
in the trusted agent; however, this validation
suffers from a crucial problem: Agents may only
have copies ofold, staleversions of the ledger.
By the time that an agent receives a copy of the ledger, the trusted
agent may have added more transactions to the ledger.Fig.4: Old Copy is a Prefix of the Block ChainAn old copy of the ledger can differ from the current copy in only one
way: the current copy may have transactions appended to the end
of the old copy. So all agents can validatepastbehavior of the
trusted agent. An agent cannot, however, treat its copy of the ledger
as the master copy because the agent may not have the transactions
added most recently to the ledger.In the next module we will see how the Bitcoin algorithm addresses
this problem.Advantages of this cryptocurrencyAny agent can get a copy of the tamper-evident ledger and verify that all
transactions in the ledger are valid. Any agent can verify that
the only way in which the ledger is modified is that elements are
append to its tail; all that the agent needs to do is to check that
the pointer to the tail is modified only by appending
elements. Because the ledger is tamper-evident, an agent can check
that the ledger doesn't change while the hash pointer to the end
of the ledger doesn't change.The bank can't forge a transaction because all payers sign the
transaction. Agents can remain anonymous because an agent's only
public information is the agent's public key, and an agent can create
multiple public keys. Every agent can verify the correctness of every
transaction.Disadvantages of this cryptocurrencyUsers may not trust the bank. Transactions are not private because the
bank has a record of all transactions. And the bank is a single point
of failure.Next we'll look at Bitcoin's algorithms for
implementing a cryptocurrency without trusted agents.ReviewWhat iscollision resistance? Why is it relevant to
  cryptocurrencies?Consider the example ofhidingin which Megan hides her
  predictions for the winner of the World Cup. Suppose you could
  choose two different random number generators for creating the
  secret \(r\); in one case the bits of \(r\) are uncorrelated, and in
  the other they are highly correlated. Which generator would you use
  and why?What isbinding? Why is it relevant to hiding information?What ispuzzle friendly?What is atamper-evident data structure? How would you
  implement a tamper-evident linear list? A tamper-evident tree?Describe the algorithm for cryptocurrency with a trusted agent to
  someone who knows absolutely nothing about cryptocurrency.How could the algorithm for cryptocurrency be used for a collection
  of distributed agents to keep track of a sequence of events, other
  than buying/selling currency?K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology



Figure Link: https://kmchandy.github.io/Crypto/CryptoCurrencyIntroduction/Slide3.jpg

Caption: Fig.3: Coins flowing into and out of a pay transaction.


By induction on \(j\), any agent can detect tampering with the list
provided malicious agents do not also modify the hash pointer to the
last element of the last.Tamper-Evident Acyclic Graphs and Merkle
TreesThe idea described in the previous paragraph to convert linear linked
lists can be used to convert directed acyclic graphs, in which nodes are
connected by pointers, into tamper-evident graphs. A specific case of
a directed acyclic graph is a rooted tree.AMerkle treeis a special case of a binary balanced tree in which
data items are stored only in the leaves.
Nodes that are not leaves 
contain only hash pointers to nodes in the next level down.
To prove that an element at the leaf is a member of the tree we need only the
\(log_{2}(n)\) hash pointers on the path from the root to that
leaf. By contrast, to prove that an element is a member of a linear
list we need to inspect \(O(n)\) elements, on average.Keys and Signed MessagesYou can create a random public-key, private-key pairby calling a function on your computer.With high probability, nobody else has this
specific pair of keys. Each individual's private key is a secret held
by that individual. Public keys are accessible by everybody.Sending messages securelyKeys are used to send messages securely.
Kamala sends a secure message to Joe by encrypting the message with
Joe's public key; Joe decrypts the message using Joe's private key.
An agent cannot decrypt the encrypted message without Joe's private key.Signing messagesSuppose Kamala needs to send a signed message to Joe while
ensuring that nobody can forge her signature. She encrypts the messageMwith her private key to get an encrypted messageM',
and sends the pair(M, M')securely to Joe, i.e., she encrypts(M, M')with Joe's public key, and sends the resulting
encrypted message to Joe.  When Joe receives the message, Joe decrypts
it using his private key to get(M, M'). Then Joe decryptsM'using Kamala's public key to get the decrypted messageM''. IfM'' = Mthen Joe knows that Kamala sentMbecause only an agent with Kamala's private signature could
have sent that message.Cryptocurrency Managed by a Trusted
AgentLet's start with a digital currency managed by a trusted agent that we will
call a bank. Later, we will
look at a consensus algorithm --- very different from Paxos and Byzantine
Generals --- which will allow cryptocurrencies without trusted
agent.The bank maintains a tamper-evident linear listLof transactions that we call a
tamper-evidentledger. 
Any agent can get a copy of the ledger.
This tamper-evident ledger is the foundation of the currency.A transaction is one of
two types:createorpay.
In a pay transaction,payersgive coins that they possess topayees. An agent can be both a 
payer and payee of the same transaction.
In acreatetransaction the bank creates coins that it gives to
agents --- the payees of the transaction; the bank acts as the payer.For this system to be trusted the bank must follow some protocol that determines
how and when the bank creates coins. We won't discuss these protocols.Apaytransaction is signed by all payers of the transaction.
Acreatetransaction is signed by the bank.
We discussed digital signatures and keys earlier.Each element of the tamper-evident ledger has:a unique id;the type of the transaction, either create or pay;list of payers: only for pay transactions --- a list
  indicating the agents who pay coins into the transaction and the
  amounts that they put in;array of value-payee pairs: for both create and pay transactions --- an
  array of pairs(value, payee public key), where each
  pair in the array indicates that coins of the specified value are
  given to the payee with the specified public key.Example of a create transactionAn example of acreatetransaction is:(3146, create,  [(2.1, 7xxxx...), (3.2, 8xxxx)]).The id of this transaction is 3146, the type of the transaction is
create, and the array of value-payee pairs is[(2.1, 7xxxx...), (3.2, 8xxx)]In this transaction the bank creates a coin of value 2.1 and gives it
to the agent with public key 7xxxx..., and the bank also
creates a coin of value 3.2 and gives it
to the agent with public key 8xxxx...The pair:(transaction id, index into array of value-payee pairs)uniquely identifies a (value, payee) tuple.For example (3146, 0) --- 
transaction id 3146, and array index 0 --- identifies value-payee[0] of
transaction 3146 which is specified by the 2-tuple: (2.1, 7xxxx...). So, the
transaction id and index, (3146, 0), tells everybody that the agent
with public key 7xxxx received 2.1 units of coin in transaction 3146.When this
transaction is in a tamper-evident ledger, every agent from that point
onwards knows that agent 7xxxx received 2.1 coins. Any modifications of this
record can be detected.Likewise, (3146, 1) --- 
transaction id 3146, and array index 1 --- identifies value-payee[1] which
is the 2-tuple (3.2, 8xxxx...).Pay transactionCoins are transferred from payers to payees in a pay transaction.Coins flowing into a pay transaction from payersThe payers are identified by a list of 2-tuples, where each 2-tuple is(transaction id, index into array of value-payee pairs)wheretransaction idis the id of the transaction in the
tamper-evident ledger. As we said earlier, this pair uniquely identifies an agent and a
value that this agent acquired in this transaction.
For example the pair --- transaction id, index --- such as (3146, 0)
identifies the 2-tuple (2.1, 7xxxx...); this 2-tuple asserts
that the agent
with public key 7xxxx received 2.1 units of coin in 
transaction 3146. Theentire amountspecified in the 2-tuple (2.1 in our
example) is value that flows into thispaytransaction from the agent
with public key 7xxxx.Suppose the payers in a pay transaction are identified by the list:[(3146, 0), (7359, 3)]and suppose pair 3 in transaction 7359 is (3.2, 8xxxx). Then the total
amount of coins flowing into this pay transaction is 2.1 + 3.2, and
this amount is disbursed to payees.Fig.3: Coins flowing into and out of a pay transaction.Coins flowing out of a pay transaction to payeesThe outflow of coins is specified by an array of value-payee pairs,
exactly as in a create transaction.
The transaction clears all coins: the total inflow from payers is
equal to the total outflow to payees in a transaction.The system may provide incentives, such as payment of coins, to
managers (e.g. banks) of cryptocurrencies. In this case, one of the
payees is the bank itself.Managing amounts spent in a transactionA transaction-id, index pair --- such as (3146, 0)
identifies a 2-tuple such as (2.1, 7xxxx...); this 2-tuple asserts
that the agent
with public key 7xxxx received 2.1 units of coin in 
transaction 3146. Theentire amountspecified in the 2-tuple (2.1 in our
example) is value that flows into the transaction. What should this
agent do if it
wants to put in more than 2.1 coins into the transaction? Or less than
2.1 coins?To put in more value, the bank identifies other transaction-id, index
pairs in which this agent received coins. For example, say that (4539, 2)
identifies a 2-tuple (3.2, 7xxxx), and assume that the payers in this transaction
are specified by the pairs (3146, 0) and (4539, 2). The pair (3146, 0)
asserts that agent (7xxxx) received 2.1 coins and the pair (4539, 2)
asserts that the same agent received 3.2 coins. So the total amount of 
coins flowing into this
transaction from this agent (7xxxx) includes 2.1 + 3.2.To put in less value, the agent acts as both payer and payee; the net
value that this agent pays out to other agents is the difference
between the amount that this agent puts in and takes out. For example,
if agent with public key 7xxxx wants to put in 1.9 coins into this
transaction its payer information can be given by the transaction-id,
index pair (3146, 0) which asserts that the agent received 2.1 coins
and the same agent is a payee that withdraws 0.2 coins.Preventing Double SpendingHow does the system prevent an agent from using the
same coin twice?For example, the transaction id, index pair
(3146, 0) identifies the 2-tuple (2.1, 7xxxx...); this tuple
asserts that the agent
with public key 7xxxx received 2.1 units of coin in 
transaction 3146. Why can't the agent with public key 7xxxx use the
2.1 coins that it received to buy items from Amazon and later use
the same 2.1 coins to buy more items from Walmart?The bank checks that the agent hasn't already spent the coin that it
is putting into a transaction.Before permitting the transaction that double-spends the 2.1 coins
with Walmart, the bank inspects the tamper-evident ledger
for all transactions after transaction 3146 and before the Walmart
transaction to ensure that the agent (7xxxx) hasn't already spent the
2.1 coins that it got in transaction 3146. The transaction that spends
the coins at Amazon will show up in the ledger, and so the transaction
with Walmart will not be allowed.Every agent that has the bank's hash pointer to the end of the
tamper-evident ledger can inspect the ledger to check that
double-spending hasn't occurred.The bank signs a valid transaction and appends it to the
tamper-evident ledger. All agents can see the bank's signature and
verify that nobody (not even the bank) has tampered with the
tamper-evident ledger.Example of a pay transactionAn example of the specification of a pay transaction is:(9431, pay,
  [(3146, 0), (4731, 2)],
  [(0.7, 7xxxx...), (4.6, 9xxxx...)]
).The id of this transaction is 9431; the type of the transaction is
pay; the payers into the transaction are identified by the pairs of
(transaction-id, index): (3146, 0), and (4731, 2); and the
payee array is [(0.7, 7xxxx...), (4.6, 9xxxx...)].Transaction validityThe bank appends a transaction toLif and only if the
transaction is valid. The bank checks for validity by carrying out the
following steps:The bank verifies that the payers into the transaction signed the
  transaction.The bank checks that the total value of coins paid out from the
  transaction does not
  exceed the total value paid in to the transaction. (If the value paid
  in exceeds the value paid out then the bank takes the difference as
  a transaction fee. More about fees later.)The bank verifies that the payers' claims to have received coins in
  previous transactions is genuine. For example,
  if the agent with
  public key 7xxxx... claims to have received coins worth 2.1 in the
  transaction with id 10, and payee array index 0, then the bank
  verifies this claim by that transaction.The bank ensures that coins paid into the transaction haven't
  already been spent.Optimizations: Blocks and Block ChainVerifying large numbers of small transactions requires more
computation than verifying small numbers of blocks of many transactions.
Ablock chainis a tamper-evident ledger in which
each element of the ledger is a block consisting of many transactions.
A block of transactions can be aggregated into a single large
transaction by aggregating all the payers and payees of the smaller
transactions.The amount of computation decreases as the number of transactions in a
block increases. The time required to fill a block with transactions
is larger when the number of transactions to fill the block increases.Checking the Trusted AgentConsider a system in which the trusted agent broadcasts its current
copy of the tamper-evident ledger to all agents.
Every agent can inspect its copy of the tamper-evident ledger to determine whether the
ledger has been tampered with. So, every agent can validate its trust
in the trusted agent; however, this validation
suffers from a crucial problem: Agents may only
have copies ofold, staleversions of the ledger.
By the time that an agent receives a copy of the ledger, the trusted
agent may have added more transactions to the ledger.Fig.4: Old Copy is a Prefix of the Block ChainAn old copy of the ledger can differ from the current copy in only one
way: the current copy may have transactions appended to the end
of the old copy. So all agents can validatepastbehavior of the
trusted agent. An agent cannot, however, treat its copy of the ledger
as the master copy because the agent may not have the transactions
added most recently to the ledger.In the next module we will see how the Bitcoin algorithm addresses
this problem.Advantages of this cryptocurrencyAny agent can get a copy of the tamper-evident ledger and verify that all
transactions in the ledger are valid. Any agent can verify that
the only way in which the ledger is modified is that elements are
append to its tail; all that the agent needs to do is to check that
the pointer to the tail is modified only by appending
elements. Because the ledger is tamper-evident, an agent can check
that the ledger doesn't change while the hash pointer to the end
of the ledger doesn't change.The bank can't forge a transaction because all payers sign the
transaction. Agents can remain anonymous because an agent's only
public information is the agent's public key, and an agent can create
multiple public keys. Every agent can verify the correctness of every
transaction.Disadvantages of this cryptocurrencyUsers may not trust the bank. Transactions are not private because the
bank has a record of all transactions. And the bank is a single point
of failure.Next we'll look at Bitcoin's algorithms for
implementing a cryptocurrency without trusted agents.ReviewWhat iscollision resistance? Why is it relevant to
  cryptocurrencies?Consider the example ofhidingin which Megan hides her
  predictions for the winner of the World Cup. Suppose you could
  choose two different random number generators for creating the
  secret \(r\); in one case the bits of \(r\) are uncorrelated, and in
  the other they are highly correlated. Which generator would you use
  and why?What isbinding? Why is it relevant to hiding information?What ispuzzle friendly?What is atamper-evident data structure? How would you
  implement a tamper-evident linear list? A tamper-evident tree?Describe the algorithm for cryptocurrency with a trusted agent to
  someone who knows absolutely nothing about cryptocurrency.How could the algorithm for cryptocurrency be used for a collection
  of distributed agents to keep track of a sequence of events, other
  than buying/selling currency?K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology



Figure Link: https://kmchandy.github.io/Crypto/CryptoCurrencyIntroduction/Slide3.jpg

Caption: Fig.3: Coins flowing into and out of a pay transaction.


AMerkle treeis a special case of a binary balanced tree in which
data items are stored only in the leaves.
Nodes that are not leaves 
contain only hash pointers to nodes in the next level down.
To prove that an element at the leaf is a member of the tree we need only the
\(log_{2}(n)\) hash pointers on the path from the root to that
leaf. By contrast, to prove that an element is a member of a linear
list we need to inspect \(O(n)\) elements, on average.Keys and Signed MessagesYou can create a random public-key, private-key pairby calling a function on your computer.With high probability, nobody else has this
specific pair of keys. Each individual's private key is a secret held
by that individual. Public keys are accessible by everybody.Sending messages securelyKeys are used to send messages securely.
Kamala sends a secure message to Joe by encrypting the message with
Joe's public key; Joe decrypts the message using Joe's private key.
An agent cannot decrypt the encrypted message without Joe's private key.Signing messagesSuppose Kamala needs to send a signed message to Joe while
ensuring that nobody can forge her signature. She encrypts the messageMwith her private key to get an encrypted messageM',
and sends the pair(M, M')securely to Joe, i.e., she encrypts(M, M')with Joe's public key, and sends the resulting
encrypted message to Joe.  When Joe receives the message, Joe decrypts
it using his private key to get(M, M'). Then Joe decryptsM'using Kamala's public key to get the decrypted messageM''. IfM'' = Mthen Joe knows that Kamala sentMbecause only an agent with Kamala's private signature could
have sent that message.Cryptocurrency Managed by a Trusted
AgentLet's start with a digital currency managed by a trusted agent that we will
call a bank. Later, we will
look at a consensus algorithm --- very different from Paxos and Byzantine
Generals --- which will allow cryptocurrencies without trusted
agent.The bank maintains a tamper-evident linear listLof transactions that we call a
tamper-evidentledger. 
Any agent can get a copy of the ledger.
This tamper-evident ledger is the foundation of the currency.A transaction is one of
two types:createorpay.
In a pay transaction,payersgive coins that they possess topayees. An agent can be both a 
payer and payee of the same transaction.
In acreatetransaction the bank creates coins that it gives to
agents --- the payees of the transaction; the bank acts as the payer.For this system to be trusted the bank must follow some protocol that determines
how and when the bank creates coins. We won't discuss these protocols.Apaytransaction is signed by all payers of the transaction.
Acreatetransaction is signed by the bank.
We discussed digital signatures and keys earlier.Each element of the tamper-evident ledger has:a unique id;the type of the transaction, either create or pay;list of payers: only for pay transactions --- a list
  indicating the agents who pay coins into the transaction and the
  amounts that they put in;array of value-payee pairs: for both create and pay transactions --- an
  array of pairs(value, payee public key), where each
  pair in the array indicates that coins of the specified value are
  given to the payee with the specified public key.Example of a create transactionAn example of acreatetransaction is:(3146, create,  [(2.1, 7xxxx...), (3.2, 8xxxx)]).The id of this transaction is 3146, the type of the transaction is
create, and the array of value-payee pairs is[(2.1, 7xxxx...), (3.2, 8xxx)]In this transaction the bank creates a coin of value 2.1 and gives it
to the agent with public key 7xxxx..., and the bank also
creates a coin of value 3.2 and gives it
to the agent with public key 8xxxx...The pair:(transaction id, index into array of value-payee pairs)uniquely identifies a (value, payee) tuple.For example (3146, 0) --- 
transaction id 3146, and array index 0 --- identifies value-payee[0] of
transaction 3146 which is specified by the 2-tuple: (2.1, 7xxxx...). So, the
transaction id and index, (3146, 0), tells everybody that the agent
with public key 7xxxx received 2.1 units of coin in transaction 3146.When this
transaction is in a tamper-evident ledger, every agent from that point
onwards knows that agent 7xxxx received 2.1 coins. Any modifications of this
record can be detected.Likewise, (3146, 1) --- 
transaction id 3146, and array index 1 --- identifies value-payee[1] which
is the 2-tuple (3.2, 8xxxx...).Pay transactionCoins are transferred from payers to payees in a pay transaction.Coins flowing into a pay transaction from payersThe payers are identified by a list of 2-tuples, where each 2-tuple is(transaction id, index into array of value-payee pairs)wheretransaction idis the id of the transaction in the
tamper-evident ledger. As we said earlier, this pair uniquely identifies an agent and a
value that this agent acquired in this transaction.
For example the pair --- transaction id, index --- such as (3146, 0)
identifies the 2-tuple (2.1, 7xxxx...); this 2-tuple asserts
that the agent
with public key 7xxxx received 2.1 units of coin in 
transaction 3146. Theentire amountspecified in the 2-tuple (2.1 in our
example) is value that flows into thispaytransaction from the agent
with public key 7xxxx.Suppose the payers in a pay transaction are identified by the list:[(3146, 0), (7359, 3)]and suppose pair 3 in transaction 7359 is (3.2, 8xxxx). Then the total
amount of coins flowing into this pay transaction is 2.1 + 3.2, and
this amount is disbursed to payees.Fig.3: Coins flowing into and out of a pay transaction.Coins flowing out of a pay transaction to payeesThe outflow of coins is specified by an array of value-payee pairs,
exactly as in a create transaction.
The transaction clears all coins: the total inflow from payers is
equal to the total outflow to payees in a transaction.The system may provide incentives, such as payment of coins, to
managers (e.g. banks) of cryptocurrencies. In this case, one of the
payees is the bank itself.Managing amounts spent in a transactionA transaction-id, index pair --- such as (3146, 0)
identifies a 2-tuple such as (2.1, 7xxxx...); this 2-tuple asserts
that the agent
with public key 7xxxx received 2.1 units of coin in 
transaction 3146. Theentire amountspecified in the 2-tuple (2.1 in our
example) is value that flows into the transaction. What should this
agent do if it
wants to put in more than 2.1 coins into the transaction? Or less than
2.1 coins?To put in more value, the bank identifies other transaction-id, index
pairs in which this agent received coins. For example, say that (4539, 2)
identifies a 2-tuple (3.2, 7xxxx), and assume that the payers in this transaction
are specified by the pairs (3146, 0) and (4539, 2). The pair (3146, 0)
asserts that agent (7xxxx) received 2.1 coins and the pair (4539, 2)
asserts that the same agent received 3.2 coins. So the total amount of 
coins flowing into this
transaction from this agent (7xxxx) includes 2.1 + 3.2.To put in less value, the agent acts as both payer and payee; the net
value that this agent pays out to other agents is the difference
between the amount that this agent puts in and takes out. For example,
if agent with public key 7xxxx wants to put in 1.9 coins into this
transaction its payer information can be given by the transaction-id,
index pair (3146, 0) which asserts that the agent received 2.1 coins
and the same agent is a payee that withdraws 0.2 coins.Preventing Double SpendingHow does the system prevent an agent from using the
same coin twice?For example, the transaction id, index pair
(3146, 0) identifies the 2-tuple (2.1, 7xxxx...); this tuple
asserts that the agent
with public key 7xxxx received 2.1 units of coin in 
transaction 3146. Why can't the agent with public key 7xxxx use the
2.1 coins that it received to buy items from Amazon and later use
the same 2.1 coins to buy more items from Walmart?The bank checks that the agent hasn't already spent the coin that it
is putting into a transaction.Before permitting the transaction that double-spends the 2.1 coins
with Walmart, the bank inspects the tamper-evident ledger
for all transactions after transaction 3146 and before the Walmart
transaction to ensure that the agent (7xxxx) hasn't already spent the
2.1 coins that it got in transaction 3146. The transaction that spends
the coins at Amazon will show up in the ledger, and so the transaction
with Walmart will not be allowed.Every agent that has the bank's hash pointer to the end of the
tamper-evident ledger can inspect the ledger to check that
double-spending hasn't occurred.The bank signs a valid transaction and appends it to the
tamper-evident ledger. All agents can see the bank's signature and
verify that nobody (not even the bank) has tampered with the
tamper-evident ledger.Example of a pay transactionAn example of the specification of a pay transaction is:(9431, pay,
  [(3146, 0), (4731, 2)],
  [(0.7, 7xxxx...), (4.6, 9xxxx...)]
).The id of this transaction is 9431; the type of the transaction is
pay; the payers into the transaction are identified by the pairs of
(transaction-id, index): (3146, 0), and (4731, 2); and the
payee array is [(0.7, 7xxxx...), (4.6, 9xxxx...)].Transaction validityThe bank appends a transaction toLif and only if the
transaction is valid. The bank checks for validity by carrying out the
following steps:The bank verifies that the payers into the transaction signed the
  transaction.The bank checks that the total value of coins paid out from the
  transaction does not
  exceed the total value paid in to the transaction. (If the value paid
  in exceeds the value paid out then the bank takes the difference as
  a transaction fee. More about fees later.)The bank verifies that the payers' claims to have received coins in
  previous transactions is genuine. For example,
  if the agent with
  public key 7xxxx... claims to have received coins worth 2.1 in the
  transaction with id 10, and payee array index 0, then the bank
  verifies this claim by that transaction.The bank ensures that coins paid into the transaction haven't
  already been spent.Optimizations: Blocks and Block ChainVerifying large numbers of small transactions requires more
computation than verifying small numbers of blocks of many transactions.
Ablock chainis a tamper-evident ledger in which
each element of the ledger is a block consisting of many transactions.
A block of transactions can be aggregated into a single large
transaction by aggregating all the payers and payees of the smaller
transactions.The amount of computation decreases as the number of transactions in a
block increases. The time required to fill a block with transactions
is larger when the number of transactions to fill the block increases.Checking the Trusted AgentConsider a system in which the trusted agent broadcasts its current
copy of the tamper-evident ledger to all agents.
Every agent can inspect its copy of the tamper-evident ledger to determine whether the
ledger has been tampered with. So, every agent can validate its trust
in the trusted agent; however, this validation
suffers from a crucial problem: Agents may only
have copies ofold, staleversions of the ledger.
By the time that an agent receives a copy of the ledger, the trusted
agent may have added more transactions to the ledger.Fig.4: Old Copy is a Prefix of the Block ChainAn old copy of the ledger can differ from the current copy in only one
way: the current copy may have transactions appended to the end
of the old copy. So all agents can validatepastbehavior of the
trusted agent. An agent cannot, however, treat its copy of the ledger
as the master copy because the agent may not have the transactions
added most recently to the ledger.In the next module we will see how the Bitcoin algorithm addresses
this problem.Advantages of this cryptocurrencyAny agent can get a copy of the tamper-evident ledger and verify that all
transactions in the ledger are valid. Any agent can verify that
the only way in which the ledger is modified is that elements are
append to its tail; all that the agent needs to do is to check that
the pointer to the tail is modified only by appending
elements. Because the ledger is tamper-evident, an agent can check
that the ledger doesn't change while the hash pointer to the end
of the ledger doesn't change.The bank can't forge a transaction because all payers sign the
transaction. Agents can remain anonymous because an agent's only
public information is the agent's public key, and an agent can create
multiple public keys. Every agent can verify the correctness of every
transaction.Disadvantages of this cryptocurrencyUsers may not trust the bank. Transactions are not private because the
bank has a record of all transactions. And the bank is a single point
of failure.Next we'll look at Bitcoin's algorithms for
implementing a cryptocurrency without trusted agents.ReviewWhat iscollision resistance? Why is it relevant to
  cryptocurrencies?Consider the example ofhidingin which Megan hides her
  predictions for the winner of the World Cup. Suppose you could
  choose two different random number generators for creating the
  secret \(r\); in one case the bits of \(r\) are uncorrelated, and in
  the other they are highly correlated. Which generator would you use
  and why?What isbinding? Why is it relevant to hiding information?What ispuzzle friendly?What is atamper-evident data structure? How would you
  implement a tamper-evident linear list? A tamper-evident tree?Describe the algorithm for cryptocurrency with a trusted agent to
  someone who knows absolutely nothing about cryptocurrency.How could the algorithm for cryptocurrency be used for a collection
  of distributed agents to keep track of a sequence of events, other
  than buying/selling currency?K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology



Figure Link: https://kmchandy.github.io/Crypto/CryptoCurrencyIntroduction/Slide3.jpg

Caption: Fig.3: Coins flowing into and out of a pay transaction.


Sending messages securely



Figure Link: https://kmchandy.github.io/Crypto/CryptoCurrencyIntroduction/Slide3.jpg

Caption: Fig.3: Coins flowing into and out of a pay transaction.


Signing messages



Figure Link: https://kmchandy.github.io/Crypto/CryptoCurrencyIntroduction/Slide3.jpg

Caption: Fig.3: Coins flowing into and out of a pay transaction.


A transaction is one of
two types:createorpay.
In a pay transaction,payersgive coins that they possess topayees. An agent can be both a 
payer and payee of the same transaction.
In acreatetransaction the bank creates coins that it gives to
agents --- the payees of the transaction; the bank acts as the payer.For this system to be trusted the bank must follow some protocol that determines
how and when the bank creates coins. We won't discuss these protocols.Apaytransaction is signed by all payers of the transaction.
Acreatetransaction is signed by the bank.
We discussed digital signatures and keys earlier.Each element of the tamper-evident ledger has:a unique id;the type of the transaction, either create or pay;list of payers: only for pay transactions --- a list
  indicating the agents who pay coins into the transaction and the
  amounts that they put in;array of value-payee pairs: for both create and pay transactions --- an
  array of pairs(value, payee public key), where each
  pair in the array indicates that coins of the specified value are
  given to the payee with the specified public key.Example of a create transactionAn example of acreatetransaction is:(3146, create,  [(2.1, 7xxxx...), (3.2, 8xxxx)]).The id of this transaction is 3146, the type of the transaction is
create, and the array of value-payee pairs is[(2.1, 7xxxx...), (3.2, 8xxx)]In this transaction the bank creates a coin of value 2.1 and gives it
to the agent with public key 7xxxx..., and the bank also
creates a coin of value 3.2 and gives it
to the agent with public key 8xxxx...The pair:(transaction id, index into array of value-payee pairs)uniquely identifies a (value, payee) tuple.For example (3146, 0) --- 
transaction id 3146, and array index 0 --- identifies value-payee[0] of
transaction 3146 which is specified by the 2-tuple: (2.1, 7xxxx...). So, the
transaction id and index, (3146, 0), tells everybody that the agent
with public key 7xxxx received 2.1 units of coin in transaction 3146.When this
transaction is in a tamper-evident ledger, every agent from that point
onwards knows that agent 7xxxx received 2.1 coins. Any modifications of this
record can be detected.Likewise, (3146, 1) --- 
transaction id 3146, and array index 1 --- identifies value-payee[1] which
is the 2-tuple (3.2, 8xxxx...).Pay transactionCoins are transferred from payers to payees in a pay transaction.Coins flowing into a pay transaction from payersThe payers are identified by a list of 2-tuples, where each 2-tuple is(transaction id, index into array of value-payee pairs)wheretransaction idis the id of the transaction in the
tamper-evident ledger. As we said earlier, this pair uniquely identifies an agent and a
value that this agent acquired in this transaction.
For example the pair --- transaction id, index --- such as (3146, 0)
identifies the 2-tuple (2.1, 7xxxx...); this 2-tuple asserts
that the agent
with public key 7xxxx received 2.1 units of coin in 
transaction 3146. Theentire amountspecified in the 2-tuple (2.1 in our
example) is value that flows into thispaytransaction from the agent
with public key 7xxxx.Suppose the payers in a pay transaction are identified by the list:[(3146, 0), (7359, 3)]and suppose pair 3 in transaction 7359 is (3.2, 8xxxx). Then the total
amount of coins flowing into this pay transaction is 2.1 + 3.2, and
this amount is disbursed to payees.Fig.3: Coins flowing into and out of a pay transaction.Coins flowing out of a pay transaction to payeesThe outflow of coins is specified by an array of value-payee pairs,
exactly as in a create transaction.
The transaction clears all coins: the total inflow from payers is
equal to the total outflow to payees in a transaction.The system may provide incentives, such as payment of coins, to
managers (e.g. banks) of cryptocurrencies. In this case, one of the
payees is the bank itself.Managing amounts spent in a transactionA transaction-id, index pair --- such as (3146, 0)
identifies a 2-tuple such as (2.1, 7xxxx...); this 2-tuple asserts
that the agent
with public key 7xxxx received 2.1 units of coin in 
transaction 3146. Theentire amountspecified in the 2-tuple (2.1 in our
example) is value that flows into the transaction. What should this
agent do if it
wants to put in more than 2.1 coins into the transaction? Or less than
2.1 coins?To put in more value, the bank identifies other transaction-id, index
pairs in which this agent received coins. For example, say that (4539, 2)
identifies a 2-tuple (3.2, 7xxxx), and assume that the payers in this transaction
are specified by the pairs (3146, 0) and (4539, 2). The pair (3146, 0)
asserts that agent (7xxxx) received 2.1 coins and the pair (4539, 2)
asserts that the same agent received 3.2 coins. So the total amount of 
coins flowing into this
transaction from this agent (7xxxx) includes 2.1 + 3.2.To put in less value, the agent acts as both payer and payee; the net
value that this agent pays out to other agents is the difference
between the amount that this agent puts in and takes out. For example,
if agent with public key 7xxxx wants to put in 1.9 coins into this
transaction its payer information can be given by the transaction-id,
index pair (3146, 0) which asserts that the agent received 2.1 coins
and the same agent is a payee that withdraws 0.2 coins.Preventing Double SpendingHow does the system prevent an agent from using the
same coin twice?For example, the transaction id, index pair
(3146, 0) identifies the 2-tuple (2.1, 7xxxx...); this tuple
asserts that the agent
with public key 7xxxx received 2.1 units of coin in 
transaction 3146. Why can't the agent with public key 7xxxx use the
2.1 coins that it received to buy items from Amazon and later use
the same 2.1 coins to buy more items from Walmart?The bank checks that the agent hasn't already spent the coin that it
is putting into a transaction.Before permitting the transaction that double-spends the 2.1 coins
with Walmart, the bank inspects the tamper-evident ledger
for all transactions after transaction 3146 and before the Walmart
transaction to ensure that the agent (7xxxx) hasn't already spent the
2.1 coins that it got in transaction 3146. The transaction that spends
the coins at Amazon will show up in the ledger, and so the transaction
with Walmart will not be allowed.Every agent that has the bank's hash pointer to the end of the
tamper-evident ledger can inspect the ledger to check that
double-spending hasn't occurred.The bank signs a valid transaction and appends it to the
tamper-evident ledger. All agents can see the bank's signature and
verify that nobody (not even the bank) has tampered with the
tamper-evident ledger.Example of a pay transactionAn example of the specification of a pay transaction is:(9431, pay,
  [(3146, 0), (4731, 2)],
  [(0.7, 7xxxx...), (4.6, 9xxxx...)]
).The id of this transaction is 9431; the type of the transaction is
pay; the payers into the transaction are identified by the pairs of
(transaction-id, index): (3146, 0), and (4731, 2); and the
payee array is [(0.7, 7xxxx...), (4.6, 9xxxx...)].Transaction validityThe bank appends a transaction toLif and only if the
transaction is valid. The bank checks for validity by carrying out the
following steps:The bank verifies that the payers into the transaction signed the
  transaction.The bank checks that the total value of coins paid out from the
  transaction does not
  exceed the total value paid in to the transaction. (If the value paid
  in exceeds the value paid out then the bank takes the difference as
  a transaction fee. More about fees later.)The bank verifies that the payers' claims to have received coins in
  previous transactions is genuine. For example,
  if the agent with
  public key 7xxxx... claims to have received coins worth 2.1 in the
  transaction with id 10, and payee array index 0, then the bank
  verifies this claim by that transaction.The bank ensures that coins paid into the transaction haven't
  already been spent.Optimizations: Blocks and Block ChainVerifying large numbers of small transactions requires more
computation than verifying small numbers of blocks of many transactions.
Ablock chainis a tamper-evident ledger in which
each element of the ledger is a block consisting of many transactions.
A block of transactions can be aggregated into a single large
transaction by aggregating all the payers and payees of the smaller
transactions.The amount of computation decreases as the number of transactions in a
block increases. The time required to fill a block with transactions
is larger when the number of transactions to fill the block increases.Checking the Trusted AgentConsider a system in which the trusted agent broadcasts its current
copy of the tamper-evident ledger to all agents.
Every agent can inspect its copy of the tamper-evident ledger to determine whether the
ledger has been tampered with. So, every agent can validate its trust
in the trusted agent; however, this validation
suffers from a crucial problem: Agents may only
have copies ofold, staleversions of the ledger.
By the time that an agent receives a copy of the ledger, the trusted
agent may have added more transactions to the ledger.Fig.4: Old Copy is a Prefix of the Block ChainAn old copy of the ledger can differ from the current copy in only one
way: the current copy may have transactions appended to the end
of the old copy. So all agents can validatepastbehavior of the
trusted agent. An agent cannot, however, treat its copy of the ledger
as the master copy because the agent may not have the transactions
added most recently to the ledger.In the next module we will see how the Bitcoin algorithm addresses
this problem.Advantages of this cryptocurrencyAny agent can get a copy of the tamper-evident ledger and verify that all
transactions in the ledger are valid. Any agent can verify that
the only way in which the ledger is modified is that elements are
append to its tail; all that the agent needs to do is to check that
the pointer to the tail is modified only by appending
elements. Because the ledger is tamper-evident, an agent can check
that the ledger doesn't change while the hash pointer to the end
of the ledger doesn't change.The bank can't forge a transaction because all payers sign the
transaction. Agents can remain anonymous because an agent's only
public information is the agent's public key, and an agent can create
multiple public keys. Every agent can verify the correctness of every
transaction.Disadvantages of this cryptocurrencyUsers may not trust the bank. Transactions are not private because the
bank has a record of all transactions. And the bank is a single point
of failure.Next we'll look at Bitcoin's algorithms for
implementing a cryptocurrency without trusted agents.ReviewWhat iscollision resistance? Why is it relevant to
  cryptocurrencies?Consider the example ofhidingin which Megan hides her
  predictions for the winner of the World Cup. Suppose you could
  choose two different random number generators for creating the
  secret \(r\); in one case the bits of \(r\) are uncorrelated, and in
  the other they are highly correlated. Which generator would you use
  and why?What isbinding? Why is it relevant to hiding information?What ispuzzle friendly?What is atamper-evident data structure? How would you
  implement a tamper-evident linear list? A tamper-evident tree?Describe the algorithm for cryptocurrency with a trusted agent to
  someone who knows absolutely nothing about cryptocurrency.How could the algorithm for cryptocurrency be used for a collection
  of distributed agents to keep track of a sequence of events, other
  than buying/selling currency?K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology



Figure Link: https://kmchandy.github.io/Crypto/CryptoCurrencyIntroduction/Slide3.jpg

Caption: Fig.3: Coins flowing into and out of a pay transaction.


For this system to be trusted the bank must follow some protocol that determines
how and when the bank creates coins. We won't discuss these protocols.Apaytransaction is signed by all payers of the transaction.
Acreatetransaction is signed by the bank.
We discussed digital signatures and keys earlier.Each element of the tamper-evident ledger has:a unique id;the type of the transaction, either create or pay;list of payers: only for pay transactions --- a list
  indicating the agents who pay coins into the transaction and the
  amounts that they put in;array of value-payee pairs: for both create and pay transactions --- an
  array of pairs(value, payee public key), where each
  pair in the array indicates that coins of the specified value are
  given to the payee with the specified public key.Example of a create transactionAn example of acreatetransaction is:(3146, create,  [(2.1, 7xxxx...), (3.2, 8xxxx)]).The id of this transaction is 3146, the type of the transaction is
create, and the array of value-payee pairs is[(2.1, 7xxxx...), (3.2, 8xxx)]In this transaction the bank creates a coin of value 2.1 and gives it
to the agent with public key 7xxxx..., and the bank also
creates a coin of value 3.2 and gives it
to the agent with public key 8xxxx...The pair:(transaction id, index into array of value-payee pairs)uniquely identifies a (value, payee) tuple.For example (3146, 0) --- 
transaction id 3146, and array index 0 --- identifies value-payee[0] of
transaction 3146 which is specified by the 2-tuple: (2.1, 7xxxx...). So, the
transaction id and index, (3146, 0), tells everybody that the agent
with public key 7xxxx received 2.1 units of coin in transaction 3146.When this
transaction is in a tamper-evident ledger, every agent from that point
onwards knows that agent 7xxxx received 2.1 coins. Any modifications of this
record can be detected.Likewise, (3146, 1) --- 
transaction id 3146, and array index 1 --- identifies value-payee[1] which
is the 2-tuple (3.2, 8xxxx...).Pay transactionCoins are transferred from payers to payees in a pay transaction.Coins flowing into a pay transaction from payersThe payers are identified by a list of 2-tuples, where each 2-tuple is(transaction id, index into array of value-payee pairs)wheretransaction idis the id of the transaction in the
tamper-evident ledger. As we said earlier, this pair uniquely identifies an agent and a
value that this agent acquired in this transaction.
For example the pair --- transaction id, index --- such as (3146, 0)
identifies the 2-tuple (2.1, 7xxxx...); this 2-tuple asserts
that the agent
with public key 7xxxx received 2.1 units of coin in 
transaction 3146. Theentire amountspecified in the 2-tuple (2.1 in our
example) is value that flows into thispaytransaction from the agent
with public key 7xxxx.Suppose the payers in a pay transaction are identified by the list:[(3146, 0), (7359, 3)]and suppose pair 3 in transaction 7359 is (3.2, 8xxxx). Then the total
amount of coins flowing into this pay transaction is 2.1 + 3.2, and
this amount is disbursed to payees.Fig.3: Coins flowing into and out of a pay transaction.Coins flowing out of a pay transaction to payeesThe outflow of coins is specified by an array of value-payee pairs,
exactly as in a create transaction.
The transaction clears all coins: the total inflow from payers is
equal to the total outflow to payees in a transaction.The system may provide incentives, such as payment of coins, to
managers (e.g. banks) of cryptocurrencies. In this case, one of the
payees is the bank itself.Managing amounts spent in a transactionA transaction-id, index pair --- such as (3146, 0)
identifies a 2-tuple such as (2.1, 7xxxx...); this 2-tuple asserts
that the agent
with public key 7xxxx received 2.1 units of coin in 
transaction 3146. Theentire amountspecified in the 2-tuple (2.1 in our
example) is value that flows into the transaction. What should this
agent do if it
wants to put in more than 2.1 coins into the transaction? Or less than
2.1 coins?To put in more value, the bank identifies other transaction-id, index
pairs in which this agent received coins. For example, say that (4539, 2)
identifies a 2-tuple (3.2, 7xxxx), and assume that the payers in this transaction
are specified by the pairs (3146, 0) and (4539, 2). The pair (3146, 0)
asserts that agent (7xxxx) received 2.1 coins and the pair (4539, 2)
asserts that the same agent received 3.2 coins. So the total amount of 
coins flowing into this
transaction from this agent (7xxxx) includes 2.1 + 3.2.To put in less value, the agent acts as both payer and payee; the net
value that this agent pays out to other agents is the difference
between the amount that this agent puts in and takes out. For example,
if agent with public key 7xxxx wants to put in 1.9 coins into this
transaction its payer information can be given by the transaction-id,
index pair (3146, 0) which asserts that the agent received 2.1 coins
and the same agent is a payee that withdraws 0.2 coins.Preventing Double SpendingHow does the system prevent an agent from using the
same coin twice?For example, the transaction id, index pair
(3146, 0) identifies the 2-tuple (2.1, 7xxxx...); this tuple
asserts that the agent
with public key 7xxxx received 2.1 units of coin in 
transaction 3146. Why can't the agent with public key 7xxxx use the
2.1 coins that it received to buy items from Amazon and later use
the same 2.1 coins to buy more items from Walmart?The bank checks that the agent hasn't already spent the coin that it
is putting into a transaction.Before permitting the transaction that double-spends the 2.1 coins
with Walmart, the bank inspects the tamper-evident ledger
for all transactions after transaction 3146 and before the Walmart
transaction to ensure that the agent (7xxxx) hasn't already spent the
2.1 coins that it got in transaction 3146. The transaction that spends
the coins at Amazon will show up in the ledger, and so the transaction
with Walmart will not be allowed.Every agent that has the bank's hash pointer to the end of the
tamper-evident ledger can inspect the ledger to check that
double-spending hasn't occurred.The bank signs a valid transaction and appends it to the
tamper-evident ledger. All agents can see the bank's signature and
verify that nobody (not even the bank) has tampered with the
tamper-evident ledger.Example of a pay transactionAn example of the specification of a pay transaction is:(9431, pay,
  [(3146, 0), (4731, 2)],
  [(0.7, 7xxxx...), (4.6, 9xxxx...)]
).The id of this transaction is 9431; the type of the transaction is
pay; the payers into the transaction are identified by the pairs of
(transaction-id, index): (3146, 0), and (4731, 2); and the
payee array is [(0.7, 7xxxx...), (4.6, 9xxxx...)].Transaction validityThe bank appends a transaction toLif and only if the
transaction is valid. The bank checks for validity by carrying out the
following steps:The bank verifies that the payers into the transaction signed the
  transaction.The bank checks that the total value of coins paid out from the
  transaction does not
  exceed the total value paid in to the transaction. (If the value paid
  in exceeds the value paid out then the bank takes the difference as
  a transaction fee. More about fees later.)The bank verifies that the payers' claims to have received coins in
  previous transactions is genuine. For example,
  if the agent with
  public key 7xxxx... claims to have received coins worth 2.1 in the
  transaction with id 10, and payee array index 0, then the bank
  verifies this claim by that transaction.The bank ensures that coins paid into the transaction haven't
  already been spent.Optimizations: Blocks and Block ChainVerifying large numbers of small transactions requires more
computation than verifying small numbers of blocks of many transactions.
Ablock chainis a tamper-evident ledger in which
each element of the ledger is a block consisting of many transactions.
A block of transactions can be aggregated into a single large
transaction by aggregating all the payers and payees of the smaller
transactions.The amount of computation decreases as the number of transactions in a
block increases. The time required to fill a block with transactions
is larger when the number of transactions to fill the block increases.Checking the Trusted AgentConsider a system in which the trusted agent broadcasts its current
copy of the tamper-evident ledger to all agents.
Every agent can inspect its copy of the tamper-evident ledger to determine whether the
ledger has been tampered with. So, every agent can validate its trust
in the trusted agent; however, this validation
suffers from a crucial problem: Agents may only
have copies ofold, staleversions of the ledger.
By the time that an agent receives a copy of the ledger, the trusted
agent may have added more transactions to the ledger.Fig.4: Old Copy is a Prefix of the Block ChainAn old copy of the ledger can differ from the current copy in only one
way: the current copy may have transactions appended to the end
of the old copy. So all agents can validatepastbehavior of the
trusted agent. An agent cannot, however, treat its copy of the ledger
as the master copy because the agent may not have the transactions
added most recently to the ledger.In the next module we will see how the Bitcoin algorithm addresses
this problem.Advantages of this cryptocurrencyAny agent can get a copy of the tamper-evident ledger and verify that all
transactions in the ledger are valid. Any agent can verify that
the only way in which the ledger is modified is that elements are
append to its tail; all that the agent needs to do is to check that
the pointer to the tail is modified only by appending
elements. Because the ledger is tamper-evident, an agent can check
that the ledger doesn't change while the hash pointer to the end
of the ledger doesn't change.The bank can't forge a transaction because all payers sign the
transaction. Agents can remain anonymous because an agent's only
public information is the agent's public key, and an agent can create
multiple public keys. Every agent can verify the correctness of every
transaction.Disadvantages of this cryptocurrencyUsers may not trust the bank. Transactions are not private because the
bank has a record of all transactions. And the bank is a single point
of failure.Next we'll look at Bitcoin's algorithms for
implementing a cryptocurrency without trusted agents.ReviewWhat iscollision resistance? Why is it relevant to
  cryptocurrencies?Consider the example ofhidingin which Megan hides her
  predictions for the winner of the World Cup. Suppose you could
  choose two different random number generators for creating the
  secret \(r\); in one case the bits of \(r\) are uncorrelated, and in
  the other they are highly correlated. Which generator would you use
  and why?What isbinding? Why is it relevant to hiding information?What ispuzzle friendly?What is atamper-evident data structure? How would you
  implement a tamper-evident linear list? A tamper-evident tree?Describe the algorithm for cryptocurrency with a trusted agent to
  someone who knows absolutely nothing about cryptocurrency.How could the algorithm for cryptocurrency be used for a collection
  of distributed agents to keep track of a sequence of events, other
  than buying/selling currency?K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology



Figure Link: https://kmchandy.github.io/Crypto/CryptoCurrencyIntroduction/Slide3.jpg

Caption: Fig.3: Coins flowing into and out of a pay transaction.


Apaytransaction is signed by all payers of the transaction.
Acreatetransaction is signed by the bank.
We discussed digital signatures and keys earlier.Each element of the tamper-evident ledger has:a unique id;the type of the transaction, either create or pay;list of payers: only for pay transactions --- a list
  indicating the agents who pay coins into the transaction and the
  amounts that they put in;array of value-payee pairs: for both create and pay transactions --- an
  array of pairs(value, payee public key), where each
  pair in the array indicates that coins of the specified value are
  given to the payee with the specified public key.Example of a create transactionAn example of acreatetransaction is:(3146, create,  [(2.1, 7xxxx...), (3.2, 8xxxx)]).The id of this transaction is 3146, the type of the transaction is
create, and the array of value-payee pairs is[(2.1, 7xxxx...), (3.2, 8xxx)]In this transaction the bank creates a coin of value 2.1 and gives it
to the agent with public key 7xxxx..., and the bank also
creates a coin of value 3.2 and gives it
to the agent with public key 8xxxx...The pair:(transaction id, index into array of value-payee pairs)uniquely identifies a (value, payee) tuple.For example (3146, 0) --- 
transaction id 3146, and array index 0 --- identifies value-payee[0] of
transaction 3146 which is specified by the 2-tuple: (2.1, 7xxxx...). So, the
transaction id and index, (3146, 0), tells everybody that the agent
with public key 7xxxx received 2.1 units of coin in transaction 3146.When this
transaction is in a tamper-evident ledger, every agent from that point
onwards knows that agent 7xxxx received 2.1 coins. Any modifications of this
record can be detected.Likewise, (3146, 1) --- 
transaction id 3146, and array index 1 --- identifies value-payee[1] which
is the 2-tuple (3.2, 8xxxx...).Pay transactionCoins are transferred from payers to payees in a pay transaction.Coins flowing into a pay transaction from payersThe payers are identified by a list of 2-tuples, where each 2-tuple is(transaction id, index into array of value-payee pairs)wheretransaction idis the id of the transaction in the
tamper-evident ledger. As we said earlier, this pair uniquely identifies an agent and a
value that this agent acquired in this transaction.
For example the pair --- transaction id, index --- such as (3146, 0)
identifies the 2-tuple (2.1, 7xxxx...); this 2-tuple asserts
that the agent
with public key 7xxxx received 2.1 units of coin in 
transaction 3146. Theentire amountspecified in the 2-tuple (2.1 in our
example) is value that flows into thispaytransaction from the agent
with public key 7xxxx.Suppose the payers in a pay transaction are identified by the list:[(3146, 0), (7359, 3)]and suppose pair 3 in transaction 7359 is (3.2, 8xxxx). Then the total
amount of coins flowing into this pay transaction is 2.1 + 3.2, and
this amount is disbursed to payees.Fig.3: Coins flowing into and out of a pay transaction.Coins flowing out of a pay transaction to payeesThe outflow of coins is specified by an array of value-payee pairs,
exactly as in a create transaction.
The transaction clears all coins: the total inflow from payers is
equal to the total outflow to payees in a transaction.The system may provide incentives, such as payment of coins, to
managers (e.g. banks) of cryptocurrencies. In this case, one of the
payees is the bank itself.Managing amounts spent in a transactionA transaction-id, index pair --- such as (3146, 0)
identifies a 2-tuple such as (2.1, 7xxxx...); this 2-tuple asserts
that the agent
with public key 7xxxx received 2.1 units of coin in 
transaction 3146. Theentire amountspecified in the 2-tuple (2.1 in our
example) is value that flows into the transaction. What should this
agent do if it
wants to put in more than 2.1 coins into the transaction? Or less than
2.1 coins?To put in more value, the bank identifies other transaction-id, index
pairs in which this agent received coins. For example, say that (4539, 2)
identifies a 2-tuple (3.2, 7xxxx), and assume that the payers in this transaction
are specified by the pairs (3146, 0) and (4539, 2). The pair (3146, 0)
asserts that agent (7xxxx) received 2.1 coins and the pair (4539, 2)
asserts that the same agent received 3.2 coins. So the total amount of 
coins flowing into this
transaction from this agent (7xxxx) includes 2.1 + 3.2.To put in less value, the agent acts as both payer and payee; the net
value that this agent pays out to other agents is the difference
between the amount that this agent puts in and takes out. For example,
if agent with public key 7xxxx wants to put in 1.9 coins into this
transaction its payer information can be given by the transaction-id,
index pair (3146, 0) which asserts that the agent received 2.1 coins
and the same agent is a payee that withdraws 0.2 coins.Preventing Double SpendingHow does the system prevent an agent from using the
same coin twice?For example, the transaction id, index pair
(3146, 0) identifies the 2-tuple (2.1, 7xxxx...); this tuple
asserts that the agent
with public key 7xxxx received 2.1 units of coin in 
transaction 3146. Why can't the agent with public key 7xxxx use the
2.1 coins that it received to buy items from Amazon and later use
the same 2.1 coins to buy more items from Walmart?The bank checks that the agent hasn't already spent the coin that it
is putting into a transaction.Before permitting the transaction that double-spends the 2.1 coins
with Walmart, the bank inspects the tamper-evident ledger
for all transactions after transaction 3146 and before the Walmart
transaction to ensure that the agent (7xxxx) hasn't already spent the
2.1 coins that it got in transaction 3146. The transaction that spends
the coins at Amazon will show up in the ledger, and so the transaction
with Walmart will not be allowed.Every agent that has the bank's hash pointer to the end of the
tamper-evident ledger can inspect the ledger to check that
double-spending hasn't occurred.The bank signs a valid transaction and appends it to the
tamper-evident ledger. All agents can see the bank's signature and
verify that nobody (not even the bank) has tampered with the
tamper-evident ledger.Example of a pay transactionAn example of the specification of a pay transaction is:(9431, pay,
  [(3146, 0), (4731, 2)],
  [(0.7, 7xxxx...), (4.6, 9xxxx...)]
).The id of this transaction is 9431; the type of the transaction is
pay; the payers into the transaction are identified by the pairs of
(transaction-id, index): (3146, 0), and (4731, 2); and the
payee array is [(0.7, 7xxxx...), (4.6, 9xxxx...)].Transaction validityThe bank appends a transaction toLif and only if the
transaction is valid. The bank checks for validity by carrying out the
following steps:The bank verifies that the payers into the transaction signed the
  transaction.The bank checks that the total value of coins paid out from the
  transaction does not
  exceed the total value paid in to the transaction. (If the value paid
  in exceeds the value paid out then the bank takes the difference as
  a transaction fee. More about fees later.)The bank verifies that the payers' claims to have received coins in
  previous transactions is genuine. For example,
  if the agent with
  public key 7xxxx... claims to have received coins worth 2.1 in the
  transaction with id 10, and payee array index 0, then the bank
  verifies this claim by that transaction.The bank ensures that coins paid into the transaction haven't
  already been spent.Optimizations: Blocks and Block ChainVerifying large numbers of small transactions requires more
computation than verifying small numbers of blocks of many transactions.
Ablock chainis a tamper-evident ledger in which
each element of the ledger is a block consisting of many transactions.
A block of transactions can be aggregated into a single large
transaction by aggregating all the payers and payees of the smaller
transactions.The amount of computation decreases as the number of transactions in a
block increases. The time required to fill a block with transactions
is larger when the number of transactions to fill the block increases.Checking the Trusted AgentConsider a system in which the trusted agent broadcasts its current
copy of the tamper-evident ledger to all agents.
Every agent can inspect its copy of the tamper-evident ledger to determine whether the
ledger has been tampered with. So, every agent can validate its trust
in the trusted agent; however, this validation
suffers from a crucial problem: Agents may only
have copies ofold, staleversions of the ledger.
By the time that an agent receives a copy of the ledger, the trusted
agent may have added more transactions to the ledger.Fig.4: Old Copy is a Prefix of the Block ChainAn old copy of the ledger can differ from the current copy in only one
way: the current copy may have transactions appended to the end
of the old copy. So all agents can validatepastbehavior of the
trusted agent. An agent cannot, however, treat its copy of the ledger
as the master copy because the agent may not have the transactions
added most recently to the ledger.In the next module we will see how the Bitcoin algorithm addresses
this problem.Advantages of this cryptocurrencyAny agent can get a copy of the tamper-evident ledger and verify that all
transactions in the ledger are valid. Any agent can verify that
the only way in which the ledger is modified is that elements are
append to its tail; all that the agent needs to do is to check that
the pointer to the tail is modified only by appending
elements. Because the ledger is tamper-evident, an agent can check
that the ledger doesn't change while the hash pointer to the end
of the ledger doesn't change.The bank can't forge a transaction because all payers sign the
transaction. Agents can remain anonymous because an agent's only
public information is the agent's public key, and an agent can create
multiple public keys. Every agent can verify the correctness of every
transaction.Disadvantages of this cryptocurrencyUsers may not trust the bank. Transactions are not private because the
bank has a record of all transactions. And the bank is a single point
of failure.Next we'll look at Bitcoin's algorithms for
implementing a cryptocurrency without trusted agents.ReviewWhat iscollision resistance? Why is it relevant to
  cryptocurrencies?Consider the example ofhidingin which Megan hides her
  predictions for the winner of the World Cup. Suppose you could
  choose two different random number generators for creating the
  secret \(r\); in one case the bits of \(r\) are uncorrelated, and in
  the other they are highly correlated. Which generator would you use
  and why?What isbinding? Why is it relevant to hiding information?What ispuzzle friendly?What is atamper-evident data structure? How would you
  implement a tamper-evident linear list? A tamper-evident tree?Describe the algorithm for cryptocurrency with a trusted agent to
  someone who knows absolutely nothing about cryptocurrency.How could the algorithm for cryptocurrency be used for a collection
  of distributed agents to keep track of a sequence of events, other
  than buying/selling currency?K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology



Figure Link: https://kmchandy.github.io/Crypto/CryptoCurrencyIntroduction/Slide3.jpg

Caption: Fig.3: Coins flowing into and out of a pay transaction.


Each element of the tamper-evident ledger has:a unique id;the type of the transaction, either create or pay;list of payers: only for pay transactions --- a list
  indicating the agents who pay coins into the transaction and the
  amounts that they put in;array of value-payee pairs: for both create and pay transactions --- an
  array of pairs(value, payee public key), where each
  pair in the array indicates that coins of the specified value are
  given to the payee with the specified public key.Example of a create transactionAn example of acreatetransaction is:(3146, create,  [(2.1, 7xxxx...), (3.2, 8xxxx)]).The id of this transaction is 3146, the type of the transaction is
create, and the array of value-payee pairs is[(2.1, 7xxxx...), (3.2, 8xxx)]In this transaction the bank creates a coin of value 2.1 and gives it
to the agent with public key 7xxxx..., and the bank also
creates a coin of value 3.2 and gives it
to the agent with public key 8xxxx...The pair:(transaction id, index into array of value-payee pairs)uniquely identifies a (value, payee) tuple.For example (3146, 0) --- 
transaction id 3146, and array index 0 --- identifies value-payee[0] of
transaction 3146 which is specified by the 2-tuple: (2.1, 7xxxx...). So, the
transaction id and index, (3146, 0), tells everybody that the agent
with public key 7xxxx received 2.1 units of coin in transaction 3146.When this
transaction is in a tamper-evident ledger, every agent from that point
onwards knows that agent 7xxxx received 2.1 coins. Any modifications of this
record can be detected.Likewise, (3146, 1) --- 
transaction id 3146, and array index 1 --- identifies value-payee[1] which
is the 2-tuple (3.2, 8xxxx...).Pay transactionCoins are transferred from payers to payees in a pay transaction.Coins flowing into a pay transaction from payersThe payers are identified by a list of 2-tuples, where each 2-tuple is(transaction id, index into array of value-payee pairs)wheretransaction idis the id of the transaction in the
tamper-evident ledger. As we said earlier, this pair uniquely identifies an agent and a
value that this agent acquired in this transaction.
For example the pair --- transaction id, index --- such as (3146, 0)
identifies the 2-tuple (2.1, 7xxxx...); this 2-tuple asserts
that the agent
with public key 7xxxx received 2.1 units of coin in 
transaction 3146. Theentire amountspecified in the 2-tuple (2.1 in our
example) is value that flows into thispaytransaction from the agent
with public key 7xxxx.Suppose the payers in a pay transaction are identified by the list:[(3146, 0), (7359, 3)]and suppose pair 3 in transaction 7359 is (3.2, 8xxxx). Then the total
amount of coins flowing into this pay transaction is 2.1 + 3.2, and
this amount is disbursed to payees.Fig.3: Coins flowing into and out of a pay transaction.Coins flowing out of a pay transaction to payeesThe outflow of coins is specified by an array of value-payee pairs,
exactly as in a create transaction.
The transaction clears all coins: the total inflow from payers is
equal to the total outflow to payees in a transaction.The system may provide incentives, such as payment of coins, to
managers (e.g. banks) of cryptocurrencies. In this case, one of the
payees is the bank itself.Managing amounts spent in a transactionA transaction-id, index pair --- such as (3146, 0)
identifies a 2-tuple such as (2.1, 7xxxx...); this 2-tuple asserts
that the agent
with public key 7xxxx received 2.1 units of coin in 
transaction 3146. Theentire amountspecified in the 2-tuple (2.1 in our
example) is value that flows into the transaction. What should this
agent do if it
wants to put in more than 2.1 coins into the transaction? Or less than
2.1 coins?To put in more value, the bank identifies other transaction-id, index
pairs in which this agent received coins. For example, say that (4539, 2)
identifies a 2-tuple (3.2, 7xxxx), and assume that the payers in this transaction
are specified by the pairs (3146, 0) and (4539, 2). The pair (3146, 0)
asserts that agent (7xxxx) received 2.1 coins and the pair (4539, 2)
asserts that the same agent received 3.2 coins. So the total amount of 
coins flowing into this
transaction from this agent (7xxxx) includes 2.1 + 3.2.To put in less value, the agent acts as both payer and payee; the net
value that this agent pays out to other agents is the difference
between the amount that this agent puts in and takes out. For example,
if agent with public key 7xxxx wants to put in 1.9 coins into this
transaction its payer information can be given by the transaction-id,
index pair (3146, 0) which asserts that the agent received 2.1 coins
and the same agent is a payee that withdraws 0.2 coins.Preventing Double SpendingHow does the system prevent an agent from using the
same coin twice?For example, the transaction id, index pair
(3146, 0) identifies the 2-tuple (2.1, 7xxxx...); this tuple
asserts that the agent
with public key 7xxxx received 2.1 units of coin in 
transaction 3146. Why can't the agent with public key 7xxxx use the
2.1 coins that it received to buy items from Amazon and later use
the same 2.1 coins to buy more items from Walmart?The bank checks that the agent hasn't already spent the coin that it
is putting into a transaction.Before permitting the transaction that double-spends the 2.1 coins
with Walmart, the bank inspects the tamper-evident ledger
for all transactions after transaction 3146 and before the Walmart
transaction to ensure that the agent (7xxxx) hasn't already spent the
2.1 coins that it got in transaction 3146. The transaction that spends
the coins at Amazon will show up in the ledger, and so the transaction
with Walmart will not be allowed.Every agent that has the bank's hash pointer to the end of the
tamper-evident ledger can inspect the ledger to check that
double-spending hasn't occurred.The bank signs a valid transaction and appends it to the
tamper-evident ledger. All agents can see the bank's signature and
verify that nobody (not even the bank) has tampered with the
tamper-evident ledger.Example of a pay transactionAn example of the specification of a pay transaction is:(9431, pay,
  [(3146, 0), (4731, 2)],
  [(0.7, 7xxxx...), (4.6, 9xxxx...)]
).The id of this transaction is 9431; the type of the transaction is
pay; the payers into the transaction are identified by the pairs of
(transaction-id, index): (3146, 0), and (4731, 2); and the
payee array is [(0.7, 7xxxx...), (4.6, 9xxxx...)].Transaction validityThe bank appends a transaction toLif and only if the
transaction is valid. The bank checks for validity by carrying out the
following steps:The bank verifies that the payers into the transaction signed the
  transaction.The bank checks that the total value of coins paid out from the
  transaction does not
  exceed the total value paid in to the transaction. (If the value paid
  in exceeds the value paid out then the bank takes the difference as
  a transaction fee. More about fees later.)The bank verifies that the payers' claims to have received coins in
  previous transactions is genuine. For example,
  if the agent with
  public key 7xxxx... claims to have received coins worth 2.1 in the
  transaction with id 10, and payee array index 0, then the bank
  verifies this claim by that transaction.The bank ensures that coins paid into the transaction haven't
  already been spent.Optimizations: Blocks and Block ChainVerifying large numbers of small transactions requires more
computation than verifying small numbers of blocks of many transactions.
Ablock chainis a tamper-evident ledger in which
each element of the ledger is a block consisting of many transactions.
A block of transactions can be aggregated into a single large
transaction by aggregating all the payers and payees of the smaller
transactions.The amount of computation decreases as the number of transactions in a
block increases. The time required to fill a block with transactions
is larger when the number of transactions to fill the block increases.Checking the Trusted AgentConsider a system in which the trusted agent broadcasts its current
copy of the tamper-evident ledger to all agents.
Every agent can inspect its copy of the tamper-evident ledger to determine whether the
ledger has been tampered with. So, every agent can validate its trust
in the trusted agent; however, this validation
suffers from a crucial problem: Agents may only
have copies ofold, staleversions of the ledger.
By the time that an agent receives a copy of the ledger, the trusted
agent may have added more transactions to the ledger.Fig.4: Old Copy is a Prefix of the Block ChainAn old copy of the ledger can differ from the current copy in only one
way: the current copy may have transactions appended to the end
of the old copy. So all agents can validatepastbehavior of the
trusted agent. An agent cannot, however, treat its copy of the ledger
as the master copy because the agent may not have the transactions
added most recently to the ledger.In the next module we will see how the Bitcoin algorithm addresses
this problem.Advantages of this cryptocurrencyAny agent can get a copy of the tamper-evident ledger and verify that all
transactions in the ledger are valid. Any agent can verify that
the only way in which the ledger is modified is that elements are
append to its tail; all that the agent needs to do is to check that
the pointer to the tail is modified only by appending
elements. Because the ledger is tamper-evident, an agent can check
that the ledger doesn't change while the hash pointer to the end
of the ledger doesn't change.The bank can't forge a transaction because all payers sign the
transaction. Agents can remain anonymous because an agent's only
public information is the agent's public key, and an agent can create
multiple public keys. Every agent can verify the correctness of every
transaction.Disadvantages of this cryptocurrencyUsers may not trust the bank. Transactions are not private because the
bank has a record of all transactions. And the bank is a single point
of failure.Next we'll look at Bitcoin's algorithms for
implementing a cryptocurrency without trusted agents.ReviewWhat iscollision resistance? Why is it relevant to
  cryptocurrencies?Consider the example ofhidingin which Megan hides her
  predictions for the winner of the World Cup. Suppose you could
  choose two different random number generators for creating the
  secret \(r\); in one case the bits of \(r\) are uncorrelated, and in
  the other they are highly correlated. Which generator would you use
  and why?What isbinding? Why is it relevant to hiding information?What ispuzzle friendly?What is atamper-evident data structure? How would you
  implement a tamper-evident linear list? A tamper-evident tree?Describe the algorithm for cryptocurrency with a trusted agent to
  someone who knows absolutely nothing about cryptocurrency.How could the algorithm for cryptocurrency be used for a collection
  of distributed agents to keep track of a sequence of events, other
  than buying/selling currency?K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology



Figure Link: https://kmchandy.github.io/Crypto/CryptoCurrencyIntroduction/Slide3.jpg

Caption: Fig.3: Coins flowing into and out of a pay transaction.


The pair:(transaction id, index into array of value-payee pairs)uniquely identifies a (value, payee) tuple.For example (3146, 0) --- 
transaction id 3146, and array index 0 --- identifies value-payee[0] of
transaction 3146 which is specified by the 2-tuple: (2.1, 7xxxx...). So, the
transaction id and index, (3146, 0), tells everybody that the agent
with public key 7xxxx received 2.1 units of coin in transaction 3146.When this
transaction is in a tamper-evident ledger, every agent from that point
onwards knows that agent 7xxxx received 2.1 coins. Any modifications of this
record can be detected.Likewise, (3146, 1) --- 
transaction id 3146, and array index 1 --- identifies value-payee[1] which
is the 2-tuple (3.2, 8xxxx...).Pay transactionCoins are transferred from payers to payees in a pay transaction.Coins flowing into a pay transaction from payersThe payers are identified by a list of 2-tuples, where each 2-tuple is(transaction id, index into array of value-payee pairs)wheretransaction idis the id of the transaction in the
tamper-evident ledger. As we said earlier, this pair uniquely identifies an agent and a
value that this agent acquired in this transaction.
For example the pair --- transaction id, index --- such as (3146, 0)
identifies the 2-tuple (2.1, 7xxxx...); this 2-tuple asserts
that the agent
with public key 7xxxx received 2.1 units of coin in 
transaction 3146. Theentire amountspecified in the 2-tuple (2.1 in our
example) is value that flows into thispaytransaction from the agent
with public key 7xxxx.Suppose the payers in a pay transaction are identified by the list:[(3146, 0), (7359, 3)]and suppose pair 3 in transaction 7359 is (3.2, 8xxxx). Then the total
amount of coins flowing into this pay transaction is 2.1 + 3.2, and
this amount is disbursed to payees.Fig.3: Coins flowing into and out of a pay transaction.Coins flowing out of a pay transaction to payeesThe outflow of coins is specified by an array of value-payee pairs,
exactly as in a create transaction.
The transaction clears all coins: the total inflow from payers is
equal to the total outflow to payees in a transaction.The system may provide incentives, such as payment of coins, to
managers (e.g. banks) of cryptocurrencies. In this case, one of the
payees is the bank itself.Managing amounts spent in a transactionA transaction-id, index pair --- such as (3146, 0)
identifies a 2-tuple such as (2.1, 7xxxx...); this 2-tuple asserts
that the agent
with public key 7xxxx received 2.1 units of coin in 
transaction 3146. Theentire amountspecified in the 2-tuple (2.1 in our
example) is value that flows into the transaction. What should this
agent do if it
wants to put in more than 2.1 coins into the transaction? Or less than
2.1 coins?To put in more value, the bank identifies other transaction-id, index
pairs in which this agent received coins. For example, say that (4539, 2)
identifies a 2-tuple (3.2, 7xxxx), and assume that the payers in this transaction
are specified by the pairs (3146, 0) and (4539, 2). The pair (3146, 0)
asserts that agent (7xxxx) received 2.1 coins and the pair (4539, 2)
asserts that the same agent received 3.2 coins. So the total amount of 
coins flowing into this
transaction from this agent (7xxxx) includes 2.1 + 3.2.To put in less value, the agent acts as both payer and payee; the net
value that this agent pays out to other agents is the difference
between the amount that this agent puts in and takes out. For example,
if agent with public key 7xxxx wants to put in 1.9 coins into this
transaction its payer information can be given by the transaction-id,
index pair (3146, 0) which asserts that the agent received 2.1 coins
and the same agent is a payee that withdraws 0.2 coins.Preventing Double SpendingHow does the system prevent an agent from using the
same coin twice?For example, the transaction id, index pair
(3146, 0) identifies the 2-tuple (2.1, 7xxxx...); this tuple
asserts that the agent
with public key 7xxxx received 2.1 units of coin in 
transaction 3146. Why can't the agent with public key 7xxxx use the
2.1 coins that it received to buy items from Amazon and later use
the same 2.1 coins to buy more items from Walmart?The bank checks that the agent hasn't already spent the coin that it
is putting into a transaction.Before permitting the transaction that double-spends the 2.1 coins
with Walmart, the bank inspects the tamper-evident ledger
for all transactions after transaction 3146 and before the Walmart
transaction to ensure that the agent (7xxxx) hasn't already spent the
2.1 coins that it got in transaction 3146. The transaction that spends
the coins at Amazon will show up in the ledger, and so the transaction
with Walmart will not be allowed.Every agent that has the bank's hash pointer to the end of the
tamper-evident ledger can inspect the ledger to check that
double-spending hasn't occurred.The bank signs a valid transaction and appends it to the
tamper-evident ledger. All agents can see the bank's signature and
verify that nobody (not even the bank) has tampered with the
tamper-evident ledger.Example of a pay transactionAn example of the specification of a pay transaction is:(9431, pay,
  [(3146, 0), (4731, 2)],
  [(0.7, 7xxxx...), (4.6, 9xxxx...)]
).The id of this transaction is 9431; the type of the transaction is
pay; the payers into the transaction are identified by the pairs of
(transaction-id, index): (3146, 0), and (4731, 2); and the
payee array is [(0.7, 7xxxx...), (4.6, 9xxxx...)].Transaction validityThe bank appends a transaction toLif and only if the
transaction is valid. The bank checks for validity by carrying out the
following steps:The bank verifies that the payers into the transaction signed the
  transaction.The bank checks that the total value of coins paid out from the
  transaction does not
  exceed the total value paid in to the transaction. (If the value paid
  in exceeds the value paid out then the bank takes the difference as
  a transaction fee. More about fees later.)The bank verifies that the payers' claims to have received coins in
  previous transactions is genuine. For example,
  if the agent with
  public key 7xxxx... claims to have received coins worth 2.1 in the
  transaction with id 10, and payee array index 0, then the bank
  verifies this claim by that transaction.The bank ensures that coins paid into the transaction haven't
  already been spent.Optimizations: Blocks and Block ChainVerifying large numbers of small transactions requires more
computation than verifying small numbers of blocks of many transactions.
Ablock chainis a tamper-evident ledger in which
each element of the ledger is a block consisting of many transactions.
A block of transactions can be aggregated into a single large
transaction by aggregating all the payers and payees of the smaller
transactions.The amount of computation decreases as the number of transactions in a
block increases. The time required to fill a block with transactions
is larger when the number of transactions to fill the block increases.Checking the Trusted AgentConsider a system in which the trusted agent broadcasts its current
copy of the tamper-evident ledger to all agents.
Every agent can inspect its copy of the tamper-evident ledger to determine whether the
ledger has been tampered with. So, every agent can validate its trust
in the trusted agent; however, this validation
suffers from a crucial problem: Agents may only
have copies ofold, staleversions of the ledger.
By the time that an agent receives a copy of the ledger, the trusted
agent may have added more transactions to the ledger.Fig.4: Old Copy is a Prefix of the Block ChainAn old copy of the ledger can differ from the current copy in only one
way: the current copy may have transactions appended to the end
of the old copy. So all agents can validatepastbehavior of the
trusted agent. An agent cannot, however, treat its copy of the ledger
as the master copy because the agent may not have the transactions
added most recently to the ledger.In the next module we will see how the Bitcoin algorithm addresses
this problem.Advantages of this cryptocurrencyAny agent can get a copy of the tamper-evident ledger and verify that all
transactions in the ledger are valid. Any agent can verify that
the only way in which the ledger is modified is that elements are
append to its tail; all that the agent needs to do is to check that
the pointer to the tail is modified only by appending
elements. Because the ledger is tamper-evident, an agent can check
that the ledger doesn't change while the hash pointer to the end
of the ledger doesn't change.The bank can't forge a transaction because all payers sign the
transaction. Agents can remain anonymous because an agent's only
public information is the agent's public key, and an agent can create
multiple public keys. Every agent can verify the correctness of every
transaction.Disadvantages of this cryptocurrencyUsers may not trust the bank. Transactions are not private because the
bank has a record of all transactions. And the bank is a single point
of failure.Next we'll look at Bitcoin's algorithms for
implementing a cryptocurrency without trusted agents.ReviewWhat iscollision resistance? Why is it relevant to
  cryptocurrencies?Consider the example ofhidingin which Megan hides her
  predictions for the winner of the World Cup. Suppose you could
  choose two different random number generators for creating the
  secret \(r\); in one case the bits of \(r\) are uncorrelated, and in
  the other they are highly correlated. Which generator would you use
  and why?What isbinding? Why is it relevant to hiding information?What ispuzzle friendly?What is atamper-evident data structure? How would you
  implement a tamper-evident linear list? A tamper-evident tree?Describe the algorithm for cryptocurrency with a trusted agent to
  someone who knows absolutely nothing about cryptocurrency.How could the algorithm for cryptocurrency be used for a collection
  of distributed agents to keep track of a sequence of events, other
  than buying/selling currency?K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology



Figure Link: https://kmchandy.github.io/Crypto/CryptoCurrencyIntroduction/Slide3.jpg

Caption: Fig.3: Coins flowing into and out of a pay transaction.


For example (3146, 0) --- 
transaction id 3146, and array index 0 --- identifies value-payee[0] of
transaction 3146 which is specified by the 2-tuple: (2.1, 7xxxx...). So, the
transaction id and index, (3146, 0), tells everybody that the agent
with public key 7xxxx received 2.1 units of coin in transaction 3146.When this
transaction is in a tamper-evident ledger, every agent from that point
onwards knows that agent 7xxxx received 2.1 coins. Any modifications of this
record can be detected.Likewise, (3146, 1) --- 
transaction id 3146, and array index 1 --- identifies value-payee[1] which
is the 2-tuple (3.2, 8xxxx...).Pay transactionCoins are transferred from payers to payees in a pay transaction.Coins flowing into a pay transaction from payersThe payers are identified by a list of 2-tuples, where each 2-tuple is(transaction id, index into array of value-payee pairs)wheretransaction idis the id of the transaction in the
tamper-evident ledger. As we said earlier, this pair uniquely identifies an agent and a
value that this agent acquired in this transaction.
For example the pair --- transaction id, index --- such as (3146, 0)
identifies the 2-tuple (2.1, 7xxxx...); this 2-tuple asserts
that the agent
with public key 7xxxx received 2.1 units of coin in 
transaction 3146. Theentire amountspecified in the 2-tuple (2.1 in our
example) is value that flows into thispaytransaction from the agent
with public key 7xxxx.Suppose the payers in a pay transaction are identified by the list:[(3146, 0), (7359, 3)]and suppose pair 3 in transaction 7359 is (3.2, 8xxxx). Then the total
amount of coins flowing into this pay transaction is 2.1 + 3.2, and
this amount is disbursed to payees.Fig.3: Coins flowing into and out of a pay transaction.Coins flowing out of a pay transaction to payeesThe outflow of coins is specified by an array of value-payee pairs,
exactly as in a create transaction.
The transaction clears all coins: the total inflow from payers is
equal to the total outflow to payees in a transaction.The system may provide incentives, such as payment of coins, to
managers (e.g. banks) of cryptocurrencies. In this case, one of the
payees is the bank itself.Managing amounts spent in a transactionA transaction-id, index pair --- such as (3146, 0)
identifies a 2-tuple such as (2.1, 7xxxx...); this 2-tuple asserts
that the agent
with public key 7xxxx received 2.1 units of coin in 
transaction 3146. Theentire amountspecified in the 2-tuple (2.1 in our
example) is value that flows into the transaction. What should this
agent do if it
wants to put in more than 2.1 coins into the transaction? Or less than
2.1 coins?To put in more value, the bank identifies other transaction-id, index
pairs in which this agent received coins. For example, say that (4539, 2)
identifies a 2-tuple (3.2, 7xxxx), and assume that the payers in this transaction
are specified by the pairs (3146, 0) and (4539, 2). The pair (3146, 0)
asserts that agent (7xxxx) received 2.1 coins and the pair (4539, 2)
asserts that the same agent received 3.2 coins. So the total amount of 
coins flowing into this
transaction from this agent (7xxxx) includes 2.1 + 3.2.To put in less value, the agent acts as both payer and payee; the net
value that this agent pays out to other agents is the difference
between the amount that this agent puts in and takes out. For example,
if agent with public key 7xxxx wants to put in 1.9 coins into this
transaction its payer information can be given by the transaction-id,
index pair (3146, 0) which asserts that the agent received 2.1 coins
and the same agent is a payee that withdraws 0.2 coins.Preventing Double SpendingHow does the system prevent an agent from using the
same coin twice?For example, the transaction id, index pair
(3146, 0) identifies the 2-tuple (2.1, 7xxxx...); this tuple
asserts that the agent
with public key 7xxxx received 2.1 units of coin in 
transaction 3146. Why can't the agent with public key 7xxxx use the
2.1 coins that it received to buy items from Amazon and later use
the same 2.1 coins to buy more items from Walmart?The bank checks that the agent hasn't already spent the coin that it
is putting into a transaction.Before permitting the transaction that double-spends the 2.1 coins
with Walmart, the bank inspects the tamper-evident ledger
for all transactions after transaction 3146 and before the Walmart
transaction to ensure that the agent (7xxxx) hasn't already spent the
2.1 coins that it got in transaction 3146. The transaction that spends
the coins at Amazon will show up in the ledger, and so the transaction
with Walmart will not be allowed.Every agent that has the bank's hash pointer to the end of the
tamper-evident ledger can inspect the ledger to check that
double-spending hasn't occurred.The bank signs a valid transaction and appends it to the
tamper-evident ledger. All agents can see the bank's signature and
verify that nobody (not even the bank) has tampered with the
tamper-evident ledger.Example of a pay transactionAn example of the specification of a pay transaction is:(9431, pay,
  [(3146, 0), (4731, 2)],
  [(0.7, 7xxxx...), (4.6, 9xxxx...)]
).The id of this transaction is 9431; the type of the transaction is
pay; the payers into the transaction are identified by the pairs of
(transaction-id, index): (3146, 0), and (4731, 2); and the
payee array is [(0.7, 7xxxx...), (4.6, 9xxxx...)].Transaction validityThe bank appends a transaction toLif and only if the
transaction is valid. The bank checks for validity by carrying out the
following steps:The bank verifies that the payers into the transaction signed the
  transaction.The bank checks that the total value of coins paid out from the
  transaction does not
  exceed the total value paid in to the transaction. (If the value paid
  in exceeds the value paid out then the bank takes the difference as
  a transaction fee. More about fees later.)The bank verifies that the payers' claims to have received coins in
  previous transactions is genuine. For example,
  if the agent with
  public key 7xxxx... claims to have received coins worth 2.1 in the
  transaction with id 10, and payee array index 0, then the bank
  verifies this claim by that transaction.The bank ensures that coins paid into the transaction haven't
  already been spent.Optimizations: Blocks and Block ChainVerifying large numbers of small transactions requires more
computation than verifying small numbers of blocks of many transactions.
Ablock chainis a tamper-evident ledger in which
each element of the ledger is a block consisting of many transactions.
A block of transactions can be aggregated into a single large
transaction by aggregating all the payers and payees of the smaller
transactions.The amount of computation decreases as the number of transactions in a
block increases. The time required to fill a block with transactions
is larger when the number of transactions to fill the block increases.Checking the Trusted AgentConsider a system in which the trusted agent broadcasts its current
copy of the tamper-evident ledger to all agents.
Every agent can inspect its copy of the tamper-evident ledger to determine whether the
ledger has been tampered with. So, every agent can validate its trust
in the trusted agent; however, this validation
suffers from a crucial problem: Agents may only
have copies ofold, staleversions of the ledger.
By the time that an agent receives a copy of the ledger, the trusted
agent may have added more transactions to the ledger.Fig.4: Old Copy is a Prefix of the Block ChainAn old copy of the ledger can differ from the current copy in only one
way: the current copy may have transactions appended to the end
of the old copy. So all agents can validatepastbehavior of the
trusted agent. An agent cannot, however, treat its copy of the ledger
as the master copy because the agent may not have the transactions
added most recently to the ledger.In the next module we will see how the Bitcoin algorithm addresses
this problem.Advantages of this cryptocurrencyAny agent can get a copy of the tamper-evident ledger and verify that all
transactions in the ledger are valid. Any agent can verify that
the only way in which the ledger is modified is that elements are
append to its tail; all that the agent needs to do is to check that
the pointer to the tail is modified only by appending
elements. Because the ledger is tamper-evident, an agent can check
that the ledger doesn't change while the hash pointer to the end
of the ledger doesn't change.The bank can't forge a transaction because all payers sign the
transaction. Agents can remain anonymous because an agent's only
public information is the agent's public key, and an agent can create
multiple public keys. Every agent can verify the correctness of every
transaction.Disadvantages of this cryptocurrencyUsers may not trust the bank. Transactions are not private because the
bank has a record of all transactions. And the bank is a single point
of failure.Next we'll look at Bitcoin's algorithms for
implementing a cryptocurrency without trusted agents.ReviewWhat iscollision resistance? Why is it relevant to
  cryptocurrencies?Consider the example ofhidingin which Megan hides her
  predictions for the winner of the World Cup. Suppose you could
  choose two different random number generators for creating the
  secret \(r\); in one case the bits of \(r\) are uncorrelated, and in
  the other they are highly correlated. Which generator would you use
  and why?What isbinding? Why is it relevant to hiding information?What ispuzzle friendly?What is atamper-evident data structure? How would you
  implement a tamper-evident linear list? A tamper-evident tree?Describe the algorithm for cryptocurrency with a trusted agent to
  someone who knows absolutely nothing about cryptocurrency.How could the algorithm for cryptocurrency be used for a collection
  of distributed agents to keep track of a sequence of events, other
  than buying/selling currency?K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology



Figure Link: https://kmchandy.github.io/Crypto/CryptoCurrencyIntroduction/Slide3.jpg

Caption: Fig.3: Coins flowing into and out of a pay transaction.


When this
transaction is in a tamper-evident ledger, every agent from that point
onwards knows that agent 7xxxx received 2.1 coins. Any modifications of this
record can be detected.Likewise, (3146, 1) --- 
transaction id 3146, and array index 1 --- identifies value-payee[1] which
is the 2-tuple (3.2, 8xxxx...).Pay transactionCoins are transferred from payers to payees in a pay transaction.Coins flowing into a pay transaction from payersThe payers are identified by a list of 2-tuples, where each 2-tuple is(transaction id, index into array of value-payee pairs)wheretransaction idis the id of the transaction in the
tamper-evident ledger. As we said earlier, this pair uniquely identifies an agent and a
value that this agent acquired in this transaction.
For example the pair --- transaction id, index --- such as (3146, 0)
identifies the 2-tuple (2.1, 7xxxx...); this 2-tuple asserts
that the agent
with public key 7xxxx received 2.1 units of coin in 
transaction 3146. Theentire amountspecified in the 2-tuple (2.1 in our
example) is value that flows into thispaytransaction from the agent
with public key 7xxxx.Suppose the payers in a pay transaction are identified by the list:[(3146, 0), (7359, 3)]and suppose pair 3 in transaction 7359 is (3.2, 8xxxx). Then the total
amount of coins flowing into this pay transaction is 2.1 + 3.2, and
this amount is disbursed to payees.Fig.3: Coins flowing into and out of a pay transaction.Coins flowing out of a pay transaction to payeesThe outflow of coins is specified by an array of value-payee pairs,
exactly as in a create transaction.
The transaction clears all coins: the total inflow from payers is
equal to the total outflow to payees in a transaction.The system may provide incentives, such as payment of coins, to
managers (e.g. banks) of cryptocurrencies. In this case, one of the
payees is the bank itself.Managing amounts spent in a transactionA transaction-id, index pair --- such as (3146, 0)
identifies a 2-tuple such as (2.1, 7xxxx...); this 2-tuple asserts
that the agent
with public key 7xxxx received 2.1 units of coin in 
transaction 3146. Theentire amountspecified in the 2-tuple (2.1 in our
example) is value that flows into the transaction. What should this
agent do if it
wants to put in more than 2.1 coins into the transaction? Or less than
2.1 coins?To put in more value, the bank identifies other transaction-id, index
pairs in which this agent received coins. For example, say that (4539, 2)
identifies a 2-tuple (3.2, 7xxxx), and assume that the payers in this transaction
are specified by the pairs (3146, 0) and (4539, 2). The pair (3146, 0)
asserts that agent (7xxxx) received 2.1 coins and the pair (4539, 2)
asserts that the same agent received 3.2 coins. So the total amount of 
coins flowing into this
transaction from this agent (7xxxx) includes 2.1 + 3.2.To put in less value, the agent acts as both payer and payee; the net
value that this agent pays out to other agents is the difference
between the amount that this agent puts in and takes out. For example,
if agent with public key 7xxxx wants to put in 1.9 coins into this
transaction its payer information can be given by the transaction-id,
index pair (3146, 0) which asserts that the agent received 2.1 coins
and the same agent is a payee that withdraws 0.2 coins.Preventing Double SpendingHow does the system prevent an agent from using the
same coin twice?For example, the transaction id, index pair
(3146, 0) identifies the 2-tuple (2.1, 7xxxx...); this tuple
asserts that the agent
with public key 7xxxx received 2.1 units of coin in 
transaction 3146. Why can't the agent with public key 7xxxx use the
2.1 coins that it received to buy items from Amazon and later use
the same 2.1 coins to buy more items from Walmart?The bank checks that the agent hasn't already spent the coin that it
is putting into a transaction.Before permitting the transaction that double-spends the 2.1 coins
with Walmart, the bank inspects the tamper-evident ledger
for all transactions after transaction 3146 and before the Walmart
transaction to ensure that the agent (7xxxx) hasn't already spent the
2.1 coins that it got in transaction 3146. The transaction that spends
the coins at Amazon will show up in the ledger, and so the transaction
with Walmart will not be allowed.Every agent that has the bank's hash pointer to the end of the
tamper-evident ledger can inspect the ledger to check that
double-spending hasn't occurred.The bank signs a valid transaction and appends it to the
tamper-evident ledger. All agents can see the bank's signature and
verify that nobody (not even the bank) has tampered with the
tamper-evident ledger.Example of a pay transactionAn example of the specification of a pay transaction is:(9431, pay,
  [(3146, 0), (4731, 2)],
  [(0.7, 7xxxx...), (4.6, 9xxxx...)]
).The id of this transaction is 9431; the type of the transaction is
pay; the payers into the transaction are identified by the pairs of
(transaction-id, index): (3146, 0), and (4731, 2); and the
payee array is [(0.7, 7xxxx...), (4.6, 9xxxx...)].Transaction validityThe bank appends a transaction toLif and only if the
transaction is valid. The bank checks for validity by carrying out the
following steps:The bank verifies that the payers into the transaction signed the
  transaction.The bank checks that the total value of coins paid out from the
  transaction does not
  exceed the total value paid in to the transaction. (If the value paid
  in exceeds the value paid out then the bank takes the difference as
  a transaction fee. More about fees later.)The bank verifies that the payers' claims to have received coins in
  previous transactions is genuine. For example,
  if the agent with
  public key 7xxxx... claims to have received coins worth 2.1 in the
  transaction with id 10, and payee array index 0, then the bank
  verifies this claim by that transaction.The bank ensures that coins paid into the transaction haven't
  already been spent.Optimizations: Blocks and Block ChainVerifying large numbers of small transactions requires more
computation than verifying small numbers of blocks of many transactions.
Ablock chainis a tamper-evident ledger in which
each element of the ledger is a block consisting of many transactions.
A block of transactions can be aggregated into a single large
transaction by aggregating all the payers and payees of the smaller
transactions.The amount of computation decreases as the number of transactions in a
block increases. The time required to fill a block with transactions
is larger when the number of transactions to fill the block increases.Checking the Trusted AgentConsider a system in which the trusted agent broadcasts its current
copy of the tamper-evident ledger to all agents.
Every agent can inspect its copy of the tamper-evident ledger to determine whether the
ledger has been tampered with. So, every agent can validate its trust
in the trusted agent; however, this validation
suffers from a crucial problem: Agents may only
have copies ofold, staleversions of the ledger.
By the time that an agent receives a copy of the ledger, the trusted
agent may have added more transactions to the ledger.Fig.4: Old Copy is a Prefix of the Block ChainAn old copy of the ledger can differ from the current copy in only one
way: the current copy may have transactions appended to the end
of the old copy. So all agents can validatepastbehavior of the
trusted agent. An agent cannot, however, treat its copy of the ledger
as the master copy because the agent may not have the transactions
added most recently to the ledger.In the next module we will see how the Bitcoin algorithm addresses
this problem.Advantages of this cryptocurrencyAny agent can get a copy of the tamper-evident ledger and verify that all
transactions in the ledger are valid. Any agent can verify that
the only way in which the ledger is modified is that elements are
append to its tail; all that the agent needs to do is to check that
the pointer to the tail is modified only by appending
elements. Because the ledger is tamper-evident, an agent can check
that the ledger doesn't change while the hash pointer to the end
of the ledger doesn't change.The bank can't forge a transaction because all payers sign the
transaction. Agents can remain anonymous because an agent's only
public information is the agent's public key, and an agent can create
multiple public keys. Every agent can verify the correctness of every
transaction.Disadvantages of this cryptocurrencyUsers may not trust the bank. Transactions are not private because the
bank has a record of all transactions. And the bank is a single point
of failure.Next we'll look at Bitcoin's algorithms for
implementing a cryptocurrency without trusted agents.ReviewWhat iscollision resistance? Why is it relevant to
  cryptocurrencies?Consider the example ofhidingin which Megan hides her
  predictions for the winner of the World Cup. Suppose you could
  choose two different random number generators for creating the
  secret \(r\); in one case the bits of \(r\) are uncorrelated, and in
  the other they are highly correlated. Which generator would you use
  and why?What isbinding? Why is it relevant to hiding information?What ispuzzle friendly?What is atamper-evident data structure? How would you
  implement a tamper-evident linear list? A tamper-evident tree?Describe the algorithm for cryptocurrency with a trusted agent to
  someone who knows absolutely nothing about cryptocurrency.How could the algorithm for cryptocurrency be used for a collection
  of distributed agents to keep track of a sequence of events, other
  than buying/selling currency?K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology



Figure Link: https://kmchandy.github.io/Crypto/CryptoCurrencyIntroduction/Slide3.jpg

Caption: Fig.3: Coins flowing into and out of a pay transaction.


Likewise, (3146, 1) --- 
transaction id 3146, and array index 1 --- identifies value-payee[1] which
is the 2-tuple (3.2, 8xxxx...).Pay transactionCoins are transferred from payers to payees in a pay transaction.Coins flowing into a pay transaction from payersThe payers are identified by a list of 2-tuples, where each 2-tuple is(transaction id, index into array of value-payee pairs)wheretransaction idis the id of the transaction in the
tamper-evident ledger. As we said earlier, this pair uniquely identifies an agent and a
value that this agent acquired in this transaction.
For example the pair --- transaction id, index --- such as (3146, 0)
identifies the 2-tuple (2.1, 7xxxx...); this 2-tuple asserts
that the agent
with public key 7xxxx received 2.1 units of coin in 
transaction 3146. Theentire amountspecified in the 2-tuple (2.1 in our
example) is value that flows into thispaytransaction from the agent
with public key 7xxxx.Suppose the payers in a pay transaction are identified by the list:[(3146, 0), (7359, 3)]and suppose pair 3 in transaction 7359 is (3.2, 8xxxx). Then the total
amount of coins flowing into this pay transaction is 2.1 + 3.2, and
this amount is disbursed to payees.Fig.3: Coins flowing into and out of a pay transaction.Coins flowing out of a pay transaction to payeesThe outflow of coins is specified by an array of value-payee pairs,
exactly as in a create transaction.
The transaction clears all coins: the total inflow from payers is
equal to the total outflow to payees in a transaction.The system may provide incentives, such as payment of coins, to
managers (e.g. banks) of cryptocurrencies. In this case, one of the
payees is the bank itself.Managing amounts spent in a transactionA transaction-id, index pair --- such as (3146, 0)
identifies a 2-tuple such as (2.1, 7xxxx...); this 2-tuple asserts
that the agent
with public key 7xxxx received 2.1 units of coin in 
transaction 3146. Theentire amountspecified in the 2-tuple (2.1 in our
example) is value that flows into the transaction. What should this
agent do if it
wants to put in more than 2.1 coins into the transaction? Or less than
2.1 coins?To put in more value, the bank identifies other transaction-id, index
pairs in which this agent received coins. For example, say that (4539, 2)
identifies a 2-tuple (3.2, 7xxxx), and assume that the payers in this transaction
are specified by the pairs (3146, 0) and (4539, 2). The pair (3146, 0)
asserts that agent (7xxxx) received 2.1 coins and the pair (4539, 2)
asserts that the same agent received 3.2 coins. So the total amount of 
coins flowing into this
transaction from this agent (7xxxx) includes 2.1 + 3.2.To put in less value, the agent acts as both payer and payee; the net
value that this agent pays out to other agents is the difference
between the amount that this agent puts in and takes out. For example,
if agent with public key 7xxxx wants to put in 1.9 coins into this
transaction its payer information can be given by the transaction-id,
index pair (3146, 0) which asserts that the agent received 2.1 coins
and the same agent is a payee that withdraws 0.2 coins.Preventing Double SpendingHow does the system prevent an agent from using the
same coin twice?For example, the transaction id, index pair
(3146, 0) identifies the 2-tuple (2.1, 7xxxx...); this tuple
asserts that the agent
with public key 7xxxx received 2.1 units of coin in 
transaction 3146. Why can't the agent with public key 7xxxx use the
2.1 coins that it received to buy items from Amazon and later use
the same 2.1 coins to buy more items from Walmart?The bank checks that the agent hasn't already spent the coin that it
is putting into a transaction.Before permitting the transaction that double-spends the 2.1 coins
with Walmart, the bank inspects the tamper-evident ledger
for all transactions after transaction 3146 and before the Walmart
transaction to ensure that the agent (7xxxx) hasn't already spent the
2.1 coins that it got in transaction 3146. The transaction that spends
the coins at Amazon will show up in the ledger, and so the transaction
with Walmart will not be allowed.Every agent that has the bank's hash pointer to the end of the
tamper-evident ledger can inspect the ledger to check that
double-spending hasn't occurred.The bank signs a valid transaction and appends it to the
tamper-evident ledger. All agents can see the bank's signature and
verify that nobody (not even the bank) has tampered with the
tamper-evident ledger.Example of a pay transactionAn example of the specification of a pay transaction is:(9431, pay,
  [(3146, 0), (4731, 2)],
  [(0.7, 7xxxx...), (4.6, 9xxxx...)]
).The id of this transaction is 9431; the type of the transaction is
pay; the payers into the transaction are identified by the pairs of
(transaction-id, index): (3146, 0), and (4731, 2); and the
payee array is [(0.7, 7xxxx...), (4.6, 9xxxx...)].Transaction validityThe bank appends a transaction toLif and only if the
transaction is valid. The bank checks for validity by carrying out the
following steps:The bank verifies that the payers into the transaction signed the
  transaction.The bank checks that the total value of coins paid out from the
  transaction does not
  exceed the total value paid in to the transaction. (If the value paid
  in exceeds the value paid out then the bank takes the difference as
  a transaction fee. More about fees later.)The bank verifies that the payers' claims to have received coins in
  previous transactions is genuine. For example,
  if the agent with
  public key 7xxxx... claims to have received coins worth 2.1 in the
  transaction with id 10, and payee array index 0, then the bank
  verifies this claim by that transaction.The bank ensures that coins paid into the transaction haven't
  already been spent.Optimizations: Blocks and Block ChainVerifying large numbers of small transactions requires more
computation than verifying small numbers of blocks of many transactions.
Ablock chainis a tamper-evident ledger in which
each element of the ledger is a block consisting of many transactions.
A block of transactions can be aggregated into a single large
transaction by aggregating all the payers and payees of the smaller
transactions.The amount of computation decreases as the number of transactions in a
block increases. The time required to fill a block with transactions
is larger when the number of transactions to fill the block increases.Checking the Trusted AgentConsider a system in which the trusted agent broadcasts its current
copy of the tamper-evident ledger to all agents.
Every agent can inspect its copy of the tamper-evident ledger to determine whether the
ledger has been tampered with. So, every agent can validate its trust
in the trusted agent; however, this validation
suffers from a crucial problem: Agents may only
have copies ofold, staleversions of the ledger.
By the time that an agent receives a copy of the ledger, the trusted
agent may have added more transactions to the ledger.Fig.4: Old Copy is a Prefix of the Block ChainAn old copy of the ledger can differ from the current copy in only one
way: the current copy may have transactions appended to the end
of the old copy. So all agents can validatepastbehavior of the
trusted agent. An agent cannot, however, treat its copy of the ledger
as the master copy because the agent may not have the transactions
added most recently to the ledger.In the next module we will see how the Bitcoin algorithm addresses
this problem.Advantages of this cryptocurrencyAny agent can get a copy of the tamper-evident ledger and verify that all
transactions in the ledger are valid. Any agent can verify that
the only way in which the ledger is modified is that elements are
append to its tail; all that the agent needs to do is to check that
the pointer to the tail is modified only by appending
elements. Because the ledger is tamper-evident, an agent can check
that the ledger doesn't change while the hash pointer to the end
of the ledger doesn't change.The bank can't forge a transaction because all payers sign the
transaction. Agents can remain anonymous because an agent's only
public information is the agent's public key, and an agent can create
multiple public keys. Every agent can verify the correctness of every
transaction.Disadvantages of this cryptocurrencyUsers may not trust the bank. Transactions are not private because the
bank has a record of all transactions. And the bank is a single point
of failure.Next we'll look at Bitcoin's algorithms for
implementing a cryptocurrency without trusted agents.ReviewWhat iscollision resistance? Why is it relevant to
  cryptocurrencies?Consider the example ofhidingin which Megan hides her
  predictions for the winner of the World Cup. Suppose you could
  choose two different random number generators for creating the
  secret \(r\); in one case the bits of \(r\) are uncorrelated, and in
  the other they are highly correlated. Which generator would you use
  and why?What isbinding? Why is it relevant to hiding information?What ispuzzle friendly?What is atamper-evident data structure? How would you
  implement a tamper-evident linear list? A tamper-evident tree?Describe the algorithm for cryptocurrency with a trusted agent to
  someone who knows absolutely nothing about cryptocurrency.How could the algorithm for cryptocurrency be used for a collection
  of distributed agents to keep track of a sequence of events, other
  than buying/selling currency?K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology



Figure Link: https://kmchandy.github.io/Crypto/CryptoCurrencyIntroduction/Slide3.jpg

Caption: Fig.3: Coins flowing into and out of a pay transaction.


Suppose the payers in a pay transaction are identified by the list:[(3146, 0), (7359, 3)]and suppose pair 3 in transaction 7359 is (3.2, 8xxxx). Then the total
amount of coins flowing into this pay transaction is 2.1 + 3.2, and
this amount is disbursed to payees.Fig.3: Coins flowing into and out of a pay transaction.Coins flowing out of a pay transaction to payeesThe outflow of coins is specified by an array of value-payee pairs,
exactly as in a create transaction.
The transaction clears all coins: the total inflow from payers is
equal to the total outflow to payees in a transaction.The system may provide incentives, such as payment of coins, to
managers (e.g. banks) of cryptocurrencies. In this case, one of the
payees is the bank itself.Managing amounts spent in a transactionA transaction-id, index pair --- such as (3146, 0)
identifies a 2-tuple such as (2.1, 7xxxx...); this 2-tuple asserts
that the agent
with public key 7xxxx received 2.1 units of coin in 
transaction 3146. Theentire amountspecified in the 2-tuple (2.1 in our
example) is value that flows into the transaction. What should this
agent do if it
wants to put in more than 2.1 coins into the transaction? Or less than
2.1 coins?To put in more value, the bank identifies other transaction-id, index
pairs in which this agent received coins. For example, say that (4539, 2)
identifies a 2-tuple (3.2, 7xxxx), and assume that the payers in this transaction
are specified by the pairs (3146, 0) and (4539, 2). The pair (3146, 0)
asserts that agent (7xxxx) received 2.1 coins and the pair (4539, 2)
asserts that the same agent received 3.2 coins. So the total amount of 
coins flowing into this
transaction from this agent (7xxxx) includes 2.1 + 3.2.To put in less value, the agent acts as both payer and payee; the net
value that this agent pays out to other agents is the difference
between the amount that this agent puts in and takes out. For example,
if agent with public key 7xxxx wants to put in 1.9 coins into this
transaction its payer information can be given by the transaction-id,
index pair (3146, 0) which asserts that the agent received 2.1 coins
and the same agent is a payee that withdraws 0.2 coins.Preventing Double SpendingHow does the system prevent an agent from using the
same coin twice?For example, the transaction id, index pair
(3146, 0) identifies the 2-tuple (2.1, 7xxxx...); this tuple
asserts that the agent
with public key 7xxxx received 2.1 units of coin in 
transaction 3146. Why can't the agent with public key 7xxxx use the
2.1 coins that it received to buy items from Amazon and later use
the same 2.1 coins to buy more items from Walmart?The bank checks that the agent hasn't already spent the coin that it
is putting into a transaction.Before permitting the transaction that double-spends the 2.1 coins
with Walmart, the bank inspects the tamper-evident ledger
for all transactions after transaction 3146 and before the Walmart
transaction to ensure that the agent (7xxxx) hasn't already spent the
2.1 coins that it got in transaction 3146. The transaction that spends
the coins at Amazon will show up in the ledger, and so the transaction
with Walmart will not be allowed.Every agent that has the bank's hash pointer to the end of the
tamper-evident ledger can inspect the ledger to check that
double-spending hasn't occurred.The bank signs a valid transaction and appends it to the
tamper-evident ledger. All agents can see the bank's signature and
verify that nobody (not even the bank) has tampered with the
tamper-evident ledger.Example of a pay transactionAn example of the specification of a pay transaction is:(9431, pay,
  [(3146, 0), (4731, 2)],
  [(0.7, 7xxxx...), (4.6, 9xxxx...)]
).The id of this transaction is 9431; the type of the transaction is
pay; the payers into the transaction are identified by the pairs of
(transaction-id, index): (3146, 0), and (4731, 2); and the
payee array is [(0.7, 7xxxx...), (4.6, 9xxxx...)].Transaction validityThe bank appends a transaction toLif and only if the
transaction is valid. The bank checks for validity by carrying out the
following steps:The bank verifies that the payers into the transaction signed the
  transaction.The bank checks that the total value of coins paid out from the
  transaction does not
  exceed the total value paid in to the transaction. (If the value paid
  in exceeds the value paid out then the bank takes the difference as
  a transaction fee. More about fees later.)The bank verifies that the payers' claims to have received coins in
  previous transactions is genuine. For example,
  if the agent with
  public key 7xxxx... claims to have received coins worth 2.1 in the
  transaction with id 10, and payee array index 0, then the bank
  verifies this claim by that transaction.The bank ensures that coins paid into the transaction haven't
  already been spent.Optimizations: Blocks and Block ChainVerifying large numbers of small transactions requires more
computation than verifying small numbers of blocks of many transactions.
Ablock chainis a tamper-evident ledger in which
each element of the ledger is a block consisting of many transactions.
A block of transactions can be aggregated into a single large
transaction by aggregating all the payers and payees of the smaller
transactions.The amount of computation decreases as the number of transactions in a
block increases. The time required to fill a block with transactions
is larger when the number of transactions to fill the block increases.Checking the Trusted AgentConsider a system in which the trusted agent broadcasts its current
copy of the tamper-evident ledger to all agents.
Every agent can inspect its copy of the tamper-evident ledger to determine whether the
ledger has been tampered with. So, every agent can validate its trust
in the trusted agent; however, this validation
suffers from a crucial problem: Agents may only
have copies ofold, staleversions of the ledger.
By the time that an agent receives a copy of the ledger, the trusted
agent may have added more transactions to the ledger.Fig.4: Old Copy is a Prefix of the Block ChainAn old copy of the ledger can differ from the current copy in only one
way: the current copy may have transactions appended to the end
of the old copy. So all agents can validatepastbehavior of the
trusted agent. An agent cannot, however, treat its copy of the ledger
as the master copy because the agent may not have the transactions
added most recently to the ledger.In the next module we will see how the Bitcoin algorithm addresses
this problem.Advantages of this cryptocurrencyAny agent can get a copy of the tamper-evident ledger and verify that all
transactions in the ledger are valid. Any agent can verify that
the only way in which the ledger is modified is that elements are
append to its tail; all that the agent needs to do is to check that
the pointer to the tail is modified only by appending
elements. Because the ledger is tamper-evident, an agent can check
that the ledger doesn't change while the hash pointer to the end
of the ledger doesn't change.The bank can't forge a transaction because all payers sign the
transaction. Agents can remain anonymous because an agent's only
public information is the agent's public key, and an agent can create
multiple public keys. Every agent can verify the correctness of every
transaction.Disadvantages of this cryptocurrencyUsers may not trust the bank. Transactions are not private because the
bank has a record of all transactions. And the bank is a single point
of failure.Next we'll look at Bitcoin's algorithms for
implementing a cryptocurrency without trusted agents.ReviewWhat iscollision resistance? Why is it relevant to
  cryptocurrencies?Consider the example ofhidingin which Megan hides her
  predictions for the winner of the World Cup. Suppose you could
  choose two different random number generators for creating the
  secret \(r\); in one case the bits of \(r\) are uncorrelated, and in
  the other they are highly correlated. Which generator would you use
  and why?What isbinding? Why is it relevant to hiding information?What ispuzzle friendly?What is atamper-evident data structure? How would you
  implement a tamper-evident linear list? A tamper-evident tree?Describe the algorithm for cryptocurrency with a trusted agent to
  someone who knows absolutely nothing about cryptocurrency.How could the algorithm for cryptocurrency be used for a collection
  of distributed agents to keep track of a sequence of events, other
  than buying/selling currency?K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology



Figure Link: https://kmchandy.github.io/Crypto/CryptoCurrencyIntroduction/Slide3.jpg

Caption: Fig.3: Coins flowing into and out of a pay transaction.


The system may provide incentives, such as payment of coins, to
managers (e.g. banks) of cryptocurrencies. In this case, one of the
payees is the bank itself.Managing amounts spent in a transactionA transaction-id, index pair --- such as (3146, 0)
identifies a 2-tuple such as (2.1, 7xxxx...); this 2-tuple asserts
that the agent
with public key 7xxxx received 2.1 units of coin in 
transaction 3146. Theentire amountspecified in the 2-tuple (2.1 in our
example) is value that flows into the transaction. What should this
agent do if it
wants to put in more than 2.1 coins into the transaction? Or less than
2.1 coins?To put in more value, the bank identifies other transaction-id, index
pairs in which this agent received coins. For example, say that (4539, 2)
identifies a 2-tuple (3.2, 7xxxx), and assume that the payers in this transaction
are specified by the pairs (3146, 0) and (4539, 2). The pair (3146, 0)
asserts that agent (7xxxx) received 2.1 coins and the pair (4539, 2)
asserts that the same agent received 3.2 coins. So the total amount of 
coins flowing into this
transaction from this agent (7xxxx) includes 2.1 + 3.2.To put in less value, the agent acts as both payer and payee; the net
value that this agent pays out to other agents is the difference
between the amount that this agent puts in and takes out. For example,
if agent with public key 7xxxx wants to put in 1.9 coins into this
transaction its payer information can be given by the transaction-id,
index pair (3146, 0) which asserts that the agent received 2.1 coins
and the same agent is a payee that withdraws 0.2 coins.Preventing Double SpendingHow does the system prevent an agent from using the
same coin twice?For example, the transaction id, index pair
(3146, 0) identifies the 2-tuple (2.1, 7xxxx...); this tuple
asserts that the agent
with public key 7xxxx received 2.1 units of coin in 
transaction 3146. Why can't the agent with public key 7xxxx use the
2.1 coins that it received to buy items from Amazon and later use
the same 2.1 coins to buy more items from Walmart?The bank checks that the agent hasn't already spent the coin that it
is putting into a transaction.Before permitting the transaction that double-spends the 2.1 coins
with Walmart, the bank inspects the tamper-evident ledger
for all transactions after transaction 3146 and before the Walmart
transaction to ensure that the agent (7xxxx) hasn't already spent the
2.1 coins that it got in transaction 3146. The transaction that spends
the coins at Amazon will show up in the ledger, and so the transaction
with Walmart will not be allowed.Every agent that has the bank's hash pointer to the end of the
tamper-evident ledger can inspect the ledger to check that
double-spending hasn't occurred.The bank signs a valid transaction and appends it to the
tamper-evident ledger. All agents can see the bank's signature and
verify that nobody (not even the bank) has tampered with the
tamper-evident ledger.Example of a pay transactionAn example of the specification of a pay transaction is:(9431, pay,
  [(3146, 0), (4731, 2)],
  [(0.7, 7xxxx...), (4.6, 9xxxx...)]
).The id of this transaction is 9431; the type of the transaction is
pay; the payers into the transaction are identified by the pairs of
(transaction-id, index): (3146, 0), and (4731, 2); and the
payee array is [(0.7, 7xxxx...), (4.6, 9xxxx...)].Transaction validityThe bank appends a transaction toLif and only if the
transaction is valid. The bank checks for validity by carrying out the
following steps:The bank verifies that the payers into the transaction signed the
  transaction.The bank checks that the total value of coins paid out from the
  transaction does not
  exceed the total value paid in to the transaction. (If the value paid
  in exceeds the value paid out then the bank takes the difference as
  a transaction fee. More about fees later.)The bank verifies that the payers' claims to have received coins in
  previous transactions is genuine. For example,
  if the agent with
  public key 7xxxx... claims to have received coins worth 2.1 in the
  transaction with id 10, and payee array index 0, then the bank
  verifies this claim by that transaction.The bank ensures that coins paid into the transaction haven't
  already been spent.Optimizations: Blocks and Block ChainVerifying large numbers of small transactions requires more
computation than verifying small numbers of blocks of many transactions.
Ablock chainis a tamper-evident ledger in which
each element of the ledger is a block consisting of many transactions.
A block of transactions can be aggregated into a single large
transaction by aggregating all the payers and payees of the smaller
transactions.The amount of computation decreases as the number of transactions in a
block increases. The time required to fill a block with transactions
is larger when the number of transactions to fill the block increases.Checking the Trusted AgentConsider a system in which the trusted agent broadcasts its current
copy of the tamper-evident ledger to all agents.
Every agent can inspect its copy of the tamper-evident ledger to determine whether the
ledger has been tampered with. So, every agent can validate its trust
in the trusted agent; however, this validation
suffers from a crucial problem: Agents may only
have copies ofold, staleversions of the ledger.
By the time that an agent receives a copy of the ledger, the trusted
agent may have added more transactions to the ledger.Fig.4: Old Copy is a Prefix of the Block ChainAn old copy of the ledger can differ from the current copy in only one
way: the current copy may have transactions appended to the end
of the old copy. So all agents can validatepastbehavior of the
trusted agent. An agent cannot, however, treat its copy of the ledger
as the master copy because the agent may not have the transactions
added most recently to the ledger.In the next module we will see how the Bitcoin algorithm addresses
this problem.Advantages of this cryptocurrencyAny agent can get a copy of the tamper-evident ledger and verify that all
transactions in the ledger are valid. Any agent can verify that
the only way in which the ledger is modified is that elements are
append to its tail; all that the agent needs to do is to check that
the pointer to the tail is modified only by appending
elements. Because the ledger is tamper-evident, an agent can check
that the ledger doesn't change while the hash pointer to the end
of the ledger doesn't change.The bank can't forge a transaction because all payers sign the
transaction. Agents can remain anonymous because an agent's only
public information is the agent's public key, and an agent can create
multiple public keys. Every agent can verify the correctness of every
transaction.Disadvantages of this cryptocurrencyUsers may not trust the bank. Transactions are not private because the
bank has a record of all transactions. And the bank is a single point
of failure.Next we'll look at Bitcoin's algorithms for
implementing a cryptocurrency without trusted agents.ReviewWhat iscollision resistance? Why is it relevant to
  cryptocurrencies?Consider the example ofhidingin which Megan hides her
  predictions for the winner of the World Cup. Suppose you could
  choose two different random number generators for creating the
  secret \(r\); in one case the bits of \(r\) are uncorrelated, and in
  the other they are highly correlated. Which generator would you use
  and why?What isbinding? Why is it relevant to hiding information?What ispuzzle friendly?What is atamper-evident data structure? How would you
  implement a tamper-evident linear list? A tamper-evident tree?Describe the algorithm for cryptocurrency with a trusted agent to
  someone who knows absolutely nothing about cryptocurrency.How could the algorithm for cryptocurrency be used for a collection
  of distributed agents to keep track of a sequence of events, other
  than buying/selling currency?K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology



Figure Link: https://kmchandy.github.io/Crypto/CryptoCurrencyIntroduction/Slide4.jpg

Caption: Fig.4: Old Copy is a Prefix of the Block Chain


To put in more value, the bank identifies other transaction-id, index
pairs in which this agent received coins. For example, say that (4539, 2)
identifies a 2-tuple (3.2, 7xxxx), and assume that the payers in this transaction
are specified by the pairs (3146, 0) and (4539, 2). The pair (3146, 0)
asserts that agent (7xxxx) received 2.1 coins and the pair (4539, 2)
asserts that the same agent received 3.2 coins. So the total amount of 
coins flowing into this
transaction from this agent (7xxxx) includes 2.1 + 3.2.To put in less value, the agent acts as both payer and payee; the net
value that this agent pays out to other agents is the difference
between the amount that this agent puts in and takes out. For example,
if agent with public key 7xxxx wants to put in 1.9 coins into this
transaction its payer information can be given by the transaction-id,
index pair (3146, 0) which asserts that the agent received 2.1 coins
and the same agent is a payee that withdraws 0.2 coins.Preventing Double SpendingHow does the system prevent an agent from using the
same coin twice?For example, the transaction id, index pair
(3146, 0) identifies the 2-tuple (2.1, 7xxxx...); this tuple
asserts that the agent
with public key 7xxxx received 2.1 units of coin in 
transaction 3146. Why can't the agent with public key 7xxxx use the
2.1 coins that it received to buy items from Amazon and later use
the same 2.1 coins to buy more items from Walmart?The bank checks that the agent hasn't already spent the coin that it
is putting into a transaction.Before permitting the transaction that double-spends the 2.1 coins
with Walmart, the bank inspects the tamper-evident ledger
for all transactions after transaction 3146 and before the Walmart
transaction to ensure that the agent (7xxxx) hasn't already spent the
2.1 coins that it got in transaction 3146. The transaction that spends
the coins at Amazon will show up in the ledger, and so the transaction
with Walmart will not be allowed.Every agent that has the bank's hash pointer to the end of the
tamper-evident ledger can inspect the ledger to check that
double-spending hasn't occurred.The bank signs a valid transaction and appends it to the
tamper-evident ledger. All agents can see the bank's signature and
verify that nobody (not even the bank) has tampered with the
tamper-evident ledger.Example of a pay transactionAn example of the specification of a pay transaction is:(9431, pay,
  [(3146, 0), (4731, 2)],
  [(0.7, 7xxxx...), (4.6, 9xxxx...)]
).The id of this transaction is 9431; the type of the transaction is
pay; the payers into the transaction are identified by the pairs of
(transaction-id, index): (3146, 0), and (4731, 2); and the
payee array is [(0.7, 7xxxx...), (4.6, 9xxxx...)].Transaction validityThe bank appends a transaction toLif and only if the
transaction is valid. The bank checks for validity by carrying out the
following steps:The bank verifies that the payers into the transaction signed the
  transaction.The bank checks that the total value of coins paid out from the
  transaction does not
  exceed the total value paid in to the transaction. (If the value paid
  in exceeds the value paid out then the bank takes the difference as
  a transaction fee. More about fees later.)The bank verifies that the payers' claims to have received coins in
  previous transactions is genuine. For example,
  if the agent with
  public key 7xxxx... claims to have received coins worth 2.1 in the
  transaction with id 10, and payee array index 0, then the bank
  verifies this claim by that transaction.The bank ensures that coins paid into the transaction haven't
  already been spent.Optimizations: Blocks and Block ChainVerifying large numbers of small transactions requires more
computation than verifying small numbers of blocks of many transactions.
Ablock chainis a tamper-evident ledger in which
each element of the ledger is a block consisting of many transactions.
A block of transactions can be aggregated into a single large
transaction by aggregating all the payers and payees of the smaller
transactions.The amount of computation decreases as the number of transactions in a
block increases. The time required to fill a block with transactions
is larger when the number of transactions to fill the block increases.Checking the Trusted AgentConsider a system in which the trusted agent broadcasts its current
copy of the tamper-evident ledger to all agents.
Every agent can inspect its copy of the tamper-evident ledger to determine whether the
ledger has been tampered with. So, every agent can validate its trust
in the trusted agent; however, this validation
suffers from a crucial problem: Agents may only
have copies ofold, staleversions of the ledger.
By the time that an agent receives a copy of the ledger, the trusted
agent may have added more transactions to the ledger.Fig.4: Old Copy is a Prefix of the Block ChainAn old copy of the ledger can differ from the current copy in only one
way: the current copy may have transactions appended to the end
of the old copy. So all agents can validatepastbehavior of the
trusted agent. An agent cannot, however, treat its copy of the ledger
as the master copy because the agent may not have the transactions
added most recently to the ledger.In the next module we will see how the Bitcoin algorithm addresses
this problem.Advantages of this cryptocurrencyAny agent can get a copy of the tamper-evident ledger and verify that all
transactions in the ledger are valid. Any agent can verify that
the only way in which the ledger is modified is that elements are
append to its tail; all that the agent needs to do is to check that
the pointer to the tail is modified only by appending
elements. Because the ledger is tamper-evident, an agent can check
that the ledger doesn't change while the hash pointer to the end
of the ledger doesn't change.The bank can't forge a transaction because all payers sign the
transaction. Agents can remain anonymous because an agent's only
public information is the agent's public key, and an agent can create
multiple public keys. Every agent can verify the correctness of every
transaction.Disadvantages of this cryptocurrencyUsers may not trust the bank. Transactions are not private because the
bank has a record of all transactions. And the bank is a single point
of failure.Next we'll look at Bitcoin's algorithms for
implementing a cryptocurrency without trusted agents.ReviewWhat iscollision resistance? Why is it relevant to
  cryptocurrencies?Consider the example ofhidingin which Megan hides her
  predictions for the winner of the World Cup. Suppose you could
  choose two different random number generators for creating the
  secret \(r\); in one case the bits of \(r\) are uncorrelated, and in
  the other they are highly correlated. Which generator would you use
  and why?What isbinding? Why is it relevant to hiding information?What ispuzzle friendly?What is atamper-evident data structure? How would you
  implement a tamper-evident linear list? A tamper-evident tree?Describe the algorithm for cryptocurrency with a trusted agent to
  someone who knows absolutely nothing about cryptocurrency.How could the algorithm for cryptocurrency be used for a collection
  of distributed agents to keep track of a sequence of events, other
  than buying/selling currency?K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology



Figure Link: https://kmchandy.github.io/Crypto/CryptoCurrencyIntroduction/Slide4.jpg

Caption: Fig.4: Old Copy is a Prefix of the Block Chain


To put in less value, the agent acts as both payer and payee; the net
value that this agent pays out to other agents is the difference
between the amount that this agent puts in and takes out. For example,
if agent with public key 7xxxx wants to put in 1.9 coins into this
transaction its payer information can be given by the transaction-id,
index pair (3146, 0) which asserts that the agent received 2.1 coins
and the same agent is a payee that withdraws 0.2 coins.Preventing Double SpendingHow does the system prevent an agent from using the
same coin twice?For example, the transaction id, index pair
(3146, 0) identifies the 2-tuple (2.1, 7xxxx...); this tuple
asserts that the agent
with public key 7xxxx received 2.1 units of coin in 
transaction 3146. Why can't the agent with public key 7xxxx use the
2.1 coins that it received to buy items from Amazon and later use
the same 2.1 coins to buy more items from Walmart?The bank checks that the agent hasn't already spent the coin that it
is putting into a transaction.Before permitting the transaction that double-spends the 2.1 coins
with Walmart, the bank inspects the tamper-evident ledger
for all transactions after transaction 3146 and before the Walmart
transaction to ensure that the agent (7xxxx) hasn't already spent the
2.1 coins that it got in transaction 3146. The transaction that spends
the coins at Amazon will show up in the ledger, and so the transaction
with Walmart will not be allowed.Every agent that has the bank's hash pointer to the end of the
tamper-evident ledger can inspect the ledger to check that
double-spending hasn't occurred.The bank signs a valid transaction and appends it to the
tamper-evident ledger. All agents can see the bank's signature and
verify that nobody (not even the bank) has tampered with the
tamper-evident ledger.Example of a pay transactionAn example of the specification of a pay transaction is:(9431, pay,
  [(3146, 0), (4731, 2)],
  [(0.7, 7xxxx...), (4.6, 9xxxx...)]
).The id of this transaction is 9431; the type of the transaction is
pay; the payers into the transaction are identified by the pairs of
(transaction-id, index): (3146, 0), and (4731, 2); and the
payee array is [(0.7, 7xxxx...), (4.6, 9xxxx...)].Transaction validityThe bank appends a transaction toLif and only if the
transaction is valid. The bank checks for validity by carrying out the
following steps:The bank verifies that the payers into the transaction signed the
  transaction.The bank checks that the total value of coins paid out from the
  transaction does not
  exceed the total value paid in to the transaction. (If the value paid
  in exceeds the value paid out then the bank takes the difference as
  a transaction fee. More about fees later.)The bank verifies that the payers' claims to have received coins in
  previous transactions is genuine. For example,
  if the agent with
  public key 7xxxx... claims to have received coins worth 2.1 in the
  transaction with id 10, and payee array index 0, then the bank
  verifies this claim by that transaction.The bank ensures that coins paid into the transaction haven't
  already been spent.Optimizations: Blocks and Block ChainVerifying large numbers of small transactions requires more
computation than verifying small numbers of blocks of many transactions.
Ablock chainis a tamper-evident ledger in which
each element of the ledger is a block consisting of many transactions.
A block of transactions can be aggregated into a single large
transaction by aggregating all the payers and payees of the smaller
transactions.The amount of computation decreases as the number of transactions in a
block increases. The time required to fill a block with transactions
is larger when the number of transactions to fill the block increases.Checking the Trusted AgentConsider a system in which the trusted agent broadcasts its current
copy of the tamper-evident ledger to all agents.
Every agent can inspect its copy of the tamper-evident ledger to determine whether the
ledger has been tampered with. So, every agent can validate its trust
in the trusted agent; however, this validation
suffers from a crucial problem: Agents may only
have copies ofold, staleversions of the ledger.
By the time that an agent receives a copy of the ledger, the trusted
agent may have added more transactions to the ledger.Fig.4: Old Copy is a Prefix of the Block ChainAn old copy of the ledger can differ from the current copy in only one
way: the current copy may have transactions appended to the end
of the old copy. So all agents can validatepastbehavior of the
trusted agent. An agent cannot, however, treat its copy of the ledger
as the master copy because the agent may not have the transactions
added most recently to the ledger.In the next module we will see how the Bitcoin algorithm addresses
this problem.Advantages of this cryptocurrencyAny agent can get a copy of the tamper-evident ledger and verify that all
transactions in the ledger are valid. Any agent can verify that
the only way in which the ledger is modified is that elements are
append to its tail; all that the agent needs to do is to check that
the pointer to the tail is modified only by appending
elements. Because the ledger is tamper-evident, an agent can check
that the ledger doesn't change while the hash pointer to the end
of the ledger doesn't change.The bank can't forge a transaction because all payers sign the
transaction. Agents can remain anonymous because an agent's only
public information is the agent's public key, and an agent can create
multiple public keys. Every agent can verify the correctness of every
transaction.Disadvantages of this cryptocurrencyUsers may not trust the bank. Transactions are not private because the
bank has a record of all transactions. And the bank is a single point
of failure.Next we'll look at Bitcoin's algorithms for
implementing a cryptocurrency without trusted agents.ReviewWhat iscollision resistance? Why is it relevant to
  cryptocurrencies?Consider the example ofhidingin which Megan hides her
  predictions for the winner of the World Cup. Suppose you could
  choose two different random number generators for creating the
  secret \(r\); in one case the bits of \(r\) are uncorrelated, and in
  the other they are highly correlated. Which generator would you use
  and why?What isbinding? Why is it relevant to hiding information?What ispuzzle friendly?What is atamper-evident data structure? How would you
  implement a tamper-evident linear list? A tamper-evident tree?Describe the algorithm for cryptocurrency with a trusted agent to
  someone who knows absolutely nothing about cryptocurrency.How could the algorithm for cryptocurrency be used for a collection
  of distributed agents to keep track of a sequence of events, other
  than buying/selling currency?K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology



Figure Link: https://kmchandy.github.io/Crypto/CryptoCurrencyIntroduction/Slide4.jpg

Caption: Fig.4: Old Copy is a Prefix of the Block Chain


For example, the transaction id, index pair
(3146, 0) identifies the 2-tuple (2.1, 7xxxx...); this tuple
asserts that the agent
with public key 7xxxx received 2.1 units of coin in 
transaction 3146. Why can't the agent with public key 7xxxx use the
2.1 coins that it received to buy items from Amazon and later use
the same 2.1 coins to buy more items from Walmart?The bank checks that the agent hasn't already spent the coin that it
is putting into a transaction.Before permitting the transaction that double-spends the 2.1 coins
with Walmart, the bank inspects the tamper-evident ledger
for all transactions after transaction 3146 and before the Walmart
transaction to ensure that the agent (7xxxx) hasn't already spent the
2.1 coins that it got in transaction 3146. The transaction that spends
the coins at Amazon will show up in the ledger, and so the transaction
with Walmart will not be allowed.Every agent that has the bank's hash pointer to the end of the
tamper-evident ledger can inspect the ledger to check that
double-spending hasn't occurred.The bank signs a valid transaction and appends it to the
tamper-evident ledger. All agents can see the bank's signature and
verify that nobody (not even the bank) has tampered with the
tamper-evident ledger.Example of a pay transactionAn example of the specification of a pay transaction is:(9431, pay,
  [(3146, 0), (4731, 2)],
  [(0.7, 7xxxx...), (4.6, 9xxxx...)]
).The id of this transaction is 9431; the type of the transaction is
pay; the payers into the transaction are identified by the pairs of
(transaction-id, index): (3146, 0), and (4731, 2); and the
payee array is [(0.7, 7xxxx...), (4.6, 9xxxx...)].Transaction validityThe bank appends a transaction toLif and only if the
transaction is valid. The bank checks for validity by carrying out the
following steps:The bank verifies that the payers into the transaction signed the
  transaction.The bank checks that the total value of coins paid out from the
  transaction does not
  exceed the total value paid in to the transaction. (If the value paid
  in exceeds the value paid out then the bank takes the difference as
  a transaction fee. More about fees later.)The bank verifies that the payers' claims to have received coins in
  previous transactions is genuine. For example,
  if the agent with
  public key 7xxxx... claims to have received coins worth 2.1 in the
  transaction with id 10, and payee array index 0, then the bank
  verifies this claim by that transaction.The bank ensures that coins paid into the transaction haven't
  already been spent.Optimizations: Blocks and Block ChainVerifying large numbers of small transactions requires more
computation than verifying small numbers of blocks of many transactions.
Ablock chainis a tamper-evident ledger in which
each element of the ledger is a block consisting of many transactions.
A block of transactions can be aggregated into a single large
transaction by aggregating all the payers and payees of the smaller
transactions.The amount of computation decreases as the number of transactions in a
block increases. The time required to fill a block with transactions
is larger when the number of transactions to fill the block increases.Checking the Trusted AgentConsider a system in which the trusted agent broadcasts its current
copy of the tamper-evident ledger to all agents.
Every agent can inspect its copy of the tamper-evident ledger to determine whether the
ledger has been tampered with. So, every agent can validate its trust
in the trusted agent; however, this validation
suffers from a crucial problem: Agents may only
have copies ofold, staleversions of the ledger.
By the time that an agent receives a copy of the ledger, the trusted
agent may have added more transactions to the ledger.Fig.4: Old Copy is a Prefix of the Block ChainAn old copy of the ledger can differ from the current copy in only one
way: the current copy may have transactions appended to the end
of the old copy. So all agents can validatepastbehavior of the
trusted agent. An agent cannot, however, treat its copy of the ledger
as the master copy because the agent may not have the transactions
added most recently to the ledger.In the next module we will see how the Bitcoin algorithm addresses
this problem.Advantages of this cryptocurrencyAny agent can get a copy of the tamper-evident ledger and verify that all
transactions in the ledger are valid. Any agent can verify that
the only way in which the ledger is modified is that elements are
append to its tail; all that the agent needs to do is to check that
the pointer to the tail is modified only by appending
elements. Because the ledger is tamper-evident, an agent can check
that the ledger doesn't change while the hash pointer to the end
of the ledger doesn't change.The bank can't forge a transaction because all payers sign the
transaction. Agents can remain anonymous because an agent's only
public information is the agent's public key, and an agent can create
multiple public keys. Every agent can verify the correctness of every
transaction.Disadvantages of this cryptocurrencyUsers may not trust the bank. Transactions are not private because the
bank has a record of all transactions. And the bank is a single point
of failure.Next we'll look at Bitcoin's algorithms for
implementing a cryptocurrency without trusted agents.ReviewWhat iscollision resistance? Why is it relevant to
  cryptocurrencies?Consider the example ofhidingin which Megan hides her
  predictions for the winner of the World Cup. Suppose you could
  choose two different random number generators for creating the
  secret \(r\); in one case the bits of \(r\) are uncorrelated, and in
  the other they are highly correlated. Which generator would you use
  and why?What isbinding? Why is it relevant to hiding information?What ispuzzle friendly?What is atamper-evident data structure? How would you
  implement a tamper-evident linear list? A tamper-evident tree?Describe the algorithm for cryptocurrency with a trusted agent to
  someone who knows absolutely nothing about cryptocurrency.How could the algorithm for cryptocurrency be used for a collection
  of distributed agents to keep track of a sequence of events, other
  than buying/selling currency?K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology



Figure Link: https://kmchandy.github.io/Crypto/CryptoCurrencyIntroduction/Slide4.jpg

Caption: Fig.4: Old Copy is a Prefix of the Block Chain


The bank checks that the agent hasn't already spent the coin that it
is putting into a transaction.Before permitting the transaction that double-spends the 2.1 coins
with Walmart, the bank inspects the tamper-evident ledger
for all transactions after transaction 3146 and before the Walmart
transaction to ensure that the agent (7xxxx) hasn't already spent the
2.1 coins that it got in transaction 3146. The transaction that spends
the coins at Amazon will show up in the ledger, and so the transaction
with Walmart will not be allowed.Every agent that has the bank's hash pointer to the end of the
tamper-evident ledger can inspect the ledger to check that
double-spending hasn't occurred.The bank signs a valid transaction and appends it to the
tamper-evident ledger. All agents can see the bank's signature and
verify that nobody (not even the bank) has tampered with the
tamper-evident ledger.Example of a pay transactionAn example of the specification of a pay transaction is:(9431, pay,
  [(3146, 0), (4731, 2)],
  [(0.7, 7xxxx...), (4.6, 9xxxx...)]
).The id of this transaction is 9431; the type of the transaction is
pay; the payers into the transaction are identified by the pairs of
(transaction-id, index): (3146, 0), and (4731, 2); and the
payee array is [(0.7, 7xxxx...), (4.6, 9xxxx...)].Transaction validityThe bank appends a transaction toLif and only if the
transaction is valid. The bank checks for validity by carrying out the
following steps:The bank verifies that the payers into the transaction signed the
  transaction.The bank checks that the total value of coins paid out from the
  transaction does not
  exceed the total value paid in to the transaction. (If the value paid
  in exceeds the value paid out then the bank takes the difference as
  a transaction fee. More about fees later.)The bank verifies that the payers' claims to have received coins in
  previous transactions is genuine. For example,
  if the agent with
  public key 7xxxx... claims to have received coins worth 2.1 in the
  transaction with id 10, and payee array index 0, then the bank
  verifies this claim by that transaction.The bank ensures that coins paid into the transaction haven't
  already been spent.Optimizations: Blocks and Block ChainVerifying large numbers of small transactions requires more
computation than verifying small numbers of blocks of many transactions.
Ablock chainis a tamper-evident ledger in which
each element of the ledger is a block consisting of many transactions.
A block of transactions can be aggregated into a single large
transaction by aggregating all the payers and payees of the smaller
transactions.The amount of computation decreases as the number of transactions in a
block increases. The time required to fill a block with transactions
is larger when the number of transactions to fill the block increases.Checking the Trusted AgentConsider a system in which the trusted agent broadcasts its current
copy of the tamper-evident ledger to all agents.
Every agent can inspect its copy of the tamper-evident ledger to determine whether the
ledger has been tampered with. So, every agent can validate its trust
in the trusted agent; however, this validation
suffers from a crucial problem: Agents may only
have copies ofold, staleversions of the ledger.
By the time that an agent receives a copy of the ledger, the trusted
agent may have added more transactions to the ledger.Fig.4: Old Copy is a Prefix of the Block ChainAn old copy of the ledger can differ from the current copy in only one
way: the current copy may have transactions appended to the end
of the old copy. So all agents can validatepastbehavior of the
trusted agent. An agent cannot, however, treat its copy of the ledger
as the master copy because the agent may not have the transactions
added most recently to the ledger.In the next module we will see how the Bitcoin algorithm addresses
this problem.Advantages of this cryptocurrencyAny agent can get a copy of the tamper-evident ledger and verify that all
transactions in the ledger are valid. Any agent can verify that
the only way in which the ledger is modified is that elements are
append to its tail; all that the agent needs to do is to check that
the pointer to the tail is modified only by appending
elements. Because the ledger is tamper-evident, an agent can check
that the ledger doesn't change while the hash pointer to the end
of the ledger doesn't change.The bank can't forge a transaction because all payers sign the
transaction. Agents can remain anonymous because an agent's only
public information is the agent's public key, and an agent can create
multiple public keys. Every agent can verify the correctness of every
transaction.Disadvantages of this cryptocurrencyUsers may not trust the bank. Transactions are not private because the
bank has a record of all transactions. And the bank is a single point
of failure.Next we'll look at Bitcoin's algorithms for
implementing a cryptocurrency without trusted agents.ReviewWhat iscollision resistance? Why is it relevant to
  cryptocurrencies?Consider the example ofhidingin which Megan hides her
  predictions for the winner of the World Cup. Suppose you could
  choose two different random number generators for creating the
  secret \(r\); in one case the bits of \(r\) are uncorrelated, and in
  the other they are highly correlated. Which generator would you use
  and why?What isbinding? Why is it relevant to hiding information?What ispuzzle friendly?What is atamper-evident data structure? How would you
  implement a tamper-evident linear list? A tamper-evident tree?Describe the algorithm for cryptocurrency with a trusted agent to
  someone who knows absolutely nothing about cryptocurrency.How could the algorithm for cryptocurrency be used for a collection
  of distributed agents to keep track of a sequence of events, other
  than buying/selling currency?K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology



Figure Link: https://kmchandy.github.io/Crypto/CryptoCurrencyIntroduction/Slide4.jpg

Caption: Fig.4: Old Copy is a Prefix of the Block Chain


Before permitting the transaction that double-spends the 2.1 coins
with Walmart, the bank inspects the tamper-evident ledger
for all transactions after transaction 3146 and before the Walmart
transaction to ensure that the agent (7xxxx) hasn't already spent the
2.1 coins that it got in transaction 3146. The transaction that spends
the coins at Amazon will show up in the ledger, and so the transaction
with Walmart will not be allowed.Every agent that has the bank's hash pointer to the end of the
tamper-evident ledger can inspect the ledger to check that
double-spending hasn't occurred.The bank signs a valid transaction and appends it to the
tamper-evident ledger. All agents can see the bank's signature and
verify that nobody (not even the bank) has tampered with the
tamper-evident ledger.Example of a pay transactionAn example of the specification of a pay transaction is:(9431, pay,
  [(3146, 0), (4731, 2)],
  [(0.7, 7xxxx...), (4.6, 9xxxx...)]
).The id of this transaction is 9431; the type of the transaction is
pay; the payers into the transaction are identified by the pairs of
(transaction-id, index): (3146, 0), and (4731, 2); and the
payee array is [(0.7, 7xxxx...), (4.6, 9xxxx...)].Transaction validityThe bank appends a transaction toLif and only if the
transaction is valid. The bank checks for validity by carrying out the
following steps:The bank verifies that the payers into the transaction signed the
  transaction.The bank checks that the total value of coins paid out from the
  transaction does not
  exceed the total value paid in to the transaction. (If the value paid
  in exceeds the value paid out then the bank takes the difference as
  a transaction fee. More about fees later.)The bank verifies that the payers' claims to have received coins in
  previous transactions is genuine. For example,
  if the agent with
  public key 7xxxx... claims to have received coins worth 2.1 in the
  transaction with id 10, and payee array index 0, then the bank
  verifies this claim by that transaction.The bank ensures that coins paid into the transaction haven't
  already been spent.Optimizations: Blocks and Block ChainVerifying large numbers of small transactions requires more
computation than verifying small numbers of blocks of many transactions.
Ablock chainis a tamper-evident ledger in which
each element of the ledger is a block consisting of many transactions.
A block of transactions can be aggregated into a single large
transaction by aggregating all the payers and payees of the smaller
transactions.The amount of computation decreases as the number of transactions in a
block increases. The time required to fill a block with transactions
is larger when the number of transactions to fill the block increases.Checking the Trusted AgentConsider a system in which the trusted agent broadcasts its current
copy of the tamper-evident ledger to all agents.
Every agent can inspect its copy of the tamper-evident ledger to determine whether the
ledger has been tampered with. So, every agent can validate its trust
in the trusted agent; however, this validation
suffers from a crucial problem: Agents may only
have copies ofold, staleversions of the ledger.
By the time that an agent receives a copy of the ledger, the trusted
agent may have added more transactions to the ledger.Fig.4: Old Copy is a Prefix of the Block ChainAn old copy of the ledger can differ from the current copy in only one
way: the current copy may have transactions appended to the end
of the old copy. So all agents can validatepastbehavior of the
trusted agent. An agent cannot, however, treat its copy of the ledger
as the master copy because the agent may not have the transactions
added most recently to the ledger.In the next module we will see how the Bitcoin algorithm addresses
this problem.Advantages of this cryptocurrencyAny agent can get a copy of the tamper-evident ledger and verify that all
transactions in the ledger are valid. Any agent can verify that
the only way in which the ledger is modified is that elements are
append to its tail; all that the agent needs to do is to check that
the pointer to the tail is modified only by appending
elements. Because the ledger is tamper-evident, an agent can check
that the ledger doesn't change while the hash pointer to the end
of the ledger doesn't change.The bank can't forge a transaction because all payers sign the
transaction. Agents can remain anonymous because an agent's only
public information is the agent's public key, and an agent can create
multiple public keys. Every agent can verify the correctness of every
transaction.Disadvantages of this cryptocurrencyUsers may not trust the bank. Transactions are not private because the
bank has a record of all transactions. And the bank is a single point
of failure.Next we'll look at Bitcoin's algorithms for
implementing a cryptocurrency without trusted agents.ReviewWhat iscollision resistance? Why is it relevant to
  cryptocurrencies?Consider the example ofhidingin which Megan hides her
  predictions for the winner of the World Cup. Suppose you could
  choose two different random number generators for creating the
  secret \(r\); in one case the bits of \(r\) are uncorrelated, and in
  the other they are highly correlated. Which generator would you use
  and why?What isbinding? Why is it relevant to hiding information?What ispuzzle friendly?What is atamper-evident data structure? How would you
  implement a tamper-evident linear list? A tamper-evident tree?Describe the algorithm for cryptocurrency with a trusted agent to
  someone who knows absolutely nothing about cryptocurrency.How could the algorithm for cryptocurrency be used for a collection
  of distributed agents to keep track of a sequence of events, other
  than buying/selling currency?K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology



Figure Link: https://kmchandy.github.io/Crypto/CryptoCurrencyIntroduction/Slide4.jpg

Caption: Fig.4: Old Copy is a Prefix of the Block Chain


Every agent that has the bank's hash pointer to the end of the
tamper-evident ledger can inspect the ledger to check that
double-spending hasn't occurred.The bank signs a valid transaction and appends it to the
tamper-evident ledger. All agents can see the bank's signature and
verify that nobody (not even the bank) has tampered with the
tamper-evident ledger.Example of a pay transactionAn example of the specification of a pay transaction is:(9431, pay,
  [(3146, 0), (4731, 2)],
  [(0.7, 7xxxx...), (4.6, 9xxxx...)]
).The id of this transaction is 9431; the type of the transaction is
pay; the payers into the transaction are identified by the pairs of
(transaction-id, index): (3146, 0), and (4731, 2); and the
payee array is [(0.7, 7xxxx...), (4.6, 9xxxx...)].Transaction validityThe bank appends a transaction toLif and only if the
transaction is valid. The bank checks for validity by carrying out the
following steps:The bank verifies that the payers into the transaction signed the
  transaction.The bank checks that the total value of coins paid out from the
  transaction does not
  exceed the total value paid in to the transaction. (If the value paid
  in exceeds the value paid out then the bank takes the difference as
  a transaction fee. More about fees later.)The bank verifies that the payers' claims to have received coins in
  previous transactions is genuine. For example,
  if the agent with
  public key 7xxxx... claims to have received coins worth 2.1 in the
  transaction with id 10, and payee array index 0, then the bank
  verifies this claim by that transaction.The bank ensures that coins paid into the transaction haven't
  already been spent.Optimizations: Blocks and Block ChainVerifying large numbers of small transactions requires more
computation than verifying small numbers of blocks of many transactions.
Ablock chainis a tamper-evident ledger in which
each element of the ledger is a block consisting of many transactions.
A block of transactions can be aggregated into a single large
transaction by aggregating all the payers and payees of the smaller
transactions.The amount of computation decreases as the number of transactions in a
block increases. The time required to fill a block with transactions
is larger when the number of transactions to fill the block increases.Checking the Trusted AgentConsider a system in which the trusted agent broadcasts its current
copy of the tamper-evident ledger to all agents.
Every agent can inspect its copy of the tamper-evident ledger to determine whether the
ledger has been tampered with. So, every agent can validate its trust
in the trusted agent; however, this validation
suffers from a crucial problem: Agents may only
have copies ofold, staleversions of the ledger.
By the time that an agent receives a copy of the ledger, the trusted
agent may have added more transactions to the ledger.Fig.4: Old Copy is a Prefix of the Block ChainAn old copy of the ledger can differ from the current copy in only one
way: the current copy may have transactions appended to the end
of the old copy. So all agents can validatepastbehavior of the
trusted agent. An agent cannot, however, treat its copy of the ledger
as the master copy because the agent may not have the transactions
added most recently to the ledger.In the next module we will see how the Bitcoin algorithm addresses
this problem.Advantages of this cryptocurrencyAny agent can get a copy of the tamper-evident ledger and verify that all
transactions in the ledger are valid. Any agent can verify that
the only way in which the ledger is modified is that elements are
append to its tail; all that the agent needs to do is to check that
the pointer to the tail is modified only by appending
elements. Because the ledger is tamper-evident, an agent can check
that the ledger doesn't change while the hash pointer to the end
of the ledger doesn't change.The bank can't forge a transaction because all payers sign the
transaction. Agents can remain anonymous because an agent's only
public information is the agent's public key, and an agent can create
multiple public keys. Every agent can verify the correctness of every
transaction.Disadvantages of this cryptocurrencyUsers may not trust the bank. Transactions are not private because the
bank has a record of all transactions. And the bank is a single point
of failure.Next we'll look at Bitcoin's algorithms for
implementing a cryptocurrency without trusted agents.ReviewWhat iscollision resistance? Why is it relevant to
  cryptocurrencies?Consider the example ofhidingin which Megan hides her
  predictions for the winner of the World Cup. Suppose you could
  choose two different random number generators for creating the
  secret \(r\); in one case the bits of \(r\) are uncorrelated, and in
  the other they are highly correlated. Which generator would you use
  and why?What isbinding? Why is it relevant to hiding information?What ispuzzle friendly?What is atamper-evident data structure? How would you
  implement a tamper-evident linear list? A tamper-evident tree?Describe the algorithm for cryptocurrency with a trusted agent to
  someone who knows absolutely nothing about cryptocurrency.How could the algorithm for cryptocurrency be used for a collection
  of distributed agents to keep track of a sequence of events, other
  than buying/selling currency?K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology



Figure Link: https://kmchandy.github.io/Crypto/CryptoCurrencyIntroduction/Slide4.jpg

Caption: Fig.4: Old Copy is a Prefix of the Block Chain


The bank signs a valid transaction and appends it to the
tamper-evident ledger. All agents can see the bank's signature and
verify that nobody (not even the bank) has tampered with the
tamper-evident ledger.Example of a pay transactionAn example of the specification of a pay transaction is:(9431, pay,
  [(3146, 0), (4731, 2)],
  [(0.7, 7xxxx...), (4.6, 9xxxx...)]
).The id of this transaction is 9431; the type of the transaction is
pay; the payers into the transaction are identified by the pairs of
(transaction-id, index): (3146, 0), and (4731, 2); and the
payee array is [(0.7, 7xxxx...), (4.6, 9xxxx...)].Transaction validityThe bank appends a transaction toLif and only if the
transaction is valid. The bank checks for validity by carrying out the
following steps:The bank verifies that the payers into the transaction signed the
  transaction.The bank checks that the total value of coins paid out from the
  transaction does not
  exceed the total value paid in to the transaction. (If the value paid
  in exceeds the value paid out then the bank takes the difference as
  a transaction fee. More about fees later.)The bank verifies that the payers' claims to have received coins in
  previous transactions is genuine. For example,
  if the agent with
  public key 7xxxx... claims to have received coins worth 2.1 in the
  transaction with id 10, and payee array index 0, then the bank
  verifies this claim by that transaction.The bank ensures that coins paid into the transaction haven't
  already been spent.Optimizations: Blocks and Block ChainVerifying large numbers of small transactions requires more
computation than verifying small numbers of blocks of many transactions.
Ablock chainis a tamper-evident ledger in which
each element of the ledger is a block consisting of many transactions.
A block of transactions can be aggregated into a single large
transaction by aggregating all the payers and payees of the smaller
transactions.The amount of computation decreases as the number of transactions in a
block increases. The time required to fill a block with transactions
is larger when the number of transactions to fill the block increases.Checking the Trusted AgentConsider a system in which the trusted agent broadcasts its current
copy of the tamper-evident ledger to all agents.
Every agent can inspect its copy of the tamper-evident ledger to determine whether the
ledger has been tampered with. So, every agent can validate its trust
in the trusted agent; however, this validation
suffers from a crucial problem: Agents may only
have copies ofold, staleversions of the ledger.
By the time that an agent receives a copy of the ledger, the trusted
agent may have added more transactions to the ledger.Fig.4: Old Copy is a Prefix of the Block ChainAn old copy of the ledger can differ from the current copy in only one
way: the current copy may have transactions appended to the end
of the old copy. So all agents can validatepastbehavior of the
trusted agent. An agent cannot, however, treat its copy of the ledger
as the master copy because the agent may not have the transactions
added most recently to the ledger.In the next module we will see how the Bitcoin algorithm addresses
this problem.Advantages of this cryptocurrencyAny agent can get a copy of the tamper-evident ledger and verify that all
transactions in the ledger are valid. Any agent can verify that
the only way in which the ledger is modified is that elements are
append to its tail; all that the agent needs to do is to check that
the pointer to the tail is modified only by appending
elements. Because the ledger is tamper-evident, an agent can check
that the ledger doesn't change while the hash pointer to the end
of the ledger doesn't change.The bank can't forge a transaction because all payers sign the
transaction. Agents can remain anonymous because an agent's only
public information is the agent's public key, and an agent can create
multiple public keys. Every agent can verify the correctness of every
transaction.Disadvantages of this cryptocurrencyUsers may not trust the bank. Transactions are not private because the
bank has a record of all transactions. And the bank is a single point
of failure.Next we'll look at Bitcoin's algorithms for
implementing a cryptocurrency without trusted agents.ReviewWhat iscollision resistance? Why is it relevant to
  cryptocurrencies?Consider the example ofhidingin which Megan hides her
  predictions for the winner of the World Cup. Suppose you could
  choose two different random number generators for creating the
  secret \(r\); in one case the bits of \(r\) are uncorrelated, and in
  the other they are highly correlated. Which generator would you use
  and why?What isbinding? Why is it relevant to hiding information?What ispuzzle friendly?What is atamper-evident data structure? How would you
  implement a tamper-evident linear list? A tamper-evident tree?Describe the algorithm for cryptocurrency with a trusted agent to
  someone who knows absolutely nothing about cryptocurrency.How could the algorithm for cryptocurrency be used for a collection
  of distributed agents to keep track of a sequence of events, other
  than buying/selling currency?K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology



Figure Link: https://kmchandy.github.io/Crypto/CryptoCurrencyIntroduction/Slide4.jpg

Caption: Fig.4: Old Copy is a Prefix of the Block Chain


The amount of computation decreases as the number of transactions in a
block increases. The time required to fill a block with transactions
is larger when the number of transactions to fill the block increases.Checking the Trusted AgentConsider a system in which the trusted agent broadcasts its current
copy of the tamper-evident ledger to all agents.
Every agent can inspect its copy of the tamper-evident ledger to determine whether the
ledger has been tampered with. So, every agent can validate its trust
in the trusted agent; however, this validation
suffers from a crucial problem: Agents may only
have copies ofold, staleversions of the ledger.
By the time that an agent receives a copy of the ledger, the trusted
agent may have added more transactions to the ledger.Fig.4: Old Copy is a Prefix of the Block ChainAn old copy of the ledger can differ from the current copy in only one
way: the current copy may have transactions appended to the end
of the old copy. So all agents can validatepastbehavior of the
trusted agent. An agent cannot, however, treat its copy of the ledger
as the master copy because the agent may not have the transactions
added most recently to the ledger.In the next module we will see how the Bitcoin algorithm addresses
this problem.Advantages of this cryptocurrencyAny agent can get a copy of the tamper-evident ledger and verify that all
transactions in the ledger are valid. Any agent can verify that
the only way in which the ledger is modified is that elements are
append to its tail; all that the agent needs to do is to check that
the pointer to the tail is modified only by appending
elements. Because the ledger is tamper-evident, an agent can check
that the ledger doesn't change while the hash pointer to the end
of the ledger doesn't change.The bank can't forge a transaction because all payers sign the
transaction. Agents can remain anonymous because an agent's only
public information is the agent's public key, and an agent can create
multiple public keys. Every agent can verify the correctness of every
transaction.Disadvantages of this cryptocurrencyUsers may not trust the bank. Transactions are not private because the
bank has a record of all transactions. And the bank is a single point
of failure.Next we'll look at Bitcoin's algorithms for
implementing a cryptocurrency without trusted agents.ReviewWhat iscollision resistance? Why is it relevant to
  cryptocurrencies?Consider the example ofhidingin which Megan hides her
  predictions for the winner of the World Cup. Suppose you could
  choose two different random number generators for creating the
  secret \(r\); in one case the bits of \(r\) are uncorrelated, and in
  the other they are highly correlated. Which generator would you use
  and why?What isbinding? Why is it relevant to hiding information?What ispuzzle friendly?What is atamper-evident data structure? How would you
  implement a tamper-evident linear list? A tamper-evident tree?Describe the algorithm for cryptocurrency with a trusted agent to
  someone who knows absolutely nothing about cryptocurrency.How could the algorithm for cryptocurrency be used for a collection
  of distributed agents to keep track of a sequence of events, other
  than buying/selling currency?K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology



Figure Link: https://kmchandy.github.io/Crypto/CryptoCurrencyIntroduction/Slide4.jpg

Caption: Fig.4: Old Copy is a Prefix of the Block Chain


An old copy of the ledger can differ from the current copy in only one
way: the current copy may have transactions appended to the end
of the old copy. So all agents can validatepastbehavior of the
trusted agent. An agent cannot, however, treat its copy of the ledger
as the master copy because the agent may not have the transactions
added most recently to the ledger.In the next module we will see how the Bitcoin algorithm addresses
this problem.Advantages of this cryptocurrencyAny agent can get a copy of the tamper-evident ledger and verify that all
transactions in the ledger are valid. Any agent can verify that
the only way in which the ledger is modified is that elements are
append to its tail; all that the agent needs to do is to check that
the pointer to the tail is modified only by appending
elements. Because the ledger is tamper-evident, an agent can check
that the ledger doesn't change while the hash pointer to the end
of the ledger doesn't change.The bank can't forge a transaction because all payers sign the
transaction. Agents can remain anonymous because an agent's only
public information is the agent's public key, and an agent can create
multiple public keys. Every agent can verify the correctness of every
transaction.Disadvantages of this cryptocurrencyUsers may not trust the bank. Transactions are not private because the
bank has a record of all transactions. And the bank is a single point
of failure.Next we'll look at Bitcoin's algorithms for
implementing a cryptocurrency without trusted agents.ReviewWhat iscollision resistance? Why is it relevant to
  cryptocurrencies?Consider the example ofhidingin which Megan hides her
  predictions for the winner of the World Cup. Suppose you could
  choose two different random number generators for creating the
  secret \(r\); in one case the bits of \(r\) are uncorrelated, and in
  the other they are highly correlated. Which generator would you use
  and why?What isbinding? Why is it relevant to hiding information?What ispuzzle friendly?What is atamper-evident data structure? How would you
  implement a tamper-evident linear list? A tamper-evident tree?Describe the algorithm for cryptocurrency with a trusted agent to
  someone who knows absolutely nothing about cryptocurrency.How could the algorithm for cryptocurrency be used for a collection
  of distributed agents to keep track of a sequence of events, other
  than buying/selling currency?K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

In the next module we will see how the Bitcoin algorithm addresses
this problem.Advantages of this cryptocurrencyAny agent can get a copy of the tamper-evident ledger and verify that all
transactions in the ledger are valid. Any agent can verify that
the only way in which the ledger is modified is that elements are
append to its tail; all that the agent needs to do is to check that
the pointer to the tail is modified only by appending
elements. Because the ledger is tamper-evident, an agent can check
that the ledger doesn't change while the hash pointer to the end
of the ledger doesn't change.The bank can't forge a transaction because all payers sign the
transaction. Agents can remain anonymous because an agent's only
public information is the agent's public key, and an agent can create
multiple public keys. Every agent can verify the correctness of every
transaction.Disadvantages of this cryptocurrencyUsers may not trust the bank. Transactions are not private because the
bank has a record of all transactions. And the bank is a single point
of failure.Next we'll look at Bitcoin's algorithms for
implementing a cryptocurrency without trusted agents.ReviewWhat iscollision resistance? Why is it relevant to
  cryptocurrencies?Consider the example ofhidingin which Megan hides her
  predictions for the winner of the World Cup. Suppose you could
  choose two different random number generators for creating the
  secret \(r\); in one case the bits of \(r\) are uncorrelated, and in
  the other they are highly correlated. Which generator would you use
  and why?What isbinding? Why is it relevant to hiding information?What ispuzzle friendly?What is atamper-evident data structure? How would you
  implement a tamper-evident linear list? A tamper-evident tree?Describe the algorithm for cryptocurrency with a trusted agent to
  someone who knows absolutely nothing about cryptocurrency.How could the algorithm for cryptocurrency be used for a collection
  of distributed agents to keep track of a sequence of events, other
  than buying/selling currency?K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

The bank can't forge a transaction because all payers sign the
transaction. Agents can remain anonymous because an agent's only
public information is the agent's public key, and an agent can create
multiple public keys. Every agent can verify the correctness of every
transaction.Disadvantages of this cryptocurrencyUsers may not trust the bank. Transactions are not private because the
bank has a record of all transactions. And the bank is a single point
of failure.Next we'll look at Bitcoin's algorithms for
implementing a cryptocurrency without trusted agents.ReviewWhat iscollision resistance? Why is it relevant to
  cryptocurrencies?Consider the example ofhidingin which Megan hides her
  predictions for the winner of the World Cup. Suppose you could
  choose two different random number generators for creating the
  secret \(r\); in one case the bits of \(r\) are uncorrelated, and in
  the other they are highly correlated. Which generator would you use
  and why?What isbinding? Why is it relevant to hiding information?What ispuzzle friendly?What is atamper-evident data structure? How would you
  implement a tamper-evident linear list? A tamper-evident tree?Describe the algorithm for cryptocurrency with a trusted agent to
  someone who knows absolutely nothing about cryptocurrency.How could the algorithm for cryptocurrency be used for a collection
  of distributed agents to keep track of a sequence of events, other
  than buying/selling currency?K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

Next we'll look at Bitcoin's algorithms for
implementing a cryptocurrency without trusted agents.ReviewWhat iscollision resistance? Why is it relevant to
  cryptocurrencies?Consider the example ofhidingin which Megan hides her
  predictions for the winner of the World Cup. Suppose you could
  choose two different random number generators for creating the
  secret \(r\); in one case the bits of \(r\) are uncorrelated, and in
  the other they are highly correlated. Which generator would you use
  and why?What isbinding? Why is it relevant to hiding information?What ispuzzle friendly?What is atamper-evident data structure? How would you
  implement a tamper-evident linear list? A tamper-evident tree?Describe the algorithm for cryptocurrency with a trusted agent to
  someone who knows absolutely nothing about cryptocurrency.How could the algorithm for cryptocurrency be used for a collection
  of distributed agents to keep track of a sequence of events, other
  than buying/selling currency?K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology
--- END Content from https://kmchandy.github.io/Crypto/CryptoCurrencyIntroduction.html ---


--- START Content from https://kmchandy.github.io/Crypto/BitCoin.html ---
TITLE: Crypto/BitCoin.html

We discussed distributed consensus in the modules onPaxosandByzantine Generals with
    writtenandoral messages.The specification of consensus is weaker in
    Bitcoin and the algorithm used to obtain consensus is different
    from those used in Paxos and Byzantine Generals written and oral
    algorithms. The Byzantine Generals algorithms assumes that the
    algorithm operates in rounds, whereas the Bitcoin algorithm
    doesn't require synchronous rounds in which all agents
    participate. The Paxos algorithm does not guarantee progress
    whereas Bitcoin requires progress with high
    probability; you wouldn't want to use coins if you had to wait
    a long time to buy anything.No trusted agentIn an earlier module we described an algorithm that had many of
    the features that we expect from a
    cryptocurrency. That algorithm had, however, a
    characteristic which is problematic to some: It relies on a
trusted agent.
The trusted agent could be theFederal Reservein the
US or acentral bankthat manages a currency.
Two of many reasons given for mistrusting banks are that (1) people may
    want to execute transactions in secret giving only their public
    keys, and (2) central banks may be able to print money whereas
    some cryptocurrencies, such as Bitcoin, limit the total amount of
    coins that can ever exist.
    The Bitcoin algorithm is a modification that eliminates the
    trusted agent from the algorithm given in the previous module.No assumptions about numbers of agentsThe Byzantine Generals algorithm uses an upper bound on the number
    of faulty agents. Paxos assumes that the total number of agents is known. 
    The Bitcoin algorithm makes no assumptions about numbers of faulty and
    non-faulty agents other than that there are a large number of agents.Incentives and transaction feesWhen a currency is managed by a single trusted agent, such as a
bank, we assume that the bank gets some reward for its
service or is paid by a government to carry out this service.
The Bitcoin algorithm pays agents with Bitcoins for checking the validity of
transactions. This payment consists of new coins that are
"mined". More about mining laterA First Proposal for an AlgorithmHow can we modify the algorithm we described earlier so that it works
without a trusted agent? 
Let's try this modification:
Select random agents to play the role of the trusted manager.A step of the algorithm is as follows:
A single agent is
chosen randomly to play the role of the trusted manager. This agent
receives and validates transactions, gathers some of the transactions
into a block, appends the block to the block chain, and
broadcasts the updated block chain. The
other agents update their copies of the block chain when they receive this
value. The system waits for all agents to update their copies and then
executes the next step.Challenges of the Proposed AlgorithmThis algorithm has several challenges.Selecting a single agent.How can the collection of agents select asingleagent to add
  a block to the block chain? The selection of a single agent requires
  all agents to reach a consensus about which agent to 
  select. So solving the problem this way would require solving
  another consensus problem. The Bitcoin algorithm does
  not select a unique agent to add a block to the block chain;
  however, it uses an ingenious mechanism to ensure that multiple agents
  don't attempt to add blocks at about the same time.Synchronization:
  How can the collection of agents wait long enough to ensure that all
  agents have updated their copies of the ledger to the most recent version before
  the ledger is modified again?Consider the following example scenario.  All agents have the same
  copy x of the ledger at some point t. Agent B, selected randomly to
  act as the trusted agent, appends transaction y to the ledger at a
  later point t' at which point B's copy is [x, y].  Then, agent C,
  selected randomly to act as the trusted agent, appends transaction z
  to the ledger at a later point t''. If C's copy is still [x],
  because it has not as yet been updated to [x, y], then after C
  appends z to the ledger, C's copy becomes [x, z].A key property of the algorithm with the trusted agent is that two
  copies of the ledger are either identical or the longer copy
  consists of additional transactions appended to the
  shorter copy.With this property, two copies of the ledger are
  either identical or the shorter copy can eventually "catch up" to the longer copy by
  merely by appending more values.In the example, B may receive
  information about transaction z after receiving information about
  transaction y, whereas C may receive this information in the reverse
  order, which leaves B's copy as [x, y, z] and C's copy as [x, z,
  y]. In this case, the copies remain different forever.The Bitcoin algorithmdoes not guarantee synchronization; however,
  its mechanism helps to make many agents append blocks in the same
  order.Incentives: 
  Why should an agent chosen to play the role of trusted agent agree
  to play that role? What's the incentive? Why wouldn't the trusted
  agent do nothing at all execute its step slowly?Untrustworthy Agents:
  The randomly-chosen trusted manager may not be trustworthy. You can
  imagine what may go wrong in the previous algorithm if the bank was
  dishonest.Next, let's look at how the Bitcoin algorithm addresses challenges 1
and 2. We'll look at challenges 3 and 4 later.Selecting a Single Random AgentHow can an arbitrary set of agents, some of whom may be
malicious, and where the size of the set is unknown, pick a random
agent?Using Puzzles to Select a Single AgentLet's look at a simple situation:
several people solve puzzles in the same room. When a
person solves her puzzle, she yells "I won!". When a
person hears that somebody else has won she stops solving her puzzle.If everybody starts at the same instant and take the same time
then there will be collisions --- many will claim to win at the same time.
If, however, the time to solve
a puzzle is a random variable with a flat distribution then collisions
are unlikely. Bitcoin uses thepuzzle-friendly propertyof cryptographic hash
functions discussed in theprevious module.Now, instead of people being all in the same room, assume that they
are competing across a network. When a person solves a puzzle,
she broadcasts a "I won" message. When a person working on a puzzle
gets a "I won" message from somebody else, she stops working on her
puzzle.Collisions are likely if the expected time to solve the puzzle
is small (say a millisecond) compared with the expected time (say a
minute) for a message broadcast by one person to reach others. Message
delays may cause multiple people to solve their puzzles before
receiving "I won" messages.
Collisions are unlikely when times to solve puzzles are much greater
than message delays.We could attempt to use timestamps: When a person solves her puzzle
she broadcasts a "I won" message and the time at which she finished
solving the puzzle. If a person gets a "I won" message with an earlier
timestamp then she concedes. This approach is problematic because a
devious agent may not solve the puzzle, or may set her timestamp to an
earlier value.Puzzles in BitcoinNext let's look at the puzzles used in Bitcoin.
Each agent has its own copy of the block chain.
An agent \(A\) collects a set \(trans\) of transactions that haven't as yet been added
to \(A\)'s copy of the block chain.
Agent \(A\) proposes
to append a block to the chain where the block
consists
of the set \(trans\) in the
following way.Let \(ptr\) be the pointer to agent \(A\)'s copy of the block chain. 
Agent \(A\) can add a block containing \(trans\) to the chain only if it
proves that it has solved the following problem --- the "puzzle."Find a number, called \(nonce\), such that:\(H(nonce + ptr + trans) < target \)where \(+\) is the concatenation operator , andtargetis a
given value.For the time being
assume that target is a constant; later, we'll see that it decreases very
slowly over time.
The smaller the value oftargetthe
greater the expected time to solve the puzzle.The time to solve aBitcoin puzzleis a
random variable with a flat distribution. Each proposer 
of a block is probably solving a different puzzle because the block of
transactions that it is aggregating is likely to be different from
that of other proposers. Agents have different amounts of
computing capacity, and the time to solve a puzzle decreases with
capacity.
Agents are unlikely to start solving their puzzles
at the same instant. For these reasons, it is possible, but unlikely, that many agents
will solve their puzzles at the same time.Using puzzles to identify a single random agent leaves us with at least three challenges: (1) Collisions
will occur; (2) agents may be devious --- they may claim to have
solved puzzles when they haven't; and (3) agents with computing power
that far exceeds those of others will solve their puzzles faster than
others do --- and so though agents are selected randomly, those with
large computing power are likely to be selected more often.Attempts at SynchronizationWhen an agent \(A\) appends a new block \(B\) to a block chain \(L\) it broadcasts
the new chain \(L + B\). This is analogous to a person shouting "I won" in the
example given earlier.When a (non-devious) agent \(A'\), which proposes to extend chain
\(L\), gets a message saying that \(L\) has already been extended to
\(L + B\) then \(A'\)
stops attempting to append a block to \(L\).
Instead, \(A\) starts again with a
new set of transactions that it proposes to append to the
extended chain \(L + B\).If the message delay between agents \(A\) and \(A'\) is small compared
to the time to solve puzzles, then a collision
between \(A\) and \(A'\) is unlikely, but still possible. So, it is
possible that \(A\) broadcasts \(L + B\) while \(A'\) broadcasts
\(L + B'\) at about the same time. So different agents may have
different copies of the block chain.
What is the equivalent of the "true" system-wide
block chain when different agents have different copies?The Bitcoin algorithm does not use synchrony to deal with this
issue. The problem of different copies of the block chain 
extant at the same time is handled in an ingenious asynchronous way that we
describe later.Managing Concurrent UpdatesA key step of the Bitcoin algorithm that updates local copies of block
chains is as follows. After an agent creates a block and appends the
newly created block to its local copy it broadcasts its copy of
the block chain.When an agent \(A\) gets a message containing a copy
of another agent's block chain, agent \(A\) sets its local copy to the
block chain in the messageif and only ifthe length of the
block chain in the message exceeds the length of \(A\)'s local copy.Let's look at a scenario.
For this scenario, \(X\) and \(Y\) are single blocks.
Assume that agent \(A\)'s copy of the block chain
is \(L\) when \(A\) receives a message containing the block chain \(L
+ X\). Because the length of the block chain \(L + X\) is bigger than \(A\)'s
copy, \(A\) sets its copy to \(L + X\).Now suppose agent \(A\) gets a
message containing the block chain \(L + Y\); what does \(A\) do?
Agent \(A\) ignores the message because the length of \(L + Y\) does not exceed
that of \(A\)'s current copy, \(L + X\).Continuing CollisionsLet's continue the above scenario.
Can block \(Y\) become part of \(A\)'s
chain, or will it remain forever an "orphan" block
as far as \(A\) is concerned?Here's a possible scenario.
An agent with a block
chain copy \(L + 
X\), solves its puzzle and appends a block \(X_{1}\) to get a new copy
\(L + X + X_{1}\) of the block chain which the agent broadcasts.
At the same time, another agent with a block
chain copy \(L + 
Y\), solves its puzzle and appends a block \(Y_{1}\) to get a new copy
\(L + Y + Y_{1}\) which is broadcast.
If \(A\) receives \(L + Y + Y_{1}\) before receiving \(L + X + X_{1}\)
then \(A\) will set its chain to \(L + Y + Y_{1}\), and then reject
\(L + X + X_{1}\).You can construct a scenario with a sequence of collisions between
agents appending blocks \(X_{i}\) and other agents appending blocks
\(Y_{j}\) so that \(A\)'s chain switches back and forth between \(X\)
and \(Y\) values.Long sequences of collisions are unlikely, and the longer the sequence
the less the probability of continuing collisions.So how can an agent determine whether a block is in the chain? And so
how can an agent find out if a transaction has been executed? You
sell your used bicycle to somebody for coins, but how do you know if
that transaction becomes part oftheblock chain when different
agents have different copies? And so how do you know that you can
spend those coins that
you should have received for your bicycle?Confidence that a Block is in the ChainIn this section we discuss the behavior of non-devious agents; we will
show how the algorithm handles devious agents later.
Suppose an agent's copy of the block chain is \(X + X_{1} + X_{2}
+ \ldots + X_{K}\), and another agent's copy is \(Y + ? + ? +
\ldots\), where \(?\) represents arbitrary values. What is the
likelihood that \(Y \neq X\)?\(Y\) can be different from \(X\) only if there are a sequence of
\(K\) or more collisions. And the likelihood of a sequence of
collisions decreases with \(K\). Likewise, the likelihood that an
agent never receives a block chain containing \(X\) decreases with
time, and so decreases with \(K\). So, if \(K\) is large then with
very high probability, \(X\) is part of every agent's block chain.Now suppose an agent's copy of the block chain is \(L + X + X_{1} + X_{2}
+ \ldots + X_{K}\). What is the likelihood
that another agent's copy is \(L + Y + ? + \ldots\) where \(L\) is an
arbitrary sequence? By the same
argument, when if \(K\) is large then with
very high probability, \(X\) is part of every block chain.For practical purposes,
many agents assume that if \(K > 6\) then \(L + X\) is a prefix of
most agents' block chains.See the Princeton Bitcoin book.Suppose agent \(A\) gets \(N\) Bitcoins in transaction \(X\). If \(K =
0\) then an agent can't be confident that \(A\) ever received these coins
because this transaction may not persist in the block chain. As \(K\)
increases agents become more confident that the transaction is in the
block chain and that \(A\) did, indeed, receive these coins.Fig.1: More Confidence in Older Blocks in the
    ChainWhat happens to Orphan Transactions?An agent may append a block \(Y\) to its chain, and this block may be
dropped from the chains of all agents and never reappear after some
point. We saw a scenario in which this happens.
Such a block is an "orphan," because no agent has a record of that
transaction after some time.
If an orphan block contains the transaction in which you
sold your bicycle in exchange for coins, then will you ever be able to
spend your coins? Yes, you will get your coins as we see next.Agents aggregate transactions that have not
appeared in the agent's block chain into blocks and propose to append these
blocks to the chain. A transaction that does not appear in block
chains will be agrregated eventually by some agent and inserted into
a block in the chian. Though the orphan block disappears the
transactions in the block do not.IncentivesNext, looks look at challenge number 3. Why should an agent create
blocks of validated transactions?Because the agent gets paid! Payment
is from either ablock rewardor transaction fees.Block rewardsAn agent that creates a block gets a specified number of Bitcoins for
itself as a reward called ablock reward.
The Bitcoins in a block reward are created by making the
block; these Bitcoins don't exist until the block is created. The process of making
blocks and acquiring block rewards is called "mining."Mining is the only way of creating new Bitcoins.When Bitcoin started the reward for creating a block was 50 Bitcoins.
The reward
halves after the creation of a certain number (210,000) of blocks. The
reward was reduced to 25 in 2013 and to 12.5 in 2018.
Block rewards will vanish at some point in the future.
The
total number of Bitcoins that can ever be created has an upper bound:
about 21 million.(Bitcoins can be lost. An agent may lose the hash pointer to the
transaction that gave the agent ownership of the coin, or an agent may
lose its private key.)Even when block rewards vanish, miners will continue to
mine provided that they get paid transaction fees. A
transaction fee is a payment by payers and payees to
miners. Transaction fees are voluntary. A high-fee for a
transaction is an incentive to miners to put this transaction into a
block quickly. So, it's possible that agents that offer no fee or low
fees may have to wait longer for their transactions to enter the
block chain.Incentives are critical for Bitcoin. Miners get paid to
get their blocks into the long-term "consensus" block chain.
Miners have an incentive to police the block chain because 
they don't get paid for appending erroneous blocks. If a miner appends
an erroneous block to the chain then other
miners won't extend chains containing the erroneous block, and so the
erroneous block will become an orphan.Any agent can check whether its copy of the block chain is
valid; however, agents making ordinary transactions don't
need to do so because there are many miners each of whom has an
incentive to ensure that the block chain is legitimate.AttacksNext, looks look at challenge number 4.Stealing coinsCan an agent steal a coin from an agent \(X\) by appending a block to
the chain where the block contains a transaction in which \(X\) gives
coins to \(Y\)?No, this can't happen thanks to cryptography. A transaction into which
\(X\) puts coins is valid only if \(X\) signs the transaction.  \(Y\)
cannot forge \(X\)'s signature, and so \(Y\) cannot create blocks that
contain such fraudulent transactions.Double spendCan an agent spend the same coin twice? Can an agent buy something
without paying for it?Consider the following transaction using conventional checks issued by
banks. A buyer gives a seller a check for $\(100,000\) for a house.
The house is put in escrow.  When the check clears and the seller
receives the payment the buyer gets possession of the house. The legal
process that includes notaries, real estate agents, and banks, helps
ensure that the transaction concludes correctly or is aborted correctly.Next, let's look at a transaction in which a person buys a video
online by paying the seller Bitcoins. The buyer and seller know each
other by their public keys and by their online addresses. 
The buyer broadcasts the transaction in which the buyer gives the seller
the payment in Bitcoins.
The amount is specified as
a pair (transaction id, array index) described in the sectionpay
transactionsinthe module introducing crypto currencies.A miner puts the transaction into a block \(X\);
appends the block to its copy \(L\) of the block chain; and broadcasts the extended
block chain \(L + X\).
When the seller gets a copy of the block chain \(L + X\), the
seller concludes that it has received the payment from the
buyer because the 
transaction has been recorded in a block chain. So the seller gives
the video to the buyer.The buyer cheats. The buyer creates a transaction in which the buyer
transfers the same Bitcoins  to the buyer itself.
A miner creates a block \(Y\) that includes this transaction.
A miner who has only received block
chain \(L\) (and hasn't yet received chain \(L + X\)) appends \(Y\) to
\(L\) to get a chain \(L + Y\), and broacasts \(L + Y\).Now we have a situation in which one
miner broadcasts a legitimate block chain \(L + X\) and a different
miner broadcasts a legitimate block chain \(L + Y\). Both chains have
the same length.
A miner with chain \(L + X\) does not know at this point that another
minder has chain \(L + Y\). So, miners will extend both block chains.
(Note that the algorithm will not permit chains \(L + X + Y\) or \(L +
Y + X\).)We've seen this situation before:  look atblock collisionsdescribed
earlier in this module. Both block chains will be extended, but
eventually, with very high probability, one of the blocks \(X\) or
\(Y\) will drop out of chain.How should sellers protect themselves?What is the equivalent of the buyer's check clearing in the bank?A seller should give the item to the buyer only after the transaction
appears with high confidence in a block in the chain --- see Figure 1.
The seller listens to block chains broadcast by miners. If the seller
gets a block chain \(L + X + L'\) where \(L'\) is itself a long block chain
then the seller has high confidence that the transaction is in the
permanent record.The seller waits to get block chains in which its transaction appears
in a block which is then followed by \(m\) blocks, for large
\(m\). The larger the value of \(m\), the greater the seller's
confidence, but the longer the buyer has to wait to get paid.
length of the extension \(L'\).A value of \(m = 6\) gives adequate confidence in most cases.Fraudulent minersA miner gets paid for every block the miner creates; so, why shouldn't
the miner create fraudulent blocks and get paid for them?The answer is the same as that for the double-spending attack. A miner
may have 
created a block and appended it to the chain; however, a suspicious
agent (and we hope that all agents are suspicious!) will not accept
the block until many blocks have been appended to the chain after
it. Other miners won't append their blocks to an invalid one. The
invalid blocks will become permanent orphans, and so the fraudulent
miner won't be able to spend the coins in these blocks.Fifty One Percent AttacksA 51% attack can be carried out by an agent that has more mining
power (e.g. 51% or more) than all other agents combined. The higher
the proportion of mining power of a single agent, the greater the
chances that attacks by that agent will succeed. You can see the
danger of a single agent having predominant mining power by thinking
about an agent with say, 99% of the total mining power. This agent
can mine so much faster than others that it can manipulate the block
chain in many ways. It can create double-spend transactions and deny
services to some transactions.A group of miners can collude to gain predominant mining power. Also,
miners canrentCloud-based systems for mining --- as opposed to having toownhuge data centers. An attacker can rent a large system for the specific purpose and
duration of an attack. The public may not know if and when such
attacks are successful becausecryptocurrencies do not have an
incentive to publish such attacks.Denial of serviceCan agents collude so that an agent \(X\)'s transactions never get
into blocks and so never get processed?An agent's identity does not appear in a transaction, only a public
key does. An agent can create new public keys at will. So let's ask
another question: can agents collude so that transactions with a
specific public key do not get into blocks?Only an agent that can solve puzzles in reasonable time can make
blocks. These agents have significant computational power. One can
concoct a situation where many agents with significant computation
power collude to avoid transactions from a specific public key. This
could have the effect of slowing processing of certain
transactions. However, such a situation isn't likely to arise because
agents have an incentive to create blocks and so they compete ---
rather than collude --- with each other.
Colluding
agents, with massive computing power, may help to deny or slow service
to a public key but not to another agent because agents can create
public keys at will.Further ReadingThere are many issues that we have not covered. This material only
covers the basics from the point of view of distributed algorithms.ReviewWhat is ablock chain?How does the algorithm deal with the situation in which one agent
receives a message to extend its copy of the block chain with a block
\(X\) while another agent receives a message to extend its copy of the
  block chain with a different block \(Y\)?Could there be an infinite sequence of collisions? Is that likely?What is mining for cryptocurrency? Suppose only a small number (say
  two organizations) mined most (say 99%) of Bitcoins. Would that be a
  problem?Why isdouble spendingunlikely to be successful?K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology



Figure Link: https://kmchandy.github.io/Crypto/Bitcoin/Slide1.jpg

Caption: Fig.1: More Confidence in Older Blocks in the
    Chain


A step of the algorithm is as follows:
A single agent is
chosen randomly to play the role of the trusted manager. This agent
receives and validates transactions, gathers some of the transactions
into a block, appends the block to the block chain, and
broadcasts the updated block chain. The
other agents update their copies of the block chain when they receive this
value. The system waits for all agents to update their copies and then
executes the next step.Challenges of the Proposed AlgorithmThis algorithm has several challenges.Selecting a single agent.How can the collection of agents select asingleagent to add
  a block to the block chain? The selection of a single agent requires
  all agents to reach a consensus about which agent to 
  select. So solving the problem this way would require solving
  another consensus problem. The Bitcoin algorithm does
  not select a unique agent to add a block to the block chain;
  however, it uses an ingenious mechanism to ensure that multiple agents
  don't attempt to add blocks at about the same time.Synchronization:
  How can the collection of agents wait long enough to ensure that all
  agents have updated their copies of the ledger to the most recent version before
  the ledger is modified again?Consider the following example scenario.  All agents have the same
  copy x of the ledger at some point t. Agent B, selected randomly to
  act as the trusted agent, appends transaction y to the ledger at a
  later point t' at which point B's copy is [x, y].  Then, agent C,
  selected randomly to act as the trusted agent, appends transaction z
  to the ledger at a later point t''. If C's copy is still [x],
  because it has not as yet been updated to [x, y], then after C
  appends z to the ledger, C's copy becomes [x, z].A key property of the algorithm with the trusted agent is that two
  copies of the ledger are either identical or the longer copy
  consists of additional transactions appended to the
  shorter copy.With this property, two copies of the ledger are
  either identical or the shorter copy can eventually "catch up" to the longer copy by
  merely by appending more values.In the example, B may receive
  information about transaction z after receiving information about
  transaction y, whereas C may receive this information in the reverse
  order, which leaves B's copy as [x, y, z] and C's copy as [x, z,
  y]. In this case, the copies remain different forever.The Bitcoin algorithmdoes not guarantee synchronization; however,
  its mechanism helps to make many agents append blocks in the same
  order.Incentives: 
  Why should an agent chosen to play the role of trusted agent agree
  to play that role? What's the incentive? Why wouldn't the trusted
  agent do nothing at all execute its step slowly?Untrustworthy Agents:
  The randomly-chosen trusted manager may not be trustworthy. You can
  imagine what may go wrong in the previous algorithm if the bank was
  dishonest.Next, let's look at how the Bitcoin algorithm addresses challenges 1
and 2. We'll look at challenges 3 and 4 later.Selecting a Single Random AgentHow can an arbitrary set of agents, some of whom may be
malicious, and where the size of the set is unknown, pick a random
agent?Using Puzzles to Select a Single AgentLet's look at a simple situation:
several people solve puzzles in the same room. When a
person solves her puzzle, she yells "I won!". When a
person hears that somebody else has won she stops solving her puzzle.If everybody starts at the same instant and take the same time
then there will be collisions --- many will claim to win at the same time.
If, however, the time to solve
a puzzle is a random variable with a flat distribution then collisions
are unlikely. Bitcoin uses thepuzzle-friendly propertyof cryptographic hash
functions discussed in theprevious module.Now, instead of people being all in the same room, assume that they
are competing across a network. When a person solves a puzzle,
she broadcasts a "I won" message. When a person working on a puzzle
gets a "I won" message from somebody else, she stops working on her
puzzle.Collisions are likely if the expected time to solve the puzzle
is small (say a millisecond) compared with the expected time (say a
minute) for a message broadcast by one person to reach others. Message
delays may cause multiple people to solve their puzzles before
receiving "I won" messages.
Collisions are unlikely when times to solve puzzles are much greater
than message delays.We could attempt to use timestamps: When a person solves her puzzle
she broadcasts a "I won" message and the time at which she finished
solving the puzzle. If a person gets a "I won" message with an earlier
timestamp then she concedes. This approach is problematic because a
devious agent may not solve the puzzle, or may set her timestamp to an
earlier value.Puzzles in BitcoinNext let's look at the puzzles used in Bitcoin.
Each agent has its own copy of the block chain.
An agent \(A\) collects a set \(trans\) of transactions that haven't as yet been added
to \(A\)'s copy of the block chain.
Agent \(A\) proposes
to append a block to the chain where the block
consists
of the set \(trans\) in the
following way.Let \(ptr\) be the pointer to agent \(A\)'s copy of the block chain. 
Agent \(A\) can add a block containing \(trans\) to the chain only if it
proves that it has solved the following problem --- the "puzzle."Find a number, called \(nonce\), such that:\(H(nonce + ptr + trans) < target \)where \(+\) is the concatenation operator , andtargetis a
given value.For the time being
assume that target is a constant; later, we'll see that it decreases very
slowly over time.
The smaller the value oftargetthe
greater the expected time to solve the puzzle.The time to solve aBitcoin puzzleis a
random variable with a flat distribution. Each proposer 
of a block is probably solving a different puzzle because the block of
transactions that it is aggregating is likely to be different from
that of other proposers. Agents have different amounts of
computing capacity, and the time to solve a puzzle decreases with
capacity.
Agents are unlikely to start solving their puzzles
at the same instant. For these reasons, it is possible, but unlikely, that many agents
will solve their puzzles at the same time.Using puzzles to identify a single random agent leaves us with at least three challenges: (1) Collisions
will occur; (2) agents may be devious --- they may claim to have
solved puzzles when they haven't; and (3) agents with computing power
that far exceeds those of others will solve their puzzles faster than
others do --- and so though agents are selected randomly, those with
large computing power are likely to be selected more often.Attempts at SynchronizationWhen an agent \(A\) appends a new block \(B\) to a block chain \(L\) it broadcasts
the new chain \(L + B\). This is analogous to a person shouting "I won" in the
example given earlier.When a (non-devious) agent \(A'\), which proposes to extend chain
\(L\), gets a message saying that \(L\) has already been extended to
\(L + B\) then \(A'\)
stops attempting to append a block to \(L\).
Instead, \(A\) starts again with a
new set of transactions that it proposes to append to the
extended chain \(L + B\).If the message delay between agents \(A\) and \(A'\) is small compared
to the time to solve puzzles, then a collision
between \(A\) and \(A'\) is unlikely, but still possible. So, it is
possible that \(A\) broadcasts \(L + B\) while \(A'\) broadcasts
\(L + B'\) at about the same time. So different agents may have
different copies of the block chain.
What is the equivalent of the "true" system-wide
block chain when different agents have different copies?The Bitcoin algorithm does not use synchrony to deal with this
issue. The problem of different copies of the block chain 
extant at the same time is handled in an ingenious asynchronous way that we
describe later.Managing Concurrent UpdatesA key step of the Bitcoin algorithm that updates local copies of block
chains is as follows. After an agent creates a block and appends the
newly created block to its local copy it broadcasts its copy of
the block chain.When an agent \(A\) gets a message containing a copy
of another agent's block chain, agent \(A\) sets its local copy to the
block chain in the messageif and only ifthe length of the
block chain in the message exceeds the length of \(A\)'s local copy.Let's look at a scenario.
For this scenario, \(X\) and \(Y\) are single blocks.
Assume that agent \(A\)'s copy of the block chain
is \(L\) when \(A\) receives a message containing the block chain \(L
+ X\). Because the length of the block chain \(L + X\) is bigger than \(A\)'s
copy, \(A\) sets its copy to \(L + X\).Now suppose agent \(A\) gets a
message containing the block chain \(L + Y\); what does \(A\) do?
Agent \(A\) ignores the message because the length of \(L + Y\) does not exceed
that of \(A\)'s current copy, \(L + X\).Continuing CollisionsLet's continue the above scenario.
Can block \(Y\) become part of \(A\)'s
chain, or will it remain forever an "orphan" block
as far as \(A\) is concerned?Here's a possible scenario.
An agent with a block
chain copy \(L + 
X\), solves its puzzle and appends a block \(X_{1}\) to get a new copy
\(L + X + X_{1}\) of the block chain which the agent broadcasts.
At the same time, another agent with a block
chain copy \(L + 
Y\), solves its puzzle and appends a block \(Y_{1}\) to get a new copy
\(L + Y + Y_{1}\) which is broadcast.
If \(A\) receives \(L + Y + Y_{1}\) before receiving \(L + X + X_{1}\)
then \(A\) will set its chain to \(L + Y + Y_{1}\), and then reject
\(L + X + X_{1}\).You can construct a scenario with a sequence of collisions between
agents appending blocks \(X_{i}\) and other agents appending blocks
\(Y_{j}\) so that \(A\)'s chain switches back and forth between \(X\)
and \(Y\) values.Long sequences of collisions are unlikely, and the longer the sequence
the less the probability of continuing collisions.So how can an agent determine whether a block is in the chain? And so
how can an agent find out if a transaction has been executed? You
sell your used bicycle to somebody for coins, but how do you know if
that transaction becomes part oftheblock chain when different
agents have different copies? And so how do you know that you can
spend those coins that
you should have received for your bicycle?Confidence that a Block is in the ChainIn this section we discuss the behavior of non-devious agents; we will
show how the algorithm handles devious agents later.
Suppose an agent's copy of the block chain is \(X + X_{1} + X_{2}
+ \ldots + X_{K}\), and another agent's copy is \(Y + ? + ? +
\ldots\), where \(?\) represents arbitrary values. What is the
likelihood that \(Y \neq X\)?\(Y\) can be different from \(X\) only if there are a sequence of
\(K\) or more collisions. And the likelihood of a sequence of
collisions decreases with \(K\). Likewise, the likelihood that an
agent never receives a block chain containing \(X\) decreases with
time, and so decreases with \(K\). So, if \(K\) is large then with
very high probability, \(X\) is part of every agent's block chain.Now suppose an agent's copy of the block chain is \(L + X + X_{1} + X_{2}
+ \ldots + X_{K}\). What is the likelihood
that another agent's copy is \(L + Y + ? + \ldots\) where \(L\) is an
arbitrary sequence? By the same
argument, when if \(K\) is large then with
very high probability, \(X\) is part of every block chain.For practical purposes,
many agents assume that if \(K > 6\) then \(L + X\) is a prefix of
most agents' block chains.See the Princeton Bitcoin book.Suppose agent \(A\) gets \(N\) Bitcoins in transaction \(X\). If \(K =
0\) then an agent can't be confident that \(A\) ever received these coins
because this transaction may not persist in the block chain. As \(K\)
increases agents become more confident that the transaction is in the
block chain and that \(A\) did, indeed, receive these coins.Fig.1: More Confidence in Older Blocks in the
    ChainWhat happens to Orphan Transactions?An agent may append a block \(Y\) to its chain, and this block may be
dropped from the chains of all agents and never reappear after some
point. We saw a scenario in which this happens.
Such a block is an "orphan," because no agent has a record of that
transaction after some time.
If an orphan block contains the transaction in which you
sold your bicycle in exchange for coins, then will you ever be able to
spend your coins? Yes, you will get your coins as we see next.Agents aggregate transactions that have not
appeared in the agent's block chain into blocks and propose to append these
blocks to the chain. A transaction that does not appear in block
chains will be agrregated eventually by some agent and inserted into
a block in the chian. Though the orphan block disappears the
transactions in the block do not.IncentivesNext, looks look at challenge number 3. Why should an agent create
blocks of validated transactions?Because the agent gets paid! Payment
is from either ablock rewardor transaction fees.Block rewardsAn agent that creates a block gets a specified number of Bitcoins for
itself as a reward called ablock reward.
The Bitcoins in a block reward are created by making the
block; these Bitcoins don't exist until the block is created. The process of making
blocks and acquiring block rewards is called "mining."Mining is the only way of creating new Bitcoins.When Bitcoin started the reward for creating a block was 50 Bitcoins.
The reward
halves after the creation of a certain number (210,000) of blocks. The
reward was reduced to 25 in 2013 and to 12.5 in 2018.
Block rewards will vanish at some point in the future.
The
total number of Bitcoins that can ever be created has an upper bound:
about 21 million.(Bitcoins can be lost. An agent may lose the hash pointer to the
transaction that gave the agent ownership of the coin, or an agent may
lose its private key.)Even when block rewards vanish, miners will continue to
mine provided that they get paid transaction fees. A
transaction fee is a payment by payers and payees to
miners. Transaction fees are voluntary. A high-fee for a
transaction is an incentive to miners to put this transaction into a
block quickly. So, it's possible that agents that offer no fee or low
fees may have to wait longer for their transactions to enter the
block chain.Incentives are critical for Bitcoin. Miners get paid to
get their blocks into the long-term "consensus" block chain.
Miners have an incentive to police the block chain because 
they don't get paid for appending erroneous blocks. If a miner appends
an erroneous block to the chain then other
miners won't extend chains containing the erroneous block, and so the
erroneous block will become an orphan.Any agent can check whether its copy of the block chain is
valid; however, agents making ordinary transactions don't
need to do so because there are many miners each of whom has an
incentive to ensure that the block chain is legitimate.AttacksNext, looks look at challenge number 4.Stealing coinsCan an agent steal a coin from an agent \(X\) by appending a block to
the chain where the block contains a transaction in which \(X\) gives
coins to \(Y\)?No, this can't happen thanks to cryptography. A transaction into which
\(X\) puts coins is valid only if \(X\) signs the transaction.  \(Y\)
cannot forge \(X\)'s signature, and so \(Y\) cannot create blocks that
contain such fraudulent transactions.Double spendCan an agent spend the same coin twice? Can an agent buy something
without paying for it?Consider the following transaction using conventional checks issued by
banks. A buyer gives a seller a check for $\(100,000\) for a house.
The house is put in escrow.  When the check clears and the seller
receives the payment the buyer gets possession of the house. The legal
process that includes notaries, real estate agents, and banks, helps
ensure that the transaction concludes correctly or is aborted correctly.Next, let's look at a transaction in which a person buys a video
online by paying the seller Bitcoins. The buyer and seller know each
other by their public keys and by their online addresses. 
The buyer broadcasts the transaction in which the buyer gives the seller
the payment in Bitcoins.
The amount is specified as
a pair (transaction id, array index) described in the sectionpay
transactionsinthe module introducing crypto currencies.A miner puts the transaction into a block \(X\);
appends the block to its copy \(L\) of the block chain; and broadcasts the extended
block chain \(L + X\).
When the seller gets a copy of the block chain \(L + X\), the
seller concludes that it has received the payment from the
buyer because the 
transaction has been recorded in a block chain. So the seller gives
the video to the buyer.The buyer cheats. The buyer creates a transaction in which the buyer
transfers the same Bitcoins  to the buyer itself.
A miner creates a block \(Y\) that includes this transaction.
A miner who has only received block
chain \(L\) (and hasn't yet received chain \(L + X\)) appends \(Y\) to
\(L\) to get a chain \(L + Y\), and broacasts \(L + Y\).Now we have a situation in which one
miner broadcasts a legitimate block chain \(L + X\) and a different
miner broadcasts a legitimate block chain \(L + Y\). Both chains have
the same length.
A miner with chain \(L + X\) does not know at this point that another
minder has chain \(L + Y\). So, miners will extend both block chains.
(Note that the algorithm will not permit chains \(L + X + Y\) or \(L +
Y + X\).)We've seen this situation before:  look atblock collisionsdescribed
earlier in this module. Both block chains will be extended, but
eventually, with very high probability, one of the blocks \(X\) or
\(Y\) will drop out of chain.How should sellers protect themselves?What is the equivalent of the buyer's check clearing in the bank?A seller should give the item to the buyer only after the transaction
appears with high confidence in a block in the chain --- see Figure 1.
The seller listens to block chains broadcast by miners. If the seller
gets a block chain \(L + X + L'\) where \(L'\) is itself a long block chain
then the seller has high confidence that the transaction is in the
permanent record.The seller waits to get block chains in which its transaction appears
in a block which is then followed by \(m\) blocks, for large
\(m\). The larger the value of \(m\), the greater the seller's
confidence, but the longer the buyer has to wait to get paid.
length of the extension \(L'\).A value of \(m = 6\) gives adequate confidence in most cases.Fraudulent minersA miner gets paid for every block the miner creates; so, why shouldn't
the miner create fraudulent blocks and get paid for them?The answer is the same as that for the double-spending attack. A miner
may have 
created a block and appended it to the chain; however, a suspicious
agent (and we hope that all agents are suspicious!) will not accept
the block until many blocks have been appended to the chain after
it. Other miners won't append their blocks to an invalid one. The
invalid blocks will become permanent orphans, and so the fraudulent
miner won't be able to spend the coins in these blocks.Fifty One Percent AttacksA 51% attack can be carried out by an agent that has more mining
power (e.g. 51% or more) than all other agents combined. The higher
the proportion of mining power of a single agent, the greater the
chances that attacks by that agent will succeed. You can see the
danger of a single agent having predominant mining power by thinking
about an agent with say, 99% of the total mining power. This agent
can mine so much faster than others that it can manipulate the block
chain in many ways. It can create double-spend transactions and deny
services to some transactions.A group of miners can collude to gain predominant mining power. Also,
miners canrentCloud-based systems for mining --- as opposed to having toownhuge data centers. An attacker can rent a large system for the specific purpose and
duration of an attack. The public may not know if and when such
attacks are successful becausecryptocurrencies do not have an
incentive to publish such attacks.Denial of serviceCan agents collude so that an agent \(X\)'s transactions never get
into blocks and so never get processed?An agent's identity does not appear in a transaction, only a public
key does. An agent can create new public keys at will. So let's ask
another question: can agents collude so that transactions with a
specific public key do not get into blocks?Only an agent that can solve puzzles in reasonable time can make
blocks. These agents have significant computational power. One can
concoct a situation where many agents with significant computation
power collude to avoid transactions from a specific public key. This
could have the effect of slowing processing of certain
transactions. However, such a situation isn't likely to arise because
agents have an incentive to create blocks and so they compete ---
rather than collude --- with each other.
Colluding
agents, with massive computing power, may help to deny or slow service
to a public key but not to another agent because agents can create
public keys at will.Further ReadingThere are many issues that we have not covered. This material only
covers the basics from the point of view of distributed algorithms.ReviewWhat is ablock chain?How does the algorithm deal with the situation in which one agent
receives a message to extend its copy of the block chain with a block
\(X\) while another agent receives a message to extend its copy of the
  block chain with a different block \(Y\)?Could there be an infinite sequence of collisions? Is that likely?What is mining for cryptocurrency? Suppose only a small number (say
  two organizations) mined most (say 99%) of Bitcoins. Would that be a
  problem?Why isdouble spendingunlikely to be successful?K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology



Figure Link: https://kmchandy.github.io/Crypto/Bitcoin/Slide1.jpg

Caption: Fig.1: More Confidence in Older Blocks in the
    Chain


Consider the following example scenario.  All agents have the same
  copy x of the ledger at some point t. Agent B, selected randomly to
  act as the trusted agent, appends transaction y to the ledger at a
  later point t' at which point B's copy is [x, y].  Then, agent C,
  selected randomly to act as the trusted agent, appends transaction z
  to the ledger at a later point t''. If C's copy is still [x],
  because it has not as yet been updated to [x, y], then after C
  appends z to the ledger, C's copy becomes [x, z].A key property of the algorithm with the trusted agent is that two
  copies of the ledger are either identical or the longer copy
  consists of additional transactions appended to the
  shorter copy.With this property, two copies of the ledger are
  either identical or the shorter copy can eventually "catch up" to the longer copy by
  merely by appending more values.In the example, B may receive
  information about transaction z after receiving information about
  transaction y, whereas C may receive this information in the reverse
  order, which leaves B's copy as [x, y, z] and C's copy as [x, z,
  y]. In this case, the copies remain different forever.The Bitcoin algorithmdoes not guarantee synchronization; however,
  its mechanism helps to make many agents append blocks in the same
  order.



Figure Link: https://kmchandy.github.io/Crypto/Bitcoin/Slide1.jpg

Caption: Fig.1: More Confidence in Older Blocks in the
    Chain


In the example, B may receive
  information about transaction z after receiving information about
  transaction y, whereas C may receive this information in the reverse
  order, which leaves B's copy as [x, y, z] and C's copy as [x, z,
  y]. In this case, the copies remain different forever.The Bitcoin algorithmdoes not guarantee synchronization; however,
  its mechanism helps to make many agents append blocks in the same
  order.



Figure Link: https://kmchandy.github.io/Crypto/Bitcoin/Slide1.jpg

Caption: Fig.1: More Confidence in Older Blocks in the
    Chain


The Bitcoin algorithmdoes not guarantee synchronization; however,
  its mechanism helps to make many agents append blocks in the same
  order.



Figure Link: https://kmchandy.github.io/Crypto/Bitcoin/Slide1.jpg

Caption: Fig.1: More Confidence in Older Blocks in the
    Chain


If everybody starts at the same instant and take the same time
then there will be collisions --- many will claim to win at the same time.
If, however, the time to solve
a puzzle is a random variable with a flat distribution then collisions
are unlikely. Bitcoin uses thepuzzle-friendly propertyof cryptographic hash
functions discussed in theprevious module.Now, instead of people being all in the same room, assume that they
are competing across a network. When a person solves a puzzle,
she broadcasts a "I won" message. When a person working on a puzzle
gets a "I won" message from somebody else, she stops working on her
puzzle.Collisions are likely if the expected time to solve the puzzle
is small (say a millisecond) compared with the expected time (say a
minute) for a message broadcast by one person to reach others. Message
delays may cause multiple people to solve their puzzles before
receiving "I won" messages.
Collisions are unlikely when times to solve puzzles are much greater
than message delays.We could attempt to use timestamps: When a person solves her puzzle
she broadcasts a "I won" message and the time at which she finished
solving the puzzle. If a person gets a "I won" message with an earlier
timestamp then she concedes. This approach is problematic because a
devious agent may not solve the puzzle, or may set her timestamp to an
earlier value.Puzzles in BitcoinNext let's look at the puzzles used in Bitcoin.
Each agent has its own copy of the block chain.
An agent \(A\) collects a set \(trans\) of transactions that haven't as yet been added
to \(A\)'s copy of the block chain.
Agent \(A\) proposes
to append a block to the chain where the block
consists
of the set \(trans\) in the
following way.Let \(ptr\) be the pointer to agent \(A\)'s copy of the block chain. 
Agent \(A\) can add a block containing \(trans\) to the chain only if it
proves that it has solved the following problem --- the "puzzle."Find a number, called \(nonce\), such that:\(H(nonce + ptr + trans) < target \)where \(+\) is the concatenation operator , andtargetis a
given value.For the time being
assume that target is a constant; later, we'll see that it decreases very
slowly over time.
The smaller the value oftargetthe
greater the expected time to solve the puzzle.The time to solve aBitcoin puzzleis a
random variable with a flat distribution. Each proposer 
of a block is probably solving a different puzzle because the block of
transactions that it is aggregating is likely to be different from
that of other proposers. Agents have different amounts of
computing capacity, and the time to solve a puzzle decreases with
capacity.
Agents are unlikely to start solving their puzzles
at the same instant. For these reasons, it is possible, but unlikely, that many agents
will solve their puzzles at the same time.Using puzzles to identify a single random agent leaves us with at least three challenges: (1) Collisions
will occur; (2) agents may be devious --- they may claim to have
solved puzzles when they haven't; and (3) agents with computing power
that far exceeds those of others will solve their puzzles faster than
others do --- and so though agents are selected randomly, those with
large computing power are likely to be selected more often.Attempts at SynchronizationWhen an agent \(A\) appends a new block \(B\) to a block chain \(L\) it broadcasts
the new chain \(L + B\). This is analogous to a person shouting "I won" in the
example given earlier.When a (non-devious) agent \(A'\), which proposes to extend chain
\(L\), gets a message saying that \(L\) has already been extended to
\(L + B\) then \(A'\)
stops attempting to append a block to \(L\).
Instead, \(A\) starts again with a
new set of transactions that it proposes to append to the
extended chain \(L + B\).If the message delay between agents \(A\) and \(A'\) is small compared
to the time to solve puzzles, then a collision
between \(A\) and \(A'\) is unlikely, but still possible. So, it is
possible that \(A\) broadcasts \(L + B\) while \(A'\) broadcasts
\(L + B'\) at about the same time. So different agents may have
different copies of the block chain.
What is the equivalent of the "true" system-wide
block chain when different agents have different copies?The Bitcoin algorithm does not use synchrony to deal with this
issue. The problem of different copies of the block chain 
extant at the same time is handled in an ingenious asynchronous way that we
describe later.Managing Concurrent UpdatesA key step of the Bitcoin algorithm that updates local copies of block
chains is as follows. After an agent creates a block and appends the
newly created block to its local copy it broadcasts its copy of
the block chain.When an agent \(A\) gets a message containing a copy
of another agent's block chain, agent \(A\) sets its local copy to the
block chain in the messageif and only ifthe length of the
block chain in the message exceeds the length of \(A\)'s local copy.Let's look at a scenario.
For this scenario, \(X\) and \(Y\) are single blocks.
Assume that agent \(A\)'s copy of the block chain
is \(L\) when \(A\) receives a message containing the block chain \(L
+ X\). Because the length of the block chain \(L + X\) is bigger than \(A\)'s
copy, \(A\) sets its copy to \(L + X\).Now suppose agent \(A\) gets a
message containing the block chain \(L + Y\); what does \(A\) do?
Agent \(A\) ignores the message because the length of \(L + Y\) does not exceed
that of \(A\)'s current copy, \(L + X\).Continuing CollisionsLet's continue the above scenario.
Can block \(Y\) become part of \(A\)'s
chain, or will it remain forever an "orphan" block
as far as \(A\) is concerned?Here's a possible scenario.
An agent with a block
chain copy \(L + 
X\), solves its puzzle and appends a block \(X_{1}\) to get a new copy
\(L + X + X_{1}\) of the block chain which the agent broadcasts.
At the same time, another agent with a block
chain copy \(L + 
Y\), solves its puzzle and appends a block \(Y_{1}\) to get a new copy
\(L + Y + Y_{1}\) which is broadcast.
If \(A\) receives \(L + Y + Y_{1}\) before receiving \(L + X + X_{1}\)
then \(A\) will set its chain to \(L + Y + Y_{1}\), and then reject
\(L + X + X_{1}\).You can construct a scenario with a sequence of collisions between
agents appending blocks \(X_{i}\) and other agents appending blocks
\(Y_{j}\) so that \(A\)'s chain switches back and forth between \(X\)
and \(Y\) values.Long sequences of collisions are unlikely, and the longer the sequence
the less the probability of continuing collisions.So how can an agent determine whether a block is in the chain? And so
how can an agent find out if a transaction has been executed? You
sell your used bicycle to somebody for coins, but how do you know if
that transaction becomes part oftheblock chain when different
agents have different copies? And so how do you know that you can
spend those coins that
you should have received for your bicycle?Confidence that a Block is in the ChainIn this section we discuss the behavior of non-devious agents; we will
show how the algorithm handles devious agents later.
Suppose an agent's copy of the block chain is \(X + X_{1} + X_{2}
+ \ldots + X_{K}\), and another agent's copy is \(Y + ? + ? +
\ldots\), where \(?\) represents arbitrary values. What is the
likelihood that \(Y \neq X\)?\(Y\) can be different from \(X\) only if there are a sequence of
\(K\) or more collisions. And the likelihood of a sequence of
collisions decreases with \(K\). Likewise, the likelihood that an
agent never receives a block chain containing \(X\) decreases with
time, and so decreases with \(K\). So, if \(K\) is large then with
very high probability, \(X\) is part of every agent's block chain.Now suppose an agent's copy of the block chain is \(L + X + X_{1} + X_{2}
+ \ldots + X_{K}\). What is the likelihood
that another agent's copy is \(L + Y + ? + \ldots\) where \(L\) is an
arbitrary sequence? By the same
argument, when if \(K\) is large then with
very high probability, \(X\) is part of every block chain.For practical purposes,
many agents assume that if \(K > 6\) then \(L + X\) is a prefix of
most agents' block chains.See the Princeton Bitcoin book.Suppose agent \(A\) gets \(N\) Bitcoins in transaction \(X\). If \(K =
0\) then an agent can't be confident that \(A\) ever received these coins
because this transaction may not persist in the block chain. As \(K\)
increases agents become more confident that the transaction is in the
block chain and that \(A\) did, indeed, receive these coins.Fig.1: More Confidence in Older Blocks in the
    ChainWhat happens to Orphan Transactions?An agent may append a block \(Y\) to its chain, and this block may be
dropped from the chains of all agents and never reappear after some
point. We saw a scenario in which this happens.
Such a block is an "orphan," because no agent has a record of that
transaction after some time.
If an orphan block contains the transaction in which you
sold your bicycle in exchange for coins, then will you ever be able to
spend your coins? Yes, you will get your coins as we see next.Agents aggregate transactions that have not
appeared in the agent's block chain into blocks and propose to append these
blocks to the chain. A transaction that does not appear in block
chains will be agrregated eventually by some agent and inserted into
a block in the chian. Though the orphan block disappears the
transactions in the block do not.IncentivesNext, looks look at challenge number 3. Why should an agent create
blocks of validated transactions?Because the agent gets paid! Payment
is from either ablock rewardor transaction fees.Block rewardsAn agent that creates a block gets a specified number of Bitcoins for
itself as a reward called ablock reward.
The Bitcoins in a block reward are created by making the
block; these Bitcoins don't exist until the block is created. The process of making
blocks and acquiring block rewards is called "mining."Mining is the only way of creating new Bitcoins.When Bitcoin started the reward for creating a block was 50 Bitcoins.
The reward
halves after the creation of a certain number (210,000) of blocks. The
reward was reduced to 25 in 2013 and to 12.5 in 2018.
Block rewards will vanish at some point in the future.
The
total number of Bitcoins that can ever be created has an upper bound:
about 21 million.(Bitcoins can be lost. An agent may lose the hash pointer to the
transaction that gave the agent ownership of the coin, or an agent may
lose its private key.)Even when block rewards vanish, miners will continue to
mine provided that they get paid transaction fees. A
transaction fee is a payment by payers and payees to
miners. Transaction fees are voluntary. A high-fee for a
transaction is an incentive to miners to put this transaction into a
block quickly. So, it's possible that agents that offer no fee or low
fees may have to wait longer for their transactions to enter the
block chain.Incentives are critical for Bitcoin. Miners get paid to
get their blocks into the long-term "consensus" block chain.
Miners have an incentive to police the block chain because 
they don't get paid for appending erroneous blocks. If a miner appends
an erroneous block to the chain then other
miners won't extend chains containing the erroneous block, and so the
erroneous block will become an orphan.Any agent can check whether its copy of the block chain is
valid; however, agents making ordinary transactions don't
need to do so because there are many miners each of whom has an
incentive to ensure that the block chain is legitimate.AttacksNext, looks look at challenge number 4.Stealing coinsCan an agent steal a coin from an agent \(X\) by appending a block to
the chain where the block contains a transaction in which \(X\) gives
coins to \(Y\)?No, this can't happen thanks to cryptography. A transaction into which
\(X\) puts coins is valid only if \(X\) signs the transaction.  \(Y\)
cannot forge \(X\)'s signature, and so \(Y\) cannot create blocks that
contain such fraudulent transactions.Double spendCan an agent spend the same coin twice? Can an agent buy something
without paying for it?Consider the following transaction using conventional checks issued by
banks. A buyer gives a seller a check for $\(100,000\) for a house.
The house is put in escrow.  When the check clears and the seller
receives the payment the buyer gets possession of the house. The legal
process that includes notaries, real estate agents, and banks, helps
ensure that the transaction concludes correctly or is aborted correctly.Next, let's look at a transaction in which a person buys a video
online by paying the seller Bitcoins. The buyer and seller know each
other by their public keys and by their online addresses. 
The buyer broadcasts the transaction in which the buyer gives the seller
the payment in Bitcoins.
The amount is specified as
a pair (transaction id, array index) described in the sectionpay
transactionsinthe module introducing crypto currencies.A miner puts the transaction into a block \(X\);
appends the block to its copy \(L\) of the block chain; and broadcasts the extended
block chain \(L + X\).
When the seller gets a copy of the block chain \(L + X\), the
seller concludes that it has received the payment from the
buyer because the 
transaction has been recorded in a block chain. So the seller gives
the video to the buyer.The buyer cheats. The buyer creates a transaction in which the buyer
transfers the same Bitcoins  to the buyer itself.
A miner creates a block \(Y\) that includes this transaction.
A miner who has only received block
chain \(L\) (and hasn't yet received chain \(L + X\)) appends \(Y\) to
\(L\) to get a chain \(L + Y\), and broacasts \(L + Y\).Now we have a situation in which one
miner broadcasts a legitimate block chain \(L + X\) and a different
miner broadcasts a legitimate block chain \(L + Y\). Both chains have
the same length.
A miner with chain \(L + X\) does not know at this point that another
minder has chain \(L + Y\). So, miners will extend both block chains.
(Note that the algorithm will not permit chains \(L + X + Y\) or \(L +
Y + X\).)We've seen this situation before:  look atblock collisionsdescribed
earlier in this module. Both block chains will be extended, but
eventually, with very high probability, one of the blocks \(X\) or
\(Y\) will drop out of chain.How should sellers protect themselves?What is the equivalent of the buyer's check clearing in the bank?A seller should give the item to the buyer only after the transaction
appears with high confidence in a block in the chain --- see Figure 1.
The seller listens to block chains broadcast by miners. If the seller
gets a block chain \(L + X + L'\) where \(L'\) is itself a long block chain
then the seller has high confidence that the transaction is in the
permanent record.The seller waits to get block chains in which its transaction appears
in a block which is then followed by \(m\) blocks, for large
\(m\). The larger the value of \(m\), the greater the seller's
confidence, but the longer the buyer has to wait to get paid.
length of the extension \(L'\).A value of \(m = 6\) gives adequate confidence in most cases.Fraudulent minersA miner gets paid for every block the miner creates; so, why shouldn't
the miner create fraudulent blocks and get paid for them?The answer is the same as that for the double-spending attack. A miner
may have 
created a block and appended it to the chain; however, a suspicious
agent (and we hope that all agents are suspicious!) will not accept
the block until many blocks have been appended to the chain after
it. Other miners won't append their blocks to an invalid one. The
invalid blocks will become permanent orphans, and so the fraudulent
miner won't be able to spend the coins in these blocks.Fifty One Percent AttacksA 51% attack can be carried out by an agent that has more mining
power (e.g. 51% or more) than all other agents combined. The higher
the proportion of mining power of a single agent, the greater the
chances that attacks by that agent will succeed. You can see the
danger of a single agent having predominant mining power by thinking
about an agent with say, 99% of the total mining power. This agent
can mine so much faster than others that it can manipulate the block
chain in many ways. It can create double-spend transactions and deny
services to some transactions.A group of miners can collude to gain predominant mining power. Also,
miners canrentCloud-based systems for mining --- as opposed to having toownhuge data centers. An attacker can rent a large system for the specific purpose and
duration of an attack. The public may not know if and when such
attacks are successful becausecryptocurrencies do not have an
incentive to publish such attacks.Denial of serviceCan agents collude so that an agent \(X\)'s transactions never get
into blocks and so never get processed?An agent's identity does not appear in a transaction, only a public
key does. An agent can create new public keys at will. So let's ask
another question: can agents collude so that transactions with a
specific public key do not get into blocks?Only an agent that can solve puzzles in reasonable time can make
blocks. These agents have significant computational power. One can
concoct a situation where many agents with significant computation
power collude to avoid transactions from a specific public key. This
could have the effect of slowing processing of certain
transactions. However, such a situation isn't likely to arise because
agents have an incentive to create blocks and so they compete ---
rather than collude --- with each other.
Colluding
agents, with massive computing power, may help to deny or slow service
to a public key but not to another agent because agents can create
public keys at will.Further ReadingThere are many issues that we have not covered. This material only
covers the basics from the point of view of distributed algorithms.ReviewWhat is ablock chain?How does the algorithm deal with the situation in which one agent
receives a message to extend its copy of the block chain with a block
\(X\) while another agent receives a message to extend its copy of the
  block chain with a different block \(Y\)?Could there be an infinite sequence of collisions? Is that likely?What is mining for cryptocurrency? Suppose only a small number (say
  two organizations) mined most (say 99%) of Bitcoins. Would that be a
  problem?Why isdouble spendingunlikely to be successful?K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology



Figure Link: https://kmchandy.github.io/Crypto/Bitcoin/Slide1.jpg

Caption: Fig.1: More Confidence in Older Blocks in the
    Chain


Now, instead of people being all in the same room, assume that they
are competing across a network. When a person solves a puzzle,
she broadcasts a "I won" message. When a person working on a puzzle
gets a "I won" message from somebody else, she stops working on her
puzzle.Collisions are likely if the expected time to solve the puzzle
is small (say a millisecond) compared with the expected time (say a
minute) for a message broadcast by one person to reach others. Message
delays may cause multiple people to solve their puzzles before
receiving "I won" messages.
Collisions are unlikely when times to solve puzzles are much greater
than message delays.We could attempt to use timestamps: When a person solves her puzzle
she broadcasts a "I won" message and the time at which she finished
solving the puzzle. If a person gets a "I won" message with an earlier
timestamp then she concedes. This approach is problematic because a
devious agent may not solve the puzzle, or may set her timestamp to an
earlier value.Puzzles in BitcoinNext let's look at the puzzles used in Bitcoin.
Each agent has its own copy of the block chain.
An agent \(A\) collects a set \(trans\) of transactions that haven't as yet been added
to \(A\)'s copy of the block chain.
Agent \(A\) proposes
to append a block to the chain where the block
consists
of the set \(trans\) in the
following way.Let \(ptr\) be the pointer to agent \(A\)'s copy of the block chain. 
Agent \(A\) can add a block containing \(trans\) to the chain only if it
proves that it has solved the following problem --- the "puzzle."Find a number, called \(nonce\), such that:\(H(nonce + ptr + trans) < target \)where \(+\) is the concatenation operator , andtargetis a
given value.For the time being
assume that target is a constant; later, we'll see that it decreases very
slowly over time.
The smaller the value oftargetthe
greater the expected time to solve the puzzle.The time to solve aBitcoin puzzleis a
random variable with a flat distribution. Each proposer 
of a block is probably solving a different puzzle because the block of
transactions that it is aggregating is likely to be different from
that of other proposers. Agents have different amounts of
computing capacity, and the time to solve a puzzle decreases with
capacity.
Agents are unlikely to start solving their puzzles
at the same instant. For these reasons, it is possible, but unlikely, that many agents
will solve their puzzles at the same time.Using puzzles to identify a single random agent leaves us with at least three challenges: (1) Collisions
will occur; (2) agents may be devious --- they may claim to have
solved puzzles when they haven't; and (3) agents with computing power
that far exceeds those of others will solve their puzzles faster than
others do --- and so though agents are selected randomly, those with
large computing power are likely to be selected more often.Attempts at SynchronizationWhen an agent \(A\) appends a new block \(B\) to a block chain \(L\) it broadcasts
the new chain \(L + B\). This is analogous to a person shouting "I won" in the
example given earlier.When a (non-devious) agent \(A'\), which proposes to extend chain
\(L\), gets a message saying that \(L\) has already been extended to
\(L + B\) then \(A'\)
stops attempting to append a block to \(L\).
Instead, \(A\) starts again with a
new set of transactions that it proposes to append to the
extended chain \(L + B\).If the message delay between agents \(A\) and \(A'\) is small compared
to the time to solve puzzles, then a collision
between \(A\) and \(A'\) is unlikely, but still possible. So, it is
possible that \(A\) broadcasts \(L + B\) while \(A'\) broadcasts
\(L + B'\) at about the same time. So different agents may have
different copies of the block chain.
What is the equivalent of the "true" system-wide
block chain when different agents have different copies?The Bitcoin algorithm does not use synchrony to deal with this
issue. The problem of different copies of the block chain 
extant at the same time is handled in an ingenious asynchronous way that we
describe later.Managing Concurrent UpdatesA key step of the Bitcoin algorithm that updates local copies of block
chains is as follows. After an agent creates a block and appends the
newly created block to its local copy it broadcasts its copy of
the block chain.When an agent \(A\) gets a message containing a copy
of another agent's block chain, agent \(A\) sets its local copy to the
block chain in the messageif and only ifthe length of the
block chain in the message exceeds the length of \(A\)'s local copy.Let's look at a scenario.
For this scenario, \(X\) and \(Y\) are single blocks.
Assume that agent \(A\)'s copy of the block chain
is \(L\) when \(A\) receives a message containing the block chain \(L
+ X\). Because the length of the block chain \(L + X\) is bigger than \(A\)'s
copy, \(A\) sets its copy to \(L + X\).Now suppose agent \(A\) gets a
message containing the block chain \(L + Y\); what does \(A\) do?
Agent \(A\) ignores the message because the length of \(L + Y\) does not exceed
that of \(A\)'s current copy, \(L + X\).Continuing CollisionsLet's continue the above scenario.
Can block \(Y\) become part of \(A\)'s
chain, or will it remain forever an "orphan" block
as far as \(A\) is concerned?Here's a possible scenario.
An agent with a block
chain copy \(L + 
X\), solves its puzzle and appends a block \(X_{1}\) to get a new copy
\(L + X + X_{1}\) of the block chain which the agent broadcasts.
At the same time, another agent with a block
chain copy \(L + 
Y\), solves its puzzle and appends a block \(Y_{1}\) to get a new copy
\(L + Y + Y_{1}\) which is broadcast.
If \(A\) receives \(L + Y + Y_{1}\) before receiving \(L + X + X_{1}\)
then \(A\) will set its chain to \(L + Y + Y_{1}\), and then reject
\(L + X + X_{1}\).You can construct a scenario with a sequence of collisions between
agents appending blocks \(X_{i}\) and other agents appending blocks
\(Y_{j}\) so that \(A\)'s chain switches back and forth between \(X\)
and \(Y\) values.Long sequences of collisions are unlikely, and the longer the sequence
the less the probability of continuing collisions.So how can an agent determine whether a block is in the chain? And so
how can an agent find out if a transaction has been executed? You
sell your used bicycle to somebody for coins, but how do you know if
that transaction becomes part oftheblock chain when different
agents have different copies? And so how do you know that you can
spend those coins that
you should have received for your bicycle?Confidence that a Block is in the ChainIn this section we discuss the behavior of non-devious agents; we will
show how the algorithm handles devious agents later.
Suppose an agent's copy of the block chain is \(X + X_{1} + X_{2}
+ \ldots + X_{K}\), and another agent's copy is \(Y + ? + ? +
\ldots\), where \(?\) represents arbitrary values. What is the
likelihood that \(Y \neq X\)?\(Y\) can be different from \(X\) only if there are a sequence of
\(K\) or more collisions. And the likelihood of a sequence of
collisions decreases with \(K\). Likewise, the likelihood that an
agent never receives a block chain containing \(X\) decreases with
time, and so decreases with \(K\). So, if \(K\) is large then with
very high probability, \(X\) is part of every agent's block chain.Now suppose an agent's copy of the block chain is \(L + X + X_{1} + X_{2}
+ \ldots + X_{K}\). What is the likelihood
that another agent's copy is \(L + Y + ? + \ldots\) where \(L\) is an
arbitrary sequence? By the same
argument, when if \(K\) is large then with
very high probability, \(X\) is part of every block chain.For practical purposes,
many agents assume that if \(K > 6\) then \(L + X\) is a prefix of
most agents' block chains.See the Princeton Bitcoin book.Suppose agent \(A\) gets \(N\) Bitcoins in transaction \(X\). If \(K =
0\) then an agent can't be confident that \(A\) ever received these coins
because this transaction may not persist in the block chain. As \(K\)
increases agents become more confident that the transaction is in the
block chain and that \(A\) did, indeed, receive these coins.Fig.1: More Confidence in Older Blocks in the
    ChainWhat happens to Orphan Transactions?An agent may append a block \(Y\) to its chain, and this block may be
dropped from the chains of all agents and never reappear after some
point. We saw a scenario in which this happens.
Such a block is an "orphan," because no agent has a record of that
transaction after some time.
If an orphan block contains the transaction in which you
sold your bicycle in exchange for coins, then will you ever be able to
spend your coins? Yes, you will get your coins as we see next.Agents aggregate transactions that have not
appeared in the agent's block chain into blocks and propose to append these
blocks to the chain. A transaction that does not appear in block
chains will be agrregated eventually by some agent and inserted into
a block in the chian. Though the orphan block disappears the
transactions in the block do not.IncentivesNext, looks look at challenge number 3. Why should an agent create
blocks of validated transactions?Because the agent gets paid! Payment
is from either ablock rewardor transaction fees.Block rewardsAn agent that creates a block gets a specified number of Bitcoins for
itself as a reward called ablock reward.
The Bitcoins in a block reward are created by making the
block; these Bitcoins don't exist until the block is created. The process of making
blocks and acquiring block rewards is called "mining."Mining is the only way of creating new Bitcoins.When Bitcoin started the reward for creating a block was 50 Bitcoins.
The reward
halves after the creation of a certain number (210,000) of blocks. The
reward was reduced to 25 in 2013 and to 12.5 in 2018.
Block rewards will vanish at some point in the future.
The
total number of Bitcoins that can ever be created has an upper bound:
about 21 million.(Bitcoins can be lost. An agent may lose the hash pointer to the
transaction that gave the agent ownership of the coin, or an agent may
lose its private key.)Even when block rewards vanish, miners will continue to
mine provided that they get paid transaction fees. A
transaction fee is a payment by payers and payees to
miners. Transaction fees are voluntary. A high-fee for a
transaction is an incentive to miners to put this transaction into a
block quickly. So, it's possible that agents that offer no fee or low
fees may have to wait longer for their transactions to enter the
block chain.Incentives are critical for Bitcoin. Miners get paid to
get their blocks into the long-term "consensus" block chain.
Miners have an incentive to police the block chain because 
they don't get paid for appending erroneous blocks. If a miner appends
an erroneous block to the chain then other
miners won't extend chains containing the erroneous block, and so the
erroneous block will become an orphan.Any agent can check whether its copy of the block chain is
valid; however, agents making ordinary transactions don't
need to do so because there are many miners each of whom has an
incentive to ensure that the block chain is legitimate.AttacksNext, looks look at challenge number 4.Stealing coinsCan an agent steal a coin from an agent \(X\) by appending a block to
the chain where the block contains a transaction in which \(X\) gives
coins to \(Y\)?No, this can't happen thanks to cryptography. A transaction into which
\(X\) puts coins is valid only if \(X\) signs the transaction.  \(Y\)
cannot forge \(X\)'s signature, and so \(Y\) cannot create blocks that
contain such fraudulent transactions.Double spendCan an agent spend the same coin twice? Can an agent buy something
without paying for it?Consider the following transaction using conventional checks issued by
banks. A buyer gives a seller a check for $\(100,000\) for a house.
The house is put in escrow.  When the check clears and the seller
receives the payment the buyer gets possession of the house. The legal
process that includes notaries, real estate agents, and banks, helps
ensure that the transaction concludes correctly or is aborted correctly.Next, let's look at a transaction in which a person buys a video
online by paying the seller Bitcoins. The buyer and seller know each
other by their public keys and by their online addresses. 
The buyer broadcasts the transaction in which the buyer gives the seller
the payment in Bitcoins.
The amount is specified as
a pair (transaction id, array index) described in the sectionpay
transactionsinthe module introducing crypto currencies.A miner puts the transaction into a block \(X\);
appends the block to its copy \(L\) of the block chain; and broadcasts the extended
block chain \(L + X\).
When the seller gets a copy of the block chain \(L + X\), the
seller concludes that it has received the payment from the
buyer because the 
transaction has been recorded in a block chain. So the seller gives
the video to the buyer.The buyer cheats. The buyer creates a transaction in which the buyer
transfers the same Bitcoins  to the buyer itself.
A miner creates a block \(Y\) that includes this transaction.
A miner who has only received block
chain \(L\) (and hasn't yet received chain \(L + X\)) appends \(Y\) to
\(L\) to get a chain \(L + Y\), and broacasts \(L + Y\).Now we have a situation in which one
miner broadcasts a legitimate block chain \(L + X\) and a different
miner broadcasts a legitimate block chain \(L + Y\). Both chains have
the same length.
A miner with chain \(L + X\) does not know at this point that another
minder has chain \(L + Y\). So, miners will extend both block chains.
(Note that the algorithm will not permit chains \(L + X + Y\) or \(L +
Y + X\).)We've seen this situation before:  look atblock collisionsdescribed
earlier in this module. Both block chains will be extended, but
eventually, with very high probability, one of the blocks \(X\) or
\(Y\) will drop out of chain.How should sellers protect themselves?What is the equivalent of the buyer's check clearing in the bank?A seller should give the item to the buyer only after the transaction
appears with high confidence in a block in the chain --- see Figure 1.
The seller listens to block chains broadcast by miners. If the seller
gets a block chain \(L + X + L'\) where \(L'\) is itself a long block chain
then the seller has high confidence that the transaction is in the
permanent record.The seller waits to get block chains in which its transaction appears
in a block which is then followed by \(m\) blocks, for large
\(m\). The larger the value of \(m\), the greater the seller's
confidence, but the longer the buyer has to wait to get paid.
length of the extension \(L'\).A value of \(m = 6\) gives adequate confidence in most cases.Fraudulent minersA miner gets paid for every block the miner creates; so, why shouldn't
the miner create fraudulent blocks and get paid for them?The answer is the same as that for the double-spending attack. A miner
may have 
created a block and appended it to the chain; however, a suspicious
agent (and we hope that all agents are suspicious!) will not accept
the block until many blocks have been appended to the chain after
it. Other miners won't append their blocks to an invalid one. The
invalid blocks will become permanent orphans, and so the fraudulent
miner won't be able to spend the coins in these blocks.Fifty One Percent AttacksA 51% attack can be carried out by an agent that has more mining
power (e.g. 51% or more) than all other agents combined. The higher
the proportion of mining power of a single agent, the greater the
chances that attacks by that agent will succeed. You can see the
danger of a single agent having predominant mining power by thinking
about an agent with say, 99% of the total mining power. This agent
can mine so much faster than others that it can manipulate the block
chain in many ways. It can create double-spend transactions and deny
services to some transactions.A group of miners can collude to gain predominant mining power. Also,
miners canrentCloud-based systems for mining --- as opposed to having toownhuge data centers. An attacker can rent a large system for the specific purpose and
duration of an attack. The public may not know if and when such
attacks are successful becausecryptocurrencies do not have an
incentive to publish such attacks.Denial of serviceCan agents collude so that an agent \(X\)'s transactions never get
into blocks and so never get processed?An agent's identity does not appear in a transaction, only a public
key does. An agent can create new public keys at will. So let's ask
another question: can agents collude so that transactions with a
specific public key do not get into blocks?Only an agent that can solve puzzles in reasonable time can make
blocks. These agents have significant computational power. One can
concoct a situation where many agents with significant computation
power collude to avoid transactions from a specific public key. This
could have the effect of slowing processing of certain
transactions. However, such a situation isn't likely to arise because
agents have an incentive to create blocks and so they compete ---
rather than collude --- with each other.
Colluding
agents, with massive computing power, may help to deny or slow service
to a public key but not to another agent because agents can create
public keys at will.Further ReadingThere are many issues that we have not covered. This material only
covers the basics from the point of view of distributed algorithms.ReviewWhat is ablock chain?How does the algorithm deal with the situation in which one agent
receives a message to extend its copy of the block chain with a block
\(X\) while another agent receives a message to extend its copy of the
  block chain with a different block \(Y\)?Could there be an infinite sequence of collisions? Is that likely?What is mining for cryptocurrency? Suppose only a small number (say
  two organizations) mined most (say 99%) of Bitcoins. Would that be a
  problem?Why isdouble spendingunlikely to be successful?K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology



Figure Link: https://kmchandy.github.io/Crypto/Bitcoin/Slide1.jpg

Caption: Fig.1: More Confidence in Older Blocks in the
    Chain


Collisions are likely if the expected time to solve the puzzle
is small (say a millisecond) compared with the expected time (say a
minute) for a message broadcast by one person to reach others. Message
delays may cause multiple people to solve their puzzles before
receiving "I won" messages.
Collisions are unlikely when times to solve puzzles are much greater
than message delays.We could attempt to use timestamps: When a person solves her puzzle
she broadcasts a "I won" message and the time at which she finished
solving the puzzle. If a person gets a "I won" message with an earlier
timestamp then she concedes. This approach is problematic because a
devious agent may not solve the puzzle, or may set her timestamp to an
earlier value.Puzzles in BitcoinNext let's look at the puzzles used in Bitcoin.
Each agent has its own copy of the block chain.
An agent \(A\) collects a set \(trans\) of transactions that haven't as yet been added
to \(A\)'s copy of the block chain.
Agent \(A\) proposes
to append a block to the chain where the block
consists
of the set \(trans\) in the
following way.Let \(ptr\) be the pointer to agent \(A\)'s copy of the block chain. 
Agent \(A\) can add a block containing \(trans\) to the chain only if it
proves that it has solved the following problem --- the "puzzle."Find a number, called \(nonce\), such that:\(H(nonce + ptr + trans) < target \)where \(+\) is the concatenation operator , andtargetis a
given value.For the time being
assume that target is a constant; later, we'll see that it decreases very
slowly over time.
The smaller the value oftargetthe
greater the expected time to solve the puzzle.The time to solve aBitcoin puzzleis a
random variable with a flat distribution. Each proposer 
of a block is probably solving a different puzzle because the block of
transactions that it is aggregating is likely to be different from
that of other proposers. Agents have different amounts of
computing capacity, and the time to solve a puzzle decreases with
capacity.
Agents are unlikely to start solving their puzzles
at the same instant. For these reasons, it is possible, but unlikely, that many agents
will solve their puzzles at the same time.Using puzzles to identify a single random agent leaves us with at least three challenges: (1) Collisions
will occur; (2) agents may be devious --- they may claim to have
solved puzzles when they haven't; and (3) agents with computing power
that far exceeds those of others will solve their puzzles faster than
others do --- and so though agents are selected randomly, those with
large computing power are likely to be selected more often.Attempts at SynchronizationWhen an agent \(A\) appends a new block \(B\) to a block chain \(L\) it broadcasts
the new chain \(L + B\). This is analogous to a person shouting "I won" in the
example given earlier.When a (non-devious) agent \(A'\), which proposes to extend chain
\(L\), gets a message saying that \(L\) has already been extended to
\(L + B\) then \(A'\)
stops attempting to append a block to \(L\).
Instead, \(A\) starts again with a
new set of transactions that it proposes to append to the
extended chain \(L + B\).If the message delay between agents \(A\) and \(A'\) is small compared
to the time to solve puzzles, then a collision
between \(A\) and \(A'\) is unlikely, but still possible. So, it is
possible that \(A\) broadcasts \(L + B\) while \(A'\) broadcasts
\(L + B'\) at about the same time. So different agents may have
different copies of the block chain.
What is the equivalent of the "true" system-wide
block chain when different agents have different copies?The Bitcoin algorithm does not use synchrony to deal with this
issue. The problem of different copies of the block chain 
extant at the same time is handled in an ingenious asynchronous way that we
describe later.Managing Concurrent UpdatesA key step of the Bitcoin algorithm that updates local copies of block
chains is as follows. After an agent creates a block and appends the
newly created block to its local copy it broadcasts its copy of
the block chain.When an agent \(A\) gets a message containing a copy
of another agent's block chain, agent \(A\) sets its local copy to the
block chain in the messageif and only ifthe length of the
block chain in the message exceeds the length of \(A\)'s local copy.Let's look at a scenario.
For this scenario, \(X\) and \(Y\) are single blocks.
Assume that agent \(A\)'s copy of the block chain
is \(L\) when \(A\) receives a message containing the block chain \(L
+ X\). Because the length of the block chain \(L + X\) is bigger than \(A\)'s
copy, \(A\) sets its copy to \(L + X\).Now suppose agent \(A\) gets a
message containing the block chain \(L + Y\); what does \(A\) do?
Agent \(A\) ignores the message because the length of \(L + Y\) does not exceed
that of \(A\)'s current copy, \(L + X\).Continuing CollisionsLet's continue the above scenario.
Can block \(Y\) become part of \(A\)'s
chain, or will it remain forever an "orphan" block
as far as \(A\) is concerned?Here's a possible scenario.
An agent with a block
chain copy \(L + 
X\), solves its puzzle and appends a block \(X_{1}\) to get a new copy
\(L + X + X_{1}\) of the block chain which the agent broadcasts.
At the same time, another agent with a block
chain copy \(L + 
Y\), solves its puzzle and appends a block \(Y_{1}\) to get a new copy
\(L + Y + Y_{1}\) which is broadcast.
If \(A\) receives \(L + Y + Y_{1}\) before receiving \(L + X + X_{1}\)
then \(A\) will set its chain to \(L + Y + Y_{1}\), and then reject
\(L + X + X_{1}\).You can construct a scenario with a sequence of collisions between
agents appending blocks \(X_{i}\) and other agents appending blocks
\(Y_{j}\) so that \(A\)'s chain switches back and forth between \(X\)
and \(Y\) values.Long sequences of collisions are unlikely, and the longer the sequence
the less the probability of continuing collisions.So how can an agent determine whether a block is in the chain? And so
how can an agent find out if a transaction has been executed? You
sell your used bicycle to somebody for coins, but how do you know if
that transaction becomes part oftheblock chain when different
agents have different copies? And so how do you know that you can
spend those coins that
you should have received for your bicycle?Confidence that a Block is in the ChainIn this section we discuss the behavior of non-devious agents; we will
show how the algorithm handles devious agents later.
Suppose an agent's copy of the block chain is \(X + X_{1} + X_{2}
+ \ldots + X_{K}\), and another agent's copy is \(Y + ? + ? +
\ldots\), where \(?\) represents arbitrary values. What is the
likelihood that \(Y \neq X\)?\(Y\) can be different from \(X\) only if there are a sequence of
\(K\) or more collisions. And the likelihood of a sequence of
collisions decreases with \(K\). Likewise, the likelihood that an
agent never receives a block chain containing \(X\) decreases with
time, and so decreases with \(K\). So, if \(K\) is large then with
very high probability, \(X\) is part of every agent's block chain.Now suppose an agent's copy of the block chain is \(L + X + X_{1} + X_{2}
+ \ldots + X_{K}\). What is the likelihood
that another agent's copy is \(L + Y + ? + \ldots\) where \(L\) is an
arbitrary sequence? By the same
argument, when if \(K\) is large then with
very high probability, \(X\) is part of every block chain.For practical purposes,
many agents assume that if \(K > 6\) then \(L + X\) is a prefix of
most agents' block chains.See the Princeton Bitcoin book.Suppose agent \(A\) gets \(N\) Bitcoins in transaction \(X\). If \(K =
0\) then an agent can't be confident that \(A\) ever received these coins
because this transaction may not persist in the block chain. As \(K\)
increases agents become more confident that the transaction is in the
block chain and that \(A\) did, indeed, receive these coins.Fig.1: More Confidence in Older Blocks in the
    ChainWhat happens to Orphan Transactions?An agent may append a block \(Y\) to its chain, and this block may be
dropped from the chains of all agents and never reappear after some
point. We saw a scenario in which this happens.
Such a block is an "orphan," because no agent has a record of that
transaction after some time.
If an orphan block contains the transaction in which you
sold your bicycle in exchange for coins, then will you ever be able to
spend your coins? Yes, you will get your coins as we see next.Agents aggregate transactions that have not
appeared in the agent's block chain into blocks and propose to append these
blocks to the chain. A transaction that does not appear in block
chains will be agrregated eventually by some agent and inserted into
a block in the chian. Though the orphan block disappears the
transactions in the block do not.IncentivesNext, looks look at challenge number 3. Why should an agent create
blocks of validated transactions?Because the agent gets paid! Payment
is from either ablock rewardor transaction fees.Block rewardsAn agent that creates a block gets a specified number of Bitcoins for
itself as a reward called ablock reward.
The Bitcoins in a block reward are created by making the
block; these Bitcoins don't exist until the block is created. The process of making
blocks and acquiring block rewards is called "mining."Mining is the only way of creating new Bitcoins.When Bitcoin started the reward for creating a block was 50 Bitcoins.
The reward
halves after the creation of a certain number (210,000) of blocks. The
reward was reduced to 25 in 2013 and to 12.5 in 2018.
Block rewards will vanish at some point in the future.
The
total number of Bitcoins that can ever be created has an upper bound:
about 21 million.(Bitcoins can be lost. An agent may lose the hash pointer to the
transaction that gave the agent ownership of the coin, or an agent may
lose its private key.)Even when block rewards vanish, miners will continue to
mine provided that they get paid transaction fees. A
transaction fee is a payment by payers and payees to
miners. Transaction fees are voluntary. A high-fee for a
transaction is an incentive to miners to put this transaction into a
block quickly. So, it's possible that agents that offer no fee or low
fees may have to wait longer for their transactions to enter the
block chain.Incentives are critical for Bitcoin. Miners get paid to
get their blocks into the long-term "consensus" block chain.
Miners have an incentive to police the block chain because 
they don't get paid for appending erroneous blocks. If a miner appends
an erroneous block to the chain then other
miners won't extend chains containing the erroneous block, and so the
erroneous block will become an orphan.Any agent can check whether its copy of the block chain is
valid; however, agents making ordinary transactions don't
need to do so because there are many miners each of whom has an
incentive to ensure that the block chain is legitimate.AttacksNext, looks look at challenge number 4.Stealing coinsCan an agent steal a coin from an agent \(X\) by appending a block to
the chain where the block contains a transaction in which \(X\) gives
coins to \(Y\)?No, this can't happen thanks to cryptography. A transaction into which
\(X\) puts coins is valid only if \(X\) signs the transaction.  \(Y\)
cannot forge \(X\)'s signature, and so \(Y\) cannot create blocks that
contain such fraudulent transactions.Double spendCan an agent spend the same coin twice? Can an agent buy something
without paying for it?Consider the following transaction using conventional checks issued by
banks. A buyer gives a seller a check for $\(100,000\) for a house.
The house is put in escrow.  When the check clears and the seller
receives the payment the buyer gets possession of the house. The legal
process that includes notaries, real estate agents, and banks, helps
ensure that the transaction concludes correctly or is aborted correctly.Next, let's look at a transaction in which a person buys a video
online by paying the seller Bitcoins. The buyer and seller know each
other by their public keys and by their online addresses. 
The buyer broadcasts the transaction in which the buyer gives the seller
the payment in Bitcoins.
The amount is specified as
a pair (transaction id, array index) described in the sectionpay
transactionsinthe module introducing crypto currencies.A miner puts the transaction into a block \(X\);
appends the block to its copy \(L\) of the block chain; and broadcasts the extended
block chain \(L + X\).
When the seller gets a copy of the block chain \(L + X\), the
seller concludes that it has received the payment from the
buyer because the 
transaction has been recorded in a block chain. So the seller gives
the video to the buyer.The buyer cheats. The buyer creates a transaction in which the buyer
transfers the same Bitcoins  to the buyer itself.
A miner creates a block \(Y\) that includes this transaction.
A miner who has only received block
chain \(L\) (and hasn't yet received chain \(L + X\)) appends \(Y\) to
\(L\) to get a chain \(L + Y\), and broacasts \(L + Y\).Now we have a situation in which one
miner broadcasts a legitimate block chain \(L + X\) and a different
miner broadcasts a legitimate block chain \(L + Y\). Both chains have
the same length.
A miner with chain \(L + X\) does not know at this point that another
minder has chain \(L + Y\). So, miners will extend both block chains.
(Note that the algorithm will not permit chains \(L + X + Y\) or \(L +
Y + X\).)We've seen this situation before:  look atblock collisionsdescribed
earlier in this module. Both block chains will be extended, but
eventually, with very high probability, one of the blocks \(X\) or
\(Y\) will drop out of chain.How should sellers protect themselves?What is the equivalent of the buyer's check clearing in the bank?A seller should give the item to the buyer only after the transaction
appears with high confidence in a block in the chain --- see Figure 1.
The seller listens to block chains broadcast by miners. If the seller
gets a block chain \(L + X + L'\) where \(L'\) is itself a long block chain
then the seller has high confidence that the transaction is in the
permanent record.The seller waits to get block chains in which its transaction appears
in a block which is then followed by \(m\) blocks, for large
\(m\). The larger the value of \(m\), the greater the seller's
confidence, but the longer the buyer has to wait to get paid.
length of the extension \(L'\).A value of \(m = 6\) gives adequate confidence in most cases.Fraudulent minersA miner gets paid for every block the miner creates; so, why shouldn't
the miner create fraudulent blocks and get paid for them?The answer is the same as that for the double-spending attack. A miner
may have 
created a block and appended it to the chain; however, a suspicious
agent (and we hope that all agents are suspicious!) will not accept
the block until many blocks have been appended to the chain after
it. Other miners won't append their blocks to an invalid one. The
invalid blocks will become permanent orphans, and so the fraudulent
miner won't be able to spend the coins in these blocks.Fifty One Percent AttacksA 51% attack can be carried out by an agent that has more mining
power (e.g. 51% or more) than all other agents combined. The higher
the proportion of mining power of a single agent, the greater the
chances that attacks by that agent will succeed. You can see the
danger of a single agent having predominant mining power by thinking
about an agent with say, 99% of the total mining power. This agent
can mine so much faster than others that it can manipulate the block
chain in many ways. It can create double-spend transactions and deny
services to some transactions.A group of miners can collude to gain predominant mining power. Also,
miners canrentCloud-based systems for mining --- as opposed to having toownhuge data centers. An attacker can rent a large system for the specific purpose and
duration of an attack. The public may not know if and when such
attacks are successful becausecryptocurrencies do not have an
incentive to publish such attacks.Denial of serviceCan agents collude so that an agent \(X\)'s transactions never get
into blocks and so never get processed?An agent's identity does not appear in a transaction, only a public
key does. An agent can create new public keys at will. So let's ask
another question: can agents collude so that transactions with a
specific public key do not get into blocks?Only an agent that can solve puzzles in reasonable time can make
blocks. These agents have significant computational power. One can
concoct a situation where many agents with significant computation
power collude to avoid transactions from a specific public key. This
could have the effect of slowing processing of certain
transactions. However, such a situation isn't likely to arise because
agents have an incentive to create blocks and so they compete ---
rather than collude --- with each other.
Colluding
agents, with massive computing power, may help to deny or slow service
to a public key but not to another agent because agents can create
public keys at will.Further ReadingThere are many issues that we have not covered. This material only
covers the basics from the point of view of distributed algorithms.ReviewWhat is ablock chain?How does the algorithm deal with the situation in which one agent
receives a message to extend its copy of the block chain with a block
\(X\) while another agent receives a message to extend its copy of the
  block chain with a different block \(Y\)?Could there be an infinite sequence of collisions? Is that likely?What is mining for cryptocurrency? Suppose only a small number (say
  two organizations) mined most (say 99%) of Bitcoins. Would that be a
  problem?Why isdouble spendingunlikely to be successful?K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology



Figure Link: https://kmchandy.github.io/Crypto/Bitcoin/Slide1.jpg

Caption: Fig.1: More Confidence in Older Blocks in the
    Chain


We could attempt to use timestamps: When a person solves her puzzle
she broadcasts a "I won" message and the time at which she finished
solving the puzzle. If a person gets a "I won" message with an earlier
timestamp then she concedes. This approach is problematic because a
devious agent may not solve the puzzle, or may set her timestamp to an
earlier value.Puzzles in BitcoinNext let's look at the puzzles used in Bitcoin.
Each agent has its own copy of the block chain.
An agent \(A\) collects a set \(trans\) of transactions that haven't as yet been added
to \(A\)'s copy of the block chain.
Agent \(A\) proposes
to append a block to the chain where the block
consists
of the set \(trans\) in the
following way.Let \(ptr\) be the pointer to agent \(A\)'s copy of the block chain. 
Agent \(A\) can add a block containing \(trans\) to the chain only if it
proves that it has solved the following problem --- the "puzzle."Find a number, called \(nonce\), such that:\(H(nonce + ptr + trans) < target \)where \(+\) is the concatenation operator , andtargetis a
given value.For the time being
assume that target is a constant; later, we'll see that it decreases very
slowly over time.
The smaller the value oftargetthe
greater the expected time to solve the puzzle.The time to solve aBitcoin puzzleis a
random variable with a flat distribution. Each proposer 
of a block is probably solving a different puzzle because the block of
transactions that it is aggregating is likely to be different from
that of other proposers. Agents have different amounts of
computing capacity, and the time to solve a puzzle decreases with
capacity.
Agents are unlikely to start solving their puzzles
at the same instant. For these reasons, it is possible, but unlikely, that many agents
will solve their puzzles at the same time.Using puzzles to identify a single random agent leaves us with at least three challenges: (1) Collisions
will occur; (2) agents may be devious --- they may claim to have
solved puzzles when they haven't; and (3) agents with computing power
that far exceeds those of others will solve their puzzles faster than
others do --- and so though agents are selected randomly, those with
large computing power are likely to be selected more often.Attempts at SynchronizationWhen an agent \(A\) appends a new block \(B\) to a block chain \(L\) it broadcasts
the new chain \(L + B\). This is analogous to a person shouting "I won" in the
example given earlier.When a (non-devious) agent \(A'\), which proposes to extend chain
\(L\), gets a message saying that \(L\) has already been extended to
\(L + B\) then \(A'\)
stops attempting to append a block to \(L\).
Instead, \(A\) starts again with a
new set of transactions that it proposes to append to the
extended chain \(L + B\).If the message delay between agents \(A\) and \(A'\) is small compared
to the time to solve puzzles, then a collision
between \(A\) and \(A'\) is unlikely, but still possible. So, it is
possible that \(A\) broadcasts \(L + B\) while \(A'\) broadcasts
\(L + B'\) at about the same time. So different agents may have
different copies of the block chain.
What is the equivalent of the "true" system-wide
block chain when different agents have different copies?The Bitcoin algorithm does not use synchrony to deal with this
issue. The problem of different copies of the block chain 
extant at the same time is handled in an ingenious asynchronous way that we
describe later.Managing Concurrent UpdatesA key step of the Bitcoin algorithm that updates local copies of block
chains is as follows. After an agent creates a block and appends the
newly created block to its local copy it broadcasts its copy of
the block chain.When an agent \(A\) gets a message containing a copy
of another agent's block chain, agent \(A\) sets its local copy to the
block chain in the messageif and only ifthe length of the
block chain in the message exceeds the length of \(A\)'s local copy.Let's look at a scenario.
For this scenario, \(X\) and \(Y\) are single blocks.
Assume that agent \(A\)'s copy of the block chain
is \(L\) when \(A\) receives a message containing the block chain \(L
+ X\). Because the length of the block chain \(L + X\) is bigger than \(A\)'s
copy, \(A\) sets its copy to \(L + X\).Now suppose agent \(A\) gets a
message containing the block chain \(L + Y\); what does \(A\) do?
Agent \(A\) ignores the message because the length of \(L + Y\) does not exceed
that of \(A\)'s current copy, \(L + X\).Continuing CollisionsLet's continue the above scenario.
Can block \(Y\) become part of \(A\)'s
chain, or will it remain forever an "orphan" block
as far as \(A\) is concerned?Here's a possible scenario.
An agent with a block
chain copy \(L + 
X\), solves its puzzle and appends a block \(X_{1}\) to get a new copy
\(L + X + X_{1}\) of the block chain which the agent broadcasts.
At the same time, another agent with a block
chain copy \(L + 
Y\), solves its puzzle and appends a block \(Y_{1}\) to get a new copy
\(L + Y + Y_{1}\) which is broadcast.
If \(A\) receives \(L + Y + Y_{1}\) before receiving \(L + X + X_{1}\)
then \(A\) will set its chain to \(L + Y + Y_{1}\), and then reject
\(L + X + X_{1}\).You can construct a scenario with a sequence of collisions between
agents appending blocks \(X_{i}\) and other agents appending blocks
\(Y_{j}\) so that \(A\)'s chain switches back and forth between \(X\)
and \(Y\) values.Long sequences of collisions are unlikely, and the longer the sequence
the less the probability of continuing collisions.So how can an agent determine whether a block is in the chain? And so
how can an agent find out if a transaction has been executed? You
sell your used bicycle to somebody for coins, but how do you know if
that transaction becomes part oftheblock chain when different
agents have different copies? And so how do you know that you can
spend those coins that
you should have received for your bicycle?Confidence that a Block is in the ChainIn this section we discuss the behavior of non-devious agents; we will
show how the algorithm handles devious agents later.
Suppose an agent's copy of the block chain is \(X + X_{1} + X_{2}
+ \ldots + X_{K}\), and another agent's copy is \(Y + ? + ? +
\ldots\), where \(?\) represents arbitrary values. What is the
likelihood that \(Y \neq X\)?\(Y\) can be different from \(X\) only if there are a sequence of
\(K\) or more collisions. And the likelihood of a sequence of
collisions decreases with \(K\). Likewise, the likelihood that an
agent never receives a block chain containing \(X\) decreases with
time, and so decreases with \(K\). So, if \(K\) is large then with
very high probability, \(X\) is part of every agent's block chain.Now suppose an agent's copy of the block chain is \(L + X + X_{1} + X_{2}
+ \ldots + X_{K}\). What is the likelihood
that another agent's copy is \(L + Y + ? + \ldots\) where \(L\) is an
arbitrary sequence? By the same
argument, when if \(K\) is large then with
very high probability, \(X\) is part of every block chain.For practical purposes,
many agents assume that if \(K > 6\) then \(L + X\) is a prefix of
most agents' block chains.See the Princeton Bitcoin book.Suppose agent \(A\) gets \(N\) Bitcoins in transaction \(X\). If \(K =
0\) then an agent can't be confident that \(A\) ever received these coins
because this transaction may not persist in the block chain. As \(K\)
increases agents become more confident that the transaction is in the
block chain and that \(A\) did, indeed, receive these coins.Fig.1: More Confidence in Older Blocks in the
    ChainWhat happens to Orphan Transactions?An agent may append a block \(Y\) to its chain, and this block may be
dropped from the chains of all agents and never reappear after some
point. We saw a scenario in which this happens.
Such a block is an "orphan," because no agent has a record of that
transaction after some time.
If an orphan block contains the transaction in which you
sold your bicycle in exchange for coins, then will you ever be able to
spend your coins? Yes, you will get your coins as we see next.Agents aggregate transactions that have not
appeared in the agent's block chain into blocks and propose to append these
blocks to the chain. A transaction that does not appear in block
chains will be agrregated eventually by some agent and inserted into
a block in the chian. Though the orphan block disappears the
transactions in the block do not.IncentivesNext, looks look at challenge number 3. Why should an agent create
blocks of validated transactions?Because the agent gets paid! Payment
is from either ablock rewardor transaction fees.Block rewardsAn agent that creates a block gets a specified number of Bitcoins for
itself as a reward called ablock reward.
The Bitcoins in a block reward are created by making the
block; these Bitcoins don't exist until the block is created. The process of making
blocks and acquiring block rewards is called "mining."Mining is the only way of creating new Bitcoins.When Bitcoin started the reward for creating a block was 50 Bitcoins.
The reward
halves after the creation of a certain number (210,000) of blocks. The
reward was reduced to 25 in 2013 and to 12.5 in 2018.
Block rewards will vanish at some point in the future.
The
total number of Bitcoins that can ever be created has an upper bound:
about 21 million.(Bitcoins can be lost. An agent may lose the hash pointer to the
transaction that gave the agent ownership of the coin, or an agent may
lose its private key.)Even when block rewards vanish, miners will continue to
mine provided that they get paid transaction fees. A
transaction fee is a payment by payers and payees to
miners. Transaction fees are voluntary. A high-fee for a
transaction is an incentive to miners to put this transaction into a
block quickly. So, it's possible that agents that offer no fee or low
fees may have to wait longer for their transactions to enter the
block chain.Incentives are critical for Bitcoin. Miners get paid to
get their blocks into the long-term "consensus" block chain.
Miners have an incentive to police the block chain because 
they don't get paid for appending erroneous blocks. If a miner appends
an erroneous block to the chain then other
miners won't extend chains containing the erroneous block, and so the
erroneous block will become an orphan.Any agent can check whether its copy of the block chain is
valid; however, agents making ordinary transactions don't
need to do so because there are many miners each of whom has an
incentive to ensure that the block chain is legitimate.AttacksNext, looks look at challenge number 4.Stealing coinsCan an agent steal a coin from an agent \(X\) by appending a block to
the chain where the block contains a transaction in which \(X\) gives
coins to \(Y\)?No, this can't happen thanks to cryptography. A transaction into which
\(X\) puts coins is valid only if \(X\) signs the transaction.  \(Y\)
cannot forge \(X\)'s signature, and so \(Y\) cannot create blocks that
contain such fraudulent transactions.Double spendCan an agent spend the same coin twice? Can an agent buy something
without paying for it?Consider the following transaction using conventional checks issued by
banks. A buyer gives a seller a check for $\(100,000\) for a house.
The house is put in escrow.  When the check clears and the seller
receives the payment the buyer gets possession of the house. The legal
process that includes notaries, real estate agents, and banks, helps
ensure that the transaction concludes correctly or is aborted correctly.Next, let's look at a transaction in which a person buys a video
online by paying the seller Bitcoins. The buyer and seller know each
other by their public keys and by their online addresses. 
The buyer broadcasts the transaction in which the buyer gives the seller
the payment in Bitcoins.
The amount is specified as
a pair (transaction id, array index) described in the sectionpay
transactionsinthe module introducing crypto currencies.A miner puts the transaction into a block \(X\);
appends the block to its copy \(L\) of the block chain; and broadcasts the extended
block chain \(L + X\).
When the seller gets a copy of the block chain \(L + X\), the
seller concludes that it has received the payment from the
buyer because the 
transaction has been recorded in a block chain. So the seller gives
the video to the buyer.The buyer cheats. The buyer creates a transaction in which the buyer
transfers the same Bitcoins  to the buyer itself.
A miner creates a block \(Y\) that includes this transaction.
A miner who has only received block
chain \(L\) (and hasn't yet received chain \(L + X\)) appends \(Y\) to
\(L\) to get a chain \(L + Y\), and broacasts \(L + Y\).Now we have a situation in which one
miner broadcasts a legitimate block chain \(L + X\) and a different
miner broadcasts a legitimate block chain \(L + Y\). Both chains have
the same length.
A miner with chain \(L + X\) does not know at this point that another
minder has chain \(L + Y\). So, miners will extend both block chains.
(Note that the algorithm will not permit chains \(L + X + Y\) or \(L +
Y + X\).)We've seen this situation before:  look atblock collisionsdescribed
earlier in this module. Both block chains will be extended, but
eventually, with very high probability, one of the blocks \(X\) or
\(Y\) will drop out of chain.How should sellers protect themselves?What is the equivalent of the buyer's check clearing in the bank?A seller should give the item to the buyer only after the transaction
appears with high confidence in a block in the chain --- see Figure 1.
The seller listens to block chains broadcast by miners. If the seller
gets a block chain \(L + X + L'\) where \(L'\) is itself a long block chain
then the seller has high confidence that the transaction is in the
permanent record.The seller waits to get block chains in which its transaction appears
in a block which is then followed by \(m\) blocks, for large
\(m\). The larger the value of \(m\), the greater the seller's
confidence, but the longer the buyer has to wait to get paid.
length of the extension \(L'\).A value of \(m = 6\) gives adequate confidence in most cases.Fraudulent minersA miner gets paid for every block the miner creates; so, why shouldn't
the miner create fraudulent blocks and get paid for them?The answer is the same as that for the double-spending attack. A miner
may have 
created a block and appended it to the chain; however, a suspicious
agent (and we hope that all agents are suspicious!) will not accept
the block until many blocks have been appended to the chain after
it. Other miners won't append their blocks to an invalid one. The
invalid blocks will become permanent orphans, and so the fraudulent
miner won't be able to spend the coins in these blocks.Fifty One Percent AttacksA 51% attack can be carried out by an agent that has more mining
power (e.g. 51% or more) than all other agents combined. The higher
the proportion of mining power of a single agent, the greater the
chances that attacks by that agent will succeed. You can see the
danger of a single agent having predominant mining power by thinking
about an agent with say, 99% of the total mining power. This agent
can mine so much faster than others that it can manipulate the block
chain in many ways. It can create double-spend transactions and deny
services to some transactions.A group of miners can collude to gain predominant mining power. Also,
miners canrentCloud-based systems for mining --- as opposed to having toownhuge data centers. An attacker can rent a large system for the specific purpose and
duration of an attack. The public may not know if and when such
attacks are successful becausecryptocurrencies do not have an
incentive to publish such attacks.Denial of serviceCan agents collude so that an agent \(X\)'s transactions never get
into blocks and so never get processed?An agent's identity does not appear in a transaction, only a public
key does. An agent can create new public keys at will. So let's ask
another question: can agents collude so that transactions with a
specific public key do not get into blocks?Only an agent that can solve puzzles in reasonable time can make
blocks. These agents have significant computational power. One can
concoct a situation where many agents with significant computation
power collude to avoid transactions from a specific public key. This
could have the effect of slowing processing of certain
transactions. However, such a situation isn't likely to arise because
agents have an incentive to create blocks and so they compete ---
rather than collude --- with each other.
Colluding
agents, with massive computing power, may help to deny or slow service
to a public key but not to another agent because agents can create
public keys at will.Further ReadingThere are many issues that we have not covered. This material only
covers the basics from the point of view of distributed algorithms.ReviewWhat is ablock chain?How does the algorithm deal with the situation in which one agent
receives a message to extend its copy of the block chain with a block
\(X\) while another agent receives a message to extend its copy of the
  block chain with a different block \(Y\)?Could there be an infinite sequence of collisions? Is that likely?What is mining for cryptocurrency? Suppose only a small number (say
  two organizations) mined most (say 99%) of Bitcoins. Would that be a
  problem?Why isdouble spendingunlikely to be successful?K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology



Figure Link: https://kmchandy.github.io/Crypto/Bitcoin/Slide1.jpg

Caption: Fig.1: More Confidence in Older Blocks in the
    Chain


Let \(ptr\) be the pointer to agent \(A\)'s copy of the block chain. 
Agent \(A\) can add a block containing \(trans\) to the chain only if it
proves that it has solved the following problem --- the "puzzle."Find a number, called \(nonce\), such that:\(H(nonce + ptr + trans) < target \)where \(+\) is the concatenation operator , andtargetis a
given value.For the time being
assume that target is a constant; later, we'll see that it decreases very
slowly over time.
The smaller the value oftargetthe
greater the expected time to solve the puzzle.The time to solve aBitcoin puzzleis a
random variable with a flat distribution. Each proposer 
of a block is probably solving a different puzzle because the block of
transactions that it is aggregating is likely to be different from
that of other proposers. Agents have different amounts of
computing capacity, and the time to solve a puzzle decreases with
capacity.
Agents are unlikely to start solving their puzzles
at the same instant. For these reasons, it is possible, but unlikely, that many agents
will solve their puzzles at the same time.Using puzzles to identify a single random agent leaves us with at least three challenges: (1) Collisions
will occur; (2) agents may be devious --- they may claim to have
solved puzzles when they haven't; and (3) agents with computing power
that far exceeds those of others will solve their puzzles faster than
others do --- and so though agents are selected randomly, those with
large computing power are likely to be selected more often.Attempts at SynchronizationWhen an agent \(A\) appends a new block \(B\) to a block chain \(L\) it broadcasts
the new chain \(L + B\). This is analogous to a person shouting "I won" in the
example given earlier.When a (non-devious) agent \(A'\), which proposes to extend chain
\(L\), gets a message saying that \(L\) has already been extended to
\(L + B\) then \(A'\)
stops attempting to append a block to \(L\).
Instead, \(A\) starts again with a
new set of transactions that it proposes to append to the
extended chain \(L + B\).If the message delay between agents \(A\) and \(A'\) is small compared
to the time to solve puzzles, then a collision
between \(A\) and \(A'\) is unlikely, but still possible. So, it is
possible that \(A\) broadcasts \(L + B\) while \(A'\) broadcasts
\(L + B'\) at about the same time. So different agents may have
different copies of the block chain.
What is the equivalent of the "true" system-wide
block chain when different agents have different copies?The Bitcoin algorithm does not use synchrony to deal with this
issue. The problem of different copies of the block chain 
extant at the same time is handled in an ingenious asynchronous way that we
describe later.Managing Concurrent UpdatesA key step of the Bitcoin algorithm that updates local copies of block
chains is as follows. After an agent creates a block and appends the
newly created block to its local copy it broadcasts its copy of
the block chain.When an agent \(A\) gets a message containing a copy
of another agent's block chain, agent \(A\) sets its local copy to the
block chain in the messageif and only ifthe length of the
block chain in the message exceeds the length of \(A\)'s local copy.Let's look at a scenario.
For this scenario, \(X\) and \(Y\) are single blocks.
Assume that agent \(A\)'s copy of the block chain
is \(L\) when \(A\) receives a message containing the block chain \(L
+ X\). Because the length of the block chain \(L + X\) is bigger than \(A\)'s
copy, \(A\) sets its copy to \(L + X\).Now suppose agent \(A\) gets a
message containing the block chain \(L + Y\); what does \(A\) do?
Agent \(A\) ignores the message because the length of \(L + Y\) does not exceed
that of \(A\)'s current copy, \(L + X\).Continuing CollisionsLet's continue the above scenario.
Can block \(Y\) become part of \(A\)'s
chain, or will it remain forever an "orphan" block
as far as \(A\) is concerned?Here's a possible scenario.
An agent with a block
chain copy \(L + 
X\), solves its puzzle and appends a block \(X_{1}\) to get a new copy
\(L + X + X_{1}\) of the block chain which the agent broadcasts.
At the same time, another agent with a block
chain copy \(L + 
Y\), solves its puzzle and appends a block \(Y_{1}\) to get a new copy
\(L + Y + Y_{1}\) which is broadcast.
If \(A\) receives \(L + Y + Y_{1}\) before receiving \(L + X + X_{1}\)
then \(A\) will set its chain to \(L + Y + Y_{1}\), and then reject
\(L + X + X_{1}\).You can construct a scenario with a sequence of collisions between
agents appending blocks \(X_{i}\) and other agents appending blocks
\(Y_{j}\) so that \(A\)'s chain switches back and forth between \(X\)
and \(Y\) values.Long sequences of collisions are unlikely, and the longer the sequence
the less the probability of continuing collisions.So how can an agent determine whether a block is in the chain? And so
how can an agent find out if a transaction has been executed? You
sell your used bicycle to somebody for coins, but how do you know if
that transaction becomes part oftheblock chain when different
agents have different copies? And so how do you know that you can
spend those coins that
you should have received for your bicycle?Confidence that a Block is in the ChainIn this section we discuss the behavior of non-devious agents; we will
show how the algorithm handles devious agents later.
Suppose an agent's copy of the block chain is \(X + X_{1} + X_{2}
+ \ldots + X_{K}\), and another agent's copy is \(Y + ? + ? +
\ldots\), where \(?\) represents arbitrary values. What is the
likelihood that \(Y \neq X\)?\(Y\) can be different from \(X\) only if there are a sequence of
\(K\) or more collisions. And the likelihood of a sequence of
collisions decreases with \(K\). Likewise, the likelihood that an
agent never receives a block chain containing \(X\) decreases with
time, and so decreases with \(K\). So, if \(K\) is large then with
very high probability, \(X\) is part of every agent's block chain.Now suppose an agent's copy of the block chain is \(L + X + X_{1} + X_{2}
+ \ldots + X_{K}\). What is the likelihood
that another agent's copy is \(L + Y + ? + \ldots\) where \(L\) is an
arbitrary sequence? By the same
argument, when if \(K\) is large then with
very high probability, \(X\) is part of every block chain.For practical purposes,
many agents assume that if \(K > 6\) then \(L + X\) is a prefix of
most agents' block chains.See the Princeton Bitcoin book.Suppose agent \(A\) gets \(N\) Bitcoins in transaction \(X\). If \(K =
0\) then an agent can't be confident that \(A\) ever received these coins
because this transaction may not persist in the block chain. As \(K\)
increases agents become more confident that the transaction is in the
block chain and that \(A\) did, indeed, receive these coins.Fig.1: More Confidence in Older Blocks in the
    ChainWhat happens to Orphan Transactions?An agent may append a block \(Y\) to its chain, and this block may be
dropped from the chains of all agents and never reappear after some
point. We saw a scenario in which this happens.
Such a block is an "orphan," because no agent has a record of that
transaction after some time.
If an orphan block contains the transaction in which you
sold your bicycle in exchange for coins, then will you ever be able to
spend your coins? Yes, you will get your coins as we see next.Agents aggregate transactions that have not
appeared in the agent's block chain into blocks and propose to append these
blocks to the chain. A transaction that does not appear in block
chains will be agrregated eventually by some agent and inserted into
a block in the chian. Though the orphan block disappears the
transactions in the block do not.IncentivesNext, looks look at challenge number 3. Why should an agent create
blocks of validated transactions?Because the agent gets paid! Payment
is from either ablock rewardor transaction fees.Block rewardsAn agent that creates a block gets a specified number of Bitcoins for
itself as a reward called ablock reward.
The Bitcoins in a block reward are created by making the
block; these Bitcoins don't exist until the block is created. The process of making
blocks and acquiring block rewards is called "mining."Mining is the only way of creating new Bitcoins.When Bitcoin started the reward for creating a block was 50 Bitcoins.
The reward
halves after the creation of a certain number (210,000) of blocks. The
reward was reduced to 25 in 2013 and to 12.5 in 2018.
Block rewards will vanish at some point in the future.
The
total number of Bitcoins that can ever be created has an upper bound:
about 21 million.(Bitcoins can be lost. An agent may lose the hash pointer to the
transaction that gave the agent ownership of the coin, or an agent may
lose its private key.)Even when block rewards vanish, miners will continue to
mine provided that they get paid transaction fees. A
transaction fee is a payment by payers and payees to
miners. Transaction fees are voluntary. A high-fee for a
transaction is an incentive to miners to put this transaction into a
block quickly. So, it's possible that agents that offer no fee or low
fees may have to wait longer for their transactions to enter the
block chain.Incentives are critical for Bitcoin. Miners get paid to
get their blocks into the long-term "consensus" block chain.
Miners have an incentive to police the block chain because 
they don't get paid for appending erroneous blocks. If a miner appends
an erroneous block to the chain then other
miners won't extend chains containing the erroneous block, and so the
erroneous block will become an orphan.Any agent can check whether its copy of the block chain is
valid; however, agents making ordinary transactions don't
need to do so because there are many miners each of whom has an
incentive to ensure that the block chain is legitimate.AttacksNext, looks look at challenge number 4.Stealing coinsCan an agent steal a coin from an agent \(X\) by appending a block to
the chain where the block contains a transaction in which \(X\) gives
coins to \(Y\)?No, this can't happen thanks to cryptography. A transaction into which
\(X\) puts coins is valid only if \(X\) signs the transaction.  \(Y\)
cannot forge \(X\)'s signature, and so \(Y\) cannot create blocks that
contain such fraudulent transactions.Double spendCan an agent spend the same coin twice? Can an agent buy something
without paying for it?Consider the following transaction using conventional checks issued by
banks. A buyer gives a seller a check for $\(100,000\) for a house.
The house is put in escrow.  When the check clears and the seller
receives the payment the buyer gets possession of the house. The legal
process that includes notaries, real estate agents, and banks, helps
ensure that the transaction concludes correctly or is aborted correctly.Next, let's look at a transaction in which a person buys a video
online by paying the seller Bitcoins. The buyer and seller know each
other by their public keys and by their online addresses. 
The buyer broadcasts the transaction in which the buyer gives the seller
the payment in Bitcoins.
The amount is specified as
a pair (transaction id, array index) described in the sectionpay
transactionsinthe module introducing crypto currencies.A miner puts the transaction into a block \(X\);
appends the block to its copy \(L\) of the block chain; and broadcasts the extended
block chain \(L + X\).
When the seller gets a copy of the block chain \(L + X\), the
seller concludes that it has received the payment from the
buyer because the 
transaction has been recorded in a block chain. So the seller gives
the video to the buyer.The buyer cheats. The buyer creates a transaction in which the buyer
transfers the same Bitcoins  to the buyer itself.
A miner creates a block \(Y\) that includes this transaction.
A miner who has only received block
chain \(L\) (and hasn't yet received chain \(L + X\)) appends \(Y\) to
\(L\) to get a chain \(L + Y\), and broacasts \(L + Y\).Now we have a situation in which one
miner broadcasts a legitimate block chain \(L + X\) and a different
miner broadcasts a legitimate block chain \(L + Y\). Both chains have
the same length.
A miner with chain \(L + X\) does not know at this point that another
minder has chain \(L + Y\). So, miners will extend both block chains.
(Note that the algorithm will not permit chains \(L + X + Y\) or \(L +
Y + X\).)We've seen this situation before:  look atblock collisionsdescribed
earlier in this module. Both block chains will be extended, but
eventually, with very high probability, one of the blocks \(X\) or
\(Y\) will drop out of chain.How should sellers protect themselves?What is the equivalent of the buyer's check clearing in the bank?A seller should give the item to the buyer only after the transaction
appears with high confidence in a block in the chain --- see Figure 1.
The seller listens to block chains broadcast by miners. If the seller
gets a block chain \(L + X + L'\) where \(L'\) is itself a long block chain
then the seller has high confidence that the transaction is in the
permanent record.The seller waits to get block chains in which its transaction appears
in a block which is then followed by \(m\) blocks, for large
\(m\). The larger the value of \(m\), the greater the seller's
confidence, but the longer the buyer has to wait to get paid.
length of the extension \(L'\).A value of \(m = 6\) gives adequate confidence in most cases.Fraudulent minersA miner gets paid for every block the miner creates; so, why shouldn't
the miner create fraudulent blocks and get paid for them?The answer is the same as that for the double-spending attack. A miner
may have 
created a block and appended it to the chain; however, a suspicious
agent (and we hope that all agents are suspicious!) will not accept
the block until many blocks have been appended to the chain after
it. Other miners won't append their blocks to an invalid one. The
invalid blocks will become permanent orphans, and so the fraudulent
miner won't be able to spend the coins in these blocks.Fifty One Percent AttacksA 51% attack can be carried out by an agent that has more mining
power (e.g. 51% or more) than all other agents combined. The higher
the proportion of mining power of a single agent, the greater the
chances that attacks by that agent will succeed. You can see the
danger of a single agent having predominant mining power by thinking
about an agent with say, 99% of the total mining power. This agent
can mine so much faster than others that it can manipulate the block
chain in many ways. It can create double-spend transactions and deny
services to some transactions.A group of miners can collude to gain predominant mining power. Also,
miners canrentCloud-based systems for mining --- as opposed to having toownhuge data centers. An attacker can rent a large system for the specific purpose and
duration of an attack. The public may not know if and when such
attacks are successful becausecryptocurrencies do not have an
incentive to publish such attacks.Denial of serviceCan agents collude so that an agent \(X\)'s transactions never get
into blocks and so never get processed?An agent's identity does not appear in a transaction, only a public
key does. An agent can create new public keys at will. So let's ask
another question: can agents collude so that transactions with a
specific public key do not get into blocks?Only an agent that can solve puzzles in reasonable time can make
blocks. These agents have significant computational power. One can
concoct a situation where many agents with significant computation
power collude to avoid transactions from a specific public key. This
could have the effect of slowing processing of certain
transactions. However, such a situation isn't likely to arise because
agents have an incentive to create blocks and so they compete ---
rather than collude --- with each other.
Colluding
agents, with massive computing power, may help to deny or slow service
to a public key but not to another agent because agents can create
public keys at will.Further ReadingThere are many issues that we have not covered. This material only
covers the basics from the point of view of distributed algorithms.ReviewWhat is ablock chain?How does the algorithm deal with the situation in which one agent
receives a message to extend its copy of the block chain with a block
\(X\) while another agent receives a message to extend its copy of the
  block chain with a different block \(Y\)?Could there be an infinite sequence of collisions? Is that likely?What is mining for cryptocurrency? Suppose only a small number (say
  two organizations) mined most (say 99%) of Bitcoins. Would that be a
  problem?Why isdouble spendingunlikely to be successful?K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology



Figure Link: https://kmchandy.github.io/Crypto/Bitcoin/Slide1.jpg

Caption: Fig.1: More Confidence in Older Blocks in the
    Chain


Find a number, called \(nonce\), such that:\(H(nonce + ptr + trans) < target \)where \(+\) is the concatenation operator , andtargetis a
given value.For the time being
assume that target is a constant; later, we'll see that it decreases very
slowly over time.
The smaller the value oftargetthe
greater the expected time to solve the puzzle.The time to solve aBitcoin puzzleis a
random variable with a flat distribution. Each proposer 
of a block is probably solving a different puzzle because the block of
transactions that it is aggregating is likely to be different from
that of other proposers. Agents have different amounts of
computing capacity, and the time to solve a puzzle decreases with
capacity.
Agents are unlikely to start solving their puzzles
at the same instant. For these reasons, it is possible, but unlikely, that many agents
will solve their puzzles at the same time.Using puzzles to identify a single random agent leaves us with at least three challenges: (1) Collisions
will occur; (2) agents may be devious --- they may claim to have
solved puzzles when they haven't; and (3) agents with computing power
that far exceeds those of others will solve their puzzles faster than
others do --- and so though agents are selected randomly, those with
large computing power are likely to be selected more often.Attempts at SynchronizationWhen an agent \(A\) appends a new block \(B\) to a block chain \(L\) it broadcasts
the new chain \(L + B\). This is analogous to a person shouting "I won" in the
example given earlier.When a (non-devious) agent \(A'\), which proposes to extend chain
\(L\), gets a message saying that \(L\) has already been extended to
\(L + B\) then \(A'\)
stops attempting to append a block to \(L\).
Instead, \(A\) starts again with a
new set of transactions that it proposes to append to the
extended chain \(L + B\).If the message delay between agents \(A\) and \(A'\) is small compared
to the time to solve puzzles, then a collision
between \(A\) and \(A'\) is unlikely, but still possible. So, it is
possible that \(A\) broadcasts \(L + B\) while \(A'\) broadcasts
\(L + B'\) at about the same time. So different agents may have
different copies of the block chain.
What is the equivalent of the "true" system-wide
block chain when different agents have different copies?The Bitcoin algorithm does not use synchrony to deal with this
issue. The problem of different copies of the block chain 
extant at the same time is handled in an ingenious asynchronous way that we
describe later.Managing Concurrent UpdatesA key step of the Bitcoin algorithm that updates local copies of block
chains is as follows. After an agent creates a block and appends the
newly created block to its local copy it broadcasts its copy of
the block chain.When an agent \(A\) gets a message containing a copy
of another agent's block chain, agent \(A\) sets its local copy to the
block chain in the messageif and only ifthe length of the
block chain in the message exceeds the length of \(A\)'s local copy.Let's look at a scenario.
For this scenario, \(X\) and \(Y\) are single blocks.
Assume that agent \(A\)'s copy of the block chain
is \(L\) when \(A\) receives a message containing the block chain \(L
+ X\). Because the length of the block chain \(L + X\) is bigger than \(A\)'s
copy, \(A\) sets its copy to \(L + X\).Now suppose agent \(A\) gets a
message containing the block chain \(L + Y\); what does \(A\) do?
Agent \(A\) ignores the message because the length of \(L + Y\) does not exceed
that of \(A\)'s current copy, \(L + X\).Continuing CollisionsLet's continue the above scenario.
Can block \(Y\) become part of \(A\)'s
chain, or will it remain forever an "orphan" block
as far as \(A\) is concerned?Here's a possible scenario.
An agent with a block
chain copy \(L + 
X\), solves its puzzle and appends a block \(X_{1}\) to get a new copy
\(L + X + X_{1}\) of the block chain which the agent broadcasts.
At the same time, another agent with a block
chain copy \(L + 
Y\), solves its puzzle and appends a block \(Y_{1}\) to get a new copy
\(L + Y + Y_{1}\) which is broadcast.
If \(A\) receives \(L + Y + Y_{1}\) before receiving \(L + X + X_{1}\)
then \(A\) will set its chain to \(L + Y + Y_{1}\), and then reject
\(L + X + X_{1}\).You can construct a scenario with a sequence of collisions between
agents appending blocks \(X_{i}\) and other agents appending blocks
\(Y_{j}\) so that \(A\)'s chain switches back and forth between \(X\)
and \(Y\) values.Long sequences of collisions are unlikely, and the longer the sequence
the less the probability of continuing collisions.So how can an agent determine whether a block is in the chain? And so
how can an agent find out if a transaction has been executed? You
sell your used bicycle to somebody for coins, but how do you know if
that transaction becomes part oftheblock chain when different
agents have different copies? And so how do you know that you can
spend those coins that
you should have received for your bicycle?Confidence that a Block is in the ChainIn this section we discuss the behavior of non-devious agents; we will
show how the algorithm handles devious agents later.
Suppose an agent's copy of the block chain is \(X + X_{1} + X_{2}
+ \ldots + X_{K}\), and another agent's copy is \(Y + ? + ? +
\ldots\), where \(?\) represents arbitrary values. What is the
likelihood that \(Y \neq X\)?\(Y\) can be different from \(X\) only if there are a sequence of
\(K\) or more collisions. And the likelihood of a sequence of
collisions decreases with \(K\). Likewise, the likelihood that an
agent never receives a block chain containing \(X\) decreases with
time, and so decreases with \(K\). So, if \(K\) is large then with
very high probability, \(X\) is part of every agent's block chain.Now suppose an agent's copy of the block chain is \(L + X + X_{1} + X_{2}
+ \ldots + X_{K}\). What is the likelihood
that another agent's copy is \(L + Y + ? + \ldots\) where \(L\) is an
arbitrary sequence? By the same
argument, when if \(K\) is large then with
very high probability, \(X\) is part of every block chain.For practical purposes,
many agents assume that if \(K > 6\) then \(L + X\) is a prefix of
most agents' block chains.See the Princeton Bitcoin book.Suppose agent \(A\) gets \(N\) Bitcoins in transaction \(X\). If \(K =
0\) then an agent can't be confident that \(A\) ever received these coins
because this transaction may not persist in the block chain. As \(K\)
increases agents become more confident that the transaction is in the
block chain and that \(A\) did, indeed, receive these coins.Fig.1: More Confidence in Older Blocks in the
    ChainWhat happens to Orphan Transactions?An agent may append a block \(Y\) to its chain, and this block may be
dropped from the chains of all agents and never reappear after some
point. We saw a scenario in which this happens.
Such a block is an "orphan," because no agent has a record of that
transaction after some time.
If an orphan block contains the transaction in which you
sold your bicycle in exchange for coins, then will you ever be able to
spend your coins? Yes, you will get your coins as we see next.Agents aggregate transactions that have not
appeared in the agent's block chain into blocks and propose to append these
blocks to the chain. A transaction that does not appear in block
chains will be agrregated eventually by some agent and inserted into
a block in the chian. Though the orphan block disappears the
transactions in the block do not.IncentivesNext, looks look at challenge number 3. Why should an agent create
blocks of validated transactions?Because the agent gets paid! Payment
is from either ablock rewardor transaction fees.Block rewardsAn agent that creates a block gets a specified number of Bitcoins for
itself as a reward called ablock reward.
The Bitcoins in a block reward are created by making the
block; these Bitcoins don't exist until the block is created. The process of making
blocks and acquiring block rewards is called "mining."Mining is the only way of creating new Bitcoins.When Bitcoin started the reward for creating a block was 50 Bitcoins.
The reward
halves after the creation of a certain number (210,000) of blocks. The
reward was reduced to 25 in 2013 and to 12.5 in 2018.
Block rewards will vanish at some point in the future.
The
total number of Bitcoins that can ever be created has an upper bound:
about 21 million.(Bitcoins can be lost. An agent may lose the hash pointer to the
transaction that gave the agent ownership of the coin, or an agent may
lose its private key.)Even when block rewards vanish, miners will continue to
mine provided that they get paid transaction fees. A
transaction fee is a payment by payers and payees to
miners. Transaction fees are voluntary. A high-fee for a
transaction is an incentive to miners to put this transaction into a
block quickly. So, it's possible that agents that offer no fee or low
fees may have to wait longer for their transactions to enter the
block chain.Incentives are critical for Bitcoin. Miners get paid to
get their blocks into the long-term "consensus" block chain.
Miners have an incentive to police the block chain because 
they don't get paid for appending erroneous blocks. If a miner appends
an erroneous block to the chain then other
miners won't extend chains containing the erroneous block, and so the
erroneous block will become an orphan.Any agent can check whether its copy of the block chain is
valid; however, agents making ordinary transactions don't
need to do so because there are many miners each of whom has an
incentive to ensure that the block chain is legitimate.AttacksNext, looks look at challenge number 4.Stealing coinsCan an agent steal a coin from an agent \(X\) by appending a block to
the chain where the block contains a transaction in which \(X\) gives
coins to \(Y\)?No, this can't happen thanks to cryptography. A transaction into which
\(X\) puts coins is valid only if \(X\) signs the transaction.  \(Y\)
cannot forge \(X\)'s signature, and so \(Y\) cannot create blocks that
contain such fraudulent transactions.Double spendCan an agent spend the same coin twice? Can an agent buy something
without paying for it?Consider the following transaction using conventional checks issued by
banks. A buyer gives a seller a check for $\(100,000\) for a house.
The house is put in escrow.  When the check clears and the seller
receives the payment the buyer gets possession of the house. The legal
process that includes notaries, real estate agents, and banks, helps
ensure that the transaction concludes correctly or is aborted correctly.Next, let's look at a transaction in which a person buys a video
online by paying the seller Bitcoins. The buyer and seller know each
other by their public keys and by their online addresses. 
The buyer broadcasts the transaction in which the buyer gives the seller
the payment in Bitcoins.
The amount is specified as
a pair (transaction id, array index) described in the sectionpay
transactionsinthe module introducing crypto currencies.A miner puts the transaction into a block \(X\);
appends the block to its copy \(L\) of the block chain; and broadcasts the extended
block chain \(L + X\).
When the seller gets a copy of the block chain \(L + X\), the
seller concludes that it has received the payment from the
buyer because the 
transaction has been recorded in a block chain. So the seller gives
the video to the buyer.The buyer cheats. The buyer creates a transaction in which the buyer
transfers the same Bitcoins  to the buyer itself.
A miner creates a block \(Y\) that includes this transaction.
A miner who has only received block
chain \(L\) (and hasn't yet received chain \(L + X\)) appends \(Y\) to
\(L\) to get a chain \(L + Y\), and broacasts \(L + Y\).Now we have a situation in which one
miner broadcasts a legitimate block chain \(L + X\) and a different
miner broadcasts a legitimate block chain \(L + Y\). Both chains have
the same length.
A miner with chain \(L + X\) does not know at this point that another
minder has chain \(L + Y\). So, miners will extend both block chains.
(Note that the algorithm will not permit chains \(L + X + Y\) or \(L +
Y + X\).)We've seen this situation before:  look atblock collisionsdescribed
earlier in this module. Both block chains will be extended, but
eventually, with very high probability, one of the blocks \(X\) or
\(Y\) will drop out of chain.How should sellers protect themselves?What is the equivalent of the buyer's check clearing in the bank?A seller should give the item to the buyer only after the transaction
appears with high confidence in a block in the chain --- see Figure 1.
The seller listens to block chains broadcast by miners. If the seller
gets a block chain \(L + X + L'\) where \(L'\) is itself a long block chain
then the seller has high confidence that the transaction is in the
permanent record.The seller waits to get block chains in which its transaction appears
in a block which is then followed by \(m\) blocks, for large
\(m\). The larger the value of \(m\), the greater the seller's
confidence, but the longer the buyer has to wait to get paid.
length of the extension \(L'\).A value of \(m = 6\) gives adequate confidence in most cases.Fraudulent minersA miner gets paid for every block the miner creates; so, why shouldn't
the miner create fraudulent blocks and get paid for them?The answer is the same as that for the double-spending attack. A miner
may have 
created a block and appended it to the chain; however, a suspicious
agent (and we hope that all agents are suspicious!) will not accept
the block until many blocks have been appended to the chain after
it. Other miners won't append their blocks to an invalid one. The
invalid blocks will become permanent orphans, and so the fraudulent
miner won't be able to spend the coins in these blocks.Fifty One Percent AttacksA 51% attack can be carried out by an agent that has more mining
power (e.g. 51% or more) than all other agents combined. The higher
the proportion of mining power of a single agent, the greater the
chances that attacks by that agent will succeed. You can see the
danger of a single agent having predominant mining power by thinking
about an agent with say, 99% of the total mining power. This agent
can mine so much faster than others that it can manipulate the block
chain in many ways. It can create double-spend transactions and deny
services to some transactions.A group of miners can collude to gain predominant mining power. Also,
miners canrentCloud-based systems for mining --- as opposed to having toownhuge data centers. An attacker can rent a large system for the specific purpose and
duration of an attack. The public may not know if and when such
attacks are successful becausecryptocurrencies do not have an
incentive to publish such attacks.Denial of serviceCan agents collude so that an agent \(X\)'s transactions never get
into blocks and so never get processed?An agent's identity does not appear in a transaction, only a public
key does. An agent can create new public keys at will. So let's ask
another question: can agents collude so that transactions with a
specific public key do not get into blocks?Only an agent that can solve puzzles in reasonable time can make
blocks. These agents have significant computational power. One can
concoct a situation where many agents with significant computation
power collude to avoid transactions from a specific public key. This
could have the effect of slowing processing of certain
transactions. However, such a situation isn't likely to arise because
agents have an incentive to create blocks and so they compete ---
rather than collude --- with each other.
Colluding
agents, with massive computing power, may help to deny or slow service
to a public key but not to another agent because agents can create
public keys at will.Further ReadingThere are many issues that we have not covered. This material only
covers the basics from the point of view of distributed algorithms.ReviewWhat is ablock chain?How does the algorithm deal with the situation in which one agent
receives a message to extend its copy of the block chain with a block
\(X\) while another agent receives a message to extend its copy of the
  block chain with a different block \(Y\)?Could there be an infinite sequence of collisions? Is that likely?What is mining for cryptocurrency? Suppose only a small number (say
  two organizations) mined most (say 99%) of Bitcoins. Would that be a
  problem?Why isdouble spendingunlikely to be successful?K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology



Figure Link: https://kmchandy.github.io/Crypto/Bitcoin/Slide1.jpg

Caption: Fig.1: More Confidence in Older Blocks in the
    Chain


\(H(nonce + ptr + trans) < target \)where \(+\) is the concatenation operator , andtargetis a
given value.For the time being
assume that target is a constant; later, we'll see that it decreases very
slowly over time.
The smaller the value oftargetthe
greater the expected time to solve the puzzle.The time to solve aBitcoin puzzleis a
random variable with a flat distribution. Each proposer 
of a block is probably solving a different puzzle because the block of
transactions that it is aggregating is likely to be different from
that of other proposers. Agents have different amounts of
computing capacity, and the time to solve a puzzle decreases with
capacity.
Agents are unlikely to start solving their puzzles
at the same instant. For these reasons, it is possible, but unlikely, that many agents
will solve their puzzles at the same time.Using puzzles to identify a single random agent leaves us with at least three challenges: (1) Collisions
will occur; (2) agents may be devious --- they may claim to have
solved puzzles when they haven't; and (3) agents with computing power
that far exceeds those of others will solve their puzzles faster than
others do --- and so though agents are selected randomly, those with
large computing power are likely to be selected more often.Attempts at SynchronizationWhen an agent \(A\) appends a new block \(B\) to a block chain \(L\) it broadcasts
the new chain \(L + B\). This is analogous to a person shouting "I won" in the
example given earlier.When a (non-devious) agent \(A'\), which proposes to extend chain
\(L\), gets a message saying that \(L\) has already been extended to
\(L + B\) then \(A'\)
stops attempting to append a block to \(L\).
Instead, \(A\) starts again with a
new set of transactions that it proposes to append to the
extended chain \(L + B\).If the message delay between agents \(A\) and \(A'\) is small compared
to the time to solve puzzles, then a collision
between \(A\) and \(A'\) is unlikely, but still possible. So, it is
possible that \(A\) broadcasts \(L + B\) while \(A'\) broadcasts
\(L + B'\) at about the same time. So different agents may have
different copies of the block chain.
What is the equivalent of the "true" system-wide
block chain when different agents have different copies?The Bitcoin algorithm does not use synchrony to deal with this
issue. The problem of different copies of the block chain 
extant at the same time is handled in an ingenious asynchronous way that we
describe later.Managing Concurrent UpdatesA key step of the Bitcoin algorithm that updates local copies of block
chains is as follows. After an agent creates a block and appends the
newly created block to its local copy it broadcasts its copy of
the block chain.When an agent \(A\) gets a message containing a copy
of another agent's block chain, agent \(A\) sets its local copy to the
block chain in the messageif and only ifthe length of the
block chain in the message exceeds the length of \(A\)'s local copy.Let's look at a scenario.
For this scenario, \(X\) and \(Y\) are single blocks.
Assume that agent \(A\)'s copy of the block chain
is \(L\) when \(A\) receives a message containing the block chain \(L
+ X\). Because the length of the block chain \(L + X\) is bigger than \(A\)'s
copy, \(A\) sets its copy to \(L + X\).Now suppose agent \(A\) gets a
message containing the block chain \(L + Y\); what does \(A\) do?
Agent \(A\) ignores the message because the length of \(L + Y\) does not exceed
that of \(A\)'s current copy, \(L + X\).Continuing CollisionsLet's continue the above scenario.
Can block \(Y\) become part of \(A\)'s
chain, or will it remain forever an "orphan" block
as far as \(A\) is concerned?Here's a possible scenario.
An agent with a block
chain copy \(L + 
X\), solves its puzzle and appends a block \(X_{1}\) to get a new copy
\(L + X + X_{1}\) of the block chain which the agent broadcasts.
At the same time, another agent with a block
chain copy \(L + 
Y\), solves its puzzle and appends a block \(Y_{1}\) to get a new copy
\(L + Y + Y_{1}\) which is broadcast.
If \(A\) receives \(L + Y + Y_{1}\) before receiving \(L + X + X_{1}\)
then \(A\) will set its chain to \(L + Y + Y_{1}\), and then reject
\(L + X + X_{1}\).You can construct a scenario with a sequence of collisions between
agents appending blocks \(X_{i}\) and other agents appending blocks
\(Y_{j}\) so that \(A\)'s chain switches back and forth between \(X\)
and \(Y\) values.Long sequences of collisions are unlikely, and the longer the sequence
the less the probability of continuing collisions.So how can an agent determine whether a block is in the chain? And so
how can an agent find out if a transaction has been executed? You
sell your used bicycle to somebody for coins, but how do you know if
that transaction becomes part oftheblock chain when different
agents have different copies? And so how do you know that you can
spend those coins that
you should have received for your bicycle?Confidence that a Block is in the ChainIn this section we discuss the behavior of non-devious agents; we will
show how the algorithm handles devious agents later.
Suppose an agent's copy of the block chain is \(X + X_{1} + X_{2}
+ \ldots + X_{K}\), and another agent's copy is \(Y + ? + ? +
\ldots\), where \(?\) represents arbitrary values. What is the
likelihood that \(Y \neq X\)?\(Y\) can be different from \(X\) only if there are a sequence of
\(K\) or more collisions. And the likelihood of a sequence of
collisions decreases with \(K\). Likewise, the likelihood that an
agent never receives a block chain containing \(X\) decreases with
time, and so decreases with \(K\). So, if \(K\) is large then with
very high probability, \(X\) is part of every agent's block chain.Now suppose an agent's copy of the block chain is \(L + X + X_{1} + X_{2}
+ \ldots + X_{K}\). What is the likelihood
that another agent's copy is \(L + Y + ? + \ldots\) where \(L\) is an
arbitrary sequence? By the same
argument, when if \(K\) is large then with
very high probability, \(X\) is part of every block chain.For practical purposes,
many agents assume that if \(K > 6\) then \(L + X\) is a prefix of
most agents' block chains.See the Princeton Bitcoin book.Suppose agent \(A\) gets \(N\) Bitcoins in transaction \(X\). If \(K =
0\) then an agent can't be confident that \(A\) ever received these coins
because this transaction may not persist in the block chain. As \(K\)
increases agents become more confident that the transaction is in the
block chain and that \(A\) did, indeed, receive these coins.Fig.1: More Confidence in Older Blocks in the
    ChainWhat happens to Orphan Transactions?An agent may append a block \(Y\) to its chain, and this block may be
dropped from the chains of all agents and never reappear after some
point. We saw a scenario in which this happens.
Such a block is an "orphan," because no agent has a record of that
transaction after some time.
If an orphan block contains the transaction in which you
sold your bicycle in exchange for coins, then will you ever be able to
spend your coins? Yes, you will get your coins as we see next.Agents aggregate transactions that have not
appeared in the agent's block chain into blocks and propose to append these
blocks to the chain. A transaction that does not appear in block
chains will be agrregated eventually by some agent and inserted into
a block in the chian. Though the orphan block disappears the
transactions in the block do not.IncentivesNext, looks look at challenge number 3. Why should an agent create
blocks of validated transactions?Because the agent gets paid! Payment
is from either ablock rewardor transaction fees.Block rewardsAn agent that creates a block gets a specified number of Bitcoins for
itself as a reward called ablock reward.
The Bitcoins in a block reward are created by making the
block; these Bitcoins don't exist until the block is created. The process of making
blocks and acquiring block rewards is called "mining."Mining is the only way of creating new Bitcoins.When Bitcoin started the reward for creating a block was 50 Bitcoins.
The reward
halves after the creation of a certain number (210,000) of blocks. The
reward was reduced to 25 in 2013 and to 12.5 in 2018.
Block rewards will vanish at some point in the future.
The
total number of Bitcoins that can ever be created has an upper bound:
about 21 million.(Bitcoins can be lost. An agent may lose the hash pointer to the
transaction that gave the agent ownership of the coin, or an agent may
lose its private key.)Even when block rewards vanish, miners will continue to
mine provided that they get paid transaction fees. A
transaction fee is a payment by payers and payees to
miners. Transaction fees are voluntary. A high-fee for a
transaction is an incentive to miners to put this transaction into a
block quickly. So, it's possible that agents that offer no fee or low
fees may have to wait longer for their transactions to enter the
block chain.Incentives are critical for Bitcoin. Miners get paid to
get their blocks into the long-term "consensus" block chain.
Miners have an incentive to police the block chain because 
they don't get paid for appending erroneous blocks. If a miner appends
an erroneous block to the chain then other
miners won't extend chains containing the erroneous block, and so the
erroneous block will become an orphan.Any agent can check whether its copy of the block chain is
valid; however, agents making ordinary transactions don't
need to do so because there are many miners each of whom has an
incentive to ensure that the block chain is legitimate.AttacksNext, looks look at challenge number 4.Stealing coinsCan an agent steal a coin from an agent \(X\) by appending a block to
the chain where the block contains a transaction in which \(X\) gives
coins to \(Y\)?No, this can't happen thanks to cryptography. A transaction into which
\(X\) puts coins is valid only if \(X\) signs the transaction.  \(Y\)
cannot forge \(X\)'s signature, and so \(Y\) cannot create blocks that
contain such fraudulent transactions.Double spendCan an agent spend the same coin twice? Can an agent buy something
without paying for it?Consider the following transaction using conventional checks issued by
banks. A buyer gives a seller a check for $\(100,000\) for a house.
The house is put in escrow.  When the check clears and the seller
receives the payment the buyer gets possession of the house. The legal
process that includes notaries, real estate agents, and banks, helps
ensure that the transaction concludes correctly or is aborted correctly.Next, let's look at a transaction in which a person buys a video
online by paying the seller Bitcoins. The buyer and seller know each
other by their public keys and by their online addresses. 
The buyer broadcasts the transaction in which the buyer gives the seller
the payment in Bitcoins.
The amount is specified as
a pair (transaction id, array index) described in the sectionpay
transactionsinthe module introducing crypto currencies.A miner puts the transaction into a block \(X\);
appends the block to its copy \(L\) of the block chain; and broadcasts the extended
block chain \(L + X\).
When the seller gets a copy of the block chain \(L + X\), the
seller concludes that it has received the payment from the
buyer because the 
transaction has been recorded in a block chain. So the seller gives
the video to the buyer.The buyer cheats. The buyer creates a transaction in which the buyer
transfers the same Bitcoins  to the buyer itself.
A miner creates a block \(Y\) that includes this transaction.
A miner who has only received block
chain \(L\) (and hasn't yet received chain \(L + X\)) appends \(Y\) to
\(L\) to get a chain \(L + Y\), and broacasts \(L + Y\).Now we have a situation in which one
miner broadcasts a legitimate block chain \(L + X\) and a different
miner broadcasts a legitimate block chain \(L + Y\). Both chains have
the same length.
A miner with chain \(L + X\) does not know at this point that another
minder has chain \(L + Y\). So, miners will extend both block chains.
(Note that the algorithm will not permit chains \(L + X + Y\) or \(L +
Y + X\).)We've seen this situation before:  look atblock collisionsdescribed
earlier in this module. Both block chains will be extended, but
eventually, with very high probability, one of the blocks \(X\) or
\(Y\) will drop out of chain.How should sellers protect themselves?What is the equivalent of the buyer's check clearing in the bank?A seller should give the item to the buyer only after the transaction
appears with high confidence in a block in the chain --- see Figure 1.
The seller listens to block chains broadcast by miners. If the seller
gets a block chain \(L + X + L'\) where \(L'\) is itself a long block chain
then the seller has high confidence that the transaction is in the
permanent record.The seller waits to get block chains in which its transaction appears
in a block which is then followed by \(m\) blocks, for large
\(m\). The larger the value of \(m\), the greater the seller's
confidence, but the longer the buyer has to wait to get paid.
length of the extension \(L'\).A value of \(m = 6\) gives adequate confidence in most cases.Fraudulent minersA miner gets paid for every block the miner creates; so, why shouldn't
the miner create fraudulent blocks and get paid for them?The answer is the same as that for the double-spending attack. A miner
may have 
created a block and appended it to the chain; however, a suspicious
agent (and we hope that all agents are suspicious!) will not accept
the block until many blocks have been appended to the chain after
it. Other miners won't append their blocks to an invalid one. The
invalid blocks will become permanent orphans, and so the fraudulent
miner won't be able to spend the coins in these blocks.Fifty One Percent AttacksA 51% attack can be carried out by an agent that has more mining
power (e.g. 51% or more) than all other agents combined. The higher
the proportion of mining power of a single agent, the greater the
chances that attacks by that agent will succeed. You can see the
danger of a single agent having predominant mining power by thinking
about an agent with say, 99% of the total mining power. This agent
can mine so much faster than others that it can manipulate the block
chain in many ways. It can create double-spend transactions and deny
services to some transactions.A group of miners can collude to gain predominant mining power. Also,
miners canrentCloud-based systems for mining --- as opposed to having toownhuge data centers. An attacker can rent a large system for the specific purpose and
duration of an attack. The public may not know if and when such
attacks are successful becausecryptocurrencies do not have an
incentive to publish such attacks.Denial of serviceCan agents collude so that an agent \(X\)'s transactions never get
into blocks and so never get processed?An agent's identity does not appear in a transaction, only a public
key does. An agent can create new public keys at will. So let's ask
another question: can agents collude so that transactions with a
specific public key do not get into blocks?Only an agent that can solve puzzles in reasonable time can make
blocks. These agents have significant computational power. One can
concoct a situation where many agents with significant computation
power collude to avoid transactions from a specific public key. This
could have the effect of slowing processing of certain
transactions. However, such a situation isn't likely to arise because
agents have an incentive to create blocks and so they compete ---
rather than collude --- with each other.
Colluding
agents, with massive computing power, may help to deny or slow service
to a public key but not to another agent because agents can create
public keys at will.Further ReadingThere are many issues that we have not covered. This material only
covers the basics from the point of view of distributed algorithms.ReviewWhat is ablock chain?How does the algorithm deal with the situation in which one agent
receives a message to extend its copy of the block chain with a block
\(X\) while another agent receives a message to extend its copy of the
  block chain with a different block \(Y\)?Could there be an infinite sequence of collisions? Is that likely?What is mining for cryptocurrency? Suppose only a small number (say
  two organizations) mined most (say 99%) of Bitcoins. Would that be a
  problem?Why isdouble spendingunlikely to be successful?K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology



Figure Link: https://kmchandy.github.io/Crypto/Bitcoin/Slide1.jpg

Caption: Fig.1: More Confidence in Older Blocks in the
    Chain


where \(+\) is the concatenation operator , andtargetis a
given value.For the time being
assume that target is a constant; later, we'll see that it decreases very
slowly over time.
The smaller the value oftargetthe
greater the expected time to solve the puzzle.The time to solve aBitcoin puzzleis a
random variable with a flat distribution. Each proposer 
of a block is probably solving a different puzzle because the block of
transactions that it is aggregating is likely to be different from
that of other proposers. Agents have different amounts of
computing capacity, and the time to solve a puzzle decreases with
capacity.
Agents are unlikely to start solving their puzzles
at the same instant. For these reasons, it is possible, but unlikely, that many agents
will solve their puzzles at the same time.Using puzzles to identify a single random agent leaves us with at least three challenges: (1) Collisions
will occur; (2) agents may be devious --- they may claim to have
solved puzzles when they haven't; and (3) agents with computing power
that far exceeds those of others will solve their puzzles faster than
others do --- and so though agents are selected randomly, those with
large computing power are likely to be selected more often.Attempts at SynchronizationWhen an agent \(A\) appends a new block \(B\) to a block chain \(L\) it broadcasts
the new chain \(L + B\). This is analogous to a person shouting "I won" in the
example given earlier.When a (non-devious) agent \(A'\), which proposes to extend chain
\(L\), gets a message saying that \(L\) has already been extended to
\(L + B\) then \(A'\)
stops attempting to append a block to \(L\).
Instead, \(A\) starts again with a
new set of transactions that it proposes to append to the
extended chain \(L + B\).If the message delay between agents \(A\) and \(A'\) is small compared
to the time to solve puzzles, then a collision
between \(A\) and \(A'\) is unlikely, but still possible. So, it is
possible that \(A\) broadcasts \(L + B\) while \(A'\) broadcasts
\(L + B'\) at about the same time. So different agents may have
different copies of the block chain.
What is the equivalent of the "true" system-wide
block chain when different agents have different copies?The Bitcoin algorithm does not use synchrony to deal with this
issue. The problem of different copies of the block chain 
extant at the same time is handled in an ingenious asynchronous way that we
describe later.Managing Concurrent UpdatesA key step of the Bitcoin algorithm that updates local copies of block
chains is as follows. After an agent creates a block and appends the
newly created block to its local copy it broadcasts its copy of
the block chain.When an agent \(A\) gets a message containing a copy
of another agent's block chain, agent \(A\) sets its local copy to the
block chain in the messageif and only ifthe length of the
block chain in the message exceeds the length of \(A\)'s local copy.Let's look at a scenario.
For this scenario, \(X\) and \(Y\) are single blocks.
Assume that agent \(A\)'s copy of the block chain
is \(L\) when \(A\) receives a message containing the block chain \(L
+ X\). Because the length of the block chain \(L + X\) is bigger than \(A\)'s
copy, \(A\) sets its copy to \(L + X\).Now suppose agent \(A\) gets a
message containing the block chain \(L + Y\); what does \(A\) do?
Agent \(A\) ignores the message because the length of \(L + Y\) does not exceed
that of \(A\)'s current copy, \(L + X\).Continuing CollisionsLet's continue the above scenario.
Can block \(Y\) become part of \(A\)'s
chain, or will it remain forever an "orphan" block
as far as \(A\) is concerned?Here's a possible scenario.
An agent with a block
chain copy \(L + 
X\), solves its puzzle and appends a block \(X_{1}\) to get a new copy
\(L + X + X_{1}\) of the block chain which the agent broadcasts.
At the same time, another agent with a block
chain copy \(L + 
Y\), solves its puzzle and appends a block \(Y_{1}\) to get a new copy
\(L + Y + Y_{1}\) which is broadcast.
If \(A\) receives \(L + Y + Y_{1}\) before receiving \(L + X + X_{1}\)
then \(A\) will set its chain to \(L + Y + Y_{1}\), and then reject
\(L + X + X_{1}\).You can construct a scenario with a sequence of collisions between
agents appending blocks \(X_{i}\) and other agents appending blocks
\(Y_{j}\) so that \(A\)'s chain switches back and forth between \(X\)
and \(Y\) values.Long sequences of collisions are unlikely, and the longer the sequence
the less the probability of continuing collisions.So how can an agent determine whether a block is in the chain? And so
how can an agent find out if a transaction has been executed? You
sell your used bicycle to somebody for coins, but how do you know if
that transaction becomes part oftheblock chain when different
agents have different copies? And so how do you know that you can
spend those coins that
you should have received for your bicycle?Confidence that a Block is in the ChainIn this section we discuss the behavior of non-devious agents; we will
show how the algorithm handles devious agents later.
Suppose an agent's copy of the block chain is \(X + X_{1} + X_{2}
+ \ldots + X_{K}\), and another agent's copy is \(Y + ? + ? +
\ldots\), where \(?\) represents arbitrary values. What is the
likelihood that \(Y \neq X\)?\(Y\) can be different from \(X\) only if there are a sequence of
\(K\) or more collisions. And the likelihood of a sequence of
collisions decreases with \(K\). Likewise, the likelihood that an
agent never receives a block chain containing \(X\) decreases with
time, and so decreases with \(K\). So, if \(K\) is large then with
very high probability, \(X\) is part of every agent's block chain.Now suppose an agent's copy of the block chain is \(L + X + X_{1} + X_{2}
+ \ldots + X_{K}\). What is the likelihood
that another agent's copy is \(L + Y + ? + \ldots\) where \(L\) is an
arbitrary sequence? By the same
argument, when if \(K\) is large then with
very high probability, \(X\) is part of every block chain.For practical purposes,
many agents assume that if \(K > 6\) then \(L + X\) is a prefix of
most agents' block chains.See the Princeton Bitcoin book.Suppose agent \(A\) gets \(N\) Bitcoins in transaction \(X\). If \(K =
0\) then an agent can't be confident that \(A\) ever received these coins
because this transaction may not persist in the block chain. As \(K\)
increases agents become more confident that the transaction is in the
block chain and that \(A\) did, indeed, receive these coins.Fig.1: More Confidence in Older Blocks in the
    ChainWhat happens to Orphan Transactions?An agent may append a block \(Y\) to its chain, and this block may be
dropped from the chains of all agents and never reappear after some
point. We saw a scenario in which this happens.
Such a block is an "orphan," because no agent has a record of that
transaction after some time.
If an orphan block contains the transaction in which you
sold your bicycle in exchange for coins, then will you ever be able to
spend your coins? Yes, you will get your coins as we see next.Agents aggregate transactions that have not
appeared in the agent's block chain into blocks and propose to append these
blocks to the chain. A transaction that does not appear in block
chains will be agrregated eventually by some agent and inserted into
a block in the chian. Though the orphan block disappears the
transactions in the block do not.IncentivesNext, looks look at challenge number 3. Why should an agent create
blocks of validated transactions?Because the agent gets paid! Payment
is from either ablock rewardor transaction fees.Block rewardsAn agent that creates a block gets a specified number of Bitcoins for
itself as a reward called ablock reward.
The Bitcoins in a block reward are created by making the
block; these Bitcoins don't exist until the block is created. The process of making
blocks and acquiring block rewards is called "mining."Mining is the only way of creating new Bitcoins.When Bitcoin started the reward for creating a block was 50 Bitcoins.
The reward
halves after the creation of a certain number (210,000) of blocks. The
reward was reduced to 25 in 2013 and to 12.5 in 2018.
Block rewards will vanish at some point in the future.
The
total number of Bitcoins that can ever be created has an upper bound:
about 21 million.(Bitcoins can be lost. An agent may lose the hash pointer to the
transaction that gave the agent ownership of the coin, or an agent may
lose its private key.)Even when block rewards vanish, miners will continue to
mine provided that they get paid transaction fees. A
transaction fee is a payment by payers and payees to
miners. Transaction fees are voluntary. A high-fee for a
transaction is an incentive to miners to put this transaction into a
block quickly. So, it's possible that agents that offer no fee or low
fees may have to wait longer for their transactions to enter the
block chain.Incentives are critical for Bitcoin. Miners get paid to
get their blocks into the long-term "consensus" block chain.
Miners have an incentive to police the block chain because 
they don't get paid for appending erroneous blocks. If a miner appends
an erroneous block to the chain then other
miners won't extend chains containing the erroneous block, and so the
erroneous block will become an orphan.Any agent can check whether its copy of the block chain is
valid; however, agents making ordinary transactions don't
need to do so because there are many miners each of whom has an
incentive to ensure that the block chain is legitimate.AttacksNext, looks look at challenge number 4.Stealing coinsCan an agent steal a coin from an agent \(X\) by appending a block to
the chain where the block contains a transaction in which \(X\) gives
coins to \(Y\)?No, this can't happen thanks to cryptography. A transaction into which
\(X\) puts coins is valid only if \(X\) signs the transaction.  \(Y\)
cannot forge \(X\)'s signature, and so \(Y\) cannot create blocks that
contain such fraudulent transactions.Double spendCan an agent spend the same coin twice? Can an agent buy something
without paying for it?Consider the following transaction using conventional checks issued by
banks. A buyer gives a seller a check for $\(100,000\) for a house.
The house is put in escrow.  When the check clears and the seller
receives the payment the buyer gets possession of the house. The legal
process that includes notaries, real estate agents, and banks, helps
ensure that the transaction concludes correctly or is aborted correctly.Next, let's look at a transaction in which a person buys a video
online by paying the seller Bitcoins. The buyer and seller know each
other by their public keys and by their online addresses. 
The buyer broadcasts the transaction in which the buyer gives the seller
the payment in Bitcoins.
The amount is specified as
a pair (transaction id, array index) described in the sectionpay
transactionsinthe module introducing crypto currencies.A miner puts the transaction into a block \(X\);
appends the block to its copy \(L\) of the block chain; and broadcasts the extended
block chain \(L + X\).
When the seller gets a copy of the block chain \(L + X\), the
seller concludes that it has received the payment from the
buyer because the 
transaction has been recorded in a block chain. So the seller gives
the video to the buyer.The buyer cheats. The buyer creates a transaction in which the buyer
transfers the same Bitcoins  to the buyer itself.
A miner creates a block \(Y\) that includes this transaction.
A miner who has only received block
chain \(L\) (and hasn't yet received chain \(L + X\)) appends \(Y\) to
\(L\) to get a chain \(L + Y\), and broacasts \(L + Y\).Now we have a situation in which one
miner broadcasts a legitimate block chain \(L + X\) and a different
miner broadcasts a legitimate block chain \(L + Y\). Both chains have
the same length.
A miner with chain \(L + X\) does not know at this point that another
minder has chain \(L + Y\). So, miners will extend both block chains.
(Note that the algorithm will not permit chains \(L + X + Y\) or \(L +
Y + X\).)We've seen this situation before:  look atblock collisionsdescribed
earlier in this module. Both block chains will be extended, but
eventually, with very high probability, one of the blocks \(X\) or
\(Y\) will drop out of chain.How should sellers protect themselves?What is the equivalent of the buyer's check clearing in the bank?A seller should give the item to the buyer only after the transaction
appears with high confidence in a block in the chain --- see Figure 1.
The seller listens to block chains broadcast by miners. If the seller
gets a block chain \(L + X + L'\) where \(L'\) is itself a long block chain
then the seller has high confidence that the transaction is in the
permanent record.The seller waits to get block chains in which its transaction appears
in a block which is then followed by \(m\) blocks, for large
\(m\). The larger the value of \(m\), the greater the seller's
confidence, but the longer the buyer has to wait to get paid.
length of the extension \(L'\).A value of \(m = 6\) gives adequate confidence in most cases.Fraudulent minersA miner gets paid for every block the miner creates; so, why shouldn't
the miner create fraudulent blocks and get paid for them?The answer is the same as that for the double-spending attack. A miner
may have 
created a block and appended it to the chain; however, a suspicious
agent (and we hope that all agents are suspicious!) will not accept
the block until many blocks have been appended to the chain after
it. Other miners won't append their blocks to an invalid one. The
invalid blocks will become permanent orphans, and so the fraudulent
miner won't be able to spend the coins in these blocks.Fifty One Percent AttacksA 51% attack can be carried out by an agent that has more mining
power (e.g. 51% or more) than all other agents combined. The higher
the proportion of mining power of a single agent, the greater the
chances that attacks by that agent will succeed. You can see the
danger of a single agent having predominant mining power by thinking
about an agent with say, 99% of the total mining power. This agent
can mine so much faster than others that it can manipulate the block
chain in many ways. It can create double-spend transactions and deny
services to some transactions.A group of miners can collude to gain predominant mining power. Also,
miners canrentCloud-based systems for mining --- as opposed to having toownhuge data centers. An attacker can rent a large system for the specific purpose and
duration of an attack. The public may not know if and when such
attacks are successful becausecryptocurrencies do not have an
incentive to publish such attacks.Denial of serviceCan agents collude so that an agent \(X\)'s transactions never get
into blocks and so never get processed?An agent's identity does not appear in a transaction, only a public
key does. An agent can create new public keys at will. So let's ask
another question: can agents collude so that transactions with a
specific public key do not get into blocks?Only an agent that can solve puzzles in reasonable time can make
blocks. These agents have significant computational power. One can
concoct a situation where many agents with significant computation
power collude to avoid transactions from a specific public key. This
could have the effect of slowing processing of certain
transactions. However, such a situation isn't likely to arise because
agents have an incentive to create blocks and so they compete ---
rather than collude --- with each other.
Colluding
agents, with massive computing power, may help to deny or slow service
to a public key but not to another agent because agents can create
public keys at will.Further ReadingThere are many issues that we have not covered. This material only
covers the basics from the point of view of distributed algorithms.ReviewWhat is ablock chain?How does the algorithm deal with the situation in which one agent
receives a message to extend its copy of the block chain with a block
\(X\) while another agent receives a message to extend its copy of the
  block chain with a different block \(Y\)?Could there be an infinite sequence of collisions? Is that likely?What is mining for cryptocurrency? Suppose only a small number (say
  two organizations) mined most (say 99%) of Bitcoins. Would that be a
  problem?Why isdouble spendingunlikely to be successful?K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology



Figure Link: https://kmchandy.github.io/Crypto/Bitcoin/Slide1.jpg

Caption: Fig.1: More Confidence in Older Blocks in the
    Chain


For the time being
assume that target is a constant; later, we'll see that it decreases very
slowly over time.
The smaller the value oftargetthe
greater the expected time to solve the puzzle.The time to solve aBitcoin puzzleis a
random variable with a flat distribution. Each proposer 
of a block is probably solving a different puzzle because the block of
transactions that it is aggregating is likely to be different from
that of other proposers. Agents have different amounts of
computing capacity, and the time to solve a puzzle decreases with
capacity.
Agents are unlikely to start solving their puzzles
at the same instant. For these reasons, it is possible, but unlikely, that many agents
will solve their puzzles at the same time.Using puzzles to identify a single random agent leaves us with at least three challenges: (1) Collisions
will occur; (2) agents may be devious --- they may claim to have
solved puzzles when they haven't; and (3) agents with computing power
that far exceeds those of others will solve their puzzles faster than
others do --- and so though agents are selected randomly, those with
large computing power are likely to be selected more often.Attempts at SynchronizationWhen an agent \(A\) appends a new block \(B\) to a block chain \(L\) it broadcasts
the new chain \(L + B\). This is analogous to a person shouting "I won" in the
example given earlier.When a (non-devious) agent \(A'\), which proposes to extend chain
\(L\), gets a message saying that \(L\) has already been extended to
\(L + B\) then \(A'\)
stops attempting to append a block to \(L\).
Instead, \(A\) starts again with a
new set of transactions that it proposes to append to the
extended chain \(L + B\).If the message delay between agents \(A\) and \(A'\) is small compared
to the time to solve puzzles, then a collision
between \(A\) and \(A'\) is unlikely, but still possible. So, it is
possible that \(A\) broadcasts \(L + B\) while \(A'\) broadcasts
\(L + B'\) at about the same time. So different agents may have
different copies of the block chain.
What is the equivalent of the "true" system-wide
block chain when different agents have different copies?The Bitcoin algorithm does not use synchrony to deal with this
issue. The problem of different copies of the block chain 
extant at the same time is handled in an ingenious asynchronous way that we
describe later.Managing Concurrent UpdatesA key step of the Bitcoin algorithm that updates local copies of block
chains is as follows. After an agent creates a block and appends the
newly created block to its local copy it broadcasts its copy of
the block chain.When an agent \(A\) gets a message containing a copy
of another agent's block chain, agent \(A\) sets its local copy to the
block chain in the messageif and only ifthe length of the
block chain in the message exceeds the length of \(A\)'s local copy.Let's look at a scenario.
For this scenario, \(X\) and \(Y\) are single blocks.
Assume that agent \(A\)'s copy of the block chain
is \(L\) when \(A\) receives a message containing the block chain \(L
+ X\). Because the length of the block chain \(L + X\) is bigger than \(A\)'s
copy, \(A\) sets its copy to \(L + X\).Now suppose agent \(A\) gets a
message containing the block chain \(L + Y\); what does \(A\) do?
Agent \(A\) ignores the message because the length of \(L + Y\) does not exceed
that of \(A\)'s current copy, \(L + X\).Continuing CollisionsLet's continue the above scenario.
Can block \(Y\) become part of \(A\)'s
chain, or will it remain forever an "orphan" block
as far as \(A\) is concerned?Here's a possible scenario.
An agent with a block
chain copy \(L + 
X\), solves its puzzle and appends a block \(X_{1}\) to get a new copy
\(L + X + X_{1}\) of the block chain which the agent broadcasts.
At the same time, another agent with a block
chain copy \(L + 
Y\), solves its puzzle and appends a block \(Y_{1}\) to get a new copy
\(L + Y + Y_{1}\) which is broadcast.
If \(A\) receives \(L + Y + Y_{1}\) before receiving \(L + X + X_{1}\)
then \(A\) will set its chain to \(L + Y + Y_{1}\), and then reject
\(L + X + X_{1}\).You can construct a scenario with a sequence of collisions between
agents appending blocks \(X_{i}\) and other agents appending blocks
\(Y_{j}\) so that \(A\)'s chain switches back and forth between \(X\)
and \(Y\) values.Long sequences of collisions are unlikely, and the longer the sequence
the less the probability of continuing collisions.So how can an agent determine whether a block is in the chain? And so
how can an agent find out if a transaction has been executed? You
sell your used bicycle to somebody for coins, but how do you know if
that transaction becomes part oftheblock chain when different
agents have different copies? And so how do you know that you can
spend those coins that
you should have received for your bicycle?Confidence that a Block is in the ChainIn this section we discuss the behavior of non-devious agents; we will
show how the algorithm handles devious agents later.
Suppose an agent's copy of the block chain is \(X + X_{1} + X_{2}
+ \ldots + X_{K}\), and another agent's copy is \(Y + ? + ? +
\ldots\), where \(?\) represents arbitrary values. What is the
likelihood that \(Y \neq X\)?\(Y\) can be different from \(X\) only if there are a sequence of
\(K\) or more collisions. And the likelihood of a sequence of
collisions decreases with \(K\). Likewise, the likelihood that an
agent never receives a block chain containing \(X\) decreases with
time, and so decreases with \(K\). So, if \(K\) is large then with
very high probability, \(X\) is part of every agent's block chain.Now suppose an agent's copy of the block chain is \(L + X + X_{1} + X_{2}
+ \ldots + X_{K}\). What is the likelihood
that another agent's copy is \(L + Y + ? + \ldots\) where \(L\) is an
arbitrary sequence? By the same
argument, when if \(K\) is large then with
very high probability, \(X\) is part of every block chain.For practical purposes,
many agents assume that if \(K > 6\) then \(L + X\) is a prefix of
most agents' block chains.See the Princeton Bitcoin book.Suppose agent \(A\) gets \(N\) Bitcoins in transaction \(X\). If \(K =
0\) then an agent can't be confident that \(A\) ever received these coins
because this transaction may not persist in the block chain. As \(K\)
increases agents become more confident that the transaction is in the
block chain and that \(A\) did, indeed, receive these coins.Fig.1: More Confidence in Older Blocks in the
    ChainWhat happens to Orphan Transactions?An agent may append a block \(Y\) to its chain, and this block may be
dropped from the chains of all agents and never reappear after some
point. We saw a scenario in which this happens.
Such a block is an "orphan," because no agent has a record of that
transaction after some time.
If an orphan block contains the transaction in which you
sold your bicycle in exchange for coins, then will you ever be able to
spend your coins? Yes, you will get your coins as we see next.Agents aggregate transactions that have not
appeared in the agent's block chain into blocks and propose to append these
blocks to the chain. A transaction that does not appear in block
chains will be agrregated eventually by some agent and inserted into
a block in the chian. Though the orphan block disappears the
transactions in the block do not.IncentivesNext, looks look at challenge number 3. Why should an agent create
blocks of validated transactions?Because the agent gets paid! Payment
is from either ablock rewardor transaction fees.Block rewardsAn agent that creates a block gets a specified number of Bitcoins for
itself as a reward called ablock reward.
The Bitcoins in a block reward are created by making the
block; these Bitcoins don't exist until the block is created. The process of making
blocks and acquiring block rewards is called "mining."Mining is the only way of creating new Bitcoins.When Bitcoin started the reward for creating a block was 50 Bitcoins.
The reward
halves after the creation of a certain number (210,000) of blocks. The
reward was reduced to 25 in 2013 and to 12.5 in 2018.
Block rewards will vanish at some point in the future.
The
total number of Bitcoins that can ever be created has an upper bound:
about 21 million.(Bitcoins can be lost. An agent may lose the hash pointer to the
transaction that gave the agent ownership of the coin, or an agent may
lose its private key.)Even when block rewards vanish, miners will continue to
mine provided that they get paid transaction fees. A
transaction fee is a payment by payers and payees to
miners. Transaction fees are voluntary. A high-fee for a
transaction is an incentive to miners to put this transaction into a
block quickly. So, it's possible that agents that offer no fee or low
fees may have to wait longer for their transactions to enter the
block chain.Incentives are critical for Bitcoin. Miners get paid to
get their blocks into the long-term "consensus" block chain.
Miners have an incentive to police the block chain because 
they don't get paid for appending erroneous blocks. If a miner appends
an erroneous block to the chain then other
miners won't extend chains containing the erroneous block, and so the
erroneous block will become an orphan.Any agent can check whether its copy of the block chain is
valid; however, agents making ordinary transactions don't
need to do so because there are many miners each of whom has an
incentive to ensure that the block chain is legitimate.AttacksNext, looks look at challenge number 4.Stealing coinsCan an agent steal a coin from an agent \(X\) by appending a block to
the chain where the block contains a transaction in which \(X\) gives
coins to \(Y\)?No, this can't happen thanks to cryptography. A transaction into which
\(X\) puts coins is valid only if \(X\) signs the transaction.  \(Y\)
cannot forge \(X\)'s signature, and so \(Y\) cannot create blocks that
contain such fraudulent transactions.Double spendCan an agent spend the same coin twice? Can an agent buy something
without paying for it?Consider the following transaction using conventional checks issued by
banks. A buyer gives a seller a check for $\(100,000\) for a house.
The house is put in escrow.  When the check clears and the seller
receives the payment the buyer gets possession of the house. The legal
process that includes notaries, real estate agents, and banks, helps
ensure that the transaction concludes correctly or is aborted correctly.Next, let's look at a transaction in which a person buys a video
online by paying the seller Bitcoins. The buyer and seller know each
other by their public keys and by their online addresses. 
The buyer broadcasts the transaction in which the buyer gives the seller
the payment in Bitcoins.
The amount is specified as
a pair (transaction id, array index) described in the sectionpay
transactionsinthe module introducing crypto currencies.A miner puts the transaction into a block \(X\);
appends the block to its copy \(L\) of the block chain; and broadcasts the extended
block chain \(L + X\).
When the seller gets a copy of the block chain \(L + X\), the
seller concludes that it has received the payment from the
buyer because the 
transaction has been recorded in a block chain. So the seller gives
the video to the buyer.The buyer cheats. The buyer creates a transaction in which the buyer
transfers the same Bitcoins  to the buyer itself.
A miner creates a block \(Y\) that includes this transaction.
A miner who has only received block
chain \(L\) (and hasn't yet received chain \(L + X\)) appends \(Y\) to
\(L\) to get a chain \(L + Y\), and broacasts \(L + Y\).Now we have a situation in which one
miner broadcasts a legitimate block chain \(L + X\) and a different
miner broadcasts a legitimate block chain \(L + Y\). Both chains have
the same length.
A miner with chain \(L + X\) does not know at this point that another
minder has chain \(L + Y\). So, miners will extend both block chains.
(Note that the algorithm will not permit chains \(L + X + Y\) or \(L +
Y + X\).)We've seen this situation before:  look atblock collisionsdescribed
earlier in this module. Both block chains will be extended, but
eventually, with very high probability, one of the blocks \(X\) or
\(Y\) will drop out of chain.How should sellers protect themselves?What is the equivalent of the buyer's check clearing in the bank?A seller should give the item to the buyer only after the transaction
appears with high confidence in a block in the chain --- see Figure 1.
The seller listens to block chains broadcast by miners. If the seller
gets a block chain \(L + X + L'\) where \(L'\) is itself a long block chain
then the seller has high confidence that the transaction is in the
permanent record.The seller waits to get block chains in which its transaction appears
in a block which is then followed by \(m\) blocks, for large
\(m\). The larger the value of \(m\), the greater the seller's
confidence, but the longer the buyer has to wait to get paid.
length of the extension \(L'\).A value of \(m = 6\) gives adequate confidence in most cases.Fraudulent minersA miner gets paid for every block the miner creates; so, why shouldn't
the miner create fraudulent blocks and get paid for them?The answer is the same as that for the double-spending attack. A miner
may have 
created a block and appended it to the chain; however, a suspicious
agent (and we hope that all agents are suspicious!) will not accept
the block until many blocks have been appended to the chain after
it. Other miners won't append their blocks to an invalid one. The
invalid blocks will become permanent orphans, and so the fraudulent
miner won't be able to spend the coins in these blocks.Fifty One Percent AttacksA 51% attack can be carried out by an agent that has more mining
power (e.g. 51% or more) than all other agents combined. The higher
the proportion of mining power of a single agent, the greater the
chances that attacks by that agent will succeed. You can see the
danger of a single agent having predominant mining power by thinking
about an agent with say, 99% of the total mining power. This agent
can mine so much faster than others that it can manipulate the block
chain in many ways. It can create double-spend transactions and deny
services to some transactions.A group of miners can collude to gain predominant mining power. Also,
miners canrentCloud-based systems for mining --- as opposed to having toownhuge data centers. An attacker can rent a large system for the specific purpose and
duration of an attack. The public may not know if and when such
attacks are successful becausecryptocurrencies do not have an
incentive to publish such attacks.Denial of serviceCan agents collude so that an agent \(X\)'s transactions never get
into blocks and so never get processed?An agent's identity does not appear in a transaction, only a public
key does. An agent can create new public keys at will. So let's ask
another question: can agents collude so that transactions with a
specific public key do not get into blocks?Only an agent that can solve puzzles in reasonable time can make
blocks. These agents have significant computational power. One can
concoct a situation where many agents with significant computation
power collude to avoid transactions from a specific public key. This
could have the effect of slowing processing of certain
transactions. However, such a situation isn't likely to arise because
agents have an incentive to create blocks and so they compete ---
rather than collude --- with each other.
Colluding
agents, with massive computing power, may help to deny or slow service
to a public key but not to another agent because agents can create
public keys at will.Further ReadingThere are many issues that we have not covered. This material only
covers the basics from the point of view of distributed algorithms.ReviewWhat is ablock chain?How does the algorithm deal with the situation in which one agent
receives a message to extend its copy of the block chain with a block
\(X\) while another agent receives a message to extend its copy of the
  block chain with a different block \(Y\)?Could there be an infinite sequence of collisions? Is that likely?What is mining for cryptocurrency? Suppose only a small number (say
  two organizations) mined most (say 99%) of Bitcoins. Would that be a
  problem?Why isdouble spendingunlikely to be successful?K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology



Figure Link: https://kmchandy.github.io/Crypto/Bitcoin/Slide1.jpg

Caption: Fig.1: More Confidence in Older Blocks in the
    Chain


The time to solve aBitcoin puzzleis a
random variable with a flat distribution. Each proposer 
of a block is probably solving a different puzzle because the block of
transactions that it is aggregating is likely to be different from
that of other proposers. Agents have different amounts of
computing capacity, and the time to solve a puzzle decreases with
capacity.
Agents are unlikely to start solving their puzzles
at the same instant. For these reasons, it is possible, but unlikely, that many agents
will solve their puzzles at the same time.Using puzzles to identify a single random agent leaves us with at least three challenges: (1) Collisions
will occur; (2) agents may be devious --- they may claim to have
solved puzzles when they haven't; and (3) agents with computing power
that far exceeds those of others will solve their puzzles faster than
others do --- and so though agents are selected randomly, those with
large computing power are likely to be selected more often.Attempts at SynchronizationWhen an agent \(A\) appends a new block \(B\) to a block chain \(L\) it broadcasts
the new chain \(L + B\). This is analogous to a person shouting "I won" in the
example given earlier.When a (non-devious) agent \(A'\), which proposes to extend chain
\(L\), gets a message saying that \(L\) has already been extended to
\(L + B\) then \(A'\)
stops attempting to append a block to \(L\).
Instead, \(A\) starts again with a
new set of transactions that it proposes to append to the
extended chain \(L + B\).If the message delay between agents \(A\) and \(A'\) is small compared
to the time to solve puzzles, then a collision
between \(A\) and \(A'\) is unlikely, but still possible. So, it is
possible that \(A\) broadcasts \(L + B\) while \(A'\) broadcasts
\(L + B'\) at about the same time. So different agents may have
different copies of the block chain.
What is the equivalent of the "true" system-wide
block chain when different agents have different copies?The Bitcoin algorithm does not use synchrony to deal with this
issue. The problem of different copies of the block chain 
extant at the same time is handled in an ingenious asynchronous way that we
describe later.Managing Concurrent UpdatesA key step of the Bitcoin algorithm that updates local copies of block
chains is as follows. After an agent creates a block and appends the
newly created block to its local copy it broadcasts its copy of
the block chain.When an agent \(A\) gets a message containing a copy
of another agent's block chain, agent \(A\) sets its local copy to the
block chain in the messageif and only ifthe length of the
block chain in the message exceeds the length of \(A\)'s local copy.Let's look at a scenario.
For this scenario, \(X\) and \(Y\) are single blocks.
Assume that agent \(A\)'s copy of the block chain
is \(L\) when \(A\) receives a message containing the block chain \(L
+ X\). Because the length of the block chain \(L + X\) is bigger than \(A\)'s
copy, \(A\) sets its copy to \(L + X\).Now suppose agent \(A\) gets a
message containing the block chain \(L + Y\); what does \(A\) do?
Agent \(A\) ignores the message because the length of \(L + Y\) does not exceed
that of \(A\)'s current copy, \(L + X\).Continuing CollisionsLet's continue the above scenario.
Can block \(Y\) become part of \(A\)'s
chain, or will it remain forever an "orphan" block
as far as \(A\) is concerned?Here's a possible scenario.
An agent with a block
chain copy \(L + 
X\), solves its puzzle and appends a block \(X_{1}\) to get a new copy
\(L + X + X_{1}\) of the block chain which the agent broadcasts.
At the same time, another agent with a block
chain copy \(L + 
Y\), solves its puzzle and appends a block \(Y_{1}\) to get a new copy
\(L + Y + Y_{1}\) which is broadcast.
If \(A\) receives \(L + Y + Y_{1}\) before receiving \(L + X + X_{1}\)
then \(A\) will set its chain to \(L + Y + Y_{1}\), and then reject
\(L + X + X_{1}\).You can construct a scenario with a sequence of collisions between
agents appending blocks \(X_{i}\) and other agents appending blocks
\(Y_{j}\) so that \(A\)'s chain switches back and forth between \(X\)
and \(Y\) values.Long sequences of collisions are unlikely, and the longer the sequence
the less the probability of continuing collisions.So how can an agent determine whether a block is in the chain? And so
how can an agent find out if a transaction has been executed? You
sell your used bicycle to somebody for coins, but how do you know if
that transaction becomes part oftheblock chain when different
agents have different copies? And so how do you know that you can
spend those coins that
you should have received for your bicycle?Confidence that a Block is in the ChainIn this section we discuss the behavior of non-devious agents; we will
show how the algorithm handles devious agents later.
Suppose an agent's copy of the block chain is \(X + X_{1} + X_{2}
+ \ldots + X_{K}\), and another agent's copy is \(Y + ? + ? +
\ldots\), where \(?\) represents arbitrary values. What is the
likelihood that \(Y \neq X\)?\(Y\) can be different from \(X\) only if there are a sequence of
\(K\) or more collisions. And the likelihood of a sequence of
collisions decreases with \(K\). Likewise, the likelihood that an
agent never receives a block chain containing \(X\) decreases with
time, and so decreases with \(K\). So, if \(K\) is large then with
very high probability, \(X\) is part of every agent's block chain.Now suppose an agent's copy of the block chain is \(L + X + X_{1} + X_{2}
+ \ldots + X_{K}\). What is the likelihood
that another agent's copy is \(L + Y + ? + \ldots\) where \(L\) is an
arbitrary sequence? By the same
argument, when if \(K\) is large then with
very high probability, \(X\) is part of every block chain.For practical purposes,
many agents assume that if \(K > 6\) then \(L + X\) is a prefix of
most agents' block chains.See the Princeton Bitcoin book.Suppose agent \(A\) gets \(N\) Bitcoins in transaction \(X\). If \(K =
0\) then an agent can't be confident that \(A\) ever received these coins
because this transaction may not persist in the block chain. As \(K\)
increases agents become more confident that the transaction is in the
block chain and that \(A\) did, indeed, receive these coins.Fig.1: More Confidence in Older Blocks in the
    ChainWhat happens to Orphan Transactions?An agent may append a block \(Y\) to its chain, and this block may be
dropped from the chains of all agents and never reappear after some
point. We saw a scenario in which this happens.
Such a block is an "orphan," because no agent has a record of that
transaction after some time.
If an orphan block contains the transaction in which you
sold your bicycle in exchange for coins, then will you ever be able to
spend your coins? Yes, you will get your coins as we see next.Agents aggregate transactions that have not
appeared in the agent's block chain into blocks and propose to append these
blocks to the chain. A transaction that does not appear in block
chains will be agrregated eventually by some agent and inserted into
a block in the chian. Though the orphan block disappears the
transactions in the block do not.IncentivesNext, looks look at challenge number 3. Why should an agent create
blocks of validated transactions?Because the agent gets paid! Payment
is from either ablock rewardor transaction fees.Block rewardsAn agent that creates a block gets a specified number of Bitcoins for
itself as a reward called ablock reward.
The Bitcoins in a block reward are created by making the
block; these Bitcoins don't exist until the block is created. The process of making
blocks and acquiring block rewards is called "mining."Mining is the only way of creating new Bitcoins.When Bitcoin started the reward for creating a block was 50 Bitcoins.
The reward
halves after the creation of a certain number (210,000) of blocks. The
reward was reduced to 25 in 2013 and to 12.5 in 2018.
Block rewards will vanish at some point in the future.
The
total number of Bitcoins that can ever be created has an upper bound:
about 21 million.(Bitcoins can be lost. An agent may lose the hash pointer to the
transaction that gave the agent ownership of the coin, or an agent may
lose its private key.)Even when block rewards vanish, miners will continue to
mine provided that they get paid transaction fees. A
transaction fee is a payment by payers and payees to
miners. Transaction fees are voluntary. A high-fee for a
transaction is an incentive to miners to put this transaction into a
block quickly. So, it's possible that agents that offer no fee or low
fees may have to wait longer for their transactions to enter the
block chain.Incentives are critical for Bitcoin. Miners get paid to
get their blocks into the long-term "consensus" block chain.
Miners have an incentive to police the block chain because 
they don't get paid for appending erroneous blocks. If a miner appends
an erroneous block to the chain then other
miners won't extend chains containing the erroneous block, and so the
erroneous block will become an orphan.Any agent can check whether its copy of the block chain is
valid; however, agents making ordinary transactions don't
need to do so because there are many miners each of whom has an
incentive to ensure that the block chain is legitimate.AttacksNext, looks look at challenge number 4.Stealing coinsCan an agent steal a coin from an agent \(X\) by appending a block to
the chain where the block contains a transaction in which \(X\) gives
coins to \(Y\)?No, this can't happen thanks to cryptography. A transaction into which
\(X\) puts coins is valid only if \(X\) signs the transaction.  \(Y\)
cannot forge \(X\)'s signature, and so \(Y\) cannot create blocks that
contain such fraudulent transactions.Double spendCan an agent spend the same coin twice? Can an agent buy something
without paying for it?Consider the following transaction using conventional checks issued by
banks. A buyer gives a seller a check for $\(100,000\) for a house.
The house is put in escrow.  When the check clears and the seller
receives the payment the buyer gets possession of the house. The legal
process that includes notaries, real estate agents, and banks, helps
ensure that the transaction concludes correctly or is aborted correctly.Next, let's look at a transaction in which a person buys a video
online by paying the seller Bitcoins. The buyer and seller know each
other by their public keys and by their online addresses. 
The buyer broadcasts the transaction in which the buyer gives the seller
the payment in Bitcoins.
The amount is specified as
a pair (transaction id, array index) described in the sectionpay
transactionsinthe module introducing crypto currencies.A miner puts the transaction into a block \(X\);
appends the block to its copy \(L\) of the block chain; and broadcasts the extended
block chain \(L + X\).
When the seller gets a copy of the block chain \(L + X\), the
seller concludes that it has received the payment from the
buyer because the 
transaction has been recorded in a block chain. So the seller gives
the video to the buyer.The buyer cheats. The buyer creates a transaction in which the buyer
transfers the same Bitcoins  to the buyer itself.
A miner creates a block \(Y\) that includes this transaction.
A miner who has only received block
chain \(L\) (and hasn't yet received chain \(L + X\)) appends \(Y\) to
\(L\) to get a chain \(L + Y\), and broacasts \(L + Y\).Now we have a situation in which one
miner broadcasts a legitimate block chain \(L + X\) and a different
miner broadcasts a legitimate block chain \(L + Y\). Both chains have
the same length.
A miner with chain \(L + X\) does not know at this point that another
minder has chain \(L + Y\). So, miners will extend both block chains.
(Note that the algorithm will not permit chains \(L + X + Y\) or \(L +
Y + X\).)We've seen this situation before:  look atblock collisionsdescribed
earlier in this module. Both block chains will be extended, but
eventually, with very high probability, one of the blocks \(X\) or
\(Y\) will drop out of chain.How should sellers protect themselves?What is the equivalent of the buyer's check clearing in the bank?A seller should give the item to the buyer only after the transaction
appears with high confidence in a block in the chain --- see Figure 1.
The seller listens to block chains broadcast by miners. If the seller
gets a block chain \(L + X + L'\) where \(L'\) is itself a long block chain
then the seller has high confidence that the transaction is in the
permanent record.The seller waits to get block chains in which its transaction appears
in a block which is then followed by \(m\) blocks, for large
\(m\). The larger the value of \(m\), the greater the seller's
confidence, but the longer the buyer has to wait to get paid.
length of the extension \(L'\).A value of \(m = 6\) gives adequate confidence in most cases.Fraudulent minersA miner gets paid for every block the miner creates; so, why shouldn't
the miner create fraudulent blocks and get paid for them?The answer is the same as that for the double-spending attack. A miner
may have 
created a block and appended it to the chain; however, a suspicious
agent (and we hope that all agents are suspicious!) will not accept
the block until many blocks have been appended to the chain after
it. Other miners won't append their blocks to an invalid one. The
invalid blocks will become permanent orphans, and so the fraudulent
miner won't be able to spend the coins in these blocks.Fifty One Percent AttacksA 51% attack can be carried out by an agent that has more mining
power (e.g. 51% or more) than all other agents combined. The higher
the proportion of mining power of a single agent, the greater the
chances that attacks by that agent will succeed. You can see the
danger of a single agent having predominant mining power by thinking
about an agent with say, 99% of the total mining power. This agent
can mine so much faster than others that it can manipulate the block
chain in many ways. It can create double-spend transactions and deny
services to some transactions.A group of miners can collude to gain predominant mining power. Also,
miners canrentCloud-based systems for mining --- as opposed to having toownhuge data centers. An attacker can rent a large system for the specific purpose and
duration of an attack. The public may not know if and when such
attacks are successful becausecryptocurrencies do not have an
incentive to publish such attacks.Denial of serviceCan agents collude so that an agent \(X\)'s transactions never get
into blocks and so never get processed?An agent's identity does not appear in a transaction, only a public
key does. An agent can create new public keys at will. So let's ask
another question: can agents collude so that transactions with a
specific public key do not get into blocks?Only an agent that can solve puzzles in reasonable time can make
blocks. These agents have significant computational power. One can
concoct a situation where many agents with significant computation
power collude to avoid transactions from a specific public key. This
could have the effect of slowing processing of certain
transactions. However, such a situation isn't likely to arise because
agents have an incentive to create blocks and so they compete ---
rather than collude --- with each other.
Colluding
agents, with massive computing power, may help to deny or slow service
to a public key but not to another agent because agents can create
public keys at will.Further ReadingThere are many issues that we have not covered. This material only
covers the basics from the point of view of distributed algorithms.ReviewWhat is ablock chain?How does the algorithm deal with the situation in which one agent
receives a message to extend its copy of the block chain with a block
\(X\) while another agent receives a message to extend its copy of the
  block chain with a different block \(Y\)?Could there be an infinite sequence of collisions? Is that likely?What is mining for cryptocurrency? Suppose only a small number (say
  two organizations) mined most (say 99%) of Bitcoins. Would that be a
  problem?Why isdouble spendingunlikely to be successful?K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology



Figure Link: https://kmchandy.github.io/Crypto/Bitcoin/Slide1.jpg

Caption: Fig.1: More Confidence in Older Blocks in the
    Chain


Using puzzles to identify a single random agent leaves us with at least three challenges: (1) Collisions
will occur; (2) agents may be devious --- they may claim to have
solved puzzles when they haven't; and (3) agents with computing power
that far exceeds those of others will solve their puzzles faster than
others do --- and so though agents are selected randomly, those with
large computing power are likely to be selected more often.Attempts at SynchronizationWhen an agent \(A\) appends a new block \(B\) to a block chain \(L\) it broadcasts
the new chain \(L + B\). This is analogous to a person shouting "I won" in the
example given earlier.When a (non-devious) agent \(A'\), which proposes to extend chain
\(L\), gets a message saying that \(L\) has already been extended to
\(L + B\) then \(A'\)
stops attempting to append a block to \(L\).
Instead, \(A\) starts again with a
new set of transactions that it proposes to append to the
extended chain \(L + B\).If the message delay between agents \(A\) and \(A'\) is small compared
to the time to solve puzzles, then a collision
between \(A\) and \(A'\) is unlikely, but still possible. So, it is
possible that \(A\) broadcasts \(L + B\) while \(A'\) broadcasts
\(L + B'\) at about the same time. So different agents may have
different copies of the block chain.
What is the equivalent of the "true" system-wide
block chain when different agents have different copies?The Bitcoin algorithm does not use synchrony to deal with this
issue. The problem of different copies of the block chain 
extant at the same time is handled in an ingenious asynchronous way that we
describe later.Managing Concurrent UpdatesA key step of the Bitcoin algorithm that updates local copies of block
chains is as follows. After an agent creates a block and appends the
newly created block to its local copy it broadcasts its copy of
the block chain.When an agent \(A\) gets a message containing a copy
of another agent's block chain, agent \(A\) sets its local copy to the
block chain in the messageif and only ifthe length of the
block chain in the message exceeds the length of \(A\)'s local copy.Let's look at a scenario.
For this scenario, \(X\) and \(Y\) are single blocks.
Assume that agent \(A\)'s copy of the block chain
is \(L\) when \(A\) receives a message containing the block chain \(L
+ X\). Because the length of the block chain \(L + X\) is bigger than \(A\)'s
copy, \(A\) sets its copy to \(L + X\).Now suppose agent \(A\) gets a
message containing the block chain \(L + Y\); what does \(A\) do?
Agent \(A\) ignores the message because the length of \(L + Y\) does not exceed
that of \(A\)'s current copy, \(L + X\).Continuing CollisionsLet's continue the above scenario.
Can block \(Y\) become part of \(A\)'s
chain, or will it remain forever an "orphan" block
as far as \(A\) is concerned?Here's a possible scenario.
An agent with a block
chain copy \(L + 
X\), solves its puzzle and appends a block \(X_{1}\) to get a new copy
\(L + X + X_{1}\) of the block chain which the agent broadcasts.
At the same time, another agent with a block
chain copy \(L + 
Y\), solves its puzzle and appends a block \(Y_{1}\) to get a new copy
\(L + Y + Y_{1}\) which is broadcast.
If \(A\) receives \(L + Y + Y_{1}\) before receiving \(L + X + X_{1}\)
then \(A\) will set its chain to \(L + Y + Y_{1}\), and then reject
\(L + X + X_{1}\).You can construct a scenario with a sequence of collisions between
agents appending blocks \(X_{i}\) and other agents appending blocks
\(Y_{j}\) so that \(A\)'s chain switches back and forth between \(X\)
and \(Y\) values.Long sequences of collisions are unlikely, and the longer the sequence
the less the probability of continuing collisions.So how can an agent determine whether a block is in the chain? And so
how can an agent find out if a transaction has been executed? You
sell your used bicycle to somebody for coins, but how do you know if
that transaction becomes part oftheblock chain when different
agents have different copies? And so how do you know that you can
spend those coins that
you should have received for your bicycle?Confidence that a Block is in the ChainIn this section we discuss the behavior of non-devious agents; we will
show how the algorithm handles devious agents later.
Suppose an agent's copy of the block chain is \(X + X_{1} + X_{2}
+ \ldots + X_{K}\), and another agent's copy is \(Y + ? + ? +
\ldots\), where \(?\) represents arbitrary values. What is the
likelihood that \(Y \neq X\)?\(Y\) can be different from \(X\) only if there are a sequence of
\(K\) or more collisions. And the likelihood of a sequence of
collisions decreases with \(K\). Likewise, the likelihood that an
agent never receives a block chain containing \(X\) decreases with
time, and so decreases with \(K\). So, if \(K\) is large then with
very high probability, \(X\) is part of every agent's block chain.Now suppose an agent's copy of the block chain is \(L + X + X_{1} + X_{2}
+ \ldots + X_{K}\). What is the likelihood
that another agent's copy is \(L + Y + ? + \ldots\) where \(L\) is an
arbitrary sequence? By the same
argument, when if \(K\) is large then with
very high probability, \(X\) is part of every block chain.For practical purposes,
many agents assume that if \(K > 6\) then \(L + X\) is a prefix of
most agents' block chains.See the Princeton Bitcoin book.Suppose agent \(A\) gets \(N\) Bitcoins in transaction \(X\). If \(K =
0\) then an agent can't be confident that \(A\) ever received these coins
because this transaction may not persist in the block chain. As \(K\)
increases agents become more confident that the transaction is in the
block chain and that \(A\) did, indeed, receive these coins.Fig.1: More Confidence in Older Blocks in the
    ChainWhat happens to Orphan Transactions?An agent may append a block \(Y\) to its chain, and this block may be
dropped from the chains of all agents and never reappear after some
point. We saw a scenario in which this happens.
Such a block is an "orphan," because no agent has a record of that
transaction after some time.
If an orphan block contains the transaction in which you
sold your bicycle in exchange for coins, then will you ever be able to
spend your coins? Yes, you will get your coins as we see next.Agents aggregate transactions that have not
appeared in the agent's block chain into blocks and propose to append these
blocks to the chain. A transaction that does not appear in block
chains will be agrregated eventually by some agent and inserted into
a block in the chian. Though the orphan block disappears the
transactions in the block do not.IncentivesNext, looks look at challenge number 3. Why should an agent create
blocks of validated transactions?Because the agent gets paid! Payment
is from either ablock rewardor transaction fees.Block rewardsAn agent that creates a block gets a specified number of Bitcoins for
itself as a reward called ablock reward.
The Bitcoins in a block reward are created by making the
block; these Bitcoins don't exist until the block is created. The process of making
blocks and acquiring block rewards is called "mining."Mining is the only way of creating new Bitcoins.When Bitcoin started the reward for creating a block was 50 Bitcoins.
The reward
halves after the creation of a certain number (210,000) of blocks. The
reward was reduced to 25 in 2013 and to 12.5 in 2018.
Block rewards will vanish at some point in the future.
The
total number of Bitcoins that can ever be created has an upper bound:
about 21 million.(Bitcoins can be lost. An agent may lose the hash pointer to the
transaction that gave the agent ownership of the coin, or an agent may
lose its private key.)Even when block rewards vanish, miners will continue to
mine provided that they get paid transaction fees. A
transaction fee is a payment by payers and payees to
miners. Transaction fees are voluntary. A high-fee for a
transaction is an incentive to miners to put this transaction into a
block quickly. So, it's possible that agents that offer no fee or low
fees may have to wait longer for their transactions to enter the
block chain.Incentives are critical for Bitcoin. Miners get paid to
get their blocks into the long-term "consensus" block chain.
Miners have an incentive to police the block chain because 
they don't get paid for appending erroneous blocks. If a miner appends
an erroneous block to the chain then other
miners won't extend chains containing the erroneous block, and so the
erroneous block will become an orphan.Any agent can check whether its copy of the block chain is
valid; however, agents making ordinary transactions don't
need to do so because there are many miners each of whom has an
incentive to ensure that the block chain is legitimate.AttacksNext, looks look at challenge number 4.Stealing coinsCan an agent steal a coin from an agent \(X\) by appending a block to
the chain where the block contains a transaction in which \(X\) gives
coins to \(Y\)?No, this can't happen thanks to cryptography. A transaction into which
\(X\) puts coins is valid only if \(X\) signs the transaction.  \(Y\)
cannot forge \(X\)'s signature, and so \(Y\) cannot create blocks that
contain such fraudulent transactions.Double spendCan an agent spend the same coin twice? Can an agent buy something
without paying for it?Consider the following transaction using conventional checks issued by
banks. A buyer gives a seller a check for $\(100,000\) for a house.
The house is put in escrow.  When the check clears and the seller
receives the payment the buyer gets possession of the house. The legal
process that includes notaries, real estate agents, and banks, helps
ensure that the transaction concludes correctly or is aborted correctly.Next, let's look at a transaction in which a person buys a video
online by paying the seller Bitcoins. The buyer and seller know each
other by their public keys and by their online addresses. 
The buyer broadcasts the transaction in which the buyer gives the seller
the payment in Bitcoins.
The amount is specified as
a pair (transaction id, array index) described in the sectionpay
transactionsinthe module introducing crypto currencies.A miner puts the transaction into a block \(X\);
appends the block to its copy \(L\) of the block chain; and broadcasts the extended
block chain \(L + X\).
When the seller gets a copy of the block chain \(L + X\), the
seller concludes that it has received the payment from the
buyer because the 
transaction has been recorded in a block chain. So the seller gives
the video to the buyer.The buyer cheats. The buyer creates a transaction in which the buyer
transfers the same Bitcoins  to the buyer itself.
A miner creates a block \(Y\) that includes this transaction.
A miner who has only received block
chain \(L\) (and hasn't yet received chain \(L + X\)) appends \(Y\) to
\(L\) to get a chain \(L + Y\), and broacasts \(L + Y\).Now we have a situation in which one
miner broadcasts a legitimate block chain \(L + X\) and a different
miner broadcasts a legitimate block chain \(L + Y\). Both chains have
the same length.
A miner with chain \(L + X\) does not know at this point that another
minder has chain \(L + Y\). So, miners will extend both block chains.
(Note that the algorithm will not permit chains \(L + X + Y\) or \(L +
Y + X\).)We've seen this situation before:  look atblock collisionsdescribed
earlier in this module. Both block chains will be extended, but
eventually, with very high probability, one of the blocks \(X\) or
\(Y\) will drop out of chain.How should sellers protect themselves?What is the equivalent of the buyer's check clearing in the bank?A seller should give the item to the buyer only after the transaction
appears with high confidence in a block in the chain --- see Figure 1.
The seller listens to block chains broadcast by miners. If the seller
gets a block chain \(L + X + L'\) where \(L'\) is itself a long block chain
then the seller has high confidence that the transaction is in the
permanent record.The seller waits to get block chains in which its transaction appears
in a block which is then followed by \(m\) blocks, for large
\(m\). The larger the value of \(m\), the greater the seller's
confidence, but the longer the buyer has to wait to get paid.
length of the extension \(L'\).A value of \(m = 6\) gives adequate confidence in most cases.Fraudulent minersA miner gets paid for every block the miner creates; so, why shouldn't
the miner create fraudulent blocks and get paid for them?The answer is the same as that for the double-spending attack. A miner
may have 
created a block and appended it to the chain; however, a suspicious
agent (and we hope that all agents are suspicious!) will not accept
the block until many blocks have been appended to the chain after
it. Other miners won't append their blocks to an invalid one. The
invalid blocks will become permanent orphans, and so the fraudulent
miner won't be able to spend the coins in these blocks.Fifty One Percent AttacksA 51% attack can be carried out by an agent that has more mining
power (e.g. 51% or more) than all other agents combined. The higher
the proportion of mining power of a single agent, the greater the
chances that attacks by that agent will succeed. You can see the
danger of a single agent having predominant mining power by thinking
about an agent with say, 99% of the total mining power. This agent
can mine so much faster than others that it can manipulate the block
chain in many ways. It can create double-spend transactions and deny
services to some transactions.A group of miners can collude to gain predominant mining power. Also,
miners canrentCloud-based systems for mining --- as opposed to having toownhuge data centers. An attacker can rent a large system for the specific purpose and
duration of an attack. The public may not know if and when such
attacks are successful becausecryptocurrencies do not have an
incentive to publish such attacks.Denial of serviceCan agents collude so that an agent \(X\)'s transactions never get
into blocks and so never get processed?An agent's identity does not appear in a transaction, only a public
key does. An agent can create new public keys at will. So let's ask
another question: can agents collude so that transactions with a
specific public key do not get into blocks?Only an agent that can solve puzzles in reasonable time can make
blocks. These agents have significant computational power. One can
concoct a situation where many agents with significant computation
power collude to avoid transactions from a specific public key. This
could have the effect of slowing processing of certain
transactions. However, such a situation isn't likely to arise because
agents have an incentive to create blocks and so they compete ---
rather than collude --- with each other.
Colluding
agents, with massive computing power, may help to deny or slow service
to a public key but not to another agent because agents can create
public keys at will.Further ReadingThere are many issues that we have not covered. This material only
covers the basics from the point of view of distributed algorithms.ReviewWhat is ablock chain?How does the algorithm deal with the situation in which one agent
receives a message to extend its copy of the block chain with a block
\(X\) while another agent receives a message to extend its copy of the
  block chain with a different block \(Y\)?Could there be an infinite sequence of collisions? Is that likely?What is mining for cryptocurrency? Suppose only a small number (say
  two organizations) mined most (say 99%) of Bitcoins. Would that be a
  problem?Why isdouble spendingunlikely to be successful?K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology



Figure Link: https://kmchandy.github.io/Crypto/Bitcoin/Slide1.jpg

Caption: Fig.1: More Confidence in Older Blocks in the
    Chain


When a (non-devious) agent \(A'\), which proposes to extend chain
\(L\), gets a message saying that \(L\) has already been extended to
\(L + B\) then \(A'\)
stops attempting to append a block to \(L\).
Instead, \(A\) starts again with a
new set of transactions that it proposes to append to the
extended chain \(L + B\).If the message delay between agents \(A\) and \(A'\) is small compared
to the time to solve puzzles, then a collision
between \(A\) and \(A'\) is unlikely, but still possible. So, it is
possible that \(A\) broadcasts \(L + B\) while \(A'\) broadcasts
\(L + B'\) at about the same time. So different agents may have
different copies of the block chain.
What is the equivalent of the "true" system-wide
block chain when different agents have different copies?The Bitcoin algorithm does not use synchrony to deal with this
issue. The problem of different copies of the block chain 
extant at the same time is handled in an ingenious asynchronous way that we
describe later.Managing Concurrent UpdatesA key step of the Bitcoin algorithm that updates local copies of block
chains is as follows. After an agent creates a block and appends the
newly created block to its local copy it broadcasts its copy of
the block chain.When an agent \(A\) gets a message containing a copy
of another agent's block chain, agent \(A\) sets its local copy to the
block chain in the messageif and only ifthe length of the
block chain in the message exceeds the length of \(A\)'s local copy.Let's look at a scenario.
For this scenario, \(X\) and \(Y\) are single blocks.
Assume that agent \(A\)'s copy of the block chain
is \(L\) when \(A\) receives a message containing the block chain \(L
+ X\). Because the length of the block chain \(L + X\) is bigger than \(A\)'s
copy, \(A\) sets its copy to \(L + X\).Now suppose agent \(A\) gets a
message containing the block chain \(L + Y\); what does \(A\) do?
Agent \(A\) ignores the message because the length of \(L + Y\) does not exceed
that of \(A\)'s current copy, \(L + X\).Continuing CollisionsLet's continue the above scenario.
Can block \(Y\) become part of \(A\)'s
chain, or will it remain forever an "orphan" block
as far as \(A\) is concerned?Here's a possible scenario.
An agent with a block
chain copy \(L + 
X\), solves its puzzle and appends a block \(X_{1}\) to get a new copy
\(L + X + X_{1}\) of the block chain which the agent broadcasts.
At the same time, another agent with a block
chain copy \(L + 
Y\), solves its puzzle and appends a block \(Y_{1}\) to get a new copy
\(L + Y + Y_{1}\) which is broadcast.
If \(A\) receives \(L + Y + Y_{1}\) before receiving \(L + X + X_{1}\)
then \(A\) will set its chain to \(L + Y + Y_{1}\), and then reject
\(L + X + X_{1}\).You can construct a scenario with a sequence of collisions between
agents appending blocks \(X_{i}\) and other agents appending blocks
\(Y_{j}\) so that \(A\)'s chain switches back and forth between \(X\)
and \(Y\) values.Long sequences of collisions are unlikely, and the longer the sequence
the less the probability of continuing collisions.So how can an agent determine whether a block is in the chain? And so
how can an agent find out if a transaction has been executed? You
sell your used bicycle to somebody for coins, but how do you know if
that transaction becomes part oftheblock chain when different
agents have different copies? And so how do you know that you can
spend those coins that
you should have received for your bicycle?Confidence that a Block is in the ChainIn this section we discuss the behavior of non-devious agents; we will
show how the algorithm handles devious agents later.
Suppose an agent's copy of the block chain is \(X + X_{1} + X_{2}
+ \ldots + X_{K}\), and another agent's copy is \(Y + ? + ? +
\ldots\), where \(?\) represents arbitrary values. What is the
likelihood that \(Y \neq X\)?\(Y\) can be different from \(X\) only if there are a sequence of
\(K\) or more collisions. And the likelihood of a sequence of
collisions decreases with \(K\). Likewise, the likelihood that an
agent never receives a block chain containing \(X\) decreases with
time, and so decreases with \(K\). So, if \(K\) is large then with
very high probability, \(X\) is part of every agent's block chain.Now suppose an agent's copy of the block chain is \(L + X + X_{1} + X_{2}
+ \ldots + X_{K}\). What is the likelihood
that another agent's copy is \(L + Y + ? + \ldots\) where \(L\) is an
arbitrary sequence? By the same
argument, when if \(K\) is large then with
very high probability, \(X\) is part of every block chain.For practical purposes,
many agents assume that if \(K > 6\) then \(L + X\) is a prefix of
most agents' block chains.See the Princeton Bitcoin book.Suppose agent \(A\) gets \(N\) Bitcoins in transaction \(X\). If \(K =
0\) then an agent can't be confident that \(A\) ever received these coins
because this transaction may not persist in the block chain. As \(K\)
increases agents become more confident that the transaction is in the
block chain and that \(A\) did, indeed, receive these coins.Fig.1: More Confidence in Older Blocks in the
    ChainWhat happens to Orphan Transactions?An agent may append a block \(Y\) to its chain, and this block may be
dropped from the chains of all agents and never reappear after some
point. We saw a scenario in which this happens.
Such a block is an "orphan," because no agent has a record of that
transaction after some time.
If an orphan block contains the transaction in which you
sold your bicycle in exchange for coins, then will you ever be able to
spend your coins? Yes, you will get your coins as we see next.Agents aggregate transactions that have not
appeared in the agent's block chain into blocks and propose to append these
blocks to the chain. A transaction that does not appear in block
chains will be agrregated eventually by some agent and inserted into
a block in the chian. Though the orphan block disappears the
transactions in the block do not.IncentivesNext, looks look at challenge number 3. Why should an agent create
blocks of validated transactions?Because the agent gets paid! Payment
is from either ablock rewardor transaction fees.Block rewardsAn agent that creates a block gets a specified number of Bitcoins for
itself as a reward called ablock reward.
The Bitcoins in a block reward are created by making the
block; these Bitcoins don't exist until the block is created. The process of making
blocks and acquiring block rewards is called "mining."Mining is the only way of creating new Bitcoins.When Bitcoin started the reward for creating a block was 50 Bitcoins.
The reward
halves after the creation of a certain number (210,000) of blocks. The
reward was reduced to 25 in 2013 and to 12.5 in 2018.
Block rewards will vanish at some point in the future.
The
total number of Bitcoins that can ever be created has an upper bound:
about 21 million.(Bitcoins can be lost. An agent may lose the hash pointer to the
transaction that gave the agent ownership of the coin, or an agent may
lose its private key.)Even when block rewards vanish, miners will continue to
mine provided that they get paid transaction fees. A
transaction fee is a payment by payers and payees to
miners. Transaction fees are voluntary. A high-fee for a
transaction is an incentive to miners to put this transaction into a
block quickly. So, it's possible that agents that offer no fee or low
fees may have to wait longer for their transactions to enter the
block chain.Incentives are critical for Bitcoin. Miners get paid to
get their blocks into the long-term "consensus" block chain.
Miners have an incentive to police the block chain because 
they don't get paid for appending erroneous blocks. If a miner appends
an erroneous block to the chain then other
miners won't extend chains containing the erroneous block, and so the
erroneous block will become an orphan.Any agent can check whether its copy of the block chain is
valid; however, agents making ordinary transactions don't
need to do so because there are many miners each of whom has an
incentive to ensure that the block chain is legitimate.AttacksNext, looks look at challenge number 4.Stealing coinsCan an agent steal a coin from an agent \(X\) by appending a block to
the chain where the block contains a transaction in which \(X\) gives
coins to \(Y\)?No, this can't happen thanks to cryptography. A transaction into which
\(X\) puts coins is valid only if \(X\) signs the transaction.  \(Y\)
cannot forge \(X\)'s signature, and so \(Y\) cannot create blocks that
contain such fraudulent transactions.Double spendCan an agent spend the same coin twice? Can an agent buy something
without paying for it?Consider the following transaction using conventional checks issued by
banks. A buyer gives a seller a check for $\(100,000\) for a house.
The house is put in escrow.  When the check clears and the seller
receives the payment the buyer gets possession of the house. The legal
process that includes notaries, real estate agents, and banks, helps
ensure that the transaction concludes correctly or is aborted correctly.Next, let's look at a transaction in which a person buys a video
online by paying the seller Bitcoins. The buyer and seller know each
other by their public keys and by their online addresses. 
The buyer broadcasts the transaction in which the buyer gives the seller
the payment in Bitcoins.
The amount is specified as
a pair (transaction id, array index) described in the sectionpay
transactionsinthe module introducing crypto currencies.A miner puts the transaction into a block \(X\);
appends the block to its copy \(L\) of the block chain; and broadcasts the extended
block chain \(L + X\).
When the seller gets a copy of the block chain \(L + X\), the
seller concludes that it has received the payment from the
buyer because the 
transaction has been recorded in a block chain. So the seller gives
the video to the buyer.The buyer cheats. The buyer creates a transaction in which the buyer
transfers the same Bitcoins  to the buyer itself.
A miner creates a block \(Y\) that includes this transaction.
A miner who has only received block
chain \(L\) (and hasn't yet received chain \(L + X\)) appends \(Y\) to
\(L\) to get a chain \(L + Y\), and broacasts \(L + Y\).Now we have a situation in which one
miner broadcasts a legitimate block chain \(L + X\) and a different
miner broadcasts a legitimate block chain \(L + Y\). Both chains have
the same length.
A miner with chain \(L + X\) does not know at this point that another
minder has chain \(L + Y\). So, miners will extend both block chains.
(Note that the algorithm will not permit chains \(L + X + Y\) or \(L +
Y + X\).)We've seen this situation before:  look atblock collisionsdescribed
earlier in this module. Both block chains will be extended, but
eventually, with very high probability, one of the blocks \(X\) or
\(Y\) will drop out of chain.How should sellers protect themselves?What is the equivalent of the buyer's check clearing in the bank?A seller should give the item to the buyer only after the transaction
appears with high confidence in a block in the chain --- see Figure 1.
The seller listens to block chains broadcast by miners. If the seller
gets a block chain \(L + X + L'\) where \(L'\) is itself a long block chain
then the seller has high confidence that the transaction is in the
permanent record.The seller waits to get block chains in which its transaction appears
in a block which is then followed by \(m\) blocks, for large
\(m\). The larger the value of \(m\), the greater the seller's
confidence, but the longer the buyer has to wait to get paid.
length of the extension \(L'\).A value of \(m = 6\) gives adequate confidence in most cases.Fraudulent minersA miner gets paid for every block the miner creates; so, why shouldn't
the miner create fraudulent blocks and get paid for them?The answer is the same as that for the double-spending attack. A miner
may have 
created a block and appended it to the chain; however, a suspicious
agent (and we hope that all agents are suspicious!) will not accept
the block until many blocks have been appended to the chain after
it. Other miners won't append their blocks to an invalid one. The
invalid blocks will become permanent orphans, and so the fraudulent
miner won't be able to spend the coins in these blocks.Fifty One Percent AttacksA 51% attack can be carried out by an agent that has more mining
power (e.g. 51% or more) than all other agents combined. The higher
the proportion of mining power of a single agent, the greater the
chances that attacks by that agent will succeed. You can see the
danger of a single agent having predominant mining power by thinking
about an agent with say, 99% of the total mining power. This agent
can mine so much faster than others that it can manipulate the block
chain in many ways. It can create double-spend transactions and deny
services to some transactions.A group of miners can collude to gain predominant mining power. Also,
miners canrentCloud-based systems for mining --- as opposed to having toownhuge data centers. An attacker can rent a large system for the specific purpose and
duration of an attack. The public may not know if and when such
attacks are successful becausecryptocurrencies do not have an
incentive to publish such attacks.Denial of serviceCan agents collude so that an agent \(X\)'s transactions never get
into blocks and so never get processed?An agent's identity does not appear in a transaction, only a public
key does. An agent can create new public keys at will. So let's ask
another question: can agents collude so that transactions with a
specific public key do not get into blocks?Only an agent that can solve puzzles in reasonable time can make
blocks. These agents have significant computational power. One can
concoct a situation where many agents with significant computation
power collude to avoid transactions from a specific public key. This
could have the effect of slowing processing of certain
transactions. However, such a situation isn't likely to arise because
agents have an incentive to create blocks and so they compete ---
rather than collude --- with each other.
Colluding
agents, with massive computing power, may help to deny or slow service
to a public key but not to another agent because agents can create
public keys at will.Further ReadingThere are many issues that we have not covered. This material only
covers the basics from the point of view of distributed algorithms.ReviewWhat is ablock chain?How does the algorithm deal with the situation in which one agent
receives a message to extend its copy of the block chain with a block
\(X\) while another agent receives a message to extend its copy of the
  block chain with a different block \(Y\)?Could there be an infinite sequence of collisions? Is that likely?What is mining for cryptocurrency? Suppose only a small number (say
  two organizations) mined most (say 99%) of Bitcoins. Would that be a
  problem?Why isdouble spendingunlikely to be successful?K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology



Figure Link: https://kmchandy.github.io/Crypto/Bitcoin/Slide1.jpg

Caption: Fig.1: More Confidence in Older Blocks in the
    Chain


If the message delay between agents \(A\) and \(A'\) is small compared
to the time to solve puzzles, then a collision
between \(A\) and \(A'\) is unlikely, but still possible. So, it is
possible that \(A\) broadcasts \(L + B\) while \(A'\) broadcasts
\(L + B'\) at about the same time. So different agents may have
different copies of the block chain.
What is the equivalent of the "true" system-wide
block chain when different agents have different copies?The Bitcoin algorithm does not use synchrony to deal with this
issue. The problem of different copies of the block chain 
extant at the same time is handled in an ingenious asynchronous way that we
describe later.Managing Concurrent UpdatesA key step of the Bitcoin algorithm that updates local copies of block
chains is as follows. After an agent creates a block and appends the
newly created block to its local copy it broadcasts its copy of
the block chain.When an agent \(A\) gets a message containing a copy
of another agent's block chain, agent \(A\) sets its local copy to the
block chain in the messageif and only ifthe length of the
block chain in the message exceeds the length of \(A\)'s local copy.Let's look at a scenario.
For this scenario, \(X\) and \(Y\) are single blocks.
Assume that agent \(A\)'s copy of the block chain
is \(L\) when \(A\) receives a message containing the block chain \(L
+ X\). Because the length of the block chain \(L + X\) is bigger than \(A\)'s
copy, \(A\) sets its copy to \(L + X\).Now suppose agent \(A\) gets a
message containing the block chain \(L + Y\); what does \(A\) do?
Agent \(A\) ignores the message because the length of \(L + Y\) does not exceed
that of \(A\)'s current copy, \(L + X\).Continuing CollisionsLet's continue the above scenario.
Can block \(Y\) become part of \(A\)'s
chain, or will it remain forever an "orphan" block
as far as \(A\) is concerned?Here's a possible scenario.
An agent with a block
chain copy \(L + 
X\), solves its puzzle and appends a block \(X_{1}\) to get a new copy
\(L + X + X_{1}\) of the block chain which the agent broadcasts.
At the same time, another agent with a block
chain copy \(L + 
Y\), solves its puzzle and appends a block \(Y_{1}\) to get a new copy
\(L + Y + Y_{1}\) which is broadcast.
If \(A\) receives \(L + Y + Y_{1}\) before receiving \(L + X + X_{1}\)
then \(A\) will set its chain to \(L + Y + Y_{1}\), and then reject
\(L + X + X_{1}\).You can construct a scenario with a sequence of collisions between
agents appending blocks \(X_{i}\) and other agents appending blocks
\(Y_{j}\) so that \(A\)'s chain switches back and forth between \(X\)
and \(Y\) values.Long sequences of collisions are unlikely, and the longer the sequence
the less the probability of continuing collisions.So how can an agent determine whether a block is in the chain? And so
how can an agent find out if a transaction has been executed? You
sell your used bicycle to somebody for coins, but how do you know if
that transaction becomes part oftheblock chain when different
agents have different copies? And so how do you know that you can
spend those coins that
you should have received for your bicycle?Confidence that a Block is in the ChainIn this section we discuss the behavior of non-devious agents; we will
show how the algorithm handles devious agents later.
Suppose an agent's copy of the block chain is \(X + X_{1} + X_{2}
+ \ldots + X_{K}\), and another agent's copy is \(Y + ? + ? +
\ldots\), where \(?\) represents arbitrary values. What is the
likelihood that \(Y \neq X\)?\(Y\) can be different from \(X\) only if there are a sequence of
\(K\) or more collisions. And the likelihood of a sequence of
collisions decreases with \(K\). Likewise, the likelihood that an
agent never receives a block chain containing \(X\) decreases with
time, and so decreases with \(K\). So, if \(K\) is large then with
very high probability, \(X\) is part of every agent's block chain.Now suppose an agent's copy of the block chain is \(L + X + X_{1} + X_{2}
+ \ldots + X_{K}\). What is the likelihood
that another agent's copy is \(L + Y + ? + \ldots\) where \(L\) is an
arbitrary sequence? By the same
argument, when if \(K\) is large then with
very high probability, \(X\) is part of every block chain.For practical purposes,
many agents assume that if \(K > 6\) then \(L + X\) is a prefix of
most agents' block chains.See the Princeton Bitcoin book.Suppose agent \(A\) gets \(N\) Bitcoins in transaction \(X\). If \(K =
0\) then an agent can't be confident that \(A\) ever received these coins
because this transaction may not persist in the block chain. As \(K\)
increases agents become more confident that the transaction is in the
block chain and that \(A\) did, indeed, receive these coins.Fig.1: More Confidence in Older Blocks in the
    ChainWhat happens to Orphan Transactions?An agent may append a block \(Y\) to its chain, and this block may be
dropped from the chains of all agents and never reappear after some
point. We saw a scenario in which this happens.
Such a block is an "orphan," because no agent has a record of that
transaction after some time.
If an orphan block contains the transaction in which you
sold your bicycle in exchange for coins, then will you ever be able to
spend your coins? Yes, you will get your coins as we see next.Agents aggregate transactions that have not
appeared in the agent's block chain into blocks and propose to append these
blocks to the chain. A transaction that does not appear in block
chains will be agrregated eventually by some agent and inserted into
a block in the chian. Though the orphan block disappears the
transactions in the block do not.IncentivesNext, looks look at challenge number 3. Why should an agent create
blocks of validated transactions?Because the agent gets paid! Payment
is from either ablock rewardor transaction fees.Block rewardsAn agent that creates a block gets a specified number of Bitcoins for
itself as a reward called ablock reward.
The Bitcoins in a block reward are created by making the
block; these Bitcoins don't exist until the block is created. The process of making
blocks and acquiring block rewards is called "mining."Mining is the only way of creating new Bitcoins.When Bitcoin started the reward for creating a block was 50 Bitcoins.
The reward
halves after the creation of a certain number (210,000) of blocks. The
reward was reduced to 25 in 2013 and to 12.5 in 2018.
Block rewards will vanish at some point in the future.
The
total number of Bitcoins that can ever be created has an upper bound:
about 21 million.(Bitcoins can be lost. An agent may lose the hash pointer to the
transaction that gave the agent ownership of the coin, or an agent may
lose its private key.)Even when block rewards vanish, miners will continue to
mine provided that they get paid transaction fees. A
transaction fee is a payment by payers and payees to
miners. Transaction fees are voluntary. A high-fee for a
transaction is an incentive to miners to put this transaction into a
block quickly. So, it's possible that agents that offer no fee or low
fees may have to wait longer for their transactions to enter the
block chain.Incentives are critical for Bitcoin. Miners get paid to
get their blocks into the long-term "consensus" block chain.
Miners have an incentive to police the block chain because 
they don't get paid for appending erroneous blocks. If a miner appends
an erroneous block to the chain then other
miners won't extend chains containing the erroneous block, and so the
erroneous block will become an orphan.Any agent can check whether its copy of the block chain is
valid; however, agents making ordinary transactions don't
need to do so because there are many miners each of whom has an
incentive to ensure that the block chain is legitimate.AttacksNext, looks look at challenge number 4.Stealing coinsCan an agent steal a coin from an agent \(X\) by appending a block to
the chain where the block contains a transaction in which \(X\) gives
coins to \(Y\)?No, this can't happen thanks to cryptography. A transaction into which
\(X\) puts coins is valid only if \(X\) signs the transaction.  \(Y\)
cannot forge \(X\)'s signature, and so \(Y\) cannot create blocks that
contain such fraudulent transactions.Double spendCan an agent spend the same coin twice? Can an agent buy something
without paying for it?Consider the following transaction using conventional checks issued by
banks. A buyer gives a seller a check for $\(100,000\) for a house.
The house is put in escrow.  When the check clears and the seller
receives the payment the buyer gets possession of the house. The legal
process that includes notaries, real estate agents, and banks, helps
ensure that the transaction concludes correctly or is aborted correctly.Next, let's look at a transaction in which a person buys a video
online by paying the seller Bitcoins. The buyer and seller know each
other by their public keys and by their online addresses. 
The buyer broadcasts the transaction in which the buyer gives the seller
the payment in Bitcoins.
The amount is specified as
a pair (transaction id, array index) described in the sectionpay
transactionsinthe module introducing crypto currencies.A miner puts the transaction into a block \(X\);
appends the block to its copy \(L\) of the block chain; and broadcasts the extended
block chain \(L + X\).
When the seller gets a copy of the block chain \(L + X\), the
seller concludes that it has received the payment from the
buyer because the 
transaction has been recorded in a block chain. So the seller gives
the video to the buyer.The buyer cheats. The buyer creates a transaction in which the buyer
transfers the same Bitcoins  to the buyer itself.
A miner creates a block \(Y\) that includes this transaction.
A miner who has only received block
chain \(L\) (and hasn't yet received chain \(L + X\)) appends \(Y\) to
\(L\) to get a chain \(L + Y\), and broacasts \(L + Y\).Now we have a situation in which one
miner broadcasts a legitimate block chain \(L + X\) and a different
miner broadcasts a legitimate block chain \(L + Y\). Both chains have
the same length.
A miner with chain \(L + X\) does not know at this point that another
minder has chain \(L + Y\). So, miners will extend both block chains.
(Note that the algorithm will not permit chains \(L + X + Y\) or \(L +
Y + X\).)We've seen this situation before:  look atblock collisionsdescribed
earlier in this module. Both block chains will be extended, but
eventually, with very high probability, one of the blocks \(X\) or
\(Y\) will drop out of chain.How should sellers protect themselves?What is the equivalent of the buyer's check clearing in the bank?A seller should give the item to the buyer only after the transaction
appears with high confidence in a block in the chain --- see Figure 1.
The seller listens to block chains broadcast by miners. If the seller
gets a block chain \(L + X + L'\) where \(L'\) is itself a long block chain
then the seller has high confidence that the transaction is in the
permanent record.The seller waits to get block chains in which its transaction appears
in a block which is then followed by \(m\) blocks, for large
\(m\). The larger the value of \(m\), the greater the seller's
confidence, but the longer the buyer has to wait to get paid.
length of the extension \(L'\).A value of \(m = 6\) gives adequate confidence in most cases.Fraudulent minersA miner gets paid for every block the miner creates; so, why shouldn't
the miner create fraudulent blocks and get paid for them?The answer is the same as that for the double-spending attack. A miner
may have 
created a block and appended it to the chain; however, a suspicious
agent (and we hope that all agents are suspicious!) will not accept
the block until many blocks have been appended to the chain after
it. Other miners won't append their blocks to an invalid one. The
invalid blocks will become permanent orphans, and so the fraudulent
miner won't be able to spend the coins in these blocks.Fifty One Percent AttacksA 51% attack can be carried out by an agent that has more mining
power (e.g. 51% or more) than all other agents combined. The higher
the proportion of mining power of a single agent, the greater the
chances that attacks by that agent will succeed. You can see the
danger of a single agent having predominant mining power by thinking
about an agent with say, 99% of the total mining power. This agent
can mine so much faster than others that it can manipulate the block
chain in many ways. It can create double-spend transactions and deny
services to some transactions.A group of miners can collude to gain predominant mining power. Also,
miners canrentCloud-based systems for mining --- as opposed to having toownhuge data centers. An attacker can rent a large system for the specific purpose and
duration of an attack. The public may not know if and when such
attacks are successful becausecryptocurrencies do not have an
incentive to publish such attacks.Denial of serviceCan agents collude so that an agent \(X\)'s transactions never get
into blocks and so never get processed?An agent's identity does not appear in a transaction, only a public
key does. An agent can create new public keys at will. So let's ask
another question: can agents collude so that transactions with a
specific public key do not get into blocks?Only an agent that can solve puzzles in reasonable time can make
blocks. These agents have significant computational power. One can
concoct a situation where many agents with significant computation
power collude to avoid transactions from a specific public key. This
could have the effect of slowing processing of certain
transactions. However, such a situation isn't likely to arise because
agents have an incentive to create blocks and so they compete ---
rather than collude --- with each other.
Colluding
agents, with massive computing power, may help to deny or slow service
to a public key but not to another agent because agents can create
public keys at will.Further ReadingThere are many issues that we have not covered. This material only
covers the basics from the point of view of distributed algorithms.ReviewWhat is ablock chain?How does the algorithm deal with the situation in which one agent
receives a message to extend its copy of the block chain with a block
\(X\) while another agent receives a message to extend its copy of the
  block chain with a different block \(Y\)?Could there be an infinite sequence of collisions? Is that likely?What is mining for cryptocurrency? Suppose only a small number (say
  two organizations) mined most (say 99%) of Bitcoins. Would that be a
  problem?Why isdouble spendingunlikely to be successful?K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology



Figure Link: https://kmchandy.github.io/Crypto/Bitcoin/Slide1.jpg

Caption: Fig.1: More Confidence in Older Blocks in the
    Chain


The Bitcoin algorithm does not use synchrony to deal with this
issue. The problem of different copies of the block chain 
extant at the same time is handled in an ingenious asynchronous way that we
describe later.Managing Concurrent UpdatesA key step of the Bitcoin algorithm that updates local copies of block
chains is as follows. After an agent creates a block and appends the
newly created block to its local copy it broadcasts its copy of
the block chain.When an agent \(A\) gets a message containing a copy
of another agent's block chain, agent \(A\) sets its local copy to the
block chain in the messageif and only ifthe length of the
block chain in the message exceeds the length of \(A\)'s local copy.Let's look at a scenario.
For this scenario, \(X\) and \(Y\) are single blocks.
Assume that agent \(A\)'s copy of the block chain
is \(L\) when \(A\) receives a message containing the block chain \(L
+ X\). Because the length of the block chain \(L + X\) is bigger than \(A\)'s
copy, \(A\) sets its copy to \(L + X\).Now suppose agent \(A\) gets a
message containing the block chain \(L + Y\); what does \(A\) do?
Agent \(A\) ignores the message because the length of \(L + Y\) does not exceed
that of \(A\)'s current copy, \(L + X\).Continuing CollisionsLet's continue the above scenario.
Can block \(Y\) become part of \(A\)'s
chain, or will it remain forever an "orphan" block
as far as \(A\) is concerned?Here's a possible scenario.
An agent with a block
chain copy \(L + 
X\), solves its puzzle and appends a block \(X_{1}\) to get a new copy
\(L + X + X_{1}\) of the block chain which the agent broadcasts.
At the same time, another agent with a block
chain copy \(L + 
Y\), solves its puzzle and appends a block \(Y_{1}\) to get a new copy
\(L + Y + Y_{1}\) which is broadcast.
If \(A\) receives \(L + Y + Y_{1}\) before receiving \(L + X + X_{1}\)
then \(A\) will set its chain to \(L + Y + Y_{1}\), and then reject
\(L + X + X_{1}\).You can construct a scenario with a sequence of collisions between
agents appending blocks \(X_{i}\) and other agents appending blocks
\(Y_{j}\) so that \(A\)'s chain switches back and forth between \(X\)
and \(Y\) values.Long sequences of collisions are unlikely, and the longer the sequence
the less the probability of continuing collisions.So how can an agent determine whether a block is in the chain? And so
how can an agent find out if a transaction has been executed? You
sell your used bicycle to somebody for coins, but how do you know if
that transaction becomes part oftheblock chain when different
agents have different copies? And so how do you know that you can
spend those coins that
you should have received for your bicycle?Confidence that a Block is in the ChainIn this section we discuss the behavior of non-devious agents; we will
show how the algorithm handles devious agents later.
Suppose an agent's copy of the block chain is \(X + X_{1} + X_{2}
+ \ldots + X_{K}\), and another agent's copy is \(Y + ? + ? +
\ldots\), where \(?\) represents arbitrary values. What is the
likelihood that \(Y \neq X\)?\(Y\) can be different from \(X\) only if there are a sequence of
\(K\) or more collisions. And the likelihood of a sequence of
collisions decreases with \(K\). Likewise, the likelihood that an
agent never receives a block chain containing \(X\) decreases with
time, and so decreases with \(K\). So, if \(K\) is large then with
very high probability, \(X\) is part of every agent's block chain.Now suppose an agent's copy of the block chain is \(L + X + X_{1} + X_{2}
+ \ldots + X_{K}\). What is the likelihood
that another agent's copy is \(L + Y + ? + \ldots\) where \(L\) is an
arbitrary sequence? By the same
argument, when if \(K\) is large then with
very high probability, \(X\) is part of every block chain.For practical purposes,
many agents assume that if \(K > 6\) then \(L + X\) is a prefix of
most agents' block chains.See the Princeton Bitcoin book.Suppose agent \(A\) gets \(N\) Bitcoins in transaction \(X\). If \(K =
0\) then an agent can't be confident that \(A\) ever received these coins
because this transaction may not persist in the block chain. As \(K\)
increases agents become more confident that the transaction is in the
block chain and that \(A\) did, indeed, receive these coins.Fig.1: More Confidence in Older Blocks in the
    ChainWhat happens to Orphan Transactions?An agent may append a block \(Y\) to its chain, and this block may be
dropped from the chains of all agents and never reappear after some
point. We saw a scenario in which this happens.
Such a block is an "orphan," because no agent has a record of that
transaction after some time.
If an orphan block contains the transaction in which you
sold your bicycle in exchange for coins, then will you ever be able to
spend your coins? Yes, you will get your coins as we see next.Agents aggregate transactions that have not
appeared in the agent's block chain into blocks and propose to append these
blocks to the chain. A transaction that does not appear in block
chains will be agrregated eventually by some agent and inserted into
a block in the chian. Though the orphan block disappears the
transactions in the block do not.IncentivesNext, looks look at challenge number 3. Why should an agent create
blocks of validated transactions?Because the agent gets paid! Payment
is from either ablock rewardor transaction fees.Block rewardsAn agent that creates a block gets a specified number of Bitcoins for
itself as a reward called ablock reward.
The Bitcoins in a block reward are created by making the
block; these Bitcoins don't exist until the block is created. The process of making
blocks and acquiring block rewards is called "mining."Mining is the only way of creating new Bitcoins.When Bitcoin started the reward for creating a block was 50 Bitcoins.
The reward
halves after the creation of a certain number (210,000) of blocks. The
reward was reduced to 25 in 2013 and to 12.5 in 2018.
Block rewards will vanish at some point in the future.
The
total number of Bitcoins that can ever be created has an upper bound:
about 21 million.(Bitcoins can be lost. An agent may lose the hash pointer to the
transaction that gave the agent ownership of the coin, or an agent may
lose its private key.)Even when block rewards vanish, miners will continue to
mine provided that they get paid transaction fees. A
transaction fee is a payment by payers and payees to
miners. Transaction fees are voluntary. A high-fee for a
transaction is an incentive to miners to put this transaction into a
block quickly. So, it's possible that agents that offer no fee or low
fees may have to wait longer for their transactions to enter the
block chain.Incentives are critical for Bitcoin. Miners get paid to
get their blocks into the long-term "consensus" block chain.
Miners have an incentive to police the block chain because 
they don't get paid for appending erroneous blocks. If a miner appends
an erroneous block to the chain then other
miners won't extend chains containing the erroneous block, and so the
erroneous block will become an orphan.Any agent can check whether its copy of the block chain is
valid; however, agents making ordinary transactions don't
need to do so because there are many miners each of whom has an
incentive to ensure that the block chain is legitimate.AttacksNext, looks look at challenge number 4.Stealing coinsCan an agent steal a coin from an agent \(X\) by appending a block to
the chain where the block contains a transaction in which \(X\) gives
coins to \(Y\)?No, this can't happen thanks to cryptography. A transaction into which
\(X\) puts coins is valid only if \(X\) signs the transaction.  \(Y\)
cannot forge \(X\)'s signature, and so \(Y\) cannot create blocks that
contain such fraudulent transactions.Double spendCan an agent spend the same coin twice? Can an agent buy something
without paying for it?Consider the following transaction using conventional checks issued by
banks. A buyer gives a seller a check for $\(100,000\) for a house.
The house is put in escrow.  When the check clears and the seller
receives the payment the buyer gets possession of the house. The legal
process that includes notaries, real estate agents, and banks, helps
ensure that the transaction concludes correctly or is aborted correctly.Next, let's look at a transaction in which a person buys a video
online by paying the seller Bitcoins. The buyer and seller know each
other by their public keys and by their online addresses. 
The buyer broadcasts the transaction in which the buyer gives the seller
the payment in Bitcoins.
The amount is specified as
a pair (transaction id, array index) described in the sectionpay
transactionsinthe module introducing crypto currencies.A miner puts the transaction into a block \(X\);
appends the block to its copy \(L\) of the block chain; and broadcasts the extended
block chain \(L + X\).
When the seller gets a copy of the block chain \(L + X\), the
seller concludes that it has received the payment from the
buyer because the 
transaction has been recorded in a block chain. So the seller gives
the video to the buyer.The buyer cheats. The buyer creates a transaction in which the buyer
transfers the same Bitcoins  to the buyer itself.
A miner creates a block \(Y\) that includes this transaction.
A miner who has only received block
chain \(L\) (and hasn't yet received chain \(L + X\)) appends \(Y\) to
\(L\) to get a chain \(L + Y\), and broacasts \(L + Y\).Now we have a situation in which one
miner broadcasts a legitimate block chain \(L + X\) and a different
miner broadcasts a legitimate block chain \(L + Y\). Both chains have
the same length.
A miner with chain \(L + X\) does not know at this point that another
minder has chain \(L + Y\). So, miners will extend both block chains.
(Note that the algorithm will not permit chains \(L + X + Y\) or \(L +
Y + X\).)We've seen this situation before:  look atblock collisionsdescribed
earlier in this module. Both block chains will be extended, but
eventually, with very high probability, one of the blocks \(X\) or
\(Y\) will drop out of chain.How should sellers protect themselves?What is the equivalent of the buyer's check clearing in the bank?A seller should give the item to the buyer only after the transaction
appears with high confidence in a block in the chain --- see Figure 1.
The seller listens to block chains broadcast by miners. If the seller
gets a block chain \(L + X + L'\) where \(L'\) is itself a long block chain
then the seller has high confidence that the transaction is in the
permanent record.The seller waits to get block chains in which its transaction appears
in a block which is then followed by \(m\) blocks, for large
\(m\). The larger the value of \(m\), the greater the seller's
confidence, but the longer the buyer has to wait to get paid.
length of the extension \(L'\).A value of \(m = 6\) gives adequate confidence in most cases.Fraudulent minersA miner gets paid for every block the miner creates; so, why shouldn't
the miner create fraudulent blocks and get paid for them?The answer is the same as that for the double-spending attack. A miner
may have 
created a block and appended it to the chain; however, a suspicious
agent (and we hope that all agents are suspicious!) will not accept
the block until many blocks have been appended to the chain after
it. Other miners won't append their blocks to an invalid one. The
invalid blocks will become permanent orphans, and so the fraudulent
miner won't be able to spend the coins in these blocks.Fifty One Percent AttacksA 51% attack can be carried out by an agent that has more mining
power (e.g. 51% or more) than all other agents combined. The higher
the proportion of mining power of a single agent, the greater the
chances that attacks by that agent will succeed. You can see the
danger of a single agent having predominant mining power by thinking
about an agent with say, 99% of the total mining power. This agent
can mine so much faster than others that it can manipulate the block
chain in many ways. It can create double-spend transactions and deny
services to some transactions.A group of miners can collude to gain predominant mining power. Also,
miners canrentCloud-based systems for mining --- as opposed to having toownhuge data centers. An attacker can rent a large system for the specific purpose and
duration of an attack. The public may not know if and when such
attacks are successful becausecryptocurrencies do not have an
incentive to publish such attacks.Denial of serviceCan agents collude so that an agent \(X\)'s transactions never get
into blocks and so never get processed?An agent's identity does not appear in a transaction, only a public
key does. An agent can create new public keys at will. So let's ask
another question: can agents collude so that transactions with a
specific public key do not get into blocks?Only an agent that can solve puzzles in reasonable time can make
blocks. These agents have significant computational power. One can
concoct a situation where many agents with significant computation
power collude to avoid transactions from a specific public key. This
could have the effect of slowing processing of certain
transactions. However, such a situation isn't likely to arise because
agents have an incentive to create blocks and so they compete ---
rather than collude --- with each other.
Colluding
agents, with massive computing power, may help to deny or slow service
to a public key but not to another agent because agents can create
public keys at will.Further ReadingThere are many issues that we have not covered. This material only
covers the basics from the point of view of distributed algorithms.ReviewWhat is ablock chain?How does the algorithm deal with the situation in which one agent
receives a message to extend its copy of the block chain with a block
\(X\) while another agent receives a message to extend its copy of the
  block chain with a different block \(Y\)?Could there be an infinite sequence of collisions? Is that likely?What is mining for cryptocurrency? Suppose only a small number (say
  two organizations) mined most (say 99%) of Bitcoins. Would that be a
  problem?Why isdouble spendingunlikely to be successful?K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology



Figure Link: https://kmchandy.github.io/Crypto/Bitcoin/Slide1.jpg

Caption: Fig.1: More Confidence in Older Blocks in the
    Chain


Let's look at a scenario.
For this scenario, \(X\) and \(Y\) are single blocks.
Assume that agent \(A\)'s copy of the block chain
is \(L\) when \(A\) receives a message containing the block chain \(L
+ X\). Because the length of the block chain \(L + X\) is bigger than \(A\)'s
copy, \(A\) sets its copy to \(L + X\).Now suppose agent \(A\) gets a
message containing the block chain \(L + Y\); what does \(A\) do?
Agent \(A\) ignores the message because the length of \(L + Y\) does not exceed
that of \(A\)'s current copy, \(L + X\).Continuing CollisionsLet's continue the above scenario.
Can block \(Y\) become part of \(A\)'s
chain, or will it remain forever an "orphan" block
as far as \(A\) is concerned?Here's a possible scenario.
An agent with a block
chain copy \(L + 
X\), solves its puzzle and appends a block \(X_{1}\) to get a new copy
\(L + X + X_{1}\) of the block chain which the agent broadcasts.
At the same time, another agent with a block
chain copy \(L + 
Y\), solves its puzzle and appends a block \(Y_{1}\) to get a new copy
\(L + Y + Y_{1}\) which is broadcast.
If \(A\) receives \(L + Y + Y_{1}\) before receiving \(L + X + X_{1}\)
then \(A\) will set its chain to \(L + Y + Y_{1}\), and then reject
\(L + X + X_{1}\).You can construct a scenario with a sequence of collisions between
agents appending blocks \(X_{i}\) and other agents appending blocks
\(Y_{j}\) so that \(A\)'s chain switches back and forth between \(X\)
and \(Y\) values.Long sequences of collisions are unlikely, and the longer the sequence
the less the probability of continuing collisions.So how can an agent determine whether a block is in the chain? And so
how can an agent find out if a transaction has been executed? You
sell your used bicycle to somebody for coins, but how do you know if
that transaction becomes part oftheblock chain when different
agents have different copies? And so how do you know that you can
spend those coins that
you should have received for your bicycle?Confidence that a Block is in the ChainIn this section we discuss the behavior of non-devious agents; we will
show how the algorithm handles devious agents later.
Suppose an agent's copy of the block chain is \(X + X_{1} + X_{2}
+ \ldots + X_{K}\), and another agent's copy is \(Y + ? + ? +
\ldots\), where \(?\) represents arbitrary values. What is the
likelihood that \(Y \neq X\)?\(Y\) can be different from \(X\) only if there are a sequence of
\(K\) or more collisions. And the likelihood of a sequence of
collisions decreases with \(K\). Likewise, the likelihood that an
agent never receives a block chain containing \(X\) decreases with
time, and so decreases with \(K\). So, if \(K\) is large then with
very high probability, \(X\) is part of every agent's block chain.Now suppose an agent's copy of the block chain is \(L + X + X_{1} + X_{2}
+ \ldots + X_{K}\). What is the likelihood
that another agent's copy is \(L + Y + ? + \ldots\) where \(L\) is an
arbitrary sequence? By the same
argument, when if \(K\) is large then with
very high probability, \(X\) is part of every block chain.For practical purposes,
many agents assume that if \(K > 6\) then \(L + X\) is a prefix of
most agents' block chains.See the Princeton Bitcoin book.Suppose agent \(A\) gets \(N\) Bitcoins in transaction \(X\). If \(K =
0\) then an agent can't be confident that \(A\) ever received these coins
because this transaction may not persist in the block chain. As \(K\)
increases agents become more confident that the transaction is in the
block chain and that \(A\) did, indeed, receive these coins.Fig.1: More Confidence in Older Blocks in the
    ChainWhat happens to Orphan Transactions?An agent may append a block \(Y\) to its chain, and this block may be
dropped from the chains of all agents and never reappear after some
point. We saw a scenario in which this happens.
Such a block is an "orphan," because no agent has a record of that
transaction after some time.
If an orphan block contains the transaction in which you
sold your bicycle in exchange for coins, then will you ever be able to
spend your coins? Yes, you will get your coins as we see next.Agents aggregate transactions that have not
appeared in the agent's block chain into blocks and propose to append these
blocks to the chain. A transaction that does not appear in block
chains will be agrregated eventually by some agent and inserted into
a block in the chian. Though the orphan block disappears the
transactions in the block do not.IncentivesNext, looks look at challenge number 3. Why should an agent create
blocks of validated transactions?Because the agent gets paid! Payment
is from either ablock rewardor transaction fees.Block rewardsAn agent that creates a block gets a specified number of Bitcoins for
itself as a reward called ablock reward.
The Bitcoins in a block reward are created by making the
block; these Bitcoins don't exist until the block is created. The process of making
blocks and acquiring block rewards is called "mining."Mining is the only way of creating new Bitcoins.When Bitcoin started the reward for creating a block was 50 Bitcoins.
The reward
halves after the creation of a certain number (210,000) of blocks. The
reward was reduced to 25 in 2013 and to 12.5 in 2018.
Block rewards will vanish at some point in the future.
The
total number of Bitcoins that can ever be created has an upper bound:
about 21 million.(Bitcoins can be lost. An agent may lose the hash pointer to the
transaction that gave the agent ownership of the coin, or an agent may
lose its private key.)Even when block rewards vanish, miners will continue to
mine provided that they get paid transaction fees. A
transaction fee is a payment by payers and payees to
miners. Transaction fees are voluntary. A high-fee for a
transaction is an incentive to miners to put this transaction into a
block quickly. So, it's possible that agents that offer no fee or low
fees may have to wait longer for their transactions to enter the
block chain.Incentives are critical for Bitcoin. Miners get paid to
get their blocks into the long-term "consensus" block chain.
Miners have an incentive to police the block chain because 
they don't get paid for appending erroneous blocks. If a miner appends
an erroneous block to the chain then other
miners won't extend chains containing the erroneous block, and so the
erroneous block will become an orphan.Any agent can check whether its copy of the block chain is
valid; however, agents making ordinary transactions don't
need to do so because there are many miners each of whom has an
incentive to ensure that the block chain is legitimate.AttacksNext, looks look at challenge number 4.Stealing coinsCan an agent steal a coin from an agent \(X\) by appending a block to
the chain where the block contains a transaction in which \(X\) gives
coins to \(Y\)?No, this can't happen thanks to cryptography. A transaction into which
\(X\) puts coins is valid only if \(X\) signs the transaction.  \(Y\)
cannot forge \(X\)'s signature, and so \(Y\) cannot create blocks that
contain such fraudulent transactions.Double spendCan an agent spend the same coin twice? Can an agent buy something
without paying for it?Consider the following transaction using conventional checks issued by
banks. A buyer gives a seller a check for $\(100,000\) for a house.
The house is put in escrow.  When the check clears and the seller
receives the payment the buyer gets possession of the house. The legal
process that includes notaries, real estate agents, and banks, helps
ensure that the transaction concludes correctly or is aborted correctly.Next, let's look at a transaction in which a person buys a video
online by paying the seller Bitcoins. The buyer and seller know each
other by their public keys and by their online addresses. 
The buyer broadcasts the transaction in which the buyer gives the seller
the payment in Bitcoins.
The amount is specified as
a pair (transaction id, array index) described in the sectionpay
transactionsinthe module introducing crypto currencies.A miner puts the transaction into a block \(X\);
appends the block to its copy \(L\) of the block chain; and broadcasts the extended
block chain \(L + X\).
When the seller gets a copy of the block chain \(L + X\), the
seller concludes that it has received the payment from the
buyer because the 
transaction has been recorded in a block chain. So the seller gives
the video to the buyer.The buyer cheats. The buyer creates a transaction in which the buyer
transfers the same Bitcoins  to the buyer itself.
A miner creates a block \(Y\) that includes this transaction.
A miner who has only received block
chain \(L\) (and hasn't yet received chain \(L + X\)) appends \(Y\) to
\(L\) to get a chain \(L + Y\), and broacasts \(L + Y\).Now we have a situation in which one
miner broadcasts a legitimate block chain \(L + X\) and a different
miner broadcasts a legitimate block chain \(L + Y\). Both chains have
the same length.
A miner with chain \(L + X\) does not know at this point that another
minder has chain \(L + Y\). So, miners will extend both block chains.
(Note that the algorithm will not permit chains \(L + X + Y\) or \(L +
Y + X\).)We've seen this situation before:  look atblock collisionsdescribed
earlier in this module. Both block chains will be extended, but
eventually, with very high probability, one of the blocks \(X\) or
\(Y\) will drop out of chain.How should sellers protect themselves?What is the equivalent of the buyer's check clearing in the bank?A seller should give the item to the buyer only after the transaction
appears with high confidence in a block in the chain --- see Figure 1.
The seller listens to block chains broadcast by miners. If the seller
gets a block chain \(L + X + L'\) where \(L'\) is itself a long block chain
then the seller has high confidence that the transaction is in the
permanent record.The seller waits to get block chains in which its transaction appears
in a block which is then followed by \(m\) blocks, for large
\(m\). The larger the value of \(m\), the greater the seller's
confidence, but the longer the buyer has to wait to get paid.
length of the extension \(L'\).A value of \(m = 6\) gives adequate confidence in most cases.Fraudulent minersA miner gets paid for every block the miner creates; so, why shouldn't
the miner create fraudulent blocks and get paid for them?The answer is the same as that for the double-spending attack. A miner
may have 
created a block and appended it to the chain; however, a suspicious
agent (and we hope that all agents are suspicious!) will not accept
the block until many blocks have been appended to the chain after
it. Other miners won't append their blocks to an invalid one. The
invalid blocks will become permanent orphans, and so the fraudulent
miner won't be able to spend the coins in these blocks.Fifty One Percent AttacksA 51% attack can be carried out by an agent that has more mining
power (e.g. 51% or more) than all other agents combined. The higher
the proportion of mining power of a single agent, the greater the
chances that attacks by that agent will succeed. You can see the
danger of a single agent having predominant mining power by thinking
about an agent with say, 99% of the total mining power. This agent
can mine so much faster than others that it can manipulate the block
chain in many ways. It can create double-spend transactions and deny
services to some transactions.A group of miners can collude to gain predominant mining power. Also,
miners canrentCloud-based systems for mining --- as opposed to having toownhuge data centers. An attacker can rent a large system for the specific purpose and
duration of an attack. The public may not know if and when such
attacks are successful becausecryptocurrencies do not have an
incentive to publish such attacks.Denial of serviceCan agents collude so that an agent \(X\)'s transactions never get
into blocks and so never get processed?An agent's identity does not appear in a transaction, only a public
key does. An agent can create new public keys at will. So let's ask
another question: can agents collude so that transactions with a
specific public key do not get into blocks?Only an agent that can solve puzzles in reasonable time can make
blocks. These agents have significant computational power. One can
concoct a situation where many agents with significant computation
power collude to avoid transactions from a specific public key. This
could have the effect of slowing processing of certain
transactions. However, such a situation isn't likely to arise because
agents have an incentive to create blocks and so they compete ---
rather than collude --- with each other.
Colluding
agents, with massive computing power, may help to deny or slow service
to a public key but not to another agent because agents can create
public keys at will.Further ReadingThere are many issues that we have not covered. This material only
covers the basics from the point of view of distributed algorithms.ReviewWhat is ablock chain?How does the algorithm deal with the situation in which one agent
receives a message to extend its copy of the block chain with a block
\(X\) while another agent receives a message to extend its copy of the
  block chain with a different block \(Y\)?Could there be an infinite sequence of collisions? Is that likely?What is mining for cryptocurrency? Suppose only a small number (say
  two organizations) mined most (say 99%) of Bitcoins. Would that be a
  problem?Why isdouble spendingunlikely to be successful?K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology



Figure Link: https://kmchandy.github.io/Crypto/Bitcoin/Slide1.jpg

Caption: Fig.1: More Confidence in Older Blocks in the
    Chain


Now suppose agent \(A\) gets a
message containing the block chain \(L + Y\); what does \(A\) do?
Agent \(A\) ignores the message because the length of \(L + Y\) does not exceed
that of \(A\)'s current copy, \(L + X\).Continuing CollisionsLet's continue the above scenario.
Can block \(Y\) become part of \(A\)'s
chain, or will it remain forever an "orphan" block
as far as \(A\) is concerned?Here's a possible scenario.
An agent with a block
chain copy \(L + 
X\), solves its puzzle and appends a block \(X_{1}\) to get a new copy
\(L + X + X_{1}\) of the block chain which the agent broadcasts.
At the same time, another agent with a block
chain copy \(L + 
Y\), solves its puzzle and appends a block \(Y_{1}\) to get a new copy
\(L + Y + Y_{1}\) which is broadcast.
If \(A\) receives \(L + Y + Y_{1}\) before receiving \(L + X + X_{1}\)
then \(A\) will set its chain to \(L + Y + Y_{1}\), and then reject
\(L + X + X_{1}\).You can construct a scenario with a sequence of collisions between
agents appending blocks \(X_{i}\) and other agents appending blocks
\(Y_{j}\) so that \(A\)'s chain switches back and forth between \(X\)
and \(Y\) values.Long sequences of collisions are unlikely, and the longer the sequence
the less the probability of continuing collisions.So how can an agent determine whether a block is in the chain? And so
how can an agent find out if a transaction has been executed? You
sell your used bicycle to somebody for coins, but how do you know if
that transaction becomes part oftheblock chain when different
agents have different copies? And so how do you know that you can
spend those coins that
you should have received for your bicycle?Confidence that a Block is in the ChainIn this section we discuss the behavior of non-devious agents; we will
show how the algorithm handles devious agents later.
Suppose an agent's copy of the block chain is \(X + X_{1} + X_{2}
+ \ldots + X_{K}\), and another agent's copy is \(Y + ? + ? +
\ldots\), where \(?\) represents arbitrary values. What is the
likelihood that \(Y \neq X\)?\(Y\) can be different from \(X\) only if there are a sequence of
\(K\) or more collisions. And the likelihood of a sequence of
collisions decreases with \(K\). Likewise, the likelihood that an
agent never receives a block chain containing \(X\) decreases with
time, and so decreases with \(K\). So, if \(K\) is large then with
very high probability, \(X\) is part of every agent's block chain.Now suppose an agent's copy of the block chain is \(L + X + X_{1} + X_{2}
+ \ldots + X_{K}\). What is the likelihood
that another agent's copy is \(L + Y + ? + \ldots\) where \(L\) is an
arbitrary sequence? By the same
argument, when if \(K\) is large then with
very high probability, \(X\) is part of every block chain.For practical purposes,
many agents assume that if \(K > 6\) then \(L + X\) is a prefix of
most agents' block chains.See the Princeton Bitcoin book.Suppose agent \(A\) gets \(N\) Bitcoins in transaction \(X\). If \(K =
0\) then an agent can't be confident that \(A\) ever received these coins
because this transaction may not persist in the block chain. As \(K\)
increases agents become more confident that the transaction is in the
block chain and that \(A\) did, indeed, receive these coins.Fig.1: More Confidence in Older Blocks in the
    ChainWhat happens to Orphan Transactions?An agent may append a block \(Y\) to its chain, and this block may be
dropped from the chains of all agents and never reappear after some
point. We saw a scenario in which this happens.
Such a block is an "orphan," because no agent has a record of that
transaction after some time.
If an orphan block contains the transaction in which you
sold your bicycle in exchange for coins, then will you ever be able to
spend your coins? Yes, you will get your coins as we see next.Agents aggregate transactions that have not
appeared in the agent's block chain into blocks and propose to append these
blocks to the chain. A transaction that does not appear in block
chains will be agrregated eventually by some agent and inserted into
a block in the chian. Though the orphan block disappears the
transactions in the block do not.IncentivesNext, looks look at challenge number 3. Why should an agent create
blocks of validated transactions?Because the agent gets paid! Payment
is from either ablock rewardor transaction fees.Block rewardsAn agent that creates a block gets a specified number of Bitcoins for
itself as a reward called ablock reward.
The Bitcoins in a block reward are created by making the
block; these Bitcoins don't exist until the block is created. The process of making
blocks and acquiring block rewards is called "mining."Mining is the only way of creating new Bitcoins.When Bitcoin started the reward for creating a block was 50 Bitcoins.
The reward
halves after the creation of a certain number (210,000) of blocks. The
reward was reduced to 25 in 2013 and to 12.5 in 2018.
Block rewards will vanish at some point in the future.
The
total number of Bitcoins that can ever be created has an upper bound:
about 21 million.(Bitcoins can be lost. An agent may lose the hash pointer to the
transaction that gave the agent ownership of the coin, or an agent may
lose its private key.)Even when block rewards vanish, miners will continue to
mine provided that they get paid transaction fees. A
transaction fee is a payment by payers and payees to
miners. Transaction fees are voluntary. A high-fee for a
transaction is an incentive to miners to put this transaction into a
block quickly. So, it's possible that agents that offer no fee or low
fees may have to wait longer for their transactions to enter the
block chain.Incentives are critical for Bitcoin. Miners get paid to
get their blocks into the long-term "consensus" block chain.
Miners have an incentive to police the block chain because 
they don't get paid for appending erroneous blocks. If a miner appends
an erroneous block to the chain then other
miners won't extend chains containing the erroneous block, and so the
erroneous block will become an orphan.Any agent can check whether its copy of the block chain is
valid; however, agents making ordinary transactions don't
need to do so because there are many miners each of whom has an
incentive to ensure that the block chain is legitimate.AttacksNext, looks look at challenge number 4.Stealing coinsCan an agent steal a coin from an agent \(X\) by appending a block to
the chain where the block contains a transaction in which \(X\) gives
coins to \(Y\)?No, this can't happen thanks to cryptography. A transaction into which
\(X\) puts coins is valid only if \(X\) signs the transaction.  \(Y\)
cannot forge \(X\)'s signature, and so \(Y\) cannot create blocks that
contain such fraudulent transactions.Double spendCan an agent spend the same coin twice? Can an agent buy something
without paying for it?Consider the following transaction using conventional checks issued by
banks. A buyer gives a seller a check for $\(100,000\) for a house.
The house is put in escrow.  When the check clears and the seller
receives the payment the buyer gets possession of the house. The legal
process that includes notaries, real estate agents, and banks, helps
ensure that the transaction concludes correctly or is aborted correctly.Next, let's look at a transaction in which a person buys a video
online by paying the seller Bitcoins. The buyer and seller know each
other by their public keys and by their online addresses. 
The buyer broadcasts the transaction in which the buyer gives the seller
the payment in Bitcoins.
The amount is specified as
a pair (transaction id, array index) described in the sectionpay
transactionsinthe module introducing crypto currencies.A miner puts the transaction into a block \(X\);
appends the block to its copy \(L\) of the block chain; and broadcasts the extended
block chain \(L + X\).
When the seller gets a copy of the block chain \(L + X\), the
seller concludes that it has received the payment from the
buyer because the 
transaction has been recorded in a block chain. So the seller gives
the video to the buyer.The buyer cheats. The buyer creates a transaction in which the buyer
transfers the same Bitcoins  to the buyer itself.
A miner creates a block \(Y\) that includes this transaction.
A miner who has only received block
chain \(L\) (and hasn't yet received chain \(L + X\)) appends \(Y\) to
\(L\) to get a chain \(L + Y\), and broacasts \(L + Y\).Now we have a situation in which one
miner broadcasts a legitimate block chain \(L + X\) and a different
miner broadcasts a legitimate block chain \(L + Y\). Both chains have
the same length.
A miner with chain \(L + X\) does not know at this point that another
minder has chain \(L + Y\). So, miners will extend both block chains.
(Note that the algorithm will not permit chains \(L + X + Y\) or \(L +
Y + X\).)We've seen this situation before:  look atblock collisionsdescribed
earlier in this module. Both block chains will be extended, but
eventually, with very high probability, one of the blocks \(X\) or
\(Y\) will drop out of chain.How should sellers protect themselves?What is the equivalent of the buyer's check clearing in the bank?A seller should give the item to the buyer only after the transaction
appears with high confidence in a block in the chain --- see Figure 1.
The seller listens to block chains broadcast by miners. If the seller
gets a block chain \(L + X + L'\) where \(L'\) is itself a long block chain
then the seller has high confidence that the transaction is in the
permanent record.The seller waits to get block chains in which its transaction appears
in a block which is then followed by \(m\) blocks, for large
\(m\). The larger the value of \(m\), the greater the seller's
confidence, but the longer the buyer has to wait to get paid.
length of the extension \(L'\).A value of \(m = 6\) gives adequate confidence in most cases.Fraudulent minersA miner gets paid for every block the miner creates; so, why shouldn't
the miner create fraudulent blocks and get paid for them?The answer is the same as that for the double-spending attack. A miner
may have 
created a block and appended it to the chain; however, a suspicious
agent (and we hope that all agents are suspicious!) will not accept
the block until many blocks have been appended to the chain after
it. Other miners won't append their blocks to an invalid one. The
invalid blocks will become permanent orphans, and so the fraudulent
miner won't be able to spend the coins in these blocks.Fifty One Percent AttacksA 51% attack can be carried out by an agent that has more mining
power (e.g. 51% or more) than all other agents combined. The higher
the proportion of mining power of a single agent, the greater the
chances that attacks by that agent will succeed. You can see the
danger of a single agent having predominant mining power by thinking
about an agent with say, 99% of the total mining power. This agent
can mine so much faster than others that it can manipulate the block
chain in many ways. It can create double-spend transactions and deny
services to some transactions.A group of miners can collude to gain predominant mining power. Also,
miners canrentCloud-based systems for mining --- as opposed to having toownhuge data centers. An attacker can rent a large system for the specific purpose and
duration of an attack. The public may not know if and when such
attacks are successful becausecryptocurrencies do not have an
incentive to publish such attacks.Denial of serviceCan agents collude so that an agent \(X\)'s transactions never get
into blocks and so never get processed?An agent's identity does not appear in a transaction, only a public
key does. An agent can create new public keys at will. So let's ask
another question: can agents collude so that transactions with a
specific public key do not get into blocks?Only an agent that can solve puzzles in reasonable time can make
blocks. These agents have significant computational power. One can
concoct a situation where many agents with significant computation
power collude to avoid transactions from a specific public key. This
could have the effect of slowing processing of certain
transactions. However, such a situation isn't likely to arise because
agents have an incentive to create blocks and so they compete ---
rather than collude --- with each other.
Colluding
agents, with massive computing power, may help to deny or slow service
to a public key but not to another agent because agents can create
public keys at will.Further ReadingThere are many issues that we have not covered. This material only
covers the basics from the point of view of distributed algorithms.ReviewWhat is ablock chain?How does the algorithm deal with the situation in which one agent
receives a message to extend its copy of the block chain with a block
\(X\) while another agent receives a message to extend its copy of the
  block chain with a different block \(Y\)?Could there be an infinite sequence of collisions? Is that likely?What is mining for cryptocurrency? Suppose only a small number (say
  two organizations) mined most (say 99%) of Bitcoins. Would that be a
  problem?Why isdouble spendingunlikely to be successful?K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology



Figure Link: https://kmchandy.github.io/Crypto/Bitcoin/Slide1.jpg

Caption: Fig.1: More Confidence in Older Blocks in the
    Chain


Here's a possible scenario.
An agent with a block
chain copy \(L + 
X\), solves its puzzle and appends a block \(X_{1}\) to get a new copy
\(L + X + X_{1}\) of the block chain which the agent broadcasts.
At the same time, another agent with a block
chain copy \(L + 
Y\), solves its puzzle and appends a block \(Y_{1}\) to get a new copy
\(L + Y + Y_{1}\) which is broadcast.
If \(A\) receives \(L + Y + Y_{1}\) before receiving \(L + X + X_{1}\)
then \(A\) will set its chain to \(L + Y + Y_{1}\), and then reject
\(L + X + X_{1}\).You can construct a scenario with a sequence of collisions between
agents appending blocks \(X_{i}\) and other agents appending blocks
\(Y_{j}\) so that \(A\)'s chain switches back and forth between \(X\)
and \(Y\) values.Long sequences of collisions are unlikely, and the longer the sequence
the less the probability of continuing collisions.So how can an agent determine whether a block is in the chain? And so
how can an agent find out if a transaction has been executed? You
sell your used bicycle to somebody for coins, but how do you know if
that transaction becomes part oftheblock chain when different
agents have different copies? And so how do you know that you can
spend those coins that
you should have received for your bicycle?Confidence that a Block is in the ChainIn this section we discuss the behavior of non-devious agents; we will
show how the algorithm handles devious agents later.
Suppose an agent's copy of the block chain is \(X + X_{1} + X_{2}
+ \ldots + X_{K}\), and another agent's copy is \(Y + ? + ? +
\ldots\), where \(?\) represents arbitrary values. What is the
likelihood that \(Y \neq X\)?\(Y\) can be different from \(X\) only if there are a sequence of
\(K\) or more collisions. And the likelihood of a sequence of
collisions decreases with \(K\). Likewise, the likelihood that an
agent never receives a block chain containing \(X\) decreases with
time, and so decreases with \(K\). So, if \(K\) is large then with
very high probability, \(X\) is part of every agent's block chain.Now suppose an agent's copy of the block chain is \(L + X + X_{1} + X_{2}
+ \ldots + X_{K}\). What is the likelihood
that another agent's copy is \(L + Y + ? + \ldots\) where \(L\) is an
arbitrary sequence? By the same
argument, when if \(K\) is large then with
very high probability, \(X\) is part of every block chain.For practical purposes,
many agents assume that if \(K > 6\) then \(L + X\) is a prefix of
most agents' block chains.See the Princeton Bitcoin book.Suppose agent \(A\) gets \(N\) Bitcoins in transaction \(X\). If \(K =
0\) then an agent can't be confident that \(A\) ever received these coins
because this transaction may not persist in the block chain. As \(K\)
increases agents become more confident that the transaction is in the
block chain and that \(A\) did, indeed, receive these coins.Fig.1: More Confidence in Older Blocks in the
    ChainWhat happens to Orphan Transactions?An agent may append a block \(Y\) to its chain, and this block may be
dropped from the chains of all agents and never reappear after some
point. We saw a scenario in which this happens.
Such a block is an "orphan," because no agent has a record of that
transaction after some time.
If an orphan block contains the transaction in which you
sold your bicycle in exchange for coins, then will you ever be able to
spend your coins? Yes, you will get your coins as we see next.Agents aggregate transactions that have not
appeared in the agent's block chain into blocks and propose to append these
blocks to the chain. A transaction that does not appear in block
chains will be agrregated eventually by some agent and inserted into
a block in the chian. Though the orphan block disappears the
transactions in the block do not.IncentivesNext, looks look at challenge number 3. Why should an agent create
blocks of validated transactions?Because the agent gets paid! Payment
is from either ablock rewardor transaction fees.Block rewardsAn agent that creates a block gets a specified number of Bitcoins for
itself as a reward called ablock reward.
The Bitcoins in a block reward are created by making the
block; these Bitcoins don't exist until the block is created. The process of making
blocks and acquiring block rewards is called "mining."Mining is the only way of creating new Bitcoins.When Bitcoin started the reward for creating a block was 50 Bitcoins.
The reward
halves after the creation of a certain number (210,000) of blocks. The
reward was reduced to 25 in 2013 and to 12.5 in 2018.
Block rewards will vanish at some point in the future.
The
total number of Bitcoins that can ever be created has an upper bound:
about 21 million.(Bitcoins can be lost. An agent may lose the hash pointer to the
transaction that gave the agent ownership of the coin, or an agent may
lose its private key.)Even when block rewards vanish, miners will continue to
mine provided that they get paid transaction fees. A
transaction fee is a payment by payers and payees to
miners. Transaction fees are voluntary. A high-fee for a
transaction is an incentive to miners to put this transaction into a
block quickly. So, it's possible that agents that offer no fee or low
fees may have to wait longer for their transactions to enter the
block chain.Incentives are critical for Bitcoin. Miners get paid to
get their blocks into the long-term "consensus" block chain.
Miners have an incentive to police the block chain because 
they don't get paid for appending erroneous blocks. If a miner appends
an erroneous block to the chain then other
miners won't extend chains containing the erroneous block, and so the
erroneous block will become an orphan.Any agent can check whether its copy of the block chain is
valid; however, agents making ordinary transactions don't
need to do so because there are many miners each of whom has an
incentive to ensure that the block chain is legitimate.AttacksNext, looks look at challenge number 4.Stealing coinsCan an agent steal a coin from an agent \(X\) by appending a block to
the chain where the block contains a transaction in which \(X\) gives
coins to \(Y\)?No, this can't happen thanks to cryptography. A transaction into which
\(X\) puts coins is valid only if \(X\) signs the transaction.  \(Y\)
cannot forge \(X\)'s signature, and so \(Y\) cannot create blocks that
contain such fraudulent transactions.Double spendCan an agent spend the same coin twice? Can an agent buy something
without paying for it?Consider the following transaction using conventional checks issued by
banks. A buyer gives a seller a check for $\(100,000\) for a house.
The house is put in escrow.  When the check clears and the seller
receives the payment the buyer gets possession of the house. The legal
process that includes notaries, real estate agents, and banks, helps
ensure that the transaction concludes correctly or is aborted correctly.Next, let's look at a transaction in which a person buys a video
online by paying the seller Bitcoins. The buyer and seller know each
other by their public keys and by their online addresses. 
The buyer broadcasts the transaction in which the buyer gives the seller
the payment in Bitcoins.
The amount is specified as
a pair (transaction id, array index) described in the sectionpay
transactionsinthe module introducing crypto currencies.A miner puts the transaction into a block \(X\);
appends the block to its copy \(L\) of the block chain; and broadcasts the extended
block chain \(L + X\).
When the seller gets a copy of the block chain \(L + X\), the
seller concludes that it has received the payment from the
buyer because the 
transaction has been recorded in a block chain. So the seller gives
the video to the buyer.The buyer cheats. The buyer creates a transaction in which the buyer
transfers the same Bitcoins  to the buyer itself.
A miner creates a block \(Y\) that includes this transaction.
A miner who has only received block
chain \(L\) (and hasn't yet received chain \(L + X\)) appends \(Y\) to
\(L\) to get a chain \(L + Y\), and broacasts \(L + Y\).Now we have a situation in which one
miner broadcasts a legitimate block chain \(L + X\) and a different
miner broadcasts a legitimate block chain \(L + Y\). Both chains have
the same length.
A miner with chain \(L + X\) does not know at this point that another
minder has chain \(L + Y\). So, miners will extend both block chains.
(Note that the algorithm will not permit chains \(L + X + Y\) or \(L +
Y + X\).)We've seen this situation before:  look atblock collisionsdescribed
earlier in this module. Both block chains will be extended, but
eventually, with very high probability, one of the blocks \(X\) or
\(Y\) will drop out of chain.How should sellers protect themselves?What is the equivalent of the buyer's check clearing in the bank?A seller should give the item to the buyer only after the transaction
appears with high confidence in a block in the chain --- see Figure 1.
The seller listens to block chains broadcast by miners. If the seller
gets a block chain \(L + X + L'\) where \(L'\) is itself a long block chain
then the seller has high confidence that the transaction is in the
permanent record.The seller waits to get block chains in which its transaction appears
in a block which is then followed by \(m\) blocks, for large
\(m\). The larger the value of \(m\), the greater the seller's
confidence, but the longer the buyer has to wait to get paid.
length of the extension \(L'\).A value of \(m = 6\) gives adequate confidence in most cases.Fraudulent minersA miner gets paid for every block the miner creates; so, why shouldn't
the miner create fraudulent blocks and get paid for them?The answer is the same as that for the double-spending attack. A miner
may have 
created a block and appended it to the chain; however, a suspicious
agent (and we hope that all agents are suspicious!) will not accept
the block until many blocks have been appended to the chain after
it. Other miners won't append their blocks to an invalid one. The
invalid blocks will become permanent orphans, and so the fraudulent
miner won't be able to spend the coins in these blocks.Fifty One Percent AttacksA 51% attack can be carried out by an agent that has more mining
power (e.g. 51% or more) than all other agents combined. The higher
the proportion of mining power of a single agent, the greater the
chances that attacks by that agent will succeed. You can see the
danger of a single agent having predominant mining power by thinking
about an agent with say, 99% of the total mining power. This agent
can mine so much faster than others that it can manipulate the block
chain in many ways. It can create double-spend transactions and deny
services to some transactions.A group of miners can collude to gain predominant mining power. Also,
miners canrentCloud-based systems for mining --- as opposed to having toownhuge data centers. An attacker can rent a large system for the specific purpose and
duration of an attack. The public may not know if and when such
attacks are successful becausecryptocurrencies do not have an
incentive to publish such attacks.Denial of serviceCan agents collude so that an agent \(X\)'s transactions never get
into blocks and so never get processed?An agent's identity does not appear in a transaction, only a public
key does. An agent can create new public keys at will. So let's ask
another question: can agents collude so that transactions with a
specific public key do not get into blocks?Only an agent that can solve puzzles in reasonable time can make
blocks. These agents have significant computational power. One can
concoct a situation where many agents with significant computation
power collude to avoid transactions from a specific public key. This
could have the effect of slowing processing of certain
transactions. However, such a situation isn't likely to arise because
agents have an incentive to create blocks and so they compete ---
rather than collude --- with each other.
Colluding
agents, with massive computing power, may help to deny or slow service
to a public key but not to another agent because agents can create
public keys at will.Further ReadingThere are many issues that we have not covered. This material only
covers the basics from the point of view of distributed algorithms.ReviewWhat is ablock chain?How does the algorithm deal with the situation in which one agent
receives a message to extend its copy of the block chain with a block
\(X\) while another agent receives a message to extend its copy of the
  block chain with a different block \(Y\)?Could there be an infinite sequence of collisions? Is that likely?What is mining for cryptocurrency? Suppose only a small number (say
  two organizations) mined most (say 99%) of Bitcoins. Would that be a
  problem?Why isdouble spendingunlikely to be successful?K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology



Figure Link: https://kmchandy.github.io/Crypto/Bitcoin/Slide1.jpg

Caption: Fig.1: More Confidence in Older Blocks in the
    Chain


You can construct a scenario with a sequence of collisions between
agents appending blocks \(X_{i}\) and other agents appending blocks
\(Y_{j}\) so that \(A\)'s chain switches back and forth between \(X\)
and \(Y\) values.Long sequences of collisions are unlikely, and the longer the sequence
the less the probability of continuing collisions.So how can an agent determine whether a block is in the chain? And so
how can an agent find out if a transaction has been executed? You
sell your used bicycle to somebody for coins, but how do you know if
that transaction becomes part oftheblock chain when different
agents have different copies? And so how do you know that you can
spend those coins that
you should have received for your bicycle?Confidence that a Block is in the ChainIn this section we discuss the behavior of non-devious agents; we will
show how the algorithm handles devious agents later.
Suppose an agent's copy of the block chain is \(X + X_{1} + X_{2}
+ \ldots + X_{K}\), and another agent's copy is \(Y + ? + ? +
\ldots\), where \(?\) represents arbitrary values. What is the
likelihood that \(Y \neq X\)?\(Y\) can be different from \(X\) only if there are a sequence of
\(K\) or more collisions. And the likelihood of a sequence of
collisions decreases with \(K\). Likewise, the likelihood that an
agent never receives a block chain containing \(X\) decreases with
time, and so decreases with \(K\). So, if \(K\) is large then with
very high probability, \(X\) is part of every agent's block chain.Now suppose an agent's copy of the block chain is \(L + X + X_{1} + X_{2}
+ \ldots + X_{K}\). What is the likelihood
that another agent's copy is \(L + Y + ? + \ldots\) where \(L\) is an
arbitrary sequence? By the same
argument, when if \(K\) is large then with
very high probability, \(X\) is part of every block chain.For practical purposes,
many agents assume that if \(K > 6\) then \(L + X\) is a prefix of
most agents' block chains.See the Princeton Bitcoin book.Suppose agent \(A\) gets \(N\) Bitcoins in transaction \(X\). If \(K =
0\) then an agent can't be confident that \(A\) ever received these coins
because this transaction may not persist in the block chain. As \(K\)
increases agents become more confident that the transaction is in the
block chain and that \(A\) did, indeed, receive these coins.Fig.1: More Confidence in Older Blocks in the
    ChainWhat happens to Orphan Transactions?An agent may append a block \(Y\) to its chain, and this block may be
dropped from the chains of all agents and never reappear after some
point. We saw a scenario in which this happens.
Such a block is an "orphan," because no agent has a record of that
transaction after some time.
If an orphan block contains the transaction in which you
sold your bicycle in exchange for coins, then will you ever be able to
spend your coins? Yes, you will get your coins as we see next.Agents aggregate transactions that have not
appeared in the agent's block chain into blocks and propose to append these
blocks to the chain. A transaction that does not appear in block
chains will be agrregated eventually by some agent and inserted into
a block in the chian. Though the orphan block disappears the
transactions in the block do not.IncentivesNext, looks look at challenge number 3. Why should an agent create
blocks of validated transactions?Because the agent gets paid! Payment
is from either ablock rewardor transaction fees.Block rewardsAn agent that creates a block gets a specified number of Bitcoins for
itself as a reward called ablock reward.
The Bitcoins in a block reward are created by making the
block; these Bitcoins don't exist until the block is created. The process of making
blocks and acquiring block rewards is called "mining."Mining is the only way of creating new Bitcoins.When Bitcoin started the reward for creating a block was 50 Bitcoins.
The reward
halves after the creation of a certain number (210,000) of blocks. The
reward was reduced to 25 in 2013 and to 12.5 in 2018.
Block rewards will vanish at some point in the future.
The
total number of Bitcoins that can ever be created has an upper bound:
about 21 million.(Bitcoins can be lost. An agent may lose the hash pointer to the
transaction that gave the agent ownership of the coin, or an agent may
lose its private key.)Even when block rewards vanish, miners will continue to
mine provided that they get paid transaction fees. A
transaction fee is a payment by payers and payees to
miners. Transaction fees are voluntary. A high-fee for a
transaction is an incentive to miners to put this transaction into a
block quickly. So, it's possible that agents that offer no fee or low
fees may have to wait longer for their transactions to enter the
block chain.Incentives are critical for Bitcoin. Miners get paid to
get their blocks into the long-term "consensus" block chain.
Miners have an incentive to police the block chain because 
they don't get paid for appending erroneous blocks. If a miner appends
an erroneous block to the chain then other
miners won't extend chains containing the erroneous block, and so the
erroneous block will become an orphan.Any agent can check whether its copy of the block chain is
valid; however, agents making ordinary transactions don't
need to do so because there are many miners each of whom has an
incentive to ensure that the block chain is legitimate.AttacksNext, looks look at challenge number 4.Stealing coinsCan an agent steal a coin from an agent \(X\) by appending a block to
the chain where the block contains a transaction in which \(X\) gives
coins to \(Y\)?No, this can't happen thanks to cryptography. A transaction into which
\(X\) puts coins is valid only if \(X\) signs the transaction.  \(Y\)
cannot forge \(X\)'s signature, and so \(Y\) cannot create blocks that
contain such fraudulent transactions.Double spendCan an agent spend the same coin twice? Can an agent buy something
without paying for it?Consider the following transaction using conventional checks issued by
banks. A buyer gives a seller a check for $\(100,000\) for a house.
The house is put in escrow.  When the check clears and the seller
receives the payment the buyer gets possession of the house. The legal
process that includes notaries, real estate agents, and banks, helps
ensure that the transaction concludes correctly or is aborted correctly.Next, let's look at a transaction in which a person buys a video
online by paying the seller Bitcoins. The buyer and seller know each
other by their public keys and by their online addresses. 
The buyer broadcasts the transaction in which the buyer gives the seller
the payment in Bitcoins.
The amount is specified as
a pair (transaction id, array index) described in the sectionpay
transactionsinthe module introducing crypto currencies.A miner puts the transaction into a block \(X\);
appends the block to its copy \(L\) of the block chain; and broadcasts the extended
block chain \(L + X\).
When the seller gets a copy of the block chain \(L + X\), the
seller concludes that it has received the payment from the
buyer because the 
transaction has been recorded in a block chain. So the seller gives
the video to the buyer.The buyer cheats. The buyer creates a transaction in which the buyer
transfers the same Bitcoins  to the buyer itself.
A miner creates a block \(Y\) that includes this transaction.
A miner who has only received block
chain \(L\) (and hasn't yet received chain \(L + X\)) appends \(Y\) to
\(L\) to get a chain \(L + Y\), and broacasts \(L + Y\).Now we have a situation in which one
miner broadcasts a legitimate block chain \(L + X\) and a different
miner broadcasts a legitimate block chain \(L + Y\). Both chains have
the same length.
A miner with chain \(L + X\) does not know at this point that another
minder has chain \(L + Y\). So, miners will extend both block chains.
(Note that the algorithm will not permit chains \(L + X + Y\) or \(L +
Y + X\).)We've seen this situation before:  look atblock collisionsdescribed
earlier in this module. Both block chains will be extended, but
eventually, with very high probability, one of the blocks \(X\) or
\(Y\) will drop out of chain.How should sellers protect themselves?What is the equivalent of the buyer's check clearing in the bank?A seller should give the item to the buyer only after the transaction
appears with high confidence in a block in the chain --- see Figure 1.
The seller listens to block chains broadcast by miners. If the seller
gets a block chain \(L + X + L'\) where \(L'\) is itself a long block chain
then the seller has high confidence that the transaction is in the
permanent record.The seller waits to get block chains in which its transaction appears
in a block which is then followed by \(m\) blocks, for large
\(m\). The larger the value of \(m\), the greater the seller's
confidence, but the longer the buyer has to wait to get paid.
length of the extension \(L'\).A value of \(m = 6\) gives adequate confidence in most cases.Fraudulent minersA miner gets paid for every block the miner creates; so, why shouldn't
the miner create fraudulent blocks and get paid for them?The answer is the same as that for the double-spending attack. A miner
may have 
created a block and appended it to the chain; however, a suspicious
agent (and we hope that all agents are suspicious!) will not accept
the block until many blocks have been appended to the chain after
it. Other miners won't append their blocks to an invalid one. The
invalid blocks will become permanent orphans, and so the fraudulent
miner won't be able to spend the coins in these blocks.Fifty One Percent AttacksA 51% attack can be carried out by an agent that has more mining
power (e.g. 51% or more) than all other agents combined. The higher
the proportion of mining power of a single agent, the greater the
chances that attacks by that agent will succeed. You can see the
danger of a single agent having predominant mining power by thinking
about an agent with say, 99% of the total mining power. This agent
can mine so much faster than others that it can manipulate the block
chain in many ways. It can create double-spend transactions and deny
services to some transactions.A group of miners can collude to gain predominant mining power. Also,
miners canrentCloud-based systems for mining --- as opposed to having toownhuge data centers. An attacker can rent a large system for the specific purpose and
duration of an attack. The public may not know if and when such
attacks are successful becausecryptocurrencies do not have an
incentive to publish such attacks.Denial of serviceCan agents collude so that an agent \(X\)'s transactions never get
into blocks and so never get processed?An agent's identity does not appear in a transaction, only a public
key does. An agent can create new public keys at will. So let's ask
another question: can agents collude so that transactions with a
specific public key do not get into blocks?Only an agent that can solve puzzles in reasonable time can make
blocks. These agents have significant computational power. One can
concoct a situation where many agents with significant computation
power collude to avoid transactions from a specific public key. This
could have the effect of slowing processing of certain
transactions. However, such a situation isn't likely to arise because
agents have an incentive to create blocks and so they compete ---
rather than collude --- with each other.
Colluding
agents, with massive computing power, may help to deny or slow service
to a public key but not to another agent because agents can create
public keys at will.Further ReadingThere are many issues that we have not covered. This material only
covers the basics from the point of view of distributed algorithms.ReviewWhat is ablock chain?How does the algorithm deal with the situation in which one agent
receives a message to extend its copy of the block chain with a block
\(X\) while another agent receives a message to extend its copy of the
  block chain with a different block \(Y\)?Could there be an infinite sequence of collisions? Is that likely?What is mining for cryptocurrency? Suppose only a small number (say
  two organizations) mined most (say 99%) of Bitcoins. Would that be a
  problem?Why isdouble spendingunlikely to be successful?K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology



Figure Link: https://kmchandy.github.io/Crypto/Bitcoin/Slide1.jpg

Caption: Fig.1: More Confidence in Older Blocks in the
    Chain


Long sequences of collisions are unlikely, and the longer the sequence
the less the probability of continuing collisions.So how can an agent determine whether a block is in the chain? And so
how can an agent find out if a transaction has been executed? You
sell your used bicycle to somebody for coins, but how do you know if
that transaction becomes part oftheblock chain when different
agents have different copies? And so how do you know that you can
spend those coins that
you should have received for your bicycle?Confidence that a Block is in the ChainIn this section we discuss the behavior of non-devious agents; we will
show how the algorithm handles devious agents later.
Suppose an agent's copy of the block chain is \(X + X_{1} + X_{2}
+ \ldots + X_{K}\), and another agent's copy is \(Y + ? + ? +
\ldots\), where \(?\) represents arbitrary values. What is the
likelihood that \(Y \neq X\)?\(Y\) can be different from \(X\) only if there are a sequence of
\(K\) or more collisions. And the likelihood of a sequence of
collisions decreases with \(K\). Likewise, the likelihood that an
agent never receives a block chain containing \(X\) decreases with
time, and so decreases with \(K\). So, if \(K\) is large then with
very high probability, \(X\) is part of every agent's block chain.Now suppose an agent's copy of the block chain is \(L + X + X_{1} + X_{2}
+ \ldots + X_{K}\). What is the likelihood
that another agent's copy is \(L + Y + ? + \ldots\) where \(L\) is an
arbitrary sequence? By the same
argument, when if \(K\) is large then with
very high probability, \(X\) is part of every block chain.For practical purposes,
many agents assume that if \(K > 6\) then \(L + X\) is a prefix of
most agents' block chains.See the Princeton Bitcoin book.Suppose agent \(A\) gets \(N\) Bitcoins in transaction \(X\). If \(K =
0\) then an agent can't be confident that \(A\) ever received these coins
because this transaction may not persist in the block chain. As \(K\)
increases agents become more confident that the transaction is in the
block chain and that \(A\) did, indeed, receive these coins.Fig.1: More Confidence in Older Blocks in the
    ChainWhat happens to Orphan Transactions?An agent may append a block \(Y\) to its chain, and this block may be
dropped from the chains of all agents and never reappear after some
point. We saw a scenario in which this happens.
Such a block is an "orphan," because no agent has a record of that
transaction after some time.
If an orphan block contains the transaction in which you
sold your bicycle in exchange for coins, then will you ever be able to
spend your coins? Yes, you will get your coins as we see next.Agents aggregate transactions that have not
appeared in the agent's block chain into blocks and propose to append these
blocks to the chain. A transaction that does not appear in block
chains will be agrregated eventually by some agent and inserted into
a block in the chian. Though the orphan block disappears the
transactions in the block do not.IncentivesNext, looks look at challenge number 3. Why should an agent create
blocks of validated transactions?Because the agent gets paid! Payment
is from either ablock rewardor transaction fees.Block rewardsAn agent that creates a block gets a specified number of Bitcoins for
itself as a reward called ablock reward.
The Bitcoins in a block reward are created by making the
block; these Bitcoins don't exist until the block is created. The process of making
blocks and acquiring block rewards is called "mining."Mining is the only way of creating new Bitcoins.When Bitcoin started the reward for creating a block was 50 Bitcoins.
The reward
halves after the creation of a certain number (210,000) of blocks. The
reward was reduced to 25 in 2013 and to 12.5 in 2018.
Block rewards will vanish at some point in the future.
The
total number of Bitcoins that can ever be created has an upper bound:
about 21 million.(Bitcoins can be lost. An agent may lose the hash pointer to the
transaction that gave the agent ownership of the coin, or an agent may
lose its private key.)Even when block rewards vanish, miners will continue to
mine provided that they get paid transaction fees. A
transaction fee is a payment by payers and payees to
miners. Transaction fees are voluntary. A high-fee for a
transaction is an incentive to miners to put this transaction into a
block quickly. So, it's possible that agents that offer no fee or low
fees may have to wait longer for their transactions to enter the
block chain.Incentives are critical for Bitcoin. Miners get paid to
get their blocks into the long-term "consensus" block chain.
Miners have an incentive to police the block chain because 
they don't get paid for appending erroneous blocks. If a miner appends
an erroneous block to the chain then other
miners won't extend chains containing the erroneous block, and so the
erroneous block will become an orphan.Any agent can check whether its copy of the block chain is
valid; however, agents making ordinary transactions don't
need to do so because there are many miners each of whom has an
incentive to ensure that the block chain is legitimate.AttacksNext, looks look at challenge number 4.Stealing coinsCan an agent steal a coin from an agent \(X\) by appending a block to
the chain where the block contains a transaction in which \(X\) gives
coins to \(Y\)?No, this can't happen thanks to cryptography. A transaction into which
\(X\) puts coins is valid only if \(X\) signs the transaction.  \(Y\)
cannot forge \(X\)'s signature, and so \(Y\) cannot create blocks that
contain such fraudulent transactions.Double spendCan an agent spend the same coin twice? Can an agent buy something
without paying for it?Consider the following transaction using conventional checks issued by
banks. A buyer gives a seller a check for $\(100,000\) for a house.
The house is put in escrow.  When the check clears and the seller
receives the payment the buyer gets possession of the house. The legal
process that includes notaries, real estate agents, and banks, helps
ensure that the transaction concludes correctly or is aborted correctly.Next, let's look at a transaction in which a person buys a video
online by paying the seller Bitcoins. The buyer and seller know each
other by their public keys and by their online addresses. 
The buyer broadcasts the transaction in which the buyer gives the seller
the payment in Bitcoins.
The amount is specified as
a pair (transaction id, array index) described in the sectionpay
transactionsinthe module introducing crypto currencies.A miner puts the transaction into a block \(X\);
appends the block to its copy \(L\) of the block chain; and broadcasts the extended
block chain \(L + X\).
When the seller gets a copy of the block chain \(L + X\), the
seller concludes that it has received the payment from the
buyer because the 
transaction has been recorded in a block chain. So the seller gives
the video to the buyer.The buyer cheats. The buyer creates a transaction in which the buyer
transfers the same Bitcoins  to the buyer itself.
A miner creates a block \(Y\) that includes this transaction.
A miner who has only received block
chain \(L\) (and hasn't yet received chain \(L + X\)) appends \(Y\) to
\(L\) to get a chain \(L + Y\), and broacasts \(L + Y\).Now we have a situation in which one
miner broadcasts a legitimate block chain \(L + X\) and a different
miner broadcasts a legitimate block chain \(L + Y\). Both chains have
the same length.
A miner with chain \(L + X\) does not know at this point that another
minder has chain \(L + Y\). So, miners will extend both block chains.
(Note that the algorithm will not permit chains \(L + X + Y\) or \(L +
Y + X\).)We've seen this situation before:  look atblock collisionsdescribed
earlier in this module. Both block chains will be extended, but
eventually, with very high probability, one of the blocks \(X\) or
\(Y\) will drop out of chain.How should sellers protect themselves?What is the equivalent of the buyer's check clearing in the bank?A seller should give the item to the buyer only after the transaction
appears with high confidence in a block in the chain --- see Figure 1.
The seller listens to block chains broadcast by miners. If the seller
gets a block chain \(L + X + L'\) where \(L'\) is itself a long block chain
then the seller has high confidence that the transaction is in the
permanent record.The seller waits to get block chains in which its transaction appears
in a block which is then followed by \(m\) blocks, for large
\(m\). The larger the value of \(m\), the greater the seller's
confidence, but the longer the buyer has to wait to get paid.
length of the extension \(L'\).A value of \(m = 6\) gives adequate confidence in most cases.Fraudulent minersA miner gets paid for every block the miner creates; so, why shouldn't
the miner create fraudulent blocks and get paid for them?The answer is the same as that for the double-spending attack. A miner
may have 
created a block and appended it to the chain; however, a suspicious
agent (and we hope that all agents are suspicious!) will not accept
the block until many blocks have been appended to the chain after
it. Other miners won't append their blocks to an invalid one. The
invalid blocks will become permanent orphans, and so the fraudulent
miner won't be able to spend the coins in these blocks.Fifty One Percent AttacksA 51% attack can be carried out by an agent that has more mining
power (e.g. 51% or more) than all other agents combined. The higher
the proportion of mining power of a single agent, the greater the
chances that attacks by that agent will succeed. You can see the
danger of a single agent having predominant mining power by thinking
about an agent with say, 99% of the total mining power. This agent
can mine so much faster than others that it can manipulate the block
chain in many ways. It can create double-spend transactions and deny
services to some transactions.A group of miners can collude to gain predominant mining power. Also,
miners canrentCloud-based systems for mining --- as opposed to having toownhuge data centers. An attacker can rent a large system for the specific purpose and
duration of an attack. The public may not know if and when such
attacks are successful becausecryptocurrencies do not have an
incentive to publish such attacks.Denial of serviceCan agents collude so that an agent \(X\)'s transactions never get
into blocks and so never get processed?An agent's identity does not appear in a transaction, only a public
key does. An agent can create new public keys at will. So let's ask
another question: can agents collude so that transactions with a
specific public key do not get into blocks?Only an agent that can solve puzzles in reasonable time can make
blocks. These agents have significant computational power. One can
concoct a situation where many agents with significant computation
power collude to avoid transactions from a specific public key. This
could have the effect of slowing processing of certain
transactions. However, such a situation isn't likely to arise because
agents have an incentive to create blocks and so they compete ---
rather than collude --- with each other.
Colluding
agents, with massive computing power, may help to deny or slow service
to a public key but not to another agent because agents can create
public keys at will.Further ReadingThere are many issues that we have not covered. This material only
covers the basics from the point of view of distributed algorithms.ReviewWhat is ablock chain?How does the algorithm deal with the situation in which one agent
receives a message to extend its copy of the block chain with a block
\(X\) while another agent receives a message to extend its copy of the
  block chain with a different block \(Y\)?Could there be an infinite sequence of collisions? Is that likely?What is mining for cryptocurrency? Suppose only a small number (say
  two organizations) mined most (say 99%) of Bitcoins. Would that be a
  problem?Why isdouble spendingunlikely to be successful?K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology



Figure Link: https://kmchandy.github.io/Crypto/Bitcoin/Slide1.jpg

Caption: Fig.1: More Confidence in Older Blocks in the
    Chain


So how can an agent determine whether a block is in the chain? And so
how can an agent find out if a transaction has been executed? You
sell your used bicycle to somebody for coins, but how do you know if
that transaction becomes part oftheblock chain when different
agents have different copies? And so how do you know that you can
spend those coins that
you should have received for your bicycle?Confidence that a Block is in the ChainIn this section we discuss the behavior of non-devious agents; we will
show how the algorithm handles devious agents later.
Suppose an agent's copy of the block chain is \(X + X_{1} + X_{2}
+ \ldots + X_{K}\), and another agent's copy is \(Y + ? + ? +
\ldots\), where \(?\) represents arbitrary values. What is the
likelihood that \(Y \neq X\)?\(Y\) can be different from \(X\) only if there are a sequence of
\(K\) or more collisions. And the likelihood of a sequence of
collisions decreases with \(K\). Likewise, the likelihood that an
agent never receives a block chain containing \(X\) decreases with
time, and so decreases with \(K\). So, if \(K\) is large then with
very high probability, \(X\) is part of every agent's block chain.Now suppose an agent's copy of the block chain is \(L + X + X_{1} + X_{2}
+ \ldots + X_{K}\). What is the likelihood
that another agent's copy is \(L + Y + ? + \ldots\) where \(L\) is an
arbitrary sequence? By the same
argument, when if \(K\) is large then with
very high probability, \(X\) is part of every block chain.For practical purposes,
many agents assume that if \(K > 6\) then \(L + X\) is a prefix of
most agents' block chains.See the Princeton Bitcoin book.Suppose agent \(A\) gets \(N\) Bitcoins in transaction \(X\). If \(K =
0\) then an agent can't be confident that \(A\) ever received these coins
because this transaction may not persist in the block chain. As \(K\)
increases agents become more confident that the transaction is in the
block chain and that \(A\) did, indeed, receive these coins.Fig.1: More Confidence in Older Blocks in the
    ChainWhat happens to Orphan Transactions?An agent may append a block \(Y\) to its chain, and this block may be
dropped from the chains of all agents and never reappear after some
point. We saw a scenario in which this happens.
Such a block is an "orphan," because no agent has a record of that
transaction after some time.
If an orphan block contains the transaction in which you
sold your bicycle in exchange for coins, then will you ever be able to
spend your coins? Yes, you will get your coins as we see next.Agents aggregate transactions that have not
appeared in the agent's block chain into blocks and propose to append these
blocks to the chain. A transaction that does not appear in block
chains will be agrregated eventually by some agent and inserted into
a block in the chian. Though the orphan block disappears the
transactions in the block do not.IncentivesNext, looks look at challenge number 3. Why should an agent create
blocks of validated transactions?Because the agent gets paid! Payment
is from either ablock rewardor transaction fees.Block rewardsAn agent that creates a block gets a specified number of Bitcoins for
itself as a reward called ablock reward.
The Bitcoins in a block reward are created by making the
block; these Bitcoins don't exist until the block is created. The process of making
blocks and acquiring block rewards is called "mining."Mining is the only way of creating new Bitcoins.When Bitcoin started the reward for creating a block was 50 Bitcoins.
The reward
halves after the creation of a certain number (210,000) of blocks. The
reward was reduced to 25 in 2013 and to 12.5 in 2018.
Block rewards will vanish at some point in the future.
The
total number of Bitcoins that can ever be created has an upper bound:
about 21 million.(Bitcoins can be lost. An agent may lose the hash pointer to the
transaction that gave the agent ownership of the coin, or an agent may
lose its private key.)Even when block rewards vanish, miners will continue to
mine provided that they get paid transaction fees. A
transaction fee is a payment by payers and payees to
miners. Transaction fees are voluntary. A high-fee for a
transaction is an incentive to miners to put this transaction into a
block quickly. So, it's possible that agents that offer no fee or low
fees may have to wait longer for their transactions to enter the
block chain.Incentives are critical for Bitcoin. Miners get paid to
get their blocks into the long-term "consensus" block chain.
Miners have an incentive to police the block chain because 
they don't get paid for appending erroneous blocks. If a miner appends
an erroneous block to the chain then other
miners won't extend chains containing the erroneous block, and so the
erroneous block will become an orphan.Any agent can check whether its copy of the block chain is
valid; however, agents making ordinary transactions don't
need to do so because there are many miners each of whom has an
incentive to ensure that the block chain is legitimate.AttacksNext, looks look at challenge number 4.Stealing coinsCan an agent steal a coin from an agent \(X\) by appending a block to
the chain where the block contains a transaction in which \(X\) gives
coins to \(Y\)?No, this can't happen thanks to cryptography. A transaction into which
\(X\) puts coins is valid only if \(X\) signs the transaction.  \(Y\)
cannot forge \(X\)'s signature, and so \(Y\) cannot create blocks that
contain such fraudulent transactions.Double spendCan an agent spend the same coin twice? Can an agent buy something
without paying for it?Consider the following transaction using conventional checks issued by
banks. A buyer gives a seller a check for $\(100,000\) for a house.
The house is put in escrow.  When the check clears and the seller
receives the payment the buyer gets possession of the house. The legal
process that includes notaries, real estate agents, and banks, helps
ensure that the transaction concludes correctly or is aborted correctly.Next, let's look at a transaction in which a person buys a video
online by paying the seller Bitcoins. The buyer and seller know each
other by their public keys and by their online addresses. 
The buyer broadcasts the transaction in which the buyer gives the seller
the payment in Bitcoins.
The amount is specified as
a pair (transaction id, array index) described in the sectionpay
transactionsinthe module introducing crypto currencies.A miner puts the transaction into a block \(X\);
appends the block to its copy \(L\) of the block chain; and broadcasts the extended
block chain \(L + X\).
When the seller gets a copy of the block chain \(L + X\), the
seller concludes that it has received the payment from the
buyer because the 
transaction has been recorded in a block chain. So the seller gives
the video to the buyer.The buyer cheats. The buyer creates a transaction in which the buyer
transfers the same Bitcoins  to the buyer itself.
A miner creates a block \(Y\) that includes this transaction.
A miner who has only received block
chain \(L\) (and hasn't yet received chain \(L + X\)) appends \(Y\) to
\(L\) to get a chain \(L + Y\), and broacasts \(L + Y\).Now we have a situation in which one
miner broadcasts a legitimate block chain \(L + X\) and a different
miner broadcasts a legitimate block chain \(L + Y\). Both chains have
the same length.
A miner with chain \(L + X\) does not know at this point that another
minder has chain \(L + Y\). So, miners will extend both block chains.
(Note that the algorithm will not permit chains \(L + X + Y\) or \(L +
Y + X\).)We've seen this situation before:  look atblock collisionsdescribed
earlier in this module. Both block chains will be extended, but
eventually, with very high probability, one of the blocks \(X\) or
\(Y\) will drop out of chain.How should sellers protect themselves?What is the equivalent of the buyer's check clearing in the bank?A seller should give the item to the buyer only after the transaction
appears with high confidence in a block in the chain --- see Figure 1.
The seller listens to block chains broadcast by miners. If the seller
gets a block chain \(L + X + L'\) where \(L'\) is itself a long block chain
then the seller has high confidence that the transaction is in the
permanent record.The seller waits to get block chains in which its transaction appears
in a block which is then followed by \(m\) blocks, for large
\(m\). The larger the value of \(m\), the greater the seller's
confidence, but the longer the buyer has to wait to get paid.
length of the extension \(L'\).A value of \(m = 6\) gives adequate confidence in most cases.Fraudulent minersA miner gets paid for every block the miner creates; so, why shouldn't
the miner create fraudulent blocks and get paid for them?The answer is the same as that for the double-spending attack. A miner
may have 
created a block and appended it to the chain; however, a suspicious
agent (and we hope that all agents are suspicious!) will not accept
the block until many blocks have been appended to the chain after
it. Other miners won't append their blocks to an invalid one. The
invalid blocks will become permanent orphans, and so the fraudulent
miner won't be able to spend the coins in these blocks.Fifty One Percent AttacksA 51% attack can be carried out by an agent that has more mining
power (e.g. 51% or more) than all other agents combined. The higher
the proportion of mining power of a single agent, the greater the
chances that attacks by that agent will succeed. You can see the
danger of a single agent having predominant mining power by thinking
about an agent with say, 99% of the total mining power. This agent
can mine so much faster than others that it can manipulate the block
chain in many ways. It can create double-spend transactions and deny
services to some transactions.A group of miners can collude to gain predominant mining power. Also,
miners canrentCloud-based systems for mining --- as opposed to having toownhuge data centers. An attacker can rent a large system for the specific purpose and
duration of an attack. The public may not know if and when such
attacks are successful becausecryptocurrencies do not have an
incentive to publish such attacks.Denial of serviceCan agents collude so that an agent \(X\)'s transactions never get
into blocks and so never get processed?An agent's identity does not appear in a transaction, only a public
key does. An agent can create new public keys at will. So let's ask
another question: can agents collude so that transactions with a
specific public key do not get into blocks?Only an agent that can solve puzzles in reasonable time can make
blocks. These agents have significant computational power. One can
concoct a situation where many agents with significant computation
power collude to avoid transactions from a specific public key. This
could have the effect of slowing processing of certain
transactions. However, such a situation isn't likely to arise because
agents have an incentive to create blocks and so they compete ---
rather than collude --- with each other.
Colluding
agents, with massive computing power, may help to deny or slow service
to a public key but not to another agent because agents can create
public keys at will.Further ReadingThere are many issues that we have not covered. This material only
covers the basics from the point of view of distributed algorithms.ReviewWhat is ablock chain?How does the algorithm deal with the situation in which one agent
receives a message to extend its copy of the block chain with a block
\(X\) while another agent receives a message to extend its copy of the
  block chain with a different block \(Y\)?Could there be an infinite sequence of collisions? Is that likely?What is mining for cryptocurrency? Suppose only a small number (say
  two organizations) mined most (say 99%) of Bitcoins. Would that be a
  problem?Why isdouble spendingunlikely to be successful?K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology



Figure Link: https://kmchandy.github.io/Crypto/Bitcoin/Slide1.jpg

Caption: Fig.1: More Confidence in Older Blocks in the
    Chain


\(Y\) can be different from \(X\) only if there are a sequence of
\(K\) or more collisions. And the likelihood of a sequence of
collisions decreases with \(K\). Likewise, the likelihood that an
agent never receives a block chain containing \(X\) decreases with
time, and so decreases with \(K\). So, if \(K\) is large then with
very high probability, \(X\) is part of every agent's block chain.Now suppose an agent's copy of the block chain is \(L + X + X_{1} + X_{2}
+ \ldots + X_{K}\). What is the likelihood
that another agent's copy is \(L + Y + ? + \ldots\) where \(L\) is an
arbitrary sequence? By the same
argument, when if \(K\) is large then with
very high probability, \(X\) is part of every block chain.For practical purposes,
many agents assume that if \(K > 6\) then \(L + X\) is a prefix of
most agents' block chains.See the Princeton Bitcoin book.Suppose agent \(A\) gets \(N\) Bitcoins in transaction \(X\). If \(K =
0\) then an agent can't be confident that \(A\) ever received these coins
because this transaction may not persist in the block chain. As \(K\)
increases agents become more confident that the transaction is in the
block chain and that \(A\) did, indeed, receive these coins.Fig.1: More Confidence in Older Blocks in the
    ChainWhat happens to Orphan Transactions?An agent may append a block \(Y\) to its chain, and this block may be
dropped from the chains of all agents and never reappear after some
point. We saw a scenario in which this happens.
Such a block is an "orphan," because no agent has a record of that
transaction after some time.
If an orphan block contains the transaction in which you
sold your bicycle in exchange for coins, then will you ever be able to
spend your coins? Yes, you will get your coins as we see next.Agents aggregate transactions that have not
appeared in the agent's block chain into blocks and propose to append these
blocks to the chain. A transaction that does not appear in block
chains will be agrregated eventually by some agent and inserted into
a block in the chian. Though the orphan block disappears the
transactions in the block do not.IncentivesNext, looks look at challenge number 3. Why should an agent create
blocks of validated transactions?Because the agent gets paid! Payment
is from either ablock rewardor transaction fees.Block rewardsAn agent that creates a block gets a specified number of Bitcoins for
itself as a reward called ablock reward.
The Bitcoins in a block reward are created by making the
block; these Bitcoins don't exist until the block is created. The process of making
blocks and acquiring block rewards is called "mining."Mining is the only way of creating new Bitcoins.When Bitcoin started the reward for creating a block was 50 Bitcoins.
The reward
halves after the creation of a certain number (210,000) of blocks. The
reward was reduced to 25 in 2013 and to 12.5 in 2018.
Block rewards will vanish at some point in the future.
The
total number of Bitcoins that can ever be created has an upper bound:
about 21 million.(Bitcoins can be lost. An agent may lose the hash pointer to the
transaction that gave the agent ownership of the coin, or an agent may
lose its private key.)Even when block rewards vanish, miners will continue to
mine provided that they get paid transaction fees. A
transaction fee is a payment by payers and payees to
miners. Transaction fees are voluntary. A high-fee for a
transaction is an incentive to miners to put this transaction into a
block quickly. So, it's possible that agents that offer no fee or low
fees may have to wait longer for their transactions to enter the
block chain.Incentives are critical for Bitcoin. Miners get paid to
get their blocks into the long-term "consensus" block chain.
Miners have an incentive to police the block chain because 
they don't get paid for appending erroneous blocks. If a miner appends
an erroneous block to the chain then other
miners won't extend chains containing the erroneous block, and so the
erroneous block will become an orphan.Any agent can check whether its copy of the block chain is
valid; however, agents making ordinary transactions don't
need to do so because there are many miners each of whom has an
incentive to ensure that the block chain is legitimate.AttacksNext, looks look at challenge number 4.Stealing coinsCan an agent steal a coin from an agent \(X\) by appending a block to
the chain where the block contains a transaction in which \(X\) gives
coins to \(Y\)?No, this can't happen thanks to cryptography. A transaction into which
\(X\) puts coins is valid only if \(X\) signs the transaction.  \(Y\)
cannot forge \(X\)'s signature, and so \(Y\) cannot create blocks that
contain such fraudulent transactions.Double spendCan an agent spend the same coin twice? Can an agent buy something
without paying for it?Consider the following transaction using conventional checks issued by
banks. A buyer gives a seller a check for $\(100,000\) for a house.
The house is put in escrow.  When the check clears and the seller
receives the payment the buyer gets possession of the house. The legal
process that includes notaries, real estate agents, and banks, helps
ensure that the transaction concludes correctly or is aborted correctly.Next, let's look at a transaction in which a person buys a video
online by paying the seller Bitcoins. The buyer and seller know each
other by their public keys and by their online addresses. 
The buyer broadcasts the transaction in which the buyer gives the seller
the payment in Bitcoins.
The amount is specified as
a pair (transaction id, array index) described in the sectionpay
transactionsinthe module introducing crypto currencies.A miner puts the transaction into a block \(X\);
appends the block to its copy \(L\) of the block chain; and broadcasts the extended
block chain \(L + X\).
When the seller gets a copy of the block chain \(L + X\), the
seller concludes that it has received the payment from the
buyer because the 
transaction has been recorded in a block chain. So the seller gives
the video to the buyer.The buyer cheats. The buyer creates a transaction in which the buyer
transfers the same Bitcoins  to the buyer itself.
A miner creates a block \(Y\) that includes this transaction.
A miner who has only received block
chain \(L\) (and hasn't yet received chain \(L + X\)) appends \(Y\) to
\(L\) to get a chain \(L + Y\), and broacasts \(L + Y\).Now we have a situation in which one
miner broadcasts a legitimate block chain \(L + X\) and a different
miner broadcasts a legitimate block chain \(L + Y\). Both chains have
the same length.
A miner with chain \(L + X\) does not know at this point that another
minder has chain \(L + Y\). So, miners will extend both block chains.
(Note that the algorithm will not permit chains \(L + X + Y\) or \(L +
Y + X\).)We've seen this situation before:  look atblock collisionsdescribed
earlier in this module. Both block chains will be extended, but
eventually, with very high probability, one of the blocks \(X\) or
\(Y\) will drop out of chain.How should sellers protect themselves?What is the equivalent of the buyer's check clearing in the bank?A seller should give the item to the buyer only after the transaction
appears with high confidence in a block in the chain --- see Figure 1.
The seller listens to block chains broadcast by miners. If the seller
gets a block chain \(L + X + L'\) where \(L'\) is itself a long block chain
then the seller has high confidence that the transaction is in the
permanent record.The seller waits to get block chains in which its transaction appears
in a block which is then followed by \(m\) blocks, for large
\(m\). The larger the value of \(m\), the greater the seller's
confidence, but the longer the buyer has to wait to get paid.
length of the extension \(L'\).A value of \(m = 6\) gives adequate confidence in most cases.Fraudulent minersA miner gets paid for every block the miner creates; so, why shouldn't
the miner create fraudulent blocks and get paid for them?The answer is the same as that for the double-spending attack. A miner
may have 
created a block and appended it to the chain; however, a suspicious
agent (and we hope that all agents are suspicious!) will not accept
the block until many blocks have been appended to the chain after
it. Other miners won't append their blocks to an invalid one. The
invalid blocks will become permanent orphans, and so the fraudulent
miner won't be able to spend the coins in these blocks.Fifty One Percent AttacksA 51% attack can be carried out by an agent that has more mining
power (e.g. 51% or more) than all other agents combined. The higher
the proportion of mining power of a single agent, the greater the
chances that attacks by that agent will succeed. You can see the
danger of a single agent having predominant mining power by thinking
about an agent with say, 99% of the total mining power. This agent
can mine so much faster than others that it can manipulate the block
chain in many ways. It can create double-spend transactions and deny
services to some transactions.A group of miners can collude to gain predominant mining power. Also,
miners canrentCloud-based systems for mining --- as opposed to having toownhuge data centers. An attacker can rent a large system for the specific purpose and
duration of an attack. The public may not know if and when such
attacks are successful becausecryptocurrencies do not have an
incentive to publish such attacks.Denial of serviceCan agents collude so that an agent \(X\)'s transactions never get
into blocks and so never get processed?An agent's identity does not appear in a transaction, only a public
key does. An agent can create new public keys at will. So let's ask
another question: can agents collude so that transactions with a
specific public key do not get into blocks?Only an agent that can solve puzzles in reasonable time can make
blocks. These agents have significant computational power. One can
concoct a situation where many agents with significant computation
power collude to avoid transactions from a specific public key. This
could have the effect of slowing processing of certain
transactions. However, such a situation isn't likely to arise because
agents have an incentive to create blocks and so they compete ---
rather than collude --- with each other.
Colluding
agents, with massive computing power, may help to deny or slow service
to a public key but not to another agent because agents can create
public keys at will.Further ReadingThere are many issues that we have not covered. This material only
covers the basics from the point of view of distributed algorithms.ReviewWhat is ablock chain?How does the algorithm deal with the situation in which one agent
receives a message to extend its copy of the block chain with a block
\(X\) while another agent receives a message to extend its copy of the
  block chain with a different block \(Y\)?Could there be an infinite sequence of collisions? Is that likely?What is mining for cryptocurrency? Suppose only a small number (say
  two organizations) mined most (say 99%) of Bitcoins. Would that be a
  problem?Why isdouble spendingunlikely to be successful?K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology



Figure Link: https://kmchandy.github.io/Crypto/Bitcoin/Slide1.jpg

Caption: Fig.1: More Confidence in Older Blocks in the
    Chain


Now suppose an agent's copy of the block chain is \(L + X + X_{1} + X_{2}
+ \ldots + X_{K}\). What is the likelihood
that another agent's copy is \(L + Y + ? + \ldots\) where \(L\) is an
arbitrary sequence? By the same
argument, when if \(K\) is large then with
very high probability, \(X\) is part of every block chain.For practical purposes,
many agents assume that if \(K > 6\) then \(L + X\) is a prefix of
most agents' block chains.See the Princeton Bitcoin book.Suppose agent \(A\) gets \(N\) Bitcoins in transaction \(X\). If \(K =
0\) then an agent can't be confident that \(A\) ever received these coins
because this transaction may not persist in the block chain. As \(K\)
increases agents become more confident that the transaction is in the
block chain and that \(A\) did, indeed, receive these coins.Fig.1: More Confidence in Older Blocks in the
    ChainWhat happens to Orphan Transactions?An agent may append a block \(Y\) to its chain, and this block may be
dropped from the chains of all agents and never reappear after some
point. We saw a scenario in which this happens.
Such a block is an "orphan," because no agent has a record of that
transaction after some time.
If an orphan block contains the transaction in which you
sold your bicycle in exchange for coins, then will you ever be able to
spend your coins? Yes, you will get your coins as we see next.Agents aggregate transactions that have not
appeared in the agent's block chain into blocks and propose to append these
blocks to the chain. A transaction that does not appear in block
chains will be agrregated eventually by some agent and inserted into
a block in the chian. Though the orphan block disappears the
transactions in the block do not.IncentivesNext, looks look at challenge number 3. Why should an agent create
blocks of validated transactions?Because the agent gets paid! Payment
is from either ablock rewardor transaction fees.Block rewardsAn agent that creates a block gets a specified number of Bitcoins for
itself as a reward called ablock reward.
The Bitcoins in a block reward are created by making the
block; these Bitcoins don't exist until the block is created. The process of making
blocks and acquiring block rewards is called "mining."Mining is the only way of creating new Bitcoins.When Bitcoin started the reward for creating a block was 50 Bitcoins.
The reward
halves after the creation of a certain number (210,000) of blocks. The
reward was reduced to 25 in 2013 and to 12.5 in 2018.
Block rewards will vanish at some point in the future.
The
total number of Bitcoins that can ever be created has an upper bound:
about 21 million.(Bitcoins can be lost. An agent may lose the hash pointer to the
transaction that gave the agent ownership of the coin, or an agent may
lose its private key.)Even when block rewards vanish, miners will continue to
mine provided that they get paid transaction fees. A
transaction fee is a payment by payers and payees to
miners. Transaction fees are voluntary. A high-fee for a
transaction is an incentive to miners to put this transaction into a
block quickly. So, it's possible that agents that offer no fee or low
fees may have to wait longer for their transactions to enter the
block chain.Incentives are critical for Bitcoin. Miners get paid to
get their blocks into the long-term "consensus" block chain.
Miners have an incentive to police the block chain because 
they don't get paid for appending erroneous blocks. If a miner appends
an erroneous block to the chain then other
miners won't extend chains containing the erroneous block, and so the
erroneous block will become an orphan.Any agent can check whether its copy of the block chain is
valid; however, agents making ordinary transactions don't
need to do so because there are many miners each of whom has an
incentive to ensure that the block chain is legitimate.AttacksNext, looks look at challenge number 4.Stealing coinsCan an agent steal a coin from an agent \(X\) by appending a block to
the chain where the block contains a transaction in which \(X\) gives
coins to \(Y\)?No, this can't happen thanks to cryptography. A transaction into which
\(X\) puts coins is valid only if \(X\) signs the transaction.  \(Y\)
cannot forge \(X\)'s signature, and so \(Y\) cannot create blocks that
contain such fraudulent transactions.Double spendCan an agent spend the same coin twice? Can an agent buy something
without paying for it?Consider the following transaction using conventional checks issued by
banks. A buyer gives a seller a check for $\(100,000\) for a house.
The house is put in escrow.  When the check clears and the seller
receives the payment the buyer gets possession of the house. The legal
process that includes notaries, real estate agents, and banks, helps
ensure that the transaction concludes correctly or is aborted correctly.Next, let's look at a transaction in which a person buys a video
online by paying the seller Bitcoins. The buyer and seller know each
other by their public keys and by their online addresses. 
The buyer broadcasts the transaction in which the buyer gives the seller
the payment in Bitcoins.
The amount is specified as
a pair (transaction id, array index) described in the sectionpay
transactionsinthe module introducing crypto currencies.A miner puts the transaction into a block \(X\);
appends the block to its copy \(L\) of the block chain; and broadcasts the extended
block chain \(L + X\).
When the seller gets a copy of the block chain \(L + X\), the
seller concludes that it has received the payment from the
buyer because the 
transaction has been recorded in a block chain. So the seller gives
the video to the buyer.The buyer cheats. The buyer creates a transaction in which the buyer
transfers the same Bitcoins  to the buyer itself.
A miner creates a block \(Y\) that includes this transaction.
A miner who has only received block
chain \(L\) (and hasn't yet received chain \(L + X\)) appends \(Y\) to
\(L\) to get a chain \(L + Y\), and broacasts \(L + Y\).Now we have a situation in which one
miner broadcasts a legitimate block chain \(L + X\) and a different
miner broadcasts a legitimate block chain \(L + Y\). Both chains have
the same length.
A miner with chain \(L + X\) does not know at this point that another
minder has chain \(L + Y\). So, miners will extend both block chains.
(Note that the algorithm will not permit chains \(L + X + Y\) or \(L +
Y + X\).)We've seen this situation before:  look atblock collisionsdescribed
earlier in this module. Both block chains will be extended, but
eventually, with very high probability, one of the blocks \(X\) or
\(Y\) will drop out of chain.How should sellers protect themselves?What is the equivalent of the buyer's check clearing in the bank?A seller should give the item to the buyer only after the transaction
appears with high confidence in a block in the chain --- see Figure 1.
The seller listens to block chains broadcast by miners. If the seller
gets a block chain \(L + X + L'\) where \(L'\) is itself a long block chain
then the seller has high confidence that the transaction is in the
permanent record.The seller waits to get block chains in which its transaction appears
in a block which is then followed by \(m\) blocks, for large
\(m\). The larger the value of \(m\), the greater the seller's
confidence, but the longer the buyer has to wait to get paid.
length of the extension \(L'\).A value of \(m = 6\) gives adequate confidence in most cases.Fraudulent minersA miner gets paid for every block the miner creates; so, why shouldn't
the miner create fraudulent blocks and get paid for them?The answer is the same as that for the double-spending attack. A miner
may have 
created a block and appended it to the chain; however, a suspicious
agent (and we hope that all agents are suspicious!) will not accept
the block until many blocks have been appended to the chain after
it. Other miners won't append their blocks to an invalid one. The
invalid blocks will become permanent orphans, and so the fraudulent
miner won't be able to spend the coins in these blocks.Fifty One Percent AttacksA 51% attack can be carried out by an agent that has more mining
power (e.g. 51% or more) than all other agents combined. The higher
the proportion of mining power of a single agent, the greater the
chances that attacks by that agent will succeed. You can see the
danger of a single agent having predominant mining power by thinking
about an agent with say, 99% of the total mining power. This agent
can mine so much faster than others that it can manipulate the block
chain in many ways. It can create double-spend transactions and deny
services to some transactions.A group of miners can collude to gain predominant mining power. Also,
miners canrentCloud-based systems for mining --- as opposed to having toownhuge data centers. An attacker can rent a large system for the specific purpose and
duration of an attack. The public may not know if and when such
attacks are successful becausecryptocurrencies do not have an
incentive to publish such attacks.Denial of serviceCan agents collude so that an agent \(X\)'s transactions never get
into blocks and so never get processed?An agent's identity does not appear in a transaction, only a public
key does. An agent can create new public keys at will. So let's ask
another question: can agents collude so that transactions with a
specific public key do not get into blocks?Only an agent that can solve puzzles in reasonable time can make
blocks. These agents have significant computational power. One can
concoct a situation where many agents with significant computation
power collude to avoid transactions from a specific public key. This
could have the effect of slowing processing of certain
transactions. However, such a situation isn't likely to arise because
agents have an incentive to create blocks and so they compete ---
rather than collude --- with each other.
Colluding
agents, with massive computing power, may help to deny or slow service
to a public key but not to another agent because agents can create
public keys at will.Further ReadingThere are many issues that we have not covered. This material only
covers the basics from the point of view of distributed algorithms.ReviewWhat is ablock chain?How does the algorithm deal with the situation in which one agent
receives a message to extend its copy of the block chain with a block
\(X\) while another agent receives a message to extend its copy of the
  block chain with a different block \(Y\)?Could there be an infinite sequence of collisions? Is that likely?What is mining for cryptocurrency? Suppose only a small number (say
  two organizations) mined most (say 99%) of Bitcoins. Would that be a
  problem?Why isdouble spendingunlikely to be successful?K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology



Figure Link: https://kmchandy.github.io/Crypto/Bitcoin/Slide1.jpg

Caption: Fig.1: More Confidence in Older Blocks in the
    Chain


For practical purposes,
many agents assume that if \(K > 6\) then \(L + X\) is a prefix of
most agents' block chains.See the Princeton Bitcoin book.Suppose agent \(A\) gets \(N\) Bitcoins in transaction \(X\). If \(K =
0\) then an agent can't be confident that \(A\) ever received these coins
because this transaction may not persist in the block chain. As \(K\)
increases agents become more confident that the transaction is in the
block chain and that \(A\) did, indeed, receive these coins.Fig.1: More Confidence in Older Blocks in the
    ChainWhat happens to Orphan Transactions?An agent may append a block \(Y\) to its chain, and this block may be
dropped from the chains of all agents and never reappear after some
point. We saw a scenario in which this happens.
Such a block is an "orphan," because no agent has a record of that
transaction after some time.
If an orphan block contains the transaction in which you
sold your bicycle in exchange for coins, then will you ever be able to
spend your coins? Yes, you will get your coins as we see next.Agents aggregate transactions that have not
appeared in the agent's block chain into blocks and propose to append these
blocks to the chain. A transaction that does not appear in block
chains will be agrregated eventually by some agent and inserted into
a block in the chian. Though the orphan block disappears the
transactions in the block do not.IncentivesNext, looks look at challenge number 3. Why should an agent create
blocks of validated transactions?Because the agent gets paid! Payment
is from either ablock rewardor transaction fees.Block rewardsAn agent that creates a block gets a specified number of Bitcoins for
itself as a reward called ablock reward.
The Bitcoins in a block reward are created by making the
block; these Bitcoins don't exist until the block is created. The process of making
blocks and acquiring block rewards is called "mining."Mining is the only way of creating new Bitcoins.When Bitcoin started the reward for creating a block was 50 Bitcoins.
The reward
halves after the creation of a certain number (210,000) of blocks. The
reward was reduced to 25 in 2013 and to 12.5 in 2018.
Block rewards will vanish at some point in the future.
The
total number of Bitcoins that can ever be created has an upper bound:
about 21 million.(Bitcoins can be lost. An agent may lose the hash pointer to the
transaction that gave the agent ownership of the coin, or an agent may
lose its private key.)Even when block rewards vanish, miners will continue to
mine provided that they get paid transaction fees. A
transaction fee is a payment by payers and payees to
miners. Transaction fees are voluntary. A high-fee for a
transaction is an incentive to miners to put this transaction into a
block quickly. So, it's possible that agents that offer no fee or low
fees may have to wait longer for their transactions to enter the
block chain.Incentives are critical for Bitcoin. Miners get paid to
get their blocks into the long-term "consensus" block chain.
Miners have an incentive to police the block chain because 
they don't get paid for appending erroneous blocks. If a miner appends
an erroneous block to the chain then other
miners won't extend chains containing the erroneous block, and so the
erroneous block will become an orphan.Any agent can check whether its copy of the block chain is
valid; however, agents making ordinary transactions don't
need to do so because there are many miners each of whom has an
incentive to ensure that the block chain is legitimate.AttacksNext, looks look at challenge number 4.Stealing coinsCan an agent steal a coin from an agent \(X\) by appending a block to
the chain where the block contains a transaction in which \(X\) gives
coins to \(Y\)?No, this can't happen thanks to cryptography. A transaction into which
\(X\) puts coins is valid only if \(X\) signs the transaction.  \(Y\)
cannot forge \(X\)'s signature, and so \(Y\) cannot create blocks that
contain such fraudulent transactions.Double spendCan an agent spend the same coin twice? Can an agent buy something
without paying for it?Consider the following transaction using conventional checks issued by
banks. A buyer gives a seller a check for $\(100,000\) for a house.
The house is put in escrow.  When the check clears and the seller
receives the payment the buyer gets possession of the house. The legal
process that includes notaries, real estate agents, and banks, helps
ensure that the transaction concludes correctly or is aborted correctly.Next, let's look at a transaction in which a person buys a video
online by paying the seller Bitcoins. The buyer and seller know each
other by their public keys and by their online addresses. 
The buyer broadcasts the transaction in which the buyer gives the seller
the payment in Bitcoins.
The amount is specified as
a pair (transaction id, array index) described in the sectionpay
transactionsinthe module introducing crypto currencies.A miner puts the transaction into a block \(X\);
appends the block to its copy \(L\) of the block chain; and broadcasts the extended
block chain \(L + X\).
When the seller gets a copy of the block chain \(L + X\), the
seller concludes that it has received the payment from the
buyer because the 
transaction has been recorded in a block chain. So the seller gives
the video to the buyer.The buyer cheats. The buyer creates a transaction in which the buyer
transfers the same Bitcoins  to the buyer itself.
A miner creates a block \(Y\) that includes this transaction.
A miner who has only received block
chain \(L\) (and hasn't yet received chain \(L + X\)) appends \(Y\) to
\(L\) to get a chain \(L + Y\), and broacasts \(L + Y\).Now we have a situation in which one
miner broadcasts a legitimate block chain \(L + X\) and a different
miner broadcasts a legitimate block chain \(L + Y\). Both chains have
the same length.
A miner with chain \(L + X\) does not know at this point that another
minder has chain \(L + Y\). So, miners will extend both block chains.
(Note that the algorithm will not permit chains \(L + X + Y\) or \(L +
Y + X\).)We've seen this situation before:  look atblock collisionsdescribed
earlier in this module. Both block chains will be extended, but
eventually, with very high probability, one of the blocks \(X\) or
\(Y\) will drop out of chain.How should sellers protect themselves?What is the equivalent of the buyer's check clearing in the bank?A seller should give the item to the buyer only after the transaction
appears with high confidence in a block in the chain --- see Figure 1.
The seller listens to block chains broadcast by miners. If the seller
gets a block chain \(L + X + L'\) where \(L'\) is itself a long block chain
then the seller has high confidence that the transaction is in the
permanent record.The seller waits to get block chains in which its transaction appears
in a block which is then followed by \(m\) blocks, for large
\(m\). The larger the value of \(m\), the greater the seller's
confidence, but the longer the buyer has to wait to get paid.
length of the extension \(L'\).A value of \(m = 6\) gives adequate confidence in most cases.Fraudulent minersA miner gets paid for every block the miner creates; so, why shouldn't
the miner create fraudulent blocks and get paid for them?The answer is the same as that for the double-spending attack. A miner
may have 
created a block and appended it to the chain; however, a suspicious
agent (and we hope that all agents are suspicious!) will not accept
the block until many blocks have been appended to the chain after
it. Other miners won't append their blocks to an invalid one. The
invalid blocks will become permanent orphans, and so the fraudulent
miner won't be able to spend the coins in these blocks.Fifty One Percent AttacksA 51% attack can be carried out by an agent that has more mining
power (e.g. 51% or more) than all other agents combined. The higher
the proportion of mining power of a single agent, the greater the
chances that attacks by that agent will succeed. You can see the
danger of a single agent having predominant mining power by thinking
about an agent with say, 99% of the total mining power. This agent
can mine so much faster than others that it can manipulate the block
chain in many ways. It can create double-spend transactions and deny
services to some transactions.A group of miners can collude to gain predominant mining power. Also,
miners canrentCloud-based systems for mining --- as opposed to having toownhuge data centers. An attacker can rent a large system for the specific purpose and
duration of an attack. The public may not know if and when such
attacks are successful becausecryptocurrencies do not have an
incentive to publish such attacks.Denial of serviceCan agents collude so that an agent \(X\)'s transactions never get
into blocks and so never get processed?An agent's identity does not appear in a transaction, only a public
key does. An agent can create new public keys at will. So let's ask
another question: can agents collude so that transactions with a
specific public key do not get into blocks?Only an agent that can solve puzzles in reasonable time can make
blocks. These agents have significant computational power. One can
concoct a situation where many agents with significant computation
power collude to avoid transactions from a specific public key. This
could have the effect of slowing processing of certain
transactions. However, such a situation isn't likely to arise because
agents have an incentive to create blocks and so they compete ---
rather than collude --- with each other.
Colluding
agents, with massive computing power, may help to deny or slow service
to a public key but not to another agent because agents can create
public keys at will.Further ReadingThere are many issues that we have not covered. This material only
covers the basics from the point of view of distributed algorithms.ReviewWhat is ablock chain?How does the algorithm deal with the situation in which one agent
receives a message to extend its copy of the block chain with a block
\(X\) while another agent receives a message to extend its copy of the
  block chain with a different block \(Y\)?Could there be an infinite sequence of collisions? Is that likely?What is mining for cryptocurrency? Suppose only a small number (say
  two organizations) mined most (say 99%) of Bitcoins. Would that be a
  problem?Why isdouble spendingunlikely to be successful?K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology



Figure Link: https://kmchandy.github.io/Crypto/Bitcoin/Slide1.jpg

Caption: Fig.1: More Confidence in Older Blocks in the
    Chain


Suppose agent \(A\) gets \(N\) Bitcoins in transaction \(X\). If \(K =
0\) then an agent can't be confident that \(A\) ever received these coins
because this transaction may not persist in the block chain. As \(K\)
increases agents become more confident that the transaction is in the
block chain and that \(A\) did, indeed, receive these coins.Fig.1: More Confidence in Older Blocks in the
    ChainWhat happens to Orphan Transactions?An agent may append a block \(Y\) to its chain, and this block may be
dropped from the chains of all agents and never reappear after some
point. We saw a scenario in which this happens.
Such a block is an "orphan," because no agent has a record of that
transaction after some time.
If an orphan block contains the transaction in which you
sold your bicycle in exchange for coins, then will you ever be able to
spend your coins? Yes, you will get your coins as we see next.Agents aggregate transactions that have not
appeared in the agent's block chain into blocks and propose to append these
blocks to the chain. A transaction that does not appear in block
chains will be agrregated eventually by some agent and inserted into
a block in the chian. Though the orphan block disappears the
transactions in the block do not.IncentivesNext, looks look at challenge number 3. Why should an agent create
blocks of validated transactions?Because the agent gets paid! Payment
is from either ablock rewardor transaction fees.Block rewardsAn agent that creates a block gets a specified number of Bitcoins for
itself as a reward called ablock reward.
The Bitcoins in a block reward are created by making the
block; these Bitcoins don't exist until the block is created. The process of making
blocks and acquiring block rewards is called "mining."Mining is the only way of creating new Bitcoins.When Bitcoin started the reward for creating a block was 50 Bitcoins.
The reward
halves after the creation of a certain number (210,000) of blocks. The
reward was reduced to 25 in 2013 and to 12.5 in 2018.
Block rewards will vanish at some point in the future.
The
total number of Bitcoins that can ever be created has an upper bound:
about 21 million.(Bitcoins can be lost. An agent may lose the hash pointer to the
transaction that gave the agent ownership of the coin, or an agent may
lose its private key.)Even when block rewards vanish, miners will continue to
mine provided that they get paid transaction fees. A
transaction fee is a payment by payers and payees to
miners. Transaction fees are voluntary. A high-fee for a
transaction is an incentive to miners to put this transaction into a
block quickly. So, it's possible that agents that offer no fee or low
fees may have to wait longer for their transactions to enter the
block chain.Incentives are critical for Bitcoin. Miners get paid to
get their blocks into the long-term "consensus" block chain.
Miners have an incentive to police the block chain because 
they don't get paid for appending erroneous blocks. If a miner appends
an erroneous block to the chain then other
miners won't extend chains containing the erroneous block, and so the
erroneous block will become an orphan.Any agent can check whether its copy of the block chain is
valid; however, agents making ordinary transactions don't
need to do so because there are many miners each of whom has an
incentive to ensure that the block chain is legitimate.AttacksNext, looks look at challenge number 4.Stealing coinsCan an agent steal a coin from an agent \(X\) by appending a block to
the chain where the block contains a transaction in which \(X\) gives
coins to \(Y\)?No, this can't happen thanks to cryptography. A transaction into which
\(X\) puts coins is valid only if \(X\) signs the transaction.  \(Y\)
cannot forge \(X\)'s signature, and so \(Y\) cannot create blocks that
contain such fraudulent transactions.Double spendCan an agent spend the same coin twice? Can an agent buy something
without paying for it?Consider the following transaction using conventional checks issued by
banks. A buyer gives a seller a check for $\(100,000\) for a house.
The house is put in escrow.  When the check clears and the seller
receives the payment the buyer gets possession of the house. The legal
process that includes notaries, real estate agents, and banks, helps
ensure that the transaction concludes correctly or is aborted correctly.Next, let's look at a transaction in which a person buys a video
online by paying the seller Bitcoins. The buyer and seller know each
other by their public keys and by their online addresses. 
The buyer broadcasts the transaction in which the buyer gives the seller
the payment in Bitcoins.
The amount is specified as
a pair (transaction id, array index) described in the sectionpay
transactionsinthe module introducing crypto currencies.A miner puts the transaction into a block \(X\);
appends the block to its copy \(L\) of the block chain; and broadcasts the extended
block chain \(L + X\).
When the seller gets a copy of the block chain \(L + X\), the
seller concludes that it has received the payment from the
buyer because the 
transaction has been recorded in a block chain. So the seller gives
the video to the buyer.The buyer cheats. The buyer creates a transaction in which the buyer
transfers the same Bitcoins  to the buyer itself.
A miner creates a block \(Y\) that includes this transaction.
A miner who has only received block
chain \(L\) (and hasn't yet received chain \(L + X\)) appends \(Y\) to
\(L\) to get a chain \(L + Y\), and broacasts \(L + Y\).Now we have a situation in which one
miner broadcasts a legitimate block chain \(L + X\) and a different
miner broadcasts a legitimate block chain \(L + Y\). Both chains have
the same length.
A miner with chain \(L + X\) does not know at this point that another
minder has chain \(L + Y\). So, miners will extend both block chains.
(Note that the algorithm will not permit chains \(L + X + Y\) or \(L +
Y + X\).)We've seen this situation before:  look atblock collisionsdescribed
earlier in this module. Both block chains will be extended, but
eventually, with very high probability, one of the blocks \(X\) or
\(Y\) will drop out of chain.How should sellers protect themselves?What is the equivalent of the buyer's check clearing in the bank?A seller should give the item to the buyer only after the transaction
appears with high confidence in a block in the chain --- see Figure 1.
The seller listens to block chains broadcast by miners. If the seller
gets a block chain \(L + X + L'\) where \(L'\) is itself a long block chain
then the seller has high confidence that the transaction is in the
permanent record.The seller waits to get block chains in which its transaction appears
in a block which is then followed by \(m\) blocks, for large
\(m\). The larger the value of \(m\), the greater the seller's
confidence, but the longer the buyer has to wait to get paid.
length of the extension \(L'\).A value of \(m = 6\) gives adequate confidence in most cases.Fraudulent minersA miner gets paid for every block the miner creates; so, why shouldn't
the miner create fraudulent blocks and get paid for them?The answer is the same as that for the double-spending attack. A miner
may have 
created a block and appended it to the chain; however, a suspicious
agent (and we hope that all agents are suspicious!) will not accept
the block until many blocks have been appended to the chain after
it. Other miners won't append their blocks to an invalid one. The
invalid blocks will become permanent orphans, and so the fraudulent
miner won't be able to spend the coins in these blocks.Fifty One Percent AttacksA 51% attack can be carried out by an agent that has more mining
power (e.g. 51% or more) than all other agents combined. The higher
the proportion of mining power of a single agent, the greater the
chances that attacks by that agent will succeed. You can see the
danger of a single agent having predominant mining power by thinking
about an agent with say, 99% of the total mining power. This agent
can mine so much faster than others that it can manipulate the block
chain in many ways. It can create double-spend transactions and deny
services to some transactions.A group of miners can collude to gain predominant mining power. Also,
miners canrentCloud-based systems for mining --- as opposed to having toownhuge data centers. An attacker can rent a large system for the specific purpose and
duration of an attack. The public may not know if and when such
attacks are successful becausecryptocurrencies do not have an
incentive to publish such attacks.Denial of serviceCan agents collude so that an agent \(X\)'s transactions never get
into blocks and so never get processed?An agent's identity does not appear in a transaction, only a public
key does. An agent can create new public keys at will. So let's ask
another question: can agents collude so that transactions with a
specific public key do not get into blocks?Only an agent that can solve puzzles in reasonable time can make
blocks. These agents have significant computational power. One can
concoct a situation where many agents with significant computation
power collude to avoid transactions from a specific public key. This
could have the effect of slowing processing of certain
transactions. However, such a situation isn't likely to arise because
agents have an incentive to create blocks and so they compete ---
rather than collude --- with each other.
Colluding
agents, with massive computing power, may help to deny or slow service
to a public key but not to another agent because agents can create
public keys at will.Further ReadingThere are many issues that we have not covered. This material only
covers the basics from the point of view of distributed algorithms.ReviewWhat is ablock chain?How does the algorithm deal with the situation in which one agent
receives a message to extend its copy of the block chain with a block
\(X\) while another agent receives a message to extend its copy of the
  block chain with a different block \(Y\)?Could there be an infinite sequence of collisions? Is that likely?What is mining for cryptocurrency? Suppose only a small number (say
  two organizations) mined most (say 99%) of Bitcoins. Would that be a
  problem?Why isdouble spendingunlikely to be successful?K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology



Figure Link: https://kmchandy.github.io/Crypto/Bitcoin/Slide1.jpg

Caption: Fig.1: More Confidence in Older Blocks in the
    Chain


Agents aggregate transactions that have not
appeared in the agent's block chain into blocks and propose to append these
blocks to the chain. A transaction that does not appear in block
chains will be agrregated eventually by some agent and inserted into
a block in the chian. Though the orphan block disappears the
transactions in the block do not.IncentivesNext, looks look at challenge number 3. Why should an agent create
blocks of validated transactions?Because the agent gets paid! Payment
is from either ablock rewardor transaction fees.Block rewardsAn agent that creates a block gets a specified number of Bitcoins for
itself as a reward called ablock reward.
The Bitcoins in a block reward are created by making the
block; these Bitcoins don't exist until the block is created. The process of making
blocks and acquiring block rewards is called "mining."Mining is the only way of creating new Bitcoins.When Bitcoin started the reward for creating a block was 50 Bitcoins.
The reward
halves after the creation of a certain number (210,000) of blocks. The
reward was reduced to 25 in 2013 and to 12.5 in 2018.
Block rewards will vanish at some point in the future.
The
total number of Bitcoins that can ever be created has an upper bound:
about 21 million.(Bitcoins can be lost. An agent may lose the hash pointer to the
transaction that gave the agent ownership of the coin, or an agent may
lose its private key.)Even when block rewards vanish, miners will continue to
mine provided that they get paid transaction fees. A
transaction fee is a payment by payers and payees to
miners. Transaction fees are voluntary. A high-fee for a
transaction is an incentive to miners to put this transaction into a
block quickly. So, it's possible that agents that offer no fee or low
fees may have to wait longer for their transactions to enter the
block chain.Incentives are critical for Bitcoin. Miners get paid to
get their blocks into the long-term "consensus" block chain.
Miners have an incentive to police the block chain because 
they don't get paid for appending erroneous blocks. If a miner appends
an erroneous block to the chain then other
miners won't extend chains containing the erroneous block, and so the
erroneous block will become an orphan.Any agent can check whether its copy of the block chain is
valid; however, agents making ordinary transactions don't
need to do so because there are many miners each of whom has an
incentive to ensure that the block chain is legitimate.AttacksNext, looks look at challenge number 4.Stealing coinsCan an agent steal a coin from an agent \(X\) by appending a block to
the chain where the block contains a transaction in which \(X\) gives
coins to \(Y\)?No, this can't happen thanks to cryptography. A transaction into which
\(X\) puts coins is valid only if \(X\) signs the transaction.  \(Y\)
cannot forge \(X\)'s signature, and so \(Y\) cannot create blocks that
contain such fraudulent transactions.Double spendCan an agent spend the same coin twice? Can an agent buy something
without paying for it?Consider the following transaction using conventional checks issued by
banks. A buyer gives a seller a check for $\(100,000\) for a house.
The house is put in escrow.  When the check clears and the seller
receives the payment the buyer gets possession of the house. The legal
process that includes notaries, real estate agents, and banks, helps
ensure that the transaction concludes correctly or is aborted correctly.Next, let's look at a transaction in which a person buys a video
online by paying the seller Bitcoins. The buyer and seller know each
other by their public keys and by their online addresses. 
The buyer broadcasts the transaction in which the buyer gives the seller
the payment in Bitcoins.
The amount is specified as
a pair (transaction id, array index) described in the sectionpay
transactionsinthe module introducing crypto currencies.A miner puts the transaction into a block \(X\);
appends the block to its copy \(L\) of the block chain; and broadcasts the extended
block chain \(L + X\).
When the seller gets a copy of the block chain \(L + X\), the
seller concludes that it has received the payment from the
buyer because the 
transaction has been recorded in a block chain. So the seller gives
the video to the buyer.The buyer cheats. The buyer creates a transaction in which the buyer
transfers the same Bitcoins  to the buyer itself.
A miner creates a block \(Y\) that includes this transaction.
A miner who has only received block
chain \(L\) (and hasn't yet received chain \(L + X\)) appends \(Y\) to
\(L\) to get a chain \(L + Y\), and broacasts \(L + Y\).Now we have a situation in which one
miner broadcasts a legitimate block chain \(L + X\) and a different
miner broadcasts a legitimate block chain \(L + Y\). Both chains have
the same length.
A miner with chain \(L + X\) does not know at this point that another
minder has chain \(L + Y\). So, miners will extend both block chains.
(Note that the algorithm will not permit chains \(L + X + Y\) or \(L +
Y + X\).)We've seen this situation before:  look atblock collisionsdescribed
earlier in this module. Both block chains will be extended, but
eventually, with very high probability, one of the blocks \(X\) or
\(Y\) will drop out of chain.How should sellers protect themselves?What is the equivalent of the buyer's check clearing in the bank?A seller should give the item to the buyer only after the transaction
appears with high confidence in a block in the chain --- see Figure 1.
The seller listens to block chains broadcast by miners. If the seller
gets a block chain \(L + X + L'\) where \(L'\) is itself a long block chain
then the seller has high confidence that the transaction is in the
permanent record.The seller waits to get block chains in which its transaction appears
in a block which is then followed by \(m\) blocks, for large
\(m\). The larger the value of \(m\), the greater the seller's
confidence, but the longer the buyer has to wait to get paid.
length of the extension \(L'\).A value of \(m = 6\) gives adequate confidence in most cases.Fraudulent minersA miner gets paid for every block the miner creates; so, why shouldn't
the miner create fraudulent blocks and get paid for them?The answer is the same as that for the double-spending attack. A miner
may have 
created a block and appended it to the chain; however, a suspicious
agent (and we hope that all agents are suspicious!) will not accept
the block until many blocks have been appended to the chain after
it. Other miners won't append their blocks to an invalid one. The
invalid blocks will become permanent orphans, and so the fraudulent
miner won't be able to spend the coins in these blocks.Fifty One Percent AttacksA 51% attack can be carried out by an agent that has more mining
power (e.g. 51% or more) than all other agents combined. The higher
the proportion of mining power of a single agent, the greater the
chances that attacks by that agent will succeed. You can see the
danger of a single agent having predominant mining power by thinking
about an agent with say, 99% of the total mining power. This agent
can mine so much faster than others that it can manipulate the block
chain in many ways. It can create double-spend transactions and deny
services to some transactions.A group of miners can collude to gain predominant mining power. Also,
miners canrentCloud-based systems for mining --- as opposed to having toownhuge data centers. An attacker can rent a large system for the specific purpose and
duration of an attack. The public may not know if and when such
attacks are successful becausecryptocurrencies do not have an
incentive to publish such attacks.Denial of serviceCan agents collude so that an agent \(X\)'s transactions never get
into blocks and so never get processed?An agent's identity does not appear in a transaction, only a public
key does. An agent can create new public keys at will. So let's ask
another question: can agents collude so that transactions with a
specific public key do not get into blocks?Only an agent that can solve puzzles in reasonable time can make
blocks. These agents have significant computational power. One can
concoct a situation where many agents with significant computation
power collude to avoid transactions from a specific public key. This
could have the effect of slowing processing of certain
transactions. However, such a situation isn't likely to arise because
agents have an incentive to create blocks and so they compete ---
rather than collude --- with each other.
Colluding
agents, with massive computing power, may help to deny or slow service
to a public key but not to another agent because agents can create
public keys at will.Further ReadingThere are many issues that we have not covered. This material only
covers the basics from the point of view of distributed algorithms.ReviewWhat is ablock chain?How does the algorithm deal with the situation in which one agent
receives a message to extend its copy of the block chain with a block
\(X\) while another agent receives a message to extend its copy of the
  block chain with a different block \(Y\)?Could there be an infinite sequence of collisions? Is that likely?What is mining for cryptocurrency? Suppose only a small number (say
  two organizations) mined most (say 99%) of Bitcoins. Would that be a
  problem?Why isdouble spendingunlikely to be successful?K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

Because the agent gets paid! Payment
is from either ablock rewardor transaction fees.Block rewardsAn agent that creates a block gets a specified number of Bitcoins for
itself as a reward called ablock reward.
The Bitcoins in a block reward are created by making the
block; these Bitcoins don't exist until the block is created. The process of making
blocks and acquiring block rewards is called "mining."Mining is the only way of creating new Bitcoins.When Bitcoin started the reward for creating a block was 50 Bitcoins.
The reward
halves after the creation of a certain number (210,000) of blocks. The
reward was reduced to 25 in 2013 and to 12.5 in 2018.
Block rewards will vanish at some point in the future.
The
total number of Bitcoins that can ever be created has an upper bound:
about 21 million.(Bitcoins can be lost. An agent may lose the hash pointer to the
transaction that gave the agent ownership of the coin, or an agent may
lose its private key.)Even when block rewards vanish, miners will continue to
mine provided that they get paid transaction fees. A
transaction fee is a payment by payers and payees to
miners. Transaction fees are voluntary. A high-fee for a
transaction is an incentive to miners to put this transaction into a
block quickly. So, it's possible that agents that offer no fee or low
fees may have to wait longer for their transactions to enter the
block chain.Incentives are critical for Bitcoin. Miners get paid to
get their blocks into the long-term "consensus" block chain.
Miners have an incentive to police the block chain because 
they don't get paid for appending erroneous blocks. If a miner appends
an erroneous block to the chain then other
miners won't extend chains containing the erroneous block, and so the
erroneous block will become an orphan.Any agent can check whether its copy of the block chain is
valid; however, agents making ordinary transactions don't
need to do so because there are many miners each of whom has an
incentive to ensure that the block chain is legitimate.AttacksNext, looks look at challenge number 4.Stealing coinsCan an agent steal a coin from an agent \(X\) by appending a block to
the chain where the block contains a transaction in which \(X\) gives
coins to \(Y\)?No, this can't happen thanks to cryptography. A transaction into which
\(X\) puts coins is valid only if \(X\) signs the transaction.  \(Y\)
cannot forge \(X\)'s signature, and so \(Y\) cannot create blocks that
contain such fraudulent transactions.Double spendCan an agent spend the same coin twice? Can an agent buy something
without paying for it?Consider the following transaction using conventional checks issued by
banks. A buyer gives a seller a check for $\(100,000\) for a house.
The house is put in escrow.  When the check clears and the seller
receives the payment the buyer gets possession of the house. The legal
process that includes notaries, real estate agents, and banks, helps
ensure that the transaction concludes correctly or is aborted correctly.Next, let's look at a transaction in which a person buys a video
online by paying the seller Bitcoins. The buyer and seller know each
other by their public keys and by their online addresses. 
The buyer broadcasts the transaction in which the buyer gives the seller
the payment in Bitcoins.
The amount is specified as
a pair (transaction id, array index) described in the sectionpay
transactionsinthe module introducing crypto currencies.A miner puts the transaction into a block \(X\);
appends the block to its copy \(L\) of the block chain; and broadcasts the extended
block chain \(L + X\).
When the seller gets a copy of the block chain \(L + X\), the
seller concludes that it has received the payment from the
buyer because the 
transaction has been recorded in a block chain. So the seller gives
the video to the buyer.The buyer cheats. The buyer creates a transaction in which the buyer
transfers the same Bitcoins  to the buyer itself.
A miner creates a block \(Y\) that includes this transaction.
A miner who has only received block
chain \(L\) (and hasn't yet received chain \(L + X\)) appends \(Y\) to
\(L\) to get a chain \(L + Y\), and broacasts \(L + Y\).Now we have a situation in which one
miner broadcasts a legitimate block chain \(L + X\) and a different
miner broadcasts a legitimate block chain \(L + Y\). Both chains have
the same length.
A miner with chain \(L + X\) does not know at this point that another
minder has chain \(L + Y\). So, miners will extend both block chains.
(Note that the algorithm will not permit chains \(L + X + Y\) or \(L +
Y + X\).)We've seen this situation before:  look atblock collisionsdescribed
earlier in this module. Both block chains will be extended, but
eventually, with very high probability, one of the blocks \(X\) or
\(Y\) will drop out of chain.How should sellers protect themselves?What is the equivalent of the buyer's check clearing in the bank?A seller should give the item to the buyer only after the transaction
appears with high confidence in a block in the chain --- see Figure 1.
The seller listens to block chains broadcast by miners. If the seller
gets a block chain \(L + X + L'\) where \(L'\) is itself a long block chain
then the seller has high confidence that the transaction is in the
permanent record.The seller waits to get block chains in which its transaction appears
in a block which is then followed by \(m\) blocks, for large
\(m\). The larger the value of \(m\), the greater the seller's
confidence, but the longer the buyer has to wait to get paid.
length of the extension \(L'\).A value of \(m = 6\) gives adequate confidence in most cases.Fraudulent minersA miner gets paid for every block the miner creates; so, why shouldn't
the miner create fraudulent blocks and get paid for them?The answer is the same as that for the double-spending attack. A miner
may have 
created a block and appended it to the chain; however, a suspicious
agent (and we hope that all agents are suspicious!) will not accept
the block until many blocks have been appended to the chain after
it. Other miners won't append their blocks to an invalid one. The
invalid blocks will become permanent orphans, and so the fraudulent
miner won't be able to spend the coins in these blocks.Fifty One Percent AttacksA 51% attack can be carried out by an agent that has more mining
power (e.g. 51% or more) than all other agents combined. The higher
the proportion of mining power of a single agent, the greater the
chances that attacks by that agent will succeed. You can see the
danger of a single agent having predominant mining power by thinking
about an agent with say, 99% of the total mining power. This agent
can mine so much faster than others that it can manipulate the block
chain in many ways. It can create double-spend transactions and deny
services to some transactions.A group of miners can collude to gain predominant mining power. Also,
miners canrentCloud-based systems for mining --- as opposed to having toownhuge data centers. An attacker can rent a large system for the specific purpose and
duration of an attack. The public may not know if and when such
attacks are successful becausecryptocurrencies do not have an
incentive to publish such attacks.Denial of serviceCan agents collude so that an agent \(X\)'s transactions never get
into blocks and so never get processed?An agent's identity does not appear in a transaction, only a public
key does. An agent can create new public keys at will. So let's ask
another question: can agents collude so that transactions with a
specific public key do not get into blocks?Only an agent that can solve puzzles in reasonable time can make
blocks. These agents have significant computational power. One can
concoct a situation where many agents with significant computation
power collude to avoid transactions from a specific public key. This
could have the effect of slowing processing of certain
transactions. However, such a situation isn't likely to arise because
agents have an incentive to create blocks and so they compete ---
rather than collude --- with each other.
Colluding
agents, with massive computing power, may help to deny or slow service
to a public key but not to another agent because agents can create
public keys at will.Further ReadingThere are many issues that we have not covered. This material only
covers the basics from the point of view of distributed algorithms.ReviewWhat is ablock chain?How does the algorithm deal with the situation in which one agent
receives a message to extend its copy of the block chain with a block
\(X\) while another agent receives a message to extend its copy of the
  block chain with a different block \(Y\)?Could there be an infinite sequence of collisions? Is that likely?What is mining for cryptocurrency? Suppose only a small number (say
  two organizations) mined most (say 99%) of Bitcoins. Would that be a
  problem?Why isdouble spendingunlikely to be successful?K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

Block rewards

When Bitcoin started the reward for creating a block was 50 Bitcoins.
The reward
halves after the creation of a certain number (210,000) of blocks. The
reward was reduced to 25 in 2013 and to 12.5 in 2018.
Block rewards will vanish at some point in the future.
The
total number of Bitcoins that can ever be created has an upper bound:
about 21 million.(Bitcoins can be lost. An agent may lose the hash pointer to the
transaction that gave the agent ownership of the coin, or an agent may
lose its private key.)Even when block rewards vanish, miners will continue to
mine provided that they get paid transaction fees. A
transaction fee is a payment by payers and payees to
miners. Transaction fees are voluntary. A high-fee for a
transaction is an incentive to miners to put this transaction into a
block quickly. So, it's possible that agents that offer no fee or low
fees may have to wait longer for their transactions to enter the
block chain.Incentives are critical for Bitcoin. Miners get paid to
get their blocks into the long-term "consensus" block chain.
Miners have an incentive to police the block chain because 
they don't get paid for appending erroneous blocks. If a miner appends
an erroneous block to the chain then other
miners won't extend chains containing the erroneous block, and so the
erroneous block will become an orphan.Any agent can check whether its copy of the block chain is
valid; however, agents making ordinary transactions don't
need to do so because there are many miners each of whom has an
incentive to ensure that the block chain is legitimate.AttacksNext, looks look at challenge number 4.Stealing coinsCan an agent steal a coin from an agent \(X\) by appending a block to
the chain where the block contains a transaction in which \(X\) gives
coins to \(Y\)?No, this can't happen thanks to cryptography. A transaction into which
\(X\) puts coins is valid only if \(X\) signs the transaction.  \(Y\)
cannot forge \(X\)'s signature, and so \(Y\) cannot create blocks that
contain such fraudulent transactions.Double spendCan an agent spend the same coin twice? Can an agent buy something
without paying for it?Consider the following transaction using conventional checks issued by
banks. A buyer gives a seller a check for $\(100,000\) for a house.
The house is put in escrow.  When the check clears and the seller
receives the payment the buyer gets possession of the house. The legal
process that includes notaries, real estate agents, and banks, helps
ensure that the transaction concludes correctly or is aborted correctly.Next, let's look at a transaction in which a person buys a video
online by paying the seller Bitcoins. The buyer and seller know each
other by their public keys and by their online addresses. 
The buyer broadcasts the transaction in which the buyer gives the seller
the payment in Bitcoins.
The amount is specified as
a pair (transaction id, array index) described in the sectionpay
transactionsinthe module introducing crypto currencies.A miner puts the transaction into a block \(X\);
appends the block to its copy \(L\) of the block chain; and broadcasts the extended
block chain \(L + X\).
When the seller gets a copy of the block chain \(L + X\), the
seller concludes that it has received the payment from the
buyer because the 
transaction has been recorded in a block chain. So the seller gives
the video to the buyer.The buyer cheats. The buyer creates a transaction in which the buyer
transfers the same Bitcoins  to the buyer itself.
A miner creates a block \(Y\) that includes this transaction.
A miner who has only received block
chain \(L\) (and hasn't yet received chain \(L + X\)) appends \(Y\) to
\(L\) to get a chain \(L + Y\), and broacasts \(L + Y\).Now we have a situation in which one
miner broadcasts a legitimate block chain \(L + X\) and a different
miner broadcasts a legitimate block chain \(L + Y\). Both chains have
the same length.
A miner with chain \(L + X\) does not know at this point that another
minder has chain \(L + Y\). So, miners will extend both block chains.
(Note that the algorithm will not permit chains \(L + X + Y\) or \(L +
Y + X\).)We've seen this situation before:  look atblock collisionsdescribed
earlier in this module. Both block chains will be extended, but
eventually, with very high probability, one of the blocks \(X\) or
\(Y\) will drop out of chain.How should sellers protect themselves?What is the equivalent of the buyer's check clearing in the bank?A seller should give the item to the buyer only after the transaction
appears with high confidence in a block in the chain --- see Figure 1.
The seller listens to block chains broadcast by miners. If the seller
gets a block chain \(L + X + L'\) where \(L'\) is itself a long block chain
then the seller has high confidence that the transaction is in the
permanent record.The seller waits to get block chains in which its transaction appears
in a block which is then followed by \(m\) blocks, for large
\(m\). The larger the value of \(m\), the greater the seller's
confidence, but the longer the buyer has to wait to get paid.
length of the extension \(L'\).A value of \(m = 6\) gives adequate confidence in most cases.Fraudulent minersA miner gets paid for every block the miner creates; so, why shouldn't
the miner create fraudulent blocks and get paid for them?The answer is the same as that for the double-spending attack. A miner
may have 
created a block and appended it to the chain; however, a suspicious
agent (and we hope that all agents are suspicious!) will not accept
the block until many blocks have been appended to the chain after
it. Other miners won't append their blocks to an invalid one. The
invalid blocks will become permanent orphans, and so the fraudulent
miner won't be able to spend the coins in these blocks.Fifty One Percent AttacksA 51% attack can be carried out by an agent that has more mining
power (e.g. 51% or more) than all other agents combined. The higher
the proportion of mining power of a single agent, the greater the
chances that attacks by that agent will succeed. You can see the
danger of a single agent having predominant mining power by thinking
about an agent with say, 99% of the total mining power. This agent
can mine so much faster than others that it can manipulate the block
chain in many ways. It can create double-spend transactions and deny
services to some transactions.A group of miners can collude to gain predominant mining power. Also,
miners canrentCloud-based systems for mining --- as opposed to having toownhuge data centers. An attacker can rent a large system for the specific purpose and
duration of an attack. The public may not know if and when such
attacks are successful becausecryptocurrencies do not have an
incentive to publish such attacks.Denial of serviceCan agents collude so that an agent \(X\)'s transactions never get
into blocks and so never get processed?An agent's identity does not appear in a transaction, only a public
key does. An agent can create new public keys at will. So let's ask
another question: can agents collude so that transactions with a
specific public key do not get into blocks?Only an agent that can solve puzzles in reasonable time can make
blocks. These agents have significant computational power. One can
concoct a situation where many agents with significant computation
power collude to avoid transactions from a specific public key. This
could have the effect of slowing processing of certain
transactions. However, such a situation isn't likely to arise because
agents have an incentive to create blocks and so they compete ---
rather than collude --- with each other.
Colluding
agents, with massive computing power, may help to deny or slow service
to a public key but not to another agent because agents can create
public keys at will.Further ReadingThere are many issues that we have not covered. This material only
covers the basics from the point of view of distributed algorithms.ReviewWhat is ablock chain?How does the algorithm deal with the situation in which one agent
receives a message to extend its copy of the block chain with a block
\(X\) while another agent receives a message to extend its copy of the
  block chain with a different block \(Y\)?Could there be an infinite sequence of collisions? Is that likely?What is mining for cryptocurrency? Suppose only a small number (say
  two organizations) mined most (say 99%) of Bitcoins. Would that be a
  problem?Why isdouble spendingunlikely to be successful?K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

(Bitcoins can be lost. An agent may lose the hash pointer to the
transaction that gave the agent ownership of the coin, or an agent may
lose its private key.)Even when block rewards vanish, miners will continue to
mine provided that they get paid transaction fees. A
transaction fee is a payment by payers and payees to
miners. Transaction fees are voluntary. A high-fee for a
transaction is an incentive to miners to put this transaction into a
block quickly. So, it's possible that agents that offer no fee or low
fees may have to wait longer for their transactions to enter the
block chain.Incentives are critical for Bitcoin. Miners get paid to
get their blocks into the long-term "consensus" block chain.
Miners have an incentive to police the block chain because 
they don't get paid for appending erroneous blocks. If a miner appends
an erroneous block to the chain then other
miners won't extend chains containing the erroneous block, and so the
erroneous block will become an orphan.Any agent can check whether its copy of the block chain is
valid; however, agents making ordinary transactions don't
need to do so because there are many miners each of whom has an
incentive to ensure that the block chain is legitimate.AttacksNext, looks look at challenge number 4.Stealing coinsCan an agent steal a coin from an agent \(X\) by appending a block to
the chain where the block contains a transaction in which \(X\) gives
coins to \(Y\)?No, this can't happen thanks to cryptography. A transaction into which
\(X\) puts coins is valid only if \(X\) signs the transaction.  \(Y\)
cannot forge \(X\)'s signature, and so \(Y\) cannot create blocks that
contain such fraudulent transactions.Double spendCan an agent spend the same coin twice? Can an agent buy something
without paying for it?Consider the following transaction using conventional checks issued by
banks. A buyer gives a seller a check for $\(100,000\) for a house.
The house is put in escrow.  When the check clears and the seller
receives the payment the buyer gets possession of the house. The legal
process that includes notaries, real estate agents, and banks, helps
ensure that the transaction concludes correctly or is aborted correctly.Next, let's look at a transaction in which a person buys a video
online by paying the seller Bitcoins. The buyer and seller know each
other by their public keys and by their online addresses. 
The buyer broadcasts the transaction in which the buyer gives the seller
the payment in Bitcoins.
The amount is specified as
a pair (transaction id, array index) described in the sectionpay
transactionsinthe module introducing crypto currencies.A miner puts the transaction into a block \(X\);
appends the block to its copy \(L\) of the block chain; and broadcasts the extended
block chain \(L + X\).
When the seller gets a copy of the block chain \(L + X\), the
seller concludes that it has received the payment from the
buyer because the 
transaction has been recorded in a block chain. So the seller gives
the video to the buyer.The buyer cheats. The buyer creates a transaction in which the buyer
transfers the same Bitcoins  to the buyer itself.
A miner creates a block \(Y\) that includes this transaction.
A miner who has only received block
chain \(L\) (and hasn't yet received chain \(L + X\)) appends \(Y\) to
\(L\) to get a chain \(L + Y\), and broacasts \(L + Y\).Now we have a situation in which one
miner broadcasts a legitimate block chain \(L + X\) and a different
miner broadcasts a legitimate block chain \(L + Y\). Both chains have
the same length.
A miner with chain \(L + X\) does not know at this point that another
minder has chain \(L + Y\). So, miners will extend both block chains.
(Note that the algorithm will not permit chains \(L + X + Y\) or \(L +
Y + X\).)We've seen this situation before:  look atblock collisionsdescribed
earlier in this module. Both block chains will be extended, but
eventually, with very high probability, one of the blocks \(X\) or
\(Y\) will drop out of chain.How should sellers protect themselves?What is the equivalent of the buyer's check clearing in the bank?A seller should give the item to the buyer only after the transaction
appears with high confidence in a block in the chain --- see Figure 1.
The seller listens to block chains broadcast by miners. If the seller
gets a block chain \(L + X + L'\) where \(L'\) is itself a long block chain
then the seller has high confidence that the transaction is in the
permanent record.The seller waits to get block chains in which its transaction appears
in a block which is then followed by \(m\) blocks, for large
\(m\). The larger the value of \(m\), the greater the seller's
confidence, but the longer the buyer has to wait to get paid.
length of the extension \(L'\).A value of \(m = 6\) gives adequate confidence in most cases.Fraudulent minersA miner gets paid for every block the miner creates; so, why shouldn't
the miner create fraudulent blocks and get paid for them?The answer is the same as that for the double-spending attack. A miner
may have 
created a block and appended it to the chain; however, a suspicious
agent (and we hope that all agents are suspicious!) will not accept
the block until many blocks have been appended to the chain after
it. Other miners won't append their blocks to an invalid one. The
invalid blocks will become permanent orphans, and so the fraudulent
miner won't be able to spend the coins in these blocks.Fifty One Percent AttacksA 51% attack can be carried out by an agent that has more mining
power (e.g. 51% or more) than all other agents combined. The higher
the proportion of mining power of a single agent, the greater the
chances that attacks by that agent will succeed. You can see the
danger of a single agent having predominant mining power by thinking
about an agent with say, 99% of the total mining power. This agent
can mine so much faster than others that it can manipulate the block
chain in many ways. It can create double-spend transactions and deny
services to some transactions.A group of miners can collude to gain predominant mining power. Also,
miners canrentCloud-based systems for mining --- as opposed to having toownhuge data centers. An attacker can rent a large system for the specific purpose and
duration of an attack. The public may not know if and when such
attacks are successful becausecryptocurrencies do not have an
incentive to publish such attacks.Denial of serviceCan agents collude so that an agent \(X\)'s transactions never get
into blocks and so never get processed?An agent's identity does not appear in a transaction, only a public
key does. An agent can create new public keys at will. So let's ask
another question: can agents collude so that transactions with a
specific public key do not get into blocks?Only an agent that can solve puzzles in reasonable time can make
blocks. These agents have significant computational power. One can
concoct a situation where many agents with significant computation
power collude to avoid transactions from a specific public key. This
could have the effect of slowing processing of certain
transactions. However, such a situation isn't likely to arise because
agents have an incentive to create blocks and so they compete ---
rather than collude --- with each other.
Colluding
agents, with massive computing power, may help to deny or slow service
to a public key but not to another agent because agents can create
public keys at will.Further ReadingThere are many issues that we have not covered. This material only
covers the basics from the point of view of distributed algorithms.ReviewWhat is ablock chain?How does the algorithm deal with the situation in which one agent
receives a message to extend its copy of the block chain with a block
\(X\) while another agent receives a message to extend its copy of the
  block chain with a different block \(Y\)?Could there be an infinite sequence of collisions? Is that likely?What is mining for cryptocurrency? Suppose only a small number (say
  two organizations) mined most (say 99%) of Bitcoins. Would that be a
  problem?Why isdouble spendingunlikely to be successful?K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

Even when block rewards vanish, miners will continue to
mine provided that they get paid transaction fees. A
transaction fee is a payment by payers and payees to
miners. Transaction fees are voluntary. A high-fee for a
transaction is an incentive to miners to put this transaction into a
block quickly. So, it's possible that agents that offer no fee or low
fees may have to wait longer for their transactions to enter the
block chain.Incentives are critical for Bitcoin. Miners get paid to
get their blocks into the long-term "consensus" block chain.
Miners have an incentive to police the block chain because 
they don't get paid for appending erroneous blocks. If a miner appends
an erroneous block to the chain then other
miners won't extend chains containing the erroneous block, and so the
erroneous block will become an orphan.Any agent can check whether its copy of the block chain is
valid; however, agents making ordinary transactions don't
need to do so because there are many miners each of whom has an
incentive to ensure that the block chain is legitimate.AttacksNext, looks look at challenge number 4.Stealing coinsCan an agent steal a coin from an agent \(X\) by appending a block to
the chain where the block contains a transaction in which \(X\) gives
coins to \(Y\)?No, this can't happen thanks to cryptography. A transaction into which
\(X\) puts coins is valid only if \(X\) signs the transaction.  \(Y\)
cannot forge \(X\)'s signature, and so \(Y\) cannot create blocks that
contain such fraudulent transactions.Double spendCan an agent spend the same coin twice? Can an agent buy something
without paying for it?Consider the following transaction using conventional checks issued by
banks. A buyer gives a seller a check for $\(100,000\) for a house.
The house is put in escrow.  When the check clears and the seller
receives the payment the buyer gets possession of the house. The legal
process that includes notaries, real estate agents, and banks, helps
ensure that the transaction concludes correctly or is aborted correctly.Next, let's look at a transaction in which a person buys a video
online by paying the seller Bitcoins. The buyer and seller know each
other by their public keys and by their online addresses. 
The buyer broadcasts the transaction in which the buyer gives the seller
the payment in Bitcoins.
The amount is specified as
a pair (transaction id, array index) described in the sectionpay
transactionsinthe module introducing crypto currencies.A miner puts the transaction into a block \(X\);
appends the block to its copy \(L\) of the block chain; and broadcasts the extended
block chain \(L + X\).
When the seller gets a copy of the block chain \(L + X\), the
seller concludes that it has received the payment from the
buyer because the 
transaction has been recorded in a block chain. So the seller gives
the video to the buyer.The buyer cheats. The buyer creates a transaction in which the buyer
transfers the same Bitcoins  to the buyer itself.
A miner creates a block \(Y\) that includes this transaction.
A miner who has only received block
chain \(L\) (and hasn't yet received chain \(L + X\)) appends \(Y\) to
\(L\) to get a chain \(L + Y\), and broacasts \(L + Y\).Now we have a situation in which one
miner broadcasts a legitimate block chain \(L + X\) and a different
miner broadcasts a legitimate block chain \(L + Y\). Both chains have
the same length.
A miner with chain \(L + X\) does not know at this point that another
minder has chain \(L + Y\). So, miners will extend both block chains.
(Note that the algorithm will not permit chains \(L + X + Y\) or \(L +
Y + X\).)We've seen this situation before:  look atblock collisionsdescribed
earlier in this module. Both block chains will be extended, but
eventually, with very high probability, one of the blocks \(X\) or
\(Y\) will drop out of chain.How should sellers protect themselves?What is the equivalent of the buyer's check clearing in the bank?A seller should give the item to the buyer only after the transaction
appears with high confidence in a block in the chain --- see Figure 1.
The seller listens to block chains broadcast by miners. If the seller
gets a block chain \(L + X + L'\) where \(L'\) is itself a long block chain
then the seller has high confidence that the transaction is in the
permanent record.The seller waits to get block chains in which its transaction appears
in a block which is then followed by \(m\) blocks, for large
\(m\). The larger the value of \(m\), the greater the seller's
confidence, but the longer the buyer has to wait to get paid.
length of the extension \(L'\).A value of \(m = 6\) gives adequate confidence in most cases.Fraudulent minersA miner gets paid for every block the miner creates; so, why shouldn't
the miner create fraudulent blocks and get paid for them?The answer is the same as that for the double-spending attack. A miner
may have 
created a block and appended it to the chain; however, a suspicious
agent (and we hope that all agents are suspicious!) will not accept
the block until many blocks have been appended to the chain after
it. Other miners won't append their blocks to an invalid one. The
invalid blocks will become permanent orphans, and so the fraudulent
miner won't be able to spend the coins in these blocks.Fifty One Percent AttacksA 51% attack can be carried out by an agent that has more mining
power (e.g. 51% or more) than all other agents combined. The higher
the proportion of mining power of a single agent, the greater the
chances that attacks by that agent will succeed. You can see the
danger of a single agent having predominant mining power by thinking
about an agent with say, 99% of the total mining power. This agent
can mine so much faster than others that it can manipulate the block
chain in many ways. It can create double-spend transactions and deny
services to some transactions.A group of miners can collude to gain predominant mining power. Also,
miners canrentCloud-based systems for mining --- as opposed to having toownhuge data centers. An attacker can rent a large system for the specific purpose and
duration of an attack. The public may not know if and when such
attacks are successful becausecryptocurrencies do not have an
incentive to publish such attacks.Denial of serviceCan agents collude so that an agent \(X\)'s transactions never get
into blocks and so never get processed?An agent's identity does not appear in a transaction, only a public
key does. An agent can create new public keys at will. So let's ask
another question: can agents collude so that transactions with a
specific public key do not get into blocks?Only an agent that can solve puzzles in reasonable time can make
blocks. These agents have significant computational power. One can
concoct a situation where many agents with significant computation
power collude to avoid transactions from a specific public key. This
could have the effect of slowing processing of certain
transactions. However, such a situation isn't likely to arise because
agents have an incentive to create blocks and so they compete ---
rather than collude --- with each other.
Colluding
agents, with massive computing power, may help to deny or slow service
to a public key but not to another agent because agents can create
public keys at will.Further ReadingThere are many issues that we have not covered. This material only
covers the basics from the point of view of distributed algorithms.ReviewWhat is ablock chain?How does the algorithm deal with the situation in which one agent
receives a message to extend its copy of the block chain with a block
\(X\) while another agent receives a message to extend its copy of the
  block chain with a different block \(Y\)?Could there be an infinite sequence of collisions? Is that likely?What is mining for cryptocurrency? Suppose only a small number (say
  two organizations) mined most (say 99%) of Bitcoins. Would that be a
  problem?Why isdouble spendingunlikely to be successful?K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

Incentives are critical for Bitcoin. Miners get paid to
get their blocks into the long-term "consensus" block chain.
Miners have an incentive to police the block chain because 
they don't get paid for appending erroneous blocks. If a miner appends
an erroneous block to the chain then other
miners won't extend chains containing the erroneous block, and so the
erroneous block will become an orphan.Any agent can check whether its copy of the block chain is
valid; however, agents making ordinary transactions don't
need to do so because there are many miners each of whom has an
incentive to ensure that the block chain is legitimate.AttacksNext, looks look at challenge number 4.Stealing coinsCan an agent steal a coin from an agent \(X\) by appending a block to
the chain where the block contains a transaction in which \(X\) gives
coins to \(Y\)?No, this can't happen thanks to cryptography. A transaction into which
\(X\) puts coins is valid only if \(X\) signs the transaction.  \(Y\)
cannot forge \(X\)'s signature, and so \(Y\) cannot create blocks that
contain such fraudulent transactions.Double spendCan an agent spend the same coin twice? Can an agent buy something
without paying for it?Consider the following transaction using conventional checks issued by
banks. A buyer gives a seller a check for $\(100,000\) for a house.
The house is put in escrow.  When the check clears and the seller
receives the payment the buyer gets possession of the house. The legal
process that includes notaries, real estate agents, and banks, helps
ensure that the transaction concludes correctly or is aborted correctly.Next, let's look at a transaction in which a person buys a video
online by paying the seller Bitcoins. The buyer and seller know each
other by their public keys and by their online addresses. 
The buyer broadcasts the transaction in which the buyer gives the seller
the payment in Bitcoins.
The amount is specified as
a pair (transaction id, array index) described in the sectionpay
transactionsinthe module introducing crypto currencies.A miner puts the transaction into a block \(X\);
appends the block to its copy \(L\) of the block chain; and broadcasts the extended
block chain \(L + X\).
When the seller gets a copy of the block chain \(L + X\), the
seller concludes that it has received the payment from the
buyer because the 
transaction has been recorded in a block chain. So the seller gives
the video to the buyer.The buyer cheats. The buyer creates a transaction in which the buyer
transfers the same Bitcoins  to the buyer itself.
A miner creates a block \(Y\) that includes this transaction.
A miner who has only received block
chain \(L\) (and hasn't yet received chain \(L + X\)) appends \(Y\) to
\(L\) to get a chain \(L + Y\), and broacasts \(L + Y\).Now we have a situation in which one
miner broadcasts a legitimate block chain \(L + X\) and a different
miner broadcasts a legitimate block chain \(L + Y\). Both chains have
the same length.
A miner with chain \(L + X\) does not know at this point that another
minder has chain \(L + Y\). So, miners will extend both block chains.
(Note that the algorithm will not permit chains \(L + X + Y\) or \(L +
Y + X\).)We've seen this situation before:  look atblock collisionsdescribed
earlier in this module. Both block chains will be extended, but
eventually, with very high probability, one of the blocks \(X\) or
\(Y\) will drop out of chain.How should sellers protect themselves?What is the equivalent of the buyer's check clearing in the bank?A seller should give the item to the buyer only after the transaction
appears with high confidence in a block in the chain --- see Figure 1.
The seller listens to block chains broadcast by miners. If the seller
gets a block chain \(L + X + L'\) where \(L'\) is itself a long block chain
then the seller has high confidence that the transaction is in the
permanent record.The seller waits to get block chains in which its transaction appears
in a block which is then followed by \(m\) blocks, for large
\(m\). The larger the value of \(m\), the greater the seller's
confidence, but the longer the buyer has to wait to get paid.
length of the extension \(L'\).A value of \(m = 6\) gives adequate confidence in most cases.Fraudulent minersA miner gets paid for every block the miner creates; so, why shouldn't
the miner create fraudulent blocks and get paid for them?The answer is the same as that for the double-spending attack. A miner
may have 
created a block and appended it to the chain; however, a suspicious
agent (and we hope that all agents are suspicious!) will not accept
the block until many blocks have been appended to the chain after
it. Other miners won't append their blocks to an invalid one. The
invalid blocks will become permanent orphans, and so the fraudulent
miner won't be able to spend the coins in these blocks.Fifty One Percent AttacksA 51% attack can be carried out by an agent that has more mining
power (e.g. 51% or more) than all other agents combined. The higher
the proportion of mining power of a single agent, the greater the
chances that attacks by that agent will succeed. You can see the
danger of a single agent having predominant mining power by thinking
about an agent with say, 99% of the total mining power. This agent
can mine so much faster than others that it can manipulate the block
chain in many ways. It can create double-spend transactions and deny
services to some transactions.A group of miners can collude to gain predominant mining power. Also,
miners canrentCloud-based systems for mining --- as opposed to having toownhuge data centers. An attacker can rent a large system for the specific purpose and
duration of an attack. The public may not know if and when such
attacks are successful becausecryptocurrencies do not have an
incentive to publish such attacks.Denial of serviceCan agents collude so that an agent \(X\)'s transactions never get
into blocks and so never get processed?An agent's identity does not appear in a transaction, only a public
key does. An agent can create new public keys at will. So let's ask
another question: can agents collude so that transactions with a
specific public key do not get into blocks?Only an agent that can solve puzzles in reasonable time can make
blocks. These agents have significant computational power. One can
concoct a situation where many agents with significant computation
power collude to avoid transactions from a specific public key. This
could have the effect of slowing processing of certain
transactions. However, such a situation isn't likely to arise because
agents have an incentive to create blocks and so they compete ---
rather than collude --- with each other.
Colluding
agents, with massive computing power, may help to deny or slow service
to a public key but not to another agent because agents can create
public keys at will.Further ReadingThere are many issues that we have not covered. This material only
covers the basics from the point of view of distributed algorithms.ReviewWhat is ablock chain?How does the algorithm deal with the situation in which one agent
receives a message to extend its copy of the block chain with a block
\(X\) while another agent receives a message to extend its copy of the
  block chain with a different block \(Y\)?Could there be an infinite sequence of collisions? Is that likely?What is mining for cryptocurrency? Suppose only a small number (say
  two organizations) mined most (say 99%) of Bitcoins. Would that be a
  problem?Why isdouble spendingunlikely to be successful?K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

Any agent can check whether its copy of the block chain is
valid; however, agents making ordinary transactions don't
need to do so because there are many miners each of whom has an
incentive to ensure that the block chain is legitimate.AttacksNext, looks look at challenge number 4.Stealing coinsCan an agent steal a coin from an agent \(X\) by appending a block to
the chain where the block contains a transaction in which \(X\) gives
coins to \(Y\)?No, this can't happen thanks to cryptography. A transaction into which
\(X\) puts coins is valid only if \(X\) signs the transaction.  \(Y\)
cannot forge \(X\)'s signature, and so \(Y\) cannot create blocks that
contain such fraudulent transactions.Double spendCan an agent spend the same coin twice? Can an agent buy something
without paying for it?Consider the following transaction using conventional checks issued by
banks. A buyer gives a seller a check for $\(100,000\) for a house.
The house is put in escrow.  When the check clears and the seller
receives the payment the buyer gets possession of the house. The legal
process that includes notaries, real estate agents, and banks, helps
ensure that the transaction concludes correctly or is aborted correctly.Next, let's look at a transaction in which a person buys a video
online by paying the seller Bitcoins. The buyer and seller know each
other by their public keys and by their online addresses. 
The buyer broadcasts the transaction in which the buyer gives the seller
the payment in Bitcoins.
The amount is specified as
a pair (transaction id, array index) described in the sectionpay
transactionsinthe module introducing crypto currencies.A miner puts the transaction into a block \(X\);
appends the block to its copy \(L\) of the block chain; and broadcasts the extended
block chain \(L + X\).
When the seller gets a copy of the block chain \(L + X\), the
seller concludes that it has received the payment from the
buyer because the 
transaction has been recorded in a block chain. So the seller gives
the video to the buyer.The buyer cheats. The buyer creates a transaction in which the buyer
transfers the same Bitcoins  to the buyer itself.
A miner creates a block \(Y\) that includes this transaction.
A miner who has only received block
chain \(L\) (and hasn't yet received chain \(L + X\)) appends \(Y\) to
\(L\) to get a chain \(L + Y\), and broacasts \(L + Y\).Now we have a situation in which one
miner broadcasts a legitimate block chain \(L + X\) and a different
miner broadcasts a legitimate block chain \(L + Y\). Both chains have
the same length.
A miner with chain \(L + X\) does not know at this point that another
minder has chain \(L + Y\). So, miners will extend both block chains.
(Note that the algorithm will not permit chains \(L + X + Y\) or \(L +
Y + X\).)We've seen this situation before:  look atblock collisionsdescribed
earlier in this module. Both block chains will be extended, but
eventually, with very high probability, one of the blocks \(X\) or
\(Y\) will drop out of chain.How should sellers protect themselves?What is the equivalent of the buyer's check clearing in the bank?A seller should give the item to the buyer only after the transaction
appears with high confidence in a block in the chain --- see Figure 1.
The seller listens to block chains broadcast by miners. If the seller
gets a block chain \(L + X + L'\) where \(L'\) is itself a long block chain
then the seller has high confidence that the transaction is in the
permanent record.The seller waits to get block chains in which its transaction appears
in a block which is then followed by \(m\) blocks, for large
\(m\). The larger the value of \(m\), the greater the seller's
confidence, but the longer the buyer has to wait to get paid.
length of the extension \(L'\).A value of \(m = 6\) gives adequate confidence in most cases.Fraudulent minersA miner gets paid for every block the miner creates; so, why shouldn't
the miner create fraudulent blocks and get paid for them?The answer is the same as that for the double-spending attack. A miner
may have 
created a block and appended it to the chain; however, a suspicious
agent (and we hope that all agents are suspicious!) will not accept
the block until many blocks have been appended to the chain after
it. Other miners won't append their blocks to an invalid one. The
invalid blocks will become permanent orphans, and so the fraudulent
miner won't be able to spend the coins in these blocks.Fifty One Percent AttacksA 51% attack can be carried out by an agent that has more mining
power (e.g. 51% or more) than all other agents combined. The higher
the proportion of mining power of a single agent, the greater the
chances that attacks by that agent will succeed. You can see the
danger of a single agent having predominant mining power by thinking
about an agent with say, 99% of the total mining power. This agent
can mine so much faster than others that it can manipulate the block
chain in many ways. It can create double-spend transactions and deny
services to some transactions.A group of miners can collude to gain predominant mining power. Also,
miners canrentCloud-based systems for mining --- as opposed to having toownhuge data centers. An attacker can rent a large system for the specific purpose and
duration of an attack. The public may not know if and when such
attacks are successful becausecryptocurrencies do not have an
incentive to publish such attacks.Denial of serviceCan agents collude so that an agent \(X\)'s transactions never get
into blocks and so never get processed?An agent's identity does not appear in a transaction, only a public
key does. An agent can create new public keys at will. So let's ask
another question: can agents collude so that transactions with a
specific public key do not get into blocks?Only an agent that can solve puzzles in reasonable time can make
blocks. These agents have significant computational power. One can
concoct a situation where many agents with significant computation
power collude to avoid transactions from a specific public key. This
could have the effect of slowing processing of certain
transactions. However, such a situation isn't likely to arise because
agents have an incentive to create blocks and so they compete ---
rather than collude --- with each other.
Colluding
agents, with massive computing power, may help to deny or slow service
to a public key but not to another agent because agents can create
public keys at will.Further ReadingThere are many issues that we have not covered. This material only
covers the basics from the point of view of distributed algorithms.ReviewWhat is ablock chain?How does the algorithm deal with the situation in which one agent
receives a message to extend its copy of the block chain with a block
\(X\) while another agent receives a message to extend its copy of the
  block chain with a different block \(Y\)?Could there be an infinite sequence of collisions? Is that likely?What is mining for cryptocurrency? Suppose only a small number (say
  two organizations) mined most (say 99%) of Bitcoins. Would that be a
  problem?Why isdouble spendingunlikely to be successful?K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

No, this can't happen thanks to cryptography. A transaction into which
\(X\) puts coins is valid only if \(X\) signs the transaction.  \(Y\)
cannot forge \(X\)'s signature, and so \(Y\) cannot create blocks that
contain such fraudulent transactions.Double spendCan an agent spend the same coin twice? Can an agent buy something
without paying for it?Consider the following transaction using conventional checks issued by
banks. A buyer gives a seller a check for $\(100,000\) for a house.
The house is put in escrow.  When the check clears and the seller
receives the payment the buyer gets possession of the house. The legal
process that includes notaries, real estate agents, and banks, helps
ensure that the transaction concludes correctly or is aborted correctly.Next, let's look at a transaction in which a person buys a video
online by paying the seller Bitcoins. The buyer and seller know each
other by their public keys and by their online addresses. 
The buyer broadcasts the transaction in which the buyer gives the seller
the payment in Bitcoins.
The amount is specified as
a pair (transaction id, array index) described in the sectionpay
transactionsinthe module introducing crypto currencies.A miner puts the transaction into a block \(X\);
appends the block to its copy \(L\) of the block chain; and broadcasts the extended
block chain \(L + X\).
When the seller gets a copy of the block chain \(L + X\), the
seller concludes that it has received the payment from the
buyer because the 
transaction has been recorded in a block chain. So the seller gives
the video to the buyer.The buyer cheats. The buyer creates a transaction in which the buyer
transfers the same Bitcoins  to the buyer itself.
A miner creates a block \(Y\) that includes this transaction.
A miner who has only received block
chain \(L\) (and hasn't yet received chain \(L + X\)) appends \(Y\) to
\(L\) to get a chain \(L + Y\), and broacasts \(L + Y\).Now we have a situation in which one
miner broadcasts a legitimate block chain \(L + X\) and a different
miner broadcasts a legitimate block chain \(L + Y\). Both chains have
the same length.
A miner with chain \(L + X\) does not know at this point that another
minder has chain \(L + Y\). So, miners will extend both block chains.
(Note that the algorithm will not permit chains \(L + X + Y\) or \(L +
Y + X\).)We've seen this situation before:  look atblock collisionsdescribed
earlier in this module. Both block chains will be extended, but
eventually, with very high probability, one of the blocks \(X\) or
\(Y\) will drop out of chain.How should sellers protect themselves?What is the equivalent of the buyer's check clearing in the bank?A seller should give the item to the buyer only after the transaction
appears with high confidence in a block in the chain --- see Figure 1.
The seller listens to block chains broadcast by miners. If the seller
gets a block chain \(L + X + L'\) where \(L'\) is itself a long block chain
then the seller has high confidence that the transaction is in the
permanent record.The seller waits to get block chains in which its transaction appears
in a block which is then followed by \(m\) blocks, for large
\(m\). The larger the value of \(m\), the greater the seller's
confidence, but the longer the buyer has to wait to get paid.
length of the extension \(L'\).A value of \(m = 6\) gives adequate confidence in most cases.Fraudulent minersA miner gets paid for every block the miner creates; so, why shouldn't
the miner create fraudulent blocks and get paid for them?The answer is the same as that for the double-spending attack. A miner
may have 
created a block and appended it to the chain; however, a suspicious
agent (and we hope that all agents are suspicious!) will not accept
the block until many blocks have been appended to the chain after
it. Other miners won't append their blocks to an invalid one. The
invalid blocks will become permanent orphans, and so the fraudulent
miner won't be able to spend the coins in these blocks.Fifty One Percent AttacksA 51% attack can be carried out by an agent that has more mining
power (e.g. 51% or more) than all other agents combined. The higher
the proportion of mining power of a single agent, the greater the
chances that attacks by that agent will succeed. You can see the
danger of a single agent having predominant mining power by thinking
about an agent with say, 99% of the total mining power. This agent
can mine so much faster than others that it can manipulate the block
chain in many ways. It can create double-spend transactions and deny
services to some transactions.A group of miners can collude to gain predominant mining power. Also,
miners canrentCloud-based systems for mining --- as opposed to having toownhuge data centers. An attacker can rent a large system for the specific purpose and
duration of an attack. The public may not know if and when such
attacks are successful becausecryptocurrencies do not have an
incentive to publish such attacks.Denial of serviceCan agents collude so that an agent \(X\)'s transactions never get
into blocks and so never get processed?An agent's identity does not appear in a transaction, only a public
key does. An agent can create new public keys at will. So let's ask
another question: can agents collude so that transactions with a
specific public key do not get into blocks?Only an agent that can solve puzzles in reasonable time can make
blocks. These agents have significant computational power. One can
concoct a situation where many agents with significant computation
power collude to avoid transactions from a specific public key. This
could have the effect of slowing processing of certain
transactions. However, such a situation isn't likely to arise because
agents have an incentive to create blocks and so they compete ---
rather than collude --- with each other.
Colluding
agents, with massive computing power, may help to deny or slow service
to a public key but not to another agent because agents can create
public keys at will.Further ReadingThere are many issues that we have not covered. This material only
covers the basics from the point of view of distributed algorithms.ReviewWhat is ablock chain?How does the algorithm deal with the situation in which one agent
receives a message to extend its copy of the block chain with a block
\(X\) while another agent receives a message to extend its copy of the
  block chain with a different block \(Y\)?Could there be an infinite sequence of collisions? Is that likely?What is mining for cryptocurrency? Suppose only a small number (say
  two organizations) mined most (say 99%) of Bitcoins. Would that be a
  problem?Why isdouble spendingunlikely to be successful?K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

Consider the following transaction using conventional checks issued by
banks. A buyer gives a seller a check for $\(100,000\) for a house.
The house is put in escrow.  When the check clears and the seller
receives the payment the buyer gets possession of the house. The legal
process that includes notaries, real estate agents, and banks, helps
ensure that the transaction concludes correctly or is aborted correctly.Next, let's look at a transaction in which a person buys a video
online by paying the seller Bitcoins. The buyer and seller know each
other by their public keys and by their online addresses. 
The buyer broadcasts the transaction in which the buyer gives the seller
the payment in Bitcoins.
The amount is specified as
a pair (transaction id, array index) described in the sectionpay
transactionsinthe module introducing crypto currencies.A miner puts the transaction into a block \(X\);
appends the block to its copy \(L\) of the block chain; and broadcasts the extended
block chain \(L + X\).
When the seller gets a copy of the block chain \(L + X\), the
seller concludes that it has received the payment from the
buyer because the 
transaction has been recorded in a block chain. So the seller gives
the video to the buyer.The buyer cheats. The buyer creates a transaction in which the buyer
transfers the same Bitcoins  to the buyer itself.
A miner creates a block \(Y\) that includes this transaction.
A miner who has only received block
chain \(L\) (and hasn't yet received chain \(L + X\)) appends \(Y\) to
\(L\) to get a chain \(L + Y\), and broacasts \(L + Y\).Now we have a situation in which one
miner broadcasts a legitimate block chain \(L + X\) and a different
miner broadcasts a legitimate block chain \(L + Y\). Both chains have
the same length.
A miner with chain \(L + X\) does not know at this point that another
minder has chain \(L + Y\). So, miners will extend both block chains.
(Note that the algorithm will not permit chains \(L + X + Y\) or \(L +
Y + X\).)We've seen this situation before:  look atblock collisionsdescribed
earlier in this module. Both block chains will be extended, but
eventually, with very high probability, one of the blocks \(X\) or
\(Y\) will drop out of chain.How should sellers protect themselves?What is the equivalent of the buyer's check clearing in the bank?A seller should give the item to the buyer only after the transaction
appears with high confidence in a block in the chain --- see Figure 1.
The seller listens to block chains broadcast by miners. If the seller
gets a block chain \(L + X + L'\) where \(L'\) is itself a long block chain
then the seller has high confidence that the transaction is in the
permanent record.The seller waits to get block chains in which its transaction appears
in a block which is then followed by \(m\) blocks, for large
\(m\). The larger the value of \(m\), the greater the seller's
confidence, but the longer the buyer has to wait to get paid.
length of the extension \(L'\).A value of \(m = 6\) gives adequate confidence in most cases.Fraudulent minersA miner gets paid for every block the miner creates; so, why shouldn't
the miner create fraudulent blocks and get paid for them?The answer is the same as that for the double-spending attack. A miner
may have 
created a block and appended it to the chain; however, a suspicious
agent (and we hope that all agents are suspicious!) will not accept
the block until many blocks have been appended to the chain after
it. Other miners won't append their blocks to an invalid one. The
invalid blocks will become permanent orphans, and so the fraudulent
miner won't be able to spend the coins in these blocks.Fifty One Percent AttacksA 51% attack can be carried out by an agent that has more mining
power (e.g. 51% or more) than all other agents combined. The higher
the proportion of mining power of a single agent, the greater the
chances that attacks by that agent will succeed. You can see the
danger of a single agent having predominant mining power by thinking
about an agent with say, 99% of the total mining power. This agent
can mine so much faster than others that it can manipulate the block
chain in many ways. It can create double-spend transactions and deny
services to some transactions.A group of miners can collude to gain predominant mining power. Also,
miners canrentCloud-based systems for mining --- as opposed to having toownhuge data centers. An attacker can rent a large system for the specific purpose and
duration of an attack. The public may not know if and when such
attacks are successful becausecryptocurrencies do not have an
incentive to publish such attacks.Denial of serviceCan agents collude so that an agent \(X\)'s transactions never get
into blocks and so never get processed?An agent's identity does not appear in a transaction, only a public
key does. An agent can create new public keys at will. So let's ask
another question: can agents collude so that transactions with a
specific public key do not get into blocks?Only an agent that can solve puzzles in reasonable time can make
blocks. These agents have significant computational power. One can
concoct a situation where many agents with significant computation
power collude to avoid transactions from a specific public key. This
could have the effect of slowing processing of certain
transactions. However, such a situation isn't likely to arise because
agents have an incentive to create blocks and so they compete ---
rather than collude --- with each other.
Colluding
agents, with massive computing power, may help to deny or slow service
to a public key but not to another agent because agents can create
public keys at will.Further ReadingThere are many issues that we have not covered. This material only
covers the basics from the point of view of distributed algorithms.ReviewWhat is ablock chain?How does the algorithm deal with the situation in which one agent
receives a message to extend its copy of the block chain with a block
\(X\) while another agent receives a message to extend its copy of the
  block chain with a different block \(Y\)?Could there be an infinite sequence of collisions? Is that likely?What is mining for cryptocurrency? Suppose only a small number (say
  two organizations) mined most (say 99%) of Bitcoins. Would that be a
  problem?Why isdouble spendingunlikely to be successful?K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

Next, let's look at a transaction in which a person buys a video
online by paying the seller Bitcoins. The buyer and seller know each
other by their public keys and by their online addresses. 
The buyer broadcasts the transaction in which the buyer gives the seller
the payment in Bitcoins.
The amount is specified as
a pair (transaction id, array index) described in the sectionpay
transactionsinthe module introducing crypto currencies.A miner puts the transaction into a block \(X\);
appends the block to its copy \(L\) of the block chain; and broadcasts the extended
block chain \(L + X\).
When the seller gets a copy of the block chain \(L + X\), the
seller concludes that it has received the payment from the
buyer because the 
transaction has been recorded in a block chain. So the seller gives
the video to the buyer.The buyer cheats. The buyer creates a transaction in which the buyer
transfers the same Bitcoins  to the buyer itself.
A miner creates a block \(Y\) that includes this transaction.
A miner who has only received block
chain \(L\) (and hasn't yet received chain \(L + X\)) appends \(Y\) to
\(L\) to get a chain \(L + Y\), and broacasts \(L + Y\).Now we have a situation in which one
miner broadcasts a legitimate block chain \(L + X\) and a different
miner broadcasts a legitimate block chain \(L + Y\). Both chains have
the same length.
A miner with chain \(L + X\) does not know at this point that another
minder has chain \(L + Y\). So, miners will extend both block chains.
(Note that the algorithm will not permit chains \(L + X + Y\) or \(L +
Y + X\).)We've seen this situation before:  look atblock collisionsdescribed
earlier in this module. Both block chains will be extended, but
eventually, with very high probability, one of the blocks \(X\) or
\(Y\) will drop out of chain.How should sellers protect themselves?What is the equivalent of the buyer's check clearing in the bank?A seller should give the item to the buyer only after the transaction
appears with high confidence in a block in the chain --- see Figure 1.
The seller listens to block chains broadcast by miners. If the seller
gets a block chain \(L + X + L'\) where \(L'\) is itself a long block chain
then the seller has high confidence that the transaction is in the
permanent record.The seller waits to get block chains in which its transaction appears
in a block which is then followed by \(m\) blocks, for large
\(m\). The larger the value of \(m\), the greater the seller's
confidence, but the longer the buyer has to wait to get paid.
length of the extension \(L'\).A value of \(m = 6\) gives adequate confidence in most cases.Fraudulent minersA miner gets paid for every block the miner creates; so, why shouldn't
the miner create fraudulent blocks and get paid for them?The answer is the same as that for the double-spending attack. A miner
may have 
created a block and appended it to the chain; however, a suspicious
agent (and we hope that all agents are suspicious!) will not accept
the block until many blocks have been appended to the chain after
it. Other miners won't append their blocks to an invalid one. The
invalid blocks will become permanent orphans, and so the fraudulent
miner won't be able to spend the coins in these blocks.Fifty One Percent AttacksA 51% attack can be carried out by an agent that has more mining
power (e.g. 51% or more) than all other agents combined. The higher
the proportion of mining power of a single agent, the greater the
chances that attacks by that agent will succeed. You can see the
danger of a single agent having predominant mining power by thinking
about an agent with say, 99% of the total mining power. This agent
can mine so much faster than others that it can manipulate the block
chain in many ways. It can create double-spend transactions and deny
services to some transactions.A group of miners can collude to gain predominant mining power. Also,
miners canrentCloud-based systems for mining --- as opposed to having toownhuge data centers. An attacker can rent a large system for the specific purpose and
duration of an attack. The public may not know if and when such
attacks are successful becausecryptocurrencies do not have an
incentive to publish such attacks.Denial of serviceCan agents collude so that an agent \(X\)'s transactions never get
into blocks and so never get processed?An agent's identity does not appear in a transaction, only a public
key does. An agent can create new public keys at will. So let's ask
another question: can agents collude so that transactions with a
specific public key do not get into blocks?Only an agent that can solve puzzles in reasonable time can make
blocks. These agents have significant computational power. One can
concoct a situation where many agents with significant computation
power collude to avoid transactions from a specific public key. This
could have the effect of slowing processing of certain
transactions. However, such a situation isn't likely to arise because
agents have an incentive to create blocks and so they compete ---
rather than collude --- with each other.
Colluding
agents, with massive computing power, may help to deny or slow service
to a public key but not to another agent because agents can create
public keys at will.Further ReadingThere are many issues that we have not covered. This material only
covers the basics from the point of view of distributed algorithms.ReviewWhat is ablock chain?How does the algorithm deal with the situation in which one agent
receives a message to extend its copy of the block chain with a block
\(X\) while another agent receives a message to extend its copy of the
  block chain with a different block \(Y\)?Could there be an infinite sequence of collisions? Is that likely?What is mining for cryptocurrency? Suppose only a small number (say
  two organizations) mined most (say 99%) of Bitcoins. Would that be a
  problem?Why isdouble spendingunlikely to be successful?K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

A miner puts the transaction into a block \(X\);
appends the block to its copy \(L\) of the block chain; and broadcasts the extended
block chain \(L + X\).
When the seller gets a copy of the block chain \(L + X\), the
seller concludes that it has received the payment from the
buyer because the 
transaction has been recorded in a block chain. So the seller gives
the video to the buyer.The buyer cheats. The buyer creates a transaction in which the buyer
transfers the same Bitcoins  to the buyer itself.
A miner creates a block \(Y\) that includes this transaction.
A miner who has only received block
chain \(L\) (and hasn't yet received chain \(L + X\)) appends \(Y\) to
\(L\) to get a chain \(L + Y\), and broacasts \(L + Y\).Now we have a situation in which one
miner broadcasts a legitimate block chain \(L + X\) and a different
miner broadcasts a legitimate block chain \(L + Y\). Both chains have
the same length.
A miner with chain \(L + X\) does not know at this point that another
minder has chain \(L + Y\). So, miners will extend both block chains.
(Note that the algorithm will not permit chains \(L + X + Y\) or \(L +
Y + X\).)We've seen this situation before:  look atblock collisionsdescribed
earlier in this module. Both block chains will be extended, but
eventually, with very high probability, one of the blocks \(X\) or
\(Y\) will drop out of chain.How should sellers protect themselves?What is the equivalent of the buyer's check clearing in the bank?A seller should give the item to the buyer only after the transaction
appears with high confidence in a block in the chain --- see Figure 1.
The seller listens to block chains broadcast by miners. If the seller
gets a block chain \(L + X + L'\) where \(L'\) is itself a long block chain
then the seller has high confidence that the transaction is in the
permanent record.The seller waits to get block chains in which its transaction appears
in a block which is then followed by \(m\) blocks, for large
\(m\). The larger the value of \(m\), the greater the seller's
confidence, but the longer the buyer has to wait to get paid.
length of the extension \(L'\).A value of \(m = 6\) gives adequate confidence in most cases.Fraudulent minersA miner gets paid for every block the miner creates; so, why shouldn't
the miner create fraudulent blocks and get paid for them?The answer is the same as that for the double-spending attack. A miner
may have 
created a block and appended it to the chain; however, a suspicious
agent (and we hope that all agents are suspicious!) will not accept
the block until many blocks have been appended to the chain after
it. Other miners won't append their blocks to an invalid one. The
invalid blocks will become permanent orphans, and so the fraudulent
miner won't be able to spend the coins in these blocks.Fifty One Percent AttacksA 51% attack can be carried out by an agent that has more mining
power (e.g. 51% or more) than all other agents combined. The higher
the proportion of mining power of a single agent, the greater the
chances that attacks by that agent will succeed. You can see the
danger of a single agent having predominant mining power by thinking
about an agent with say, 99% of the total mining power. This agent
can mine so much faster than others that it can manipulate the block
chain in many ways. It can create double-spend transactions and deny
services to some transactions.A group of miners can collude to gain predominant mining power. Also,
miners canrentCloud-based systems for mining --- as opposed to having toownhuge data centers. An attacker can rent a large system for the specific purpose and
duration of an attack. The public may not know if and when such
attacks are successful becausecryptocurrencies do not have an
incentive to publish such attacks.Denial of serviceCan agents collude so that an agent \(X\)'s transactions never get
into blocks and so never get processed?An agent's identity does not appear in a transaction, only a public
key does. An agent can create new public keys at will. So let's ask
another question: can agents collude so that transactions with a
specific public key do not get into blocks?Only an agent that can solve puzzles in reasonable time can make
blocks. These agents have significant computational power. One can
concoct a situation where many agents with significant computation
power collude to avoid transactions from a specific public key. This
could have the effect of slowing processing of certain
transactions. However, such a situation isn't likely to arise because
agents have an incentive to create blocks and so they compete ---
rather than collude --- with each other.
Colluding
agents, with massive computing power, may help to deny or slow service
to a public key but not to another agent because agents can create
public keys at will.Further ReadingThere are many issues that we have not covered. This material only
covers the basics from the point of view of distributed algorithms.ReviewWhat is ablock chain?How does the algorithm deal with the situation in which one agent
receives a message to extend its copy of the block chain with a block
\(X\) while another agent receives a message to extend its copy of the
  block chain with a different block \(Y\)?Could there be an infinite sequence of collisions? Is that likely?What is mining for cryptocurrency? Suppose only a small number (say
  two organizations) mined most (say 99%) of Bitcoins. Would that be a
  problem?Why isdouble spendingunlikely to be successful?K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

The buyer cheats. The buyer creates a transaction in which the buyer
transfers the same Bitcoins  to the buyer itself.
A miner creates a block \(Y\) that includes this transaction.
A miner who has only received block
chain \(L\) (and hasn't yet received chain \(L + X\)) appends \(Y\) to
\(L\) to get a chain \(L + Y\), and broacasts \(L + Y\).Now we have a situation in which one
miner broadcasts a legitimate block chain \(L + X\) and a different
miner broadcasts a legitimate block chain \(L + Y\). Both chains have
the same length.
A miner with chain \(L + X\) does not know at this point that another
minder has chain \(L + Y\). So, miners will extend both block chains.
(Note that the algorithm will not permit chains \(L + X + Y\) or \(L +
Y + X\).)We've seen this situation before:  look atblock collisionsdescribed
earlier in this module. Both block chains will be extended, but
eventually, with very high probability, one of the blocks \(X\) or
\(Y\) will drop out of chain.How should sellers protect themselves?What is the equivalent of the buyer's check clearing in the bank?A seller should give the item to the buyer only after the transaction
appears with high confidence in a block in the chain --- see Figure 1.
The seller listens to block chains broadcast by miners. If the seller
gets a block chain \(L + X + L'\) where \(L'\) is itself a long block chain
then the seller has high confidence that the transaction is in the
permanent record.The seller waits to get block chains in which its transaction appears
in a block which is then followed by \(m\) blocks, for large
\(m\). The larger the value of \(m\), the greater the seller's
confidence, but the longer the buyer has to wait to get paid.
length of the extension \(L'\).A value of \(m = 6\) gives adequate confidence in most cases.Fraudulent minersA miner gets paid for every block the miner creates; so, why shouldn't
the miner create fraudulent blocks and get paid for them?The answer is the same as that for the double-spending attack. A miner
may have 
created a block and appended it to the chain; however, a suspicious
agent (and we hope that all agents are suspicious!) will not accept
the block until many blocks have been appended to the chain after
it. Other miners won't append their blocks to an invalid one. The
invalid blocks will become permanent orphans, and so the fraudulent
miner won't be able to spend the coins in these blocks.Fifty One Percent AttacksA 51% attack can be carried out by an agent that has more mining
power (e.g. 51% or more) than all other agents combined. The higher
the proportion of mining power of a single agent, the greater the
chances that attacks by that agent will succeed. You can see the
danger of a single agent having predominant mining power by thinking
about an agent with say, 99% of the total mining power. This agent
can mine so much faster than others that it can manipulate the block
chain in many ways. It can create double-spend transactions and deny
services to some transactions.A group of miners can collude to gain predominant mining power. Also,
miners canrentCloud-based systems for mining --- as opposed to having toownhuge data centers. An attacker can rent a large system for the specific purpose and
duration of an attack. The public may not know if and when such
attacks are successful becausecryptocurrencies do not have an
incentive to publish such attacks.Denial of serviceCan agents collude so that an agent \(X\)'s transactions never get
into blocks and so never get processed?An agent's identity does not appear in a transaction, only a public
key does. An agent can create new public keys at will. So let's ask
another question: can agents collude so that transactions with a
specific public key do not get into blocks?Only an agent that can solve puzzles in reasonable time can make
blocks. These agents have significant computational power. One can
concoct a situation where many agents with significant computation
power collude to avoid transactions from a specific public key. This
could have the effect of slowing processing of certain
transactions. However, such a situation isn't likely to arise because
agents have an incentive to create blocks and so they compete ---
rather than collude --- with each other.
Colluding
agents, with massive computing power, may help to deny or slow service
to a public key but not to another agent because agents can create
public keys at will.Further ReadingThere are many issues that we have not covered. This material only
covers the basics from the point of view of distributed algorithms.ReviewWhat is ablock chain?How does the algorithm deal with the situation in which one agent
receives a message to extend its copy of the block chain with a block
\(X\) while another agent receives a message to extend its copy of the
  block chain with a different block \(Y\)?Could there be an infinite sequence of collisions? Is that likely?What is mining for cryptocurrency? Suppose only a small number (say
  two organizations) mined most (say 99%) of Bitcoins. Would that be a
  problem?Why isdouble spendingunlikely to be successful?K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

Now we have a situation in which one
miner broadcasts a legitimate block chain \(L + X\) and a different
miner broadcasts a legitimate block chain \(L + Y\). Both chains have
the same length.
A miner with chain \(L + X\) does not know at this point that another
minder has chain \(L + Y\). So, miners will extend both block chains.
(Note that the algorithm will not permit chains \(L + X + Y\) or \(L +
Y + X\).)We've seen this situation before:  look atblock collisionsdescribed
earlier in this module. Both block chains will be extended, but
eventually, with very high probability, one of the blocks \(X\) or
\(Y\) will drop out of chain.How should sellers protect themselves?What is the equivalent of the buyer's check clearing in the bank?A seller should give the item to the buyer only after the transaction
appears with high confidence in a block in the chain --- see Figure 1.
The seller listens to block chains broadcast by miners. If the seller
gets a block chain \(L + X + L'\) where \(L'\) is itself a long block chain
then the seller has high confidence that the transaction is in the
permanent record.The seller waits to get block chains in which its transaction appears
in a block which is then followed by \(m\) blocks, for large
\(m\). The larger the value of \(m\), the greater the seller's
confidence, but the longer the buyer has to wait to get paid.
length of the extension \(L'\).A value of \(m = 6\) gives adequate confidence in most cases.Fraudulent minersA miner gets paid for every block the miner creates; so, why shouldn't
the miner create fraudulent blocks and get paid for them?The answer is the same as that for the double-spending attack. A miner
may have 
created a block and appended it to the chain; however, a suspicious
agent (and we hope that all agents are suspicious!) will not accept
the block until many blocks have been appended to the chain after
it. Other miners won't append their blocks to an invalid one. The
invalid blocks will become permanent orphans, and so the fraudulent
miner won't be able to spend the coins in these blocks.Fifty One Percent AttacksA 51% attack can be carried out by an agent that has more mining
power (e.g. 51% or more) than all other agents combined. The higher
the proportion of mining power of a single agent, the greater the
chances that attacks by that agent will succeed. You can see the
danger of a single agent having predominant mining power by thinking
about an agent with say, 99% of the total mining power. This agent
can mine so much faster than others that it can manipulate the block
chain in many ways. It can create double-spend transactions and deny
services to some transactions.A group of miners can collude to gain predominant mining power. Also,
miners canrentCloud-based systems for mining --- as opposed to having toownhuge data centers. An attacker can rent a large system for the specific purpose and
duration of an attack. The public may not know if and when such
attacks are successful becausecryptocurrencies do not have an
incentive to publish such attacks.Denial of serviceCan agents collude so that an agent \(X\)'s transactions never get
into blocks and so never get processed?An agent's identity does not appear in a transaction, only a public
key does. An agent can create new public keys at will. So let's ask
another question: can agents collude so that transactions with a
specific public key do not get into blocks?Only an agent that can solve puzzles in reasonable time can make
blocks. These agents have significant computational power. One can
concoct a situation where many agents with significant computation
power collude to avoid transactions from a specific public key. This
could have the effect of slowing processing of certain
transactions. However, such a situation isn't likely to arise because
agents have an incentive to create blocks and so they compete ---
rather than collude --- with each other.
Colluding
agents, with massive computing power, may help to deny or slow service
to a public key but not to another agent because agents can create
public keys at will.Further ReadingThere are many issues that we have not covered. This material only
covers the basics from the point of view of distributed algorithms.ReviewWhat is ablock chain?How does the algorithm deal with the situation in which one agent
receives a message to extend its copy of the block chain with a block
\(X\) while another agent receives a message to extend its copy of the
  block chain with a different block \(Y\)?Could there be an infinite sequence of collisions? Is that likely?What is mining for cryptocurrency? Suppose only a small number (say
  two organizations) mined most (say 99%) of Bitcoins. Would that be a
  problem?Why isdouble spendingunlikely to be successful?K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

We've seen this situation before:  look atblock collisionsdescribed
earlier in this module. Both block chains will be extended, but
eventually, with very high probability, one of the blocks \(X\) or
\(Y\) will drop out of chain.How should sellers protect themselves?What is the equivalent of the buyer's check clearing in the bank?A seller should give the item to the buyer only after the transaction
appears with high confidence in a block in the chain --- see Figure 1.
The seller listens to block chains broadcast by miners. If the seller
gets a block chain \(L + X + L'\) where \(L'\) is itself a long block chain
then the seller has high confidence that the transaction is in the
permanent record.The seller waits to get block chains in which its transaction appears
in a block which is then followed by \(m\) blocks, for large
\(m\). The larger the value of \(m\), the greater the seller's
confidence, but the longer the buyer has to wait to get paid.
length of the extension \(L'\).A value of \(m = 6\) gives adequate confidence in most cases.Fraudulent minersA miner gets paid for every block the miner creates; so, why shouldn't
the miner create fraudulent blocks and get paid for them?The answer is the same as that for the double-spending attack. A miner
may have 
created a block and appended it to the chain; however, a suspicious
agent (and we hope that all agents are suspicious!) will not accept
the block until many blocks have been appended to the chain after
it. Other miners won't append their blocks to an invalid one. The
invalid blocks will become permanent orphans, and so the fraudulent
miner won't be able to spend the coins in these blocks.Fifty One Percent AttacksA 51% attack can be carried out by an agent that has more mining
power (e.g. 51% or more) than all other agents combined. The higher
the proportion of mining power of a single agent, the greater the
chances that attacks by that agent will succeed. You can see the
danger of a single agent having predominant mining power by thinking
about an agent with say, 99% of the total mining power. This agent
can mine so much faster than others that it can manipulate the block
chain in many ways. It can create double-spend transactions and deny
services to some transactions.A group of miners can collude to gain predominant mining power. Also,
miners canrentCloud-based systems for mining --- as opposed to having toownhuge data centers. An attacker can rent a large system for the specific purpose and
duration of an attack. The public may not know if and when such
attacks are successful becausecryptocurrencies do not have an
incentive to publish such attacks.Denial of serviceCan agents collude so that an agent \(X\)'s transactions never get
into blocks and so never get processed?An agent's identity does not appear in a transaction, only a public
key does. An agent can create new public keys at will. So let's ask
another question: can agents collude so that transactions with a
specific public key do not get into blocks?Only an agent that can solve puzzles in reasonable time can make
blocks. These agents have significant computational power. One can
concoct a situation where many agents with significant computation
power collude to avoid transactions from a specific public key. This
could have the effect of slowing processing of certain
transactions. However, such a situation isn't likely to arise because
agents have an incentive to create blocks and so they compete ---
rather than collude --- with each other.
Colluding
agents, with massive computing power, may help to deny or slow service
to a public key but not to another agent because agents can create
public keys at will.Further ReadingThere are many issues that we have not covered. This material only
covers the basics from the point of view of distributed algorithms.ReviewWhat is ablock chain?How does the algorithm deal with the situation in which one agent
receives a message to extend its copy of the block chain with a block
\(X\) while another agent receives a message to extend its copy of the
  block chain with a different block \(Y\)?Could there be an infinite sequence of collisions? Is that likely?What is mining for cryptocurrency? Suppose only a small number (say
  two organizations) mined most (say 99%) of Bitcoins. Would that be a
  problem?Why isdouble spendingunlikely to be successful?K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

How should sellers protect themselves?

What is the equivalent of the buyer's check clearing in the bank?A seller should give the item to the buyer only after the transaction
appears with high confidence in a block in the chain --- see Figure 1.
The seller listens to block chains broadcast by miners. If the seller
gets a block chain \(L + X + L'\) where \(L'\) is itself a long block chain
then the seller has high confidence that the transaction is in the
permanent record.The seller waits to get block chains in which its transaction appears
in a block which is then followed by \(m\) blocks, for large
\(m\). The larger the value of \(m\), the greater the seller's
confidence, but the longer the buyer has to wait to get paid.
length of the extension \(L'\).A value of \(m = 6\) gives adequate confidence in most cases.Fraudulent minersA miner gets paid for every block the miner creates; so, why shouldn't
the miner create fraudulent blocks and get paid for them?The answer is the same as that for the double-spending attack. A miner
may have 
created a block and appended it to the chain; however, a suspicious
agent (and we hope that all agents are suspicious!) will not accept
the block until many blocks have been appended to the chain after
it. Other miners won't append their blocks to an invalid one. The
invalid blocks will become permanent orphans, and so the fraudulent
miner won't be able to spend the coins in these blocks.Fifty One Percent AttacksA 51% attack can be carried out by an agent that has more mining
power (e.g. 51% or more) than all other agents combined. The higher
the proportion of mining power of a single agent, the greater the
chances that attacks by that agent will succeed. You can see the
danger of a single agent having predominant mining power by thinking
about an agent with say, 99% of the total mining power. This agent
can mine so much faster than others that it can manipulate the block
chain in many ways. It can create double-spend transactions and deny
services to some transactions.A group of miners can collude to gain predominant mining power. Also,
miners canrentCloud-based systems for mining --- as opposed to having toownhuge data centers. An attacker can rent a large system for the specific purpose and
duration of an attack. The public may not know if and when such
attacks are successful becausecryptocurrencies do not have an
incentive to publish such attacks.Denial of serviceCan agents collude so that an agent \(X\)'s transactions never get
into blocks and so never get processed?An agent's identity does not appear in a transaction, only a public
key does. An agent can create new public keys at will. So let's ask
another question: can agents collude so that transactions with a
specific public key do not get into blocks?Only an agent that can solve puzzles in reasonable time can make
blocks. These agents have significant computational power. One can
concoct a situation where many agents with significant computation
power collude to avoid transactions from a specific public key. This
could have the effect of slowing processing of certain
transactions. However, such a situation isn't likely to arise because
agents have an incentive to create blocks and so they compete ---
rather than collude --- with each other.
Colluding
agents, with massive computing power, may help to deny or slow service
to a public key but not to another agent because agents can create
public keys at will.Further ReadingThere are many issues that we have not covered. This material only
covers the basics from the point of view of distributed algorithms.ReviewWhat is ablock chain?How does the algorithm deal with the situation in which one agent
receives a message to extend its copy of the block chain with a block
\(X\) while another agent receives a message to extend its copy of the
  block chain with a different block \(Y\)?Could there be an infinite sequence of collisions? Is that likely?What is mining for cryptocurrency? Suppose only a small number (say
  two organizations) mined most (say 99%) of Bitcoins. Would that be a
  problem?Why isdouble spendingunlikely to be successful?K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

A seller should give the item to the buyer only after the transaction
appears with high confidence in a block in the chain --- see Figure 1.
The seller listens to block chains broadcast by miners. If the seller
gets a block chain \(L + X + L'\) where \(L'\) is itself a long block chain
then the seller has high confidence that the transaction is in the
permanent record.The seller waits to get block chains in which its transaction appears
in a block which is then followed by \(m\) blocks, for large
\(m\). The larger the value of \(m\), the greater the seller's
confidence, but the longer the buyer has to wait to get paid.
length of the extension \(L'\).A value of \(m = 6\) gives adequate confidence in most cases.Fraudulent minersA miner gets paid for every block the miner creates; so, why shouldn't
the miner create fraudulent blocks and get paid for them?The answer is the same as that for the double-spending attack. A miner
may have 
created a block and appended it to the chain; however, a suspicious
agent (and we hope that all agents are suspicious!) will not accept
the block until many blocks have been appended to the chain after
it. Other miners won't append their blocks to an invalid one. The
invalid blocks will become permanent orphans, and so the fraudulent
miner won't be able to spend the coins in these blocks.Fifty One Percent AttacksA 51% attack can be carried out by an agent that has more mining
power (e.g. 51% or more) than all other agents combined. The higher
the proportion of mining power of a single agent, the greater the
chances that attacks by that agent will succeed. You can see the
danger of a single agent having predominant mining power by thinking
about an agent with say, 99% of the total mining power. This agent
can mine so much faster than others that it can manipulate the block
chain in many ways. It can create double-spend transactions and deny
services to some transactions.A group of miners can collude to gain predominant mining power. Also,
miners canrentCloud-based systems for mining --- as opposed to having toownhuge data centers. An attacker can rent a large system for the specific purpose and
duration of an attack. The public may not know if and when such
attacks are successful becausecryptocurrencies do not have an
incentive to publish such attacks.Denial of serviceCan agents collude so that an agent \(X\)'s transactions never get
into blocks and so never get processed?An agent's identity does not appear in a transaction, only a public
key does. An agent can create new public keys at will. So let's ask
another question: can agents collude so that transactions with a
specific public key do not get into blocks?Only an agent that can solve puzzles in reasonable time can make
blocks. These agents have significant computational power. One can
concoct a situation where many agents with significant computation
power collude to avoid transactions from a specific public key. This
could have the effect of slowing processing of certain
transactions. However, such a situation isn't likely to arise because
agents have an incentive to create blocks and so they compete ---
rather than collude --- with each other.
Colluding
agents, with massive computing power, may help to deny or slow service
to a public key but not to another agent because agents can create
public keys at will.Further ReadingThere are many issues that we have not covered. This material only
covers the basics from the point of view of distributed algorithms.ReviewWhat is ablock chain?How does the algorithm deal with the situation in which one agent
receives a message to extend its copy of the block chain with a block
\(X\) while another agent receives a message to extend its copy of the
  block chain with a different block \(Y\)?Could there be an infinite sequence of collisions? Is that likely?What is mining for cryptocurrency? Suppose only a small number (say
  two organizations) mined most (say 99%) of Bitcoins. Would that be a
  problem?Why isdouble spendingunlikely to be successful?K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

The seller waits to get block chains in which its transaction appears
in a block which is then followed by \(m\) blocks, for large
\(m\). The larger the value of \(m\), the greater the seller's
confidence, but the longer the buyer has to wait to get paid.
length of the extension \(L'\).A value of \(m = 6\) gives adequate confidence in most cases.Fraudulent minersA miner gets paid for every block the miner creates; so, why shouldn't
the miner create fraudulent blocks and get paid for them?The answer is the same as that for the double-spending attack. A miner
may have 
created a block and appended it to the chain; however, a suspicious
agent (and we hope that all agents are suspicious!) will not accept
the block until many blocks have been appended to the chain after
it. Other miners won't append their blocks to an invalid one. The
invalid blocks will become permanent orphans, and so the fraudulent
miner won't be able to spend the coins in these blocks.Fifty One Percent AttacksA 51% attack can be carried out by an agent that has more mining
power (e.g. 51% or more) than all other agents combined. The higher
the proportion of mining power of a single agent, the greater the
chances that attacks by that agent will succeed. You can see the
danger of a single agent having predominant mining power by thinking
about an agent with say, 99% of the total mining power. This agent
can mine so much faster than others that it can manipulate the block
chain in many ways. It can create double-spend transactions and deny
services to some transactions.A group of miners can collude to gain predominant mining power. Also,
miners canrentCloud-based systems for mining --- as opposed to having toownhuge data centers. An attacker can rent a large system for the specific purpose and
duration of an attack. The public may not know if and when such
attacks are successful becausecryptocurrencies do not have an
incentive to publish such attacks.Denial of serviceCan agents collude so that an agent \(X\)'s transactions never get
into blocks and so never get processed?An agent's identity does not appear in a transaction, only a public
key does. An agent can create new public keys at will. So let's ask
another question: can agents collude so that transactions with a
specific public key do not get into blocks?Only an agent that can solve puzzles in reasonable time can make
blocks. These agents have significant computational power. One can
concoct a situation where many agents with significant computation
power collude to avoid transactions from a specific public key. This
could have the effect of slowing processing of certain
transactions. However, such a situation isn't likely to arise because
agents have an incentive to create blocks and so they compete ---
rather than collude --- with each other.
Colluding
agents, with massive computing power, may help to deny or slow service
to a public key but not to another agent because agents can create
public keys at will.Further ReadingThere are many issues that we have not covered. This material only
covers the basics from the point of view of distributed algorithms.ReviewWhat is ablock chain?How does the algorithm deal with the situation in which one agent
receives a message to extend its copy of the block chain with a block
\(X\) while another agent receives a message to extend its copy of the
  block chain with a different block \(Y\)?Could there be an infinite sequence of collisions? Is that likely?What is mining for cryptocurrency? Suppose only a small number (say
  two organizations) mined most (say 99%) of Bitcoins. Would that be a
  problem?Why isdouble spendingunlikely to be successful?K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

The answer is the same as that for the double-spending attack. A miner
may have 
created a block and appended it to the chain; however, a suspicious
agent (and we hope that all agents are suspicious!) will not accept
the block until many blocks have been appended to the chain after
it. Other miners won't append their blocks to an invalid one. The
invalid blocks will become permanent orphans, and so the fraudulent
miner won't be able to spend the coins in these blocks.Fifty One Percent AttacksA 51% attack can be carried out by an agent that has more mining
power (e.g. 51% or more) than all other agents combined. The higher
the proportion of mining power of a single agent, the greater the
chances that attacks by that agent will succeed. You can see the
danger of a single agent having predominant mining power by thinking
about an agent with say, 99% of the total mining power. This agent
can mine so much faster than others that it can manipulate the block
chain in many ways. It can create double-spend transactions and deny
services to some transactions.A group of miners can collude to gain predominant mining power. Also,
miners canrentCloud-based systems for mining --- as opposed to having toownhuge data centers. An attacker can rent a large system for the specific purpose and
duration of an attack. The public may not know if and when such
attacks are successful becausecryptocurrencies do not have an
incentive to publish such attacks.Denial of serviceCan agents collude so that an agent \(X\)'s transactions never get
into blocks and so never get processed?An agent's identity does not appear in a transaction, only a public
key does. An agent can create new public keys at will. So let's ask
another question: can agents collude so that transactions with a
specific public key do not get into blocks?Only an agent that can solve puzzles in reasonable time can make
blocks. These agents have significant computational power. One can
concoct a situation where many agents with significant computation
power collude to avoid transactions from a specific public key. This
could have the effect of slowing processing of certain
transactions. However, such a situation isn't likely to arise because
agents have an incentive to create blocks and so they compete ---
rather than collude --- with each other.
Colluding
agents, with massive computing power, may help to deny or slow service
to a public key but not to another agent because agents can create
public keys at will.Further ReadingThere are many issues that we have not covered. This material only
covers the basics from the point of view of distributed algorithms.ReviewWhat is ablock chain?How does the algorithm deal with the situation in which one agent
receives a message to extend its copy of the block chain with a block
\(X\) while another agent receives a message to extend its copy of the
  block chain with a different block \(Y\)?Could there be an infinite sequence of collisions? Is that likely?What is mining for cryptocurrency? Suppose only a small number (say
  two organizations) mined most (say 99%) of Bitcoins. Would that be a
  problem?Why isdouble spendingunlikely to be successful?K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

A group of miners can collude to gain predominant mining power. Also,
miners canrentCloud-based systems for mining --- as opposed to having toownhuge data centers. An attacker can rent a large system for the specific purpose and
duration of an attack. The public may not know if and when such
attacks are successful becausecryptocurrencies do not have an
incentive to publish such attacks.Denial of serviceCan agents collude so that an agent \(X\)'s transactions never get
into blocks and so never get processed?An agent's identity does not appear in a transaction, only a public
key does. An agent can create new public keys at will. So let's ask
another question: can agents collude so that transactions with a
specific public key do not get into blocks?Only an agent that can solve puzzles in reasonable time can make
blocks. These agents have significant computational power. One can
concoct a situation where many agents with significant computation
power collude to avoid transactions from a specific public key. This
could have the effect of slowing processing of certain
transactions. However, such a situation isn't likely to arise because
agents have an incentive to create blocks and so they compete ---
rather than collude --- with each other.
Colluding
agents, with massive computing power, may help to deny or slow service
to a public key but not to another agent because agents can create
public keys at will.Further ReadingThere are many issues that we have not covered. This material only
covers the basics from the point of view of distributed algorithms.ReviewWhat is ablock chain?How does the algorithm deal with the situation in which one agent
receives a message to extend its copy of the block chain with a block
\(X\) while another agent receives a message to extend its copy of the
  block chain with a different block \(Y\)?Could there be an infinite sequence of collisions? Is that likely?What is mining for cryptocurrency? Suppose only a small number (say
  two organizations) mined most (say 99%) of Bitcoins. Would that be a
  problem?Why isdouble spendingunlikely to be successful?K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

An agent's identity does not appear in a transaction, only a public
key does. An agent can create new public keys at will. So let's ask
another question: can agents collude so that transactions with a
specific public key do not get into blocks?Only an agent that can solve puzzles in reasonable time can make
blocks. These agents have significant computational power. One can
concoct a situation where many agents with significant computation
power collude to avoid transactions from a specific public key. This
could have the effect of slowing processing of certain
transactions. However, such a situation isn't likely to arise because
agents have an incentive to create blocks and so they compete ---
rather than collude --- with each other.
Colluding
agents, with massive computing power, may help to deny or slow service
to a public key but not to another agent because agents can create
public keys at will.Further ReadingThere are many issues that we have not covered. This material only
covers the basics from the point of view of distributed algorithms.ReviewWhat is ablock chain?How does the algorithm deal with the situation in which one agent
receives a message to extend its copy of the block chain with a block
\(X\) while another agent receives a message to extend its copy of the
  block chain with a different block \(Y\)?Could there be an infinite sequence of collisions? Is that likely?What is mining for cryptocurrency? Suppose only a small number (say
  two organizations) mined most (say 99%) of Bitcoins. Would that be a
  problem?Why isdouble spendingunlikely to be successful?K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

Only an agent that can solve puzzles in reasonable time can make
blocks. These agents have significant computational power. One can
concoct a situation where many agents with significant computation
power collude to avoid transactions from a specific public key. This
could have the effect of slowing processing of certain
transactions. However, such a situation isn't likely to arise because
agents have an incentive to create blocks and so they compete ---
rather than collude --- with each other.
Colluding
agents, with massive computing power, may help to deny or slow service
to a public key but not to another agent because agents can create
public keys at will.Further ReadingThere are many issues that we have not covered. This material only
covers the basics from the point of view of distributed algorithms.ReviewWhat is ablock chain?How does the algorithm deal with the situation in which one agent
receives a message to extend its copy of the block chain with a block
\(X\) while another agent receives a message to extend its copy of the
  block chain with a different block \(Y\)?Could there be an infinite sequence of collisions? Is that likely?What is mining for cryptocurrency? Suppose only a small number (say
  two organizations) mined most (say 99%) of Bitcoins. Would that be a
  problem?Why isdouble spendingunlikely to be successful?K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology
--- END Content from https://kmchandy.github.io/Crypto/BitCoin.html ---


--- START Content from https://kmchandy.github.io/Knowledge/Knowledge.html ---
TITLE: Knowledge/Knowledge.html

In this module we define a predicate "agent \(x\)
    knows \(P\)" where \(P\) is a predicate on states of a system.
    In later modules, we will use this definition to discuss
    algorithms.This module presents theorems about what agents know. The proofs
    of these theorems follow from the definition ofconsistent
    cuts.What an Agent KnowsLet \(x\) be an agent, \(P\) a predicate on system states, and
    \(Q\) a predicate on states of \(x\).\(P\) can be aglobal predicate, i.e., a predicate on
    states of all agents and channels. \(Q\) is alocal
    predicateof \(x\) because it is a predicate only on the states of
    agent \(x\) and is independent of other agents and only channels.Let \(init\) be the predicate that defines the initial condition of
the system.We
    define the predicate "\(x\)knows\(P\)" as follows.\(x\) knows \(P\) is the weakest local predicate \(Q\) of
    \(x\) such that:\(
    [init \; \Rightarrow \; always([Q \;
    \Rightarrow \; P])]
    \)\(x\) knows \(P\) holds in a local state \(s_{x}\) of agent \(x\)
exactly when \(P\) holds in all states of all trajectories
    (that start from an initial state) when the
local state of agent \(x\) is \(s_{x}\).ExplanationFor any predicate \(R\):\(
[init  \; \Rightarrow \; always(R)]
\)means that \(R\) holds in every state of every trajectory that starts
in an initial state. So,\(
[init  \; \Rightarrow \; always([Q \Rightarrow P])]
\)means that in \([Q \Rightarrow P]\) holds in every state of every
trajectory that starts in an initial state.
So, if local predicate \(Q\) of agent \(x\) holds in any
state of any trajectory that starts from an initial state then global predicate \(P\)
also holds in that state.ExampleA system consisting of agents \(0, \ldots,  N\) has two indivisible
    tokens which are not created or destroyed."Agent \(0\) knows no other agent holds a token" is a predicate on the states of agent
\(0\); this predicate holds for a local state \(s_{0}\) of agent \(0\)
if and only if no other agent holds a token when agent \(0\) is in
state \(s_{0}\).So, "agent \(0\) knows P" holds exactly when agent \(0\)
holds both tokens.ExampleThis example deals with a system consisting of two agents \(x\) and
\(y\) and channels in both directions between the agents. The system
has a single indivisible token that is not created or destroyed.
Let
\(P\) be the predicate: "\(y\) doesnothold the token."
Agent \(x\) knows \(P\) when \(x\) holds the token.\(\neg\) (\(x\) knows \(P\)) is a predicate too, and sometimes
programmers refer to this predicate as "\(x\) does not know \(P\)."ExampleIn the previous example, let \(Q\) be the predicate: "\(y\) holds the token."
In what local states of agent \(x\) does \(x\) know that "\(y\) holds the token?"There are no local states of agent \(x\) in which \(x\) knows that
"\(y\) holds the token." Even when \(x\) does not hold the token,
\(x\) does not know that \(y\) holds the token because
the token could be in a channel.When \(x\) does not hold the token, \(x\) does not know that \(y\) holds
the token and \(x\) does not know that \(y\) does not hold the token."NOT(x knows Q)" AND "NOT(x knows NOT Q)"is a predicate which holds in the
state which \(x\) does not hold the token.Notation"\(x\) knows \(P\) at a point \(T\)" in a timeline means that at
    point \(T\) agent \(x\) is in a state where the predicate "\(x\)
knows \(P\)" holds.Theorem: Knowledge and Consistent CutsLet \(P\) be a predicate on the states of a system.If  \(x\) knows \(P\) at a point \(T\) in \(x\)'s timeline
then \(P\) holds in every consistent cut through that point.ProofThe states corresponding to all consistent cuts that pass through the same
point on \(x\)'s timeline have the same common value for \(x\)'s local
state.Example>Fig. 2. Agent A knows P in all consistent cuts that cross
    point TThe top figure in both diagrams above show a time T at which agent A knows
that P holds. This implies that P holds in all consistent cuts through
point T. The lower figures in the diagrams show consistent cuts which
passes point T on A's timeline; the theorem says that P holds for the
state at these cuts too.Theorem: A Silent Agent retains
KnowledgeAn agent that sends no information between a point \(T\) and a later
point \(T'\) retains all the knowledge it has at \(T\) at \(T'\).Let \(x\) be an agent in a system, and let \(P\) be a
predicate on a subsystem that does not include \(x\).
Let \(T\) and \(T'\) be points on \(x\)'s timeline with \(T < T'\). If
\(x\) knows \(P\) at point \(T\) and \(x\) sends no messages in the
interval \([T, T']\) then \(x\) knows \(P\) at \(T'\).ProofLet \(c'\) be any consistent cut through point \(T'\) on \(x\)'s
timeline. We will prove that \(P\) holds for the state at cut \(c'\).Let \(c\) be the
cut that is identical to \(c'\) except that it passes through point \(T\) on
\(x\)'s timeline. \(c\) is consistent because there are no
outgoing edges from \(x\)'s timeline between cuts \(c\) and
\(c'\).Because \(x\) knows \(P\) at \(T\), \(P\) holds at \(c\). Since \(c\) and \(c'\)
are identical except for the intersection with \(x\)'s timeline,
it follows that \(P\) holds \(c'\).Example>Fig. 3. Illustration of Proof of Silent AgentsConsequence of the TheoremAn agent doesn't lose knowledge by getting information from other
agents.An agent can only lose knowledge by sending information to
other agents. This seems counterintuitive; we'll look at the reasoning
underlying this in a later theorem.Theorem: Agents who don't listen remain
IgnorantAn agent that receives no information between a point \(T\) and a later
point \(T'\) learns no new knowledge between \(T\) and \(T'\).Let \(x\) be an agent in a system, and let \(P\) be a
predicate on a subsystem that does not include \(x\).
Let \(T\) and \(T'\) be points on \(x\)'s timeline with \(T < T'\). If
\(x\) knows \(P\) at point \(T'\) and \(x\) received no messages in the
interval \([T, T']\) then \(x\) knows \(P\) at \(T\).The proof has exactly the same structure as the proof of the
previous theorem.Consequence of the Theorem\(x\) didn't learn anything in the interval  \([T, T']\); everything
\(x\) knows at the later point \(T'\) is knowledge it already had at
the earlier point \(T\).The only way for an agent to gain knowledge is
to receive messages. An agent cannot learn about other agents by only
sending messages or making internal state transitions.Theorem: Knowledge implies ControlLet \(x\) and \(y\) be agents in a system, and let \(P\) be a
predicate on the states of \(y\).
Let \(T\) and \(T'\) be instants in a trajectory with \(T < T'\). If
\(x\) knows \(P\) at  \(T\), and \(\neg P\) holds at \(T'\), then there
is a path in the timeline diagram from point \(T\) on \(x\)'s timeline
to point \(T'\) on \(y\)'s timeline.Proof: If there is no path from point \(T\) on \(x\)'s timeline
to point \(T'\) on \(y\)'s timeline then there exists a consistent cut
which crosses \(x\)'s timeline at \(T\) and
crosses \(y\)'s timeline at \(T'\).ExampleIn the figure below, agent \(A\) at point \(T\) knows that agent \(C\)
holds no tokens. At a later point \(T'\) agent \(C\) holds a
token. What must happen between points \(T\) and \(T'\)?>Fig. 4. What must happen between T and T'?There must be a path in the timeline diagram from point \(T\) on agent
\(A\)'s timeline to point \(T'\) on agent \(C\)'s timeline. This path
is represented by edges that show time elapsing on a timeline and
message edges between timelines.>Fig. 5. There must be a path from A at T to C at T'?Consequence of the TheoremSuppose you and your friend communicate only by means of messages
that are delayed by arbitrary (finite) amounts. Consider a situation
where your friend knows
that you are in the library at 9 pm. Then, from our definition of
knowledge, because agents only know truth, you must be in the library
at 9 pm. Moreover, you can't leave the library until
you receive a message from your friend; this message may go through
intermediate agents.In one of the exercises we'll look at knowledge when agents have
clocks that may drift from each other but are not more than a some
constant \(M\) units apart. If your friend knows that you will be in
the library till her watch reads 9:00 pm, and your watches may drift apart
by a minute, then you can leave the library at 9:01 pm. Clocks are
useful even if they aren't perfect. More about clocks later.Theorem: Communication to learn about
ChangeThis theorem is similar to the "knowledge is control" theorem.Let \(x\) and \(y\) be agents in a system, and let \(P\) be a
predicate on the states of \(y\).
Let \(T\) and \(T'\) be instants in a trajectory with \(T < T'\). If
\(\neg P\) holds at \(T\) and \(x\) knows \(P\) at \(T'\) then there
is a path in the timeline diagram from point \(T\) on \(y\)'s timeline
to point \(T'\) on \(x\)'s timeline.ExampleIn the figure below, agent \(C\) holds a token at point \(T\).
At a later point \(T'\) agent \(A\) knows that agent \(C\) holds no
tokens. What must happen between points \(T\) and \(T'\)?>Fig. 6. What must happen between T and T'?There must be a path in the timeline diagram from point \(T\) on agent
\(C\)'s timeline to point \(T'\) on agent \(A\)'s timeline. This path
is represented by edges that show time elapsing on a timeline and
message edges between timelines.>Fig. 7. There must be a path from C at T to A at T'?What Agents Know about Channel StatesNext, let's look at systems in which messages are acknowledged.
For a pair of agents \(x, y\), let \(ms\) and \(mr\)
be the number of messages that \(x\) has sent to \(y\), and  the number 
of messages that \(y\) has received from \(x\), respectively.
Let \(as\) and \(ar\) be
the number of acknowledgements that \(y\) has sent to \(x\), and
the number of acknowledgements that \(x\) has received from \(y\),
respectively.
The following is an invariant:\(ms \geq mr \geq as \geq ar \)The number of messages in the channel from \(x\) to \(y\) is \(ms -
mr\).
Because \(ms\) and \(ar\) are variables of agent \(x\),
agent \(x\)knowsan upper bound,\(\; ms - ar\), on the number of
messages in channel \((x, y)\).
So, \(x\) knows that the channel is empty when \(\; ms = ar\).What an agent cannot knowAn agent cannot know that there are
exactly \(n\) messages in a channel, for \(n > 0\).
You can prove this result using the concept of consistent
cuts. Intuitively, the agent cannot know whether a message is in the
channel or has been received.Chains of KnowledgeLet \(x, y, z\) be agents of a system and \(P\) be a predicate on
states of the system. Then the following are all predicates:\(z\) knows \(P\)\(y\) knows that \(z\) knows \(P\)\(x\) knows that \(y\) knows that \(z\) knows \(P\)The theorems given earlier apply to any predicate. For example, if
\(x\) knows that \(y\) knows that \(z\) knows \(P\) at a point \(t\) in a
trajectory, and \(\neg P\) holds at a later point \(t'\)
then there must be path in the timeline diagram from point \(t\) on
\(x\)'s timeline to point \(t'\) on \(z\)'s timeline.Concurrent Systems with Shared VariablesThe theorems
and proofs given in this module apply to systems with shared
variables, and indeed any system with trajectories that are
representable by timeline diagrams and with consistent cuts.SummaryMany people working on distributed systems use the phrase "an agent
knows." This module gives a definition of the concept that is
consistent with intuitive definitions of knowledge.
The central idea in this module is the
relationship between what agents know and consistent cuts of
timelines. We presented several theorems about agent knowledge which
are intuitive when applied to human agents. The proofs are
straightforward and are all based on consistent cuts of timelines.ReviewIs the following true? Why?(\(z\) knows \(P\)) and (\(z\) knows \(Q\)) \(\equiv\) \(z\) knows
  \(\(P \wedge Q\))Is the following true? Why?(\(z\) knows \(P\)) or (\(z\) knows \(Q\)) \(\equiv\) \(z\) knows
  \(\( P \vee Q\))Suppose you and your friend communicate only using messages sent in
  channels, just as agents do in our model of distributed
  systems. When youknowthat your friend is wearing a cap does
  that mean that (a) your friend is wearing a cap, and (b) your friend
  can't take the cap off until the friend hears from you?K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology



Figure Link: https://kmchandy.github.io/Knowledge/KnowledgeDiagrams/Slide02.jpg

Caption: Fig. 2. Agent A knows P in all consistent cuts that cross
    point T


This module presents theorems about what agents know. The proofs
    of these theorems follow from the definition ofconsistent
    cuts.What an Agent KnowsLet \(x\) be an agent, \(P\) a predicate on system states, and
    \(Q\) a predicate on states of \(x\).\(P\) can be aglobal predicate, i.e., a predicate on
    states of all agents and channels. \(Q\) is alocal
    predicateof \(x\) because it is a predicate only on the states of
    agent \(x\) and is independent of other agents and only channels.Let \(init\) be the predicate that defines the initial condition of
the system.We
    define the predicate "\(x\)knows\(P\)" as follows.\(x\) knows \(P\) is the weakest local predicate \(Q\) of
    \(x\) such that:\(
    [init \; \Rightarrow \; always([Q \;
    \Rightarrow \; P])]
    \)\(x\) knows \(P\) holds in a local state \(s_{x}\) of agent \(x\)
exactly when \(P\) holds in all states of all trajectories
    (that start from an initial state) when the
local state of agent \(x\) is \(s_{x}\).ExplanationFor any predicate \(R\):\(
[init  \; \Rightarrow \; always(R)]
\)means that \(R\) holds in every state of every trajectory that starts
in an initial state. So,\(
[init  \; \Rightarrow \; always([Q \Rightarrow P])]
\)means that in \([Q \Rightarrow P]\) holds in every state of every
trajectory that starts in an initial state.
So, if local predicate \(Q\) of agent \(x\) holds in any
state of any trajectory that starts from an initial state then global predicate \(P\)
also holds in that state.ExampleA system consisting of agents \(0, \ldots,  N\) has two indivisible
    tokens which are not created or destroyed."Agent \(0\) knows no other agent holds a token" is a predicate on the states of agent
\(0\); this predicate holds for a local state \(s_{0}\) of agent \(0\)
if and only if no other agent holds a token when agent \(0\) is in
state \(s_{0}\).So, "agent \(0\) knows P" holds exactly when agent \(0\)
holds both tokens.ExampleThis example deals with a system consisting of two agents \(x\) and
\(y\) and channels in both directions between the agents. The system
has a single indivisible token that is not created or destroyed.
Let
\(P\) be the predicate: "\(y\) doesnothold the token."
Agent \(x\) knows \(P\) when \(x\) holds the token.\(\neg\) (\(x\) knows \(P\)) is a predicate too, and sometimes
programmers refer to this predicate as "\(x\) does not know \(P\)."ExampleIn the previous example, let \(Q\) be the predicate: "\(y\) holds the token."
In what local states of agent \(x\) does \(x\) know that "\(y\) holds the token?"There are no local states of agent \(x\) in which \(x\) knows that
"\(y\) holds the token." Even when \(x\) does not hold the token,
\(x\) does not know that \(y\) holds the token because
the token could be in a channel.When \(x\) does not hold the token, \(x\) does not know that \(y\) holds
the token and \(x\) does not know that \(y\) does not hold the token."NOT(x knows Q)" AND "NOT(x knows NOT Q)"is a predicate which holds in the
state which \(x\) does not hold the token.Notation"\(x\) knows \(P\) at a point \(T\)" in a timeline means that at
    point \(T\) agent \(x\) is in a state where the predicate "\(x\)
knows \(P\)" holds.Theorem: Knowledge and Consistent CutsLet \(P\) be a predicate on the states of a system.If  \(x\) knows \(P\) at a point \(T\) in \(x\)'s timeline
then \(P\) holds in every consistent cut through that point.ProofThe states corresponding to all consistent cuts that pass through the same
point on \(x\)'s timeline have the same common value for \(x\)'s local
state.Example>Fig. 2. Agent A knows P in all consistent cuts that cross
    point TThe top figure in both diagrams above show a time T at which agent A knows
that P holds. This implies that P holds in all consistent cuts through
point T. The lower figures in the diagrams show consistent cuts which
passes point T on A's timeline; the theorem says that P holds for the
state at these cuts too.Theorem: A Silent Agent retains
KnowledgeAn agent that sends no information between a point \(T\) and a later
point \(T'\) retains all the knowledge it has at \(T\) at \(T'\).Let \(x\) be an agent in a system, and let \(P\) be a
predicate on a subsystem that does not include \(x\).
Let \(T\) and \(T'\) be points on \(x\)'s timeline with \(T < T'\). If
\(x\) knows \(P\) at point \(T\) and \(x\) sends no messages in the
interval \([T, T']\) then \(x\) knows \(P\) at \(T'\).ProofLet \(c'\) be any consistent cut through point \(T'\) on \(x\)'s
timeline. We will prove that \(P\) holds for the state at cut \(c'\).Let \(c\) be the
cut that is identical to \(c'\) except that it passes through point \(T\) on
\(x\)'s timeline. \(c\) is consistent because there are no
outgoing edges from \(x\)'s timeline between cuts \(c\) and
\(c'\).Because \(x\) knows \(P\) at \(T\), \(P\) holds at \(c\). Since \(c\) and \(c'\)
are identical except for the intersection with \(x\)'s timeline,
it follows that \(P\) holds \(c'\).Example>Fig. 3. Illustration of Proof of Silent AgentsConsequence of the TheoremAn agent doesn't lose knowledge by getting information from other
agents.An agent can only lose knowledge by sending information to
other agents. This seems counterintuitive; we'll look at the reasoning
underlying this in a later theorem.Theorem: Agents who don't listen remain
IgnorantAn agent that receives no information between a point \(T\) and a later
point \(T'\) learns no new knowledge between \(T\) and \(T'\).Let \(x\) be an agent in a system, and let \(P\) be a
predicate on a subsystem that does not include \(x\).
Let \(T\) and \(T'\) be points on \(x\)'s timeline with \(T < T'\). If
\(x\) knows \(P\) at point \(T'\) and \(x\) received no messages in the
interval \([T, T']\) then \(x\) knows \(P\) at \(T\).The proof has exactly the same structure as the proof of the
previous theorem.Consequence of the Theorem\(x\) didn't learn anything in the interval  \([T, T']\); everything
\(x\) knows at the later point \(T'\) is knowledge it already had at
the earlier point \(T\).The only way for an agent to gain knowledge is
to receive messages. An agent cannot learn about other agents by only
sending messages or making internal state transitions.Theorem: Knowledge implies ControlLet \(x\) and \(y\) be agents in a system, and let \(P\) be a
predicate on the states of \(y\).
Let \(T\) and \(T'\) be instants in a trajectory with \(T < T'\). If
\(x\) knows \(P\) at  \(T\), and \(\neg P\) holds at \(T'\), then there
is a path in the timeline diagram from point \(T\) on \(x\)'s timeline
to point \(T'\) on \(y\)'s timeline.Proof: If there is no path from point \(T\) on \(x\)'s timeline
to point \(T'\) on \(y\)'s timeline then there exists a consistent cut
which crosses \(x\)'s timeline at \(T\) and
crosses \(y\)'s timeline at \(T'\).ExampleIn the figure below, agent \(A\) at point \(T\) knows that agent \(C\)
holds no tokens. At a later point \(T'\) agent \(C\) holds a
token. What must happen between points \(T\) and \(T'\)?>Fig. 4. What must happen between T and T'?There must be a path in the timeline diagram from point \(T\) on agent
\(A\)'s timeline to point \(T'\) on agent \(C\)'s timeline. This path
is represented by edges that show time elapsing on a timeline and
message edges between timelines.>Fig. 5. There must be a path from A at T to C at T'?Consequence of the TheoremSuppose you and your friend communicate only by means of messages
that are delayed by arbitrary (finite) amounts. Consider a situation
where your friend knows
that you are in the library at 9 pm. Then, from our definition of
knowledge, because agents only know truth, you must be in the library
at 9 pm. Moreover, you can't leave the library until
you receive a message from your friend; this message may go through
intermediate agents.In one of the exercises we'll look at knowledge when agents have
clocks that may drift from each other but are not more than a some
constant \(M\) units apart. If your friend knows that you will be in
the library till her watch reads 9:00 pm, and your watches may drift apart
by a minute, then you can leave the library at 9:01 pm. Clocks are
useful even if they aren't perfect. More about clocks later.Theorem: Communication to learn about
ChangeThis theorem is similar to the "knowledge is control" theorem.Let \(x\) and \(y\) be agents in a system, and let \(P\) be a
predicate on the states of \(y\).
Let \(T\) and \(T'\) be instants in a trajectory with \(T < T'\). If
\(\neg P\) holds at \(T\) and \(x\) knows \(P\) at \(T'\) then there
is a path in the timeline diagram from point \(T\) on \(y\)'s timeline
to point \(T'\) on \(x\)'s timeline.ExampleIn the figure below, agent \(C\) holds a token at point \(T\).
At a later point \(T'\) agent \(A\) knows that agent \(C\) holds no
tokens. What must happen between points \(T\) and \(T'\)?>Fig. 6. What must happen between T and T'?There must be a path in the timeline diagram from point \(T\) on agent
\(C\)'s timeline to point \(T'\) on agent \(A\)'s timeline. This path
is represented by edges that show time elapsing on a timeline and
message edges between timelines.>Fig. 7. There must be a path from C at T to A at T'?What Agents Know about Channel StatesNext, let's look at systems in which messages are acknowledged.
For a pair of agents \(x, y\), let \(ms\) and \(mr\)
be the number of messages that \(x\) has sent to \(y\), and  the number 
of messages that \(y\) has received from \(x\), respectively.
Let \(as\) and \(ar\) be
the number of acknowledgements that \(y\) has sent to \(x\), and
the number of acknowledgements that \(x\) has received from \(y\),
respectively.
The following is an invariant:\(ms \geq mr \geq as \geq ar \)The number of messages in the channel from \(x\) to \(y\) is \(ms -
mr\).
Because \(ms\) and \(ar\) are variables of agent \(x\),
agent \(x\)knowsan upper bound,\(\; ms - ar\), on the number of
messages in channel \((x, y)\).
So, \(x\) knows that the channel is empty when \(\; ms = ar\).What an agent cannot knowAn agent cannot know that there are
exactly \(n\) messages in a channel, for \(n > 0\).
You can prove this result using the concept of consistent
cuts. Intuitively, the agent cannot know whether a message is in the
channel or has been received.Chains of KnowledgeLet \(x, y, z\) be agents of a system and \(P\) be a predicate on
states of the system. Then the following are all predicates:\(z\) knows \(P\)\(y\) knows that \(z\) knows \(P\)\(x\) knows that \(y\) knows that \(z\) knows \(P\)The theorems given earlier apply to any predicate. For example, if
\(x\) knows that \(y\) knows that \(z\) knows \(P\) at a point \(t\) in a
trajectory, and \(\neg P\) holds at a later point \(t'\)
then there must be path in the timeline diagram from point \(t\) on
\(x\)'s timeline to point \(t'\) on \(z\)'s timeline.Concurrent Systems with Shared VariablesThe theorems
and proofs given in this module apply to systems with shared
variables, and indeed any system with trajectories that are
representable by timeline diagrams and with consistent cuts.SummaryMany people working on distributed systems use the phrase "an agent
knows." This module gives a definition of the concept that is
consistent with intuitive definitions of knowledge.
The central idea in this module is the
relationship between what agents know and consistent cuts of
timelines. We presented several theorems about agent knowledge which
are intuitive when applied to human agents. The proofs are
straightforward and are all based on consistent cuts of timelines.ReviewIs the following true? Why?(\(z\) knows \(P\)) and (\(z\) knows \(Q\)) \(\equiv\) \(z\) knows
  \(\(P \wedge Q\))Is the following true? Why?(\(z\) knows \(P\)) or (\(z\) knows \(Q\)) \(\equiv\) \(z\) knows
  \(\( P \vee Q\))Suppose you and your friend communicate only using messages sent in
  channels, just as agents do in our model of distributed
  systems. When youknowthat your friend is wearing a cap does
  that mean that (a) your friend is wearing a cap, and (b) your friend
  can't take the cap off until the friend hears from you?K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology



Figure Link: https://kmchandy.github.io/Knowledge/KnowledgeDiagrams/Slide02.jpg

Caption: Fig. 2. Agent A knows P in all consistent cuts that cross
    point T


\(P\) can be aglobal predicate, i.e., a predicate on
    states of all agents and channels. \(Q\) is alocal
    predicateof \(x\) because it is a predicate only on the states of
    agent \(x\) and is independent of other agents and only channels.Let \(init\) be the predicate that defines the initial condition of
the system.We
    define the predicate "\(x\)knows\(P\)" as follows.\(x\) knows \(P\) is the weakest local predicate \(Q\) of
    \(x\) such that:\(
    [init \; \Rightarrow \; always([Q \;
    \Rightarrow \; P])]
    \)\(x\) knows \(P\) holds in a local state \(s_{x}\) of agent \(x\)
exactly when \(P\) holds in all states of all trajectories
    (that start from an initial state) when the
local state of agent \(x\) is \(s_{x}\).ExplanationFor any predicate \(R\):\(
[init  \; \Rightarrow \; always(R)]
\)means that \(R\) holds in every state of every trajectory that starts
in an initial state. So,\(
[init  \; \Rightarrow \; always([Q \Rightarrow P])]
\)means that in \([Q \Rightarrow P]\) holds in every state of every
trajectory that starts in an initial state.
So, if local predicate \(Q\) of agent \(x\) holds in any
state of any trajectory that starts from an initial state then global predicate \(P\)
also holds in that state.ExampleA system consisting of agents \(0, \ldots,  N\) has two indivisible
    tokens which are not created or destroyed."Agent \(0\) knows no other agent holds a token" is a predicate on the states of agent
\(0\); this predicate holds for a local state \(s_{0}\) of agent \(0\)
if and only if no other agent holds a token when agent \(0\) is in
state \(s_{0}\).So, "agent \(0\) knows P" holds exactly when agent \(0\)
holds both tokens.ExampleThis example deals with a system consisting of two agents \(x\) and
\(y\) and channels in both directions between the agents. The system
has a single indivisible token that is not created or destroyed.
Let
\(P\) be the predicate: "\(y\) doesnothold the token."
Agent \(x\) knows \(P\) when \(x\) holds the token.\(\neg\) (\(x\) knows \(P\)) is a predicate too, and sometimes
programmers refer to this predicate as "\(x\) does not know \(P\)."ExampleIn the previous example, let \(Q\) be the predicate: "\(y\) holds the token."
In what local states of agent \(x\) does \(x\) know that "\(y\) holds the token?"There are no local states of agent \(x\) in which \(x\) knows that
"\(y\) holds the token." Even when \(x\) does not hold the token,
\(x\) does not know that \(y\) holds the token because
the token could be in a channel.When \(x\) does not hold the token, \(x\) does not know that \(y\) holds
the token and \(x\) does not know that \(y\) does not hold the token."NOT(x knows Q)" AND "NOT(x knows NOT Q)"is a predicate which holds in the
state which \(x\) does not hold the token.Notation"\(x\) knows \(P\) at a point \(T\)" in a timeline means that at
    point \(T\) agent \(x\) is in a state where the predicate "\(x\)
knows \(P\)" holds.Theorem: Knowledge and Consistent CutsLet \(P\) be a predicate on the states of a system.If  \(x\) knows \(P\) at a point \(T\) in \(x\)'s timeline
then \(P\) holds in every consistent cut through that point.ProofThe states corresponding to all consistent cuts that pass through the same
point on \(x\)'s timeline have the same common value for \(x\)'s local
state.Example>Fig. 2. Agent A knows P in all consistent cuts that cross
    point TThe top figure in both diagrams above show a time T at which agent A knows
that P holds. This implies that P holds in all consistent cuts through
point T. The lower figures in the diagrams show consistent cuts which
passes point T on A's timeline; the theorem says that P holds for the
state at these cuts too.Theorem: A Silent Agent retains
KnowledgeAn agent that sends no information between a point \(T\) and a later
point \(T'\) retains all the knowledge it has at \(T\) at \(T'\).Let \(x\) be an agent in a system, and let \(P\) be a
predicate on a subsystem that does not include \(x\).
Let \(T\) and \(T'\) be points on \(x\)'s timeline with \(T < T'\). If
\(x\) knows \(P\) at point \(T\) and \(x\) sends no messages in the
interval \([T, T']\) then \(x\) knows \(P\) at \(T'\).ProofLet \(c'\) be any consistent cut through point \(T'\) on \(x\)'s
timeline. We will prove that \(P\) holds for the state at cut \(c'\).Let \(c\) be the
cut that is identical to \(c'\) except that it passes through point \(T\) on
\(x\)'s timeline. \(c\) is consistent because there are no
outgoing edges from \(x\)'s timeline between cuts \(c\) and
\(c'\).Because \(x\) knows \(P\) at \(T\), \(P\) holds at \(c\). Since \(c\) and \(c'\)
are identical except for the intersection with \(x\)'s timeline,
it follows that \(P\) holds \(c'\).Example>Fig. 3. Illustration of Proof of Silent AgentsConsequence of the TheoremAn agent doesn't lose knowledge by getting information from other
agents.An agent can only lose knowledge by sending information to
other agents. This seems counterintuitive; we'll look at the reasoning
underlying this in a later theorem.Theorem: Agents who don't listen remain
IgnorantAn agent that receives no information between a point \(T\) and a later
point \(T'\) learns no new knowledge between \(T\) and \(T'\).Let \(x\) be an agent in a system, and let \(P\) be a
predicate on a subsystem that does not include \(x\).
Let \(T\) and \(T'\) be points on \(x\)'s timeline with \(T < T'\). If
\(x\) knows \(P\) at point \(T'\) and \(x\) received no messages in the
interval \([T, T']\) then \(x\) knows \(P\) at \(T\).The proof has exactly the same structure as the proof of the
previous theorem.Consequence of the Theorem\(x\) didn't learn anything in the interval  \([T, T']\); everything
\(x\) knows at the later point \(T'\) is knowledge it already had at
the earlier point \(T\).The only way for an agent to gain knowledge is
to receive messages. An agent cannot learn about other agents by only
sending messages or making internal state transitions.Theorem: Knowledge implies ControlLet \(x\) and \(y\) be agents in a system, and let \(P\) be a
predicate on the states of \(y\).
Let \(T\) and \(T'\) be instants in a trajectory with \(T < T'\). If
\(x\) knows \(P\) at  \(T\), and \(\neg P\) holds at \(T'\), then there
is a path in the timeline diagram from point \(T\) on \(x\)'s timeline
to point \(T'\) on \(y\)'s timeline.Proof: If there is no path from point \(T\) on \(x\)'s timeline
to point \(T'\) on \(y\)'s timeline then there exists a consistent cut
which crosses \(x\)'s timeline at \(T\) and
crosses \(y\)'s timeline at \(T'\).ExampleIn the figure below, agent \(A\) at point \(T\) knows that agent \(C\)
holds no tokens. At a later point \(T'\) agent \(C\) holds a
token. What must happen between points \(T\) and \(T'\)?>Fig. 4. What must happen between T and T'?There must be a path in the timeline diagram from point \(T\) on agent
\(A\)'s timeline to point \(T'\) on agent \(C\)'s timeline. This path
is represented by edges that show time elapsing on a timeline and
message edges between timelines.>Fig. 5. There must be a path from A at T to C at T'?Consequence of the TheoremSuppose you and your friend communicate only by means of messages
that are delayed by arbitrary (finite) amounts. Consider a situation
where your friend knows
that you are in the library at 9 pm. Then, from our definition of
knowledge, because agents only know truth, you must be in the library
at 9 pm. Moreover, you can't leave the library until
you receive a message from your friend; this message may go through
intermediate agents.In one of the exercises we'll look at knowledge when agents have
clocks that may drift from each other but are not more than a some
constant \(M\) units apart. If your friend knows that you will be in
the library till her watch reads 9:00 pm, and your watches may drift apart
by a minute, then you can leave the library at 9:01 pm. Clocks are
useful even if they aren't perfect. More about clocks later.Theorem: Communication to learn about
ChangeThis theorem is similar to the "knowledge is control" theorem.Let \(x\) and \(y\) be agents in a system, and let \(P\) be a
predicate on the states of \(y\).
Let \(T\) and \(T'\) be instants in a trajectory with \(T < T'\). If
\(\neg P\) holds at \(T\) and \(x\) knows \(P\) at \(T'\) then there
is a path in the timeline diagram from point \(T\) on \(y\)'s timeline
to point \(T'\) on \(x\)'s timeline.ExampleIn the figure below, agent \(C\) holds a token at point \(T\).
At a later point \(T'\) agent \(A\) knows that agent \(C\) holds no
tokens. What must happen between points \(T\) and \(T'\)?>Fig. 6. What must happen between T and T'?There must be a path in the timeline diagram from point \(T\) on agent
\(C\)'s timeline to point \(T'\) on agent \(A\)'s timeline. This path
is represented by edges that show time elapsing on a timeline and
message edges between timelines.>Fig. 7. There must be a path from C at T to A at T'?What Agents Know about Channel StatesNext, let's look at systems in which messages are acknowledged.
For a pair of agents \(x, y\), let \(ms\) and \(mr\)
be the number of messages that \(x\) has sent to \(y\), and  the number 
of messages that \(y\) has received from \(x\), respectively.
Let \(as\) and \(ar\) be
the number of acknowledgements that \(y\) has sent to \(x\), and
the number of acknowledgements that \(x\) has received from \(y\),
respectively.
The following is an invariant:\(ms \geq mr \geq as \geq ar \)The number of messages in the channel from \(x\) to \(y\) is \(ms -
mr\).
Because \(ms\) and \(ar\) are variables of agent \(x\),
agent \(x\)knowsan upper bound,\(\; ms - ar\), on the number of
messages in channel \((x, y)\).
So, \(x\) knows that the channel is empty when \(\; ms = ar\).What an agent cannot knowAn agent cannot know that there are
exactly \(n\) messages in a channel, for \(n > 0\).
You can prove this result using the concept of consistent
cuts. Intuitively, the agent cannot know whether a message is in the
channel or has been received.Chains of KnowledgeLet \(x, y, z\) be agents of a system and \(P\) be a predicate on
states of the system. Then the following are all predicates:\(z\) knows \(P\)\(y\) knows that \(z\) knows \(P\)\(x\) knows that \(y\) knows that \(z\) knows \(P\)The theorems given earlier apply to any predicate. For example, if
\(x\) knows that \(y\) knows that \(z\) knows \(P\) at a point \(t\) in a
trajectory, and \(\neg P\) holds at a later point \(t'\)
then there must be path in the timeline diagram from point \(t\) on
\(x\)'s timeline to point \(t'\) on \(z\)'s timeline.Concurrent Systems with Shared VariablesThe theorems
and proofs given in this module apply to systems with shared
variables, and indeed any system with trajectories that are
representable by timeline diagrams and with consistent cuts.SummaryMany people working on distributed systems use the phrase "an agent
knows." This module gives a definition of the concept that is
consistent with intuitive definitions of knowledge.
The central idea in this module is the
relationship between what agents know and consistent cuts of
timelines. We presented several theorems about agent knowledge which
are intuitive when applied to human agents. The proofs are
straightforward and are all based on consistent cuts of timelines.ReviewIs the following true? Why?(\(z\) knows \(P\)) and (\(z\) knows \(Q\)) \(\equiv\) \(z\) knows
  \(\(P \wedge Q\))Is the following true? Why?(\(z\) knows \(P\)) or (\(z\) knows \(Q\)) \(\equiv\) \(z\) knows
  \(\( P \vee Q\))Suppose you and your friend communicate only using messages sent in
  channels, just as agents do in our model of distributed
  systems. When youknowthat your friend is wearing a cap does
  that mean that (a) your friend is wearing a cap, and (b) your friend
  can't take the cap off until the friend hears from you?K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology



Figure Link: https://kmchandy.github.io/Knowledge/KnowledgeDiagrams/Slide02.jpg

Caption: Fig. 2. Agent A knows P in all consistent cuts that cross
    point T


Let \(init\) be the predicate that defines the initial condition of
the system.We
    define the predicate "\(x\)knows\(P\)" as follows.\(x\) knows \(P\) is the weakest local predicate \(Q\) of
    \(x\) such that:\(
    [init \; \Rightarrow \; always([Q \;
    \Rightarrow \; P])]
    \)\(x\) knows \(P\) holds in a local state \(s_{x}\) of agent \(x\)
exactly when \(P\) holds in all states of all trajectories
    (that start from an initial state) when the
local state of agent \(x\) is \(s_{x}\).ExplanationFor any predicate \(R\):\(
[init  \; \Rightarrow \; always(R)]
\)means that \(R\) holds in every state of every trajectory that starts
in an initial state. So,\(
[init  \; \Rightarrow \; always([Q \Rightarrow P])]
\)means that in \([Q \Rightarrow P]\) holds in every state of every
trajectory that starts in an initial state.
So, if local predicate \(Q\) of agent \(x\) holds in any
state of any trajectory that starts from an initial state then global predicate \(P\)
also holds in that state.ExampleA system consisting of agents \(0, \ldots,  N\) has two indivisible
    tokens which are not created or destroyed."Agent \(0\) knows no other agent holds a token" is a predicate on the states of agent
\(0\); this predicate holds for a local state \(s_{0}\) of agent \(0\)
if and only if no other agent holds a token when agent \(0\) is in
state \(s_{0}\).So, "agent \(0\) knows P" holds exactly when agent \(0\)
holds both tokens.ExampleThis example deals with a system consisting of two agents \(x\) and
\(y\) and channels in both directions between the agents. The system
has a single indivisible token that is not created or destroyed.
Let
\(P\) be the predicate: "\(y\) doesnothold the token."
Agent \(x\) knows \(P\) when \(x\) holds the token.\(\neg\) (\(x\) knows \(P\)) is a predicate too, and sometimes
programmers refer to this predicate as "\(x\) does not know \(P\)."ExampleIn the previous example, let \(Q\) be the predicate: "\(y\) holds the token."
In what local states of agent \(x\) does \(x\) know that "\(y\) holds the token?"There are no local states of agent \(x\) in which \(x\) knows that
"\(y\) holds the token." Even when \(x\) does not hold the token,
\(x\) does not know that \(y\) holds the token because
the token could be in a channel.When \(x\) does not hold the token, \(x\) does not know that \(y\) holds
the token and \(x\) does not know that \(y\) does not hold the token."NOT(x knows Q)" AND "NOT(x knows NOT Q)"is a predicate which holds in the
state which \(x\) does not hold the token.Notation"\(x\) knows \(P\) at a point \(T\)" in a timeline means that at
    point \(T\) agent \(x\) is in a state where the predicate "\(x\)
knows \(P\)" holds.Theorem: Knowledge and Consistent CutsLet \(P\) be a predicate on the states of a system.If  \(x\) knows \(P\) at a point \(T\) in \(x\)'s timeline
then \(P\) holds in every consistent cut through that point.ProofThe states corresponding to all consistent cuts that pass through the same
point on \(x\)'s timeline have the same common value for \(x\)'s local
state.Example>Fig. 2. Agent A knows P in all consistent cuts that cross
    point TThe top figure in both diagrams above show a time T at which agent A knows
that P holds. This implies that P holds in all consistent cuts through
point T. The lower figures in the diagrams show consistent cuts which
passes point T on A's timeline; the theorem says that P holds for the
state at these cuts too.Theorem: A Silent Agent retains
KnowledgeAn agent that sends no information between a point \(T\) and a later
point \(T'\) retains all the knowledge it has at \(T\) at \(T'\).Let \(x\) be an agent in a system, and let \(P\) be a
predicate on a subsystem that does not include \(x\).
Let \(T\) and \(T'\) be points on \(x\)'s timeline with \(T < T'\). If
\(x\) knows \(P\) at point \(T\) and \(x\) sends no messages in the
interval \([T, T']\) then \(x\) knows \(P\) at \(T'\).ProofLet \(c'\) be any consistent cut through point \(T'\) on \(x\)'s
timeline. We will prove that \(P\) holds for the state at cut \(c'\).Let \(c\) be the
cut that is identical to \(c'\) except that it passes through point \(T\) on
\(x\)'s timeline. \(c\) is consistent because there are no
outgoing edges from \(x\)'s timeline between cuts \(c\) and
\(c'\).Because \(x\) knows \(P\) at \(T\), \(P\) holds at \(c\). Since \(c\) and \(c'\)
are identical except for the intersection with \(x\)'s timeline,
it follows that \(P\) holds \(c'\).Example>Fig. 3. Illustration of Proof of Silent AgentsConsequence of the TheoremAn agent doesn't lose knowledge by getting information from other
agents.An agent can only lose knowledge by sending information to
other agents. This seems counterintuitive; we'll look at the reasoning
underlying this in a later theorem.Theorem: Agents who don't listen remain
IgnorantAn agent that receives no information between a point \(T\) and a later
point \(T'\) learns no new knowledge between \(T\) and \(T'\).Let \(x\) be an agent in a system, and let \(P\) be a
predicate on a subsystem that does not include \(x\).
Let \(T\) and \(T'\) be points on \(x\)'s timeline with \(T < T'\). If
\(x\) knows \(P\) at point \(T'\) and \(x\) received no messages in the
interval \([T, T']\) then \(x\) knows \(P\) at \(T\).The proof has exactly the same structure as the proof of the
previous theorem.Consequence of the Theorem\(x\) didn't learn anything in the interval  \([T, T']\); everything
\(x\) knows at the later point \(T'\) is knowledge it already had at
the earlier point \(T\).The only way for an agent to gain knowledge is
to receive messages. An agent cannot learn about other agents by only
sending messages or making internal state transitions.Theorem: Knowledge implies ControlLet \(x\) and \(y\) be agents in a system, and let \(P\) be a
predicate on the states of \(y\).
Let \(T\) and \(T'\) be instants in a trajectory with \(T < T'\). If
\(x\) knows \(P\) at  \(T\), and \(\neg P\) holds at \(T'\), then there
is a path in the timeline diagram from point \(T\) on \(x\)'s timeline
to point \(T'\) on \(y\)'s timeline.Proof: If there is no path from point \(T\) on \(x\)'s timeline
to point \(T'\) on \(y\)'s timeline then there exists a consistent cut
which crosses \(x\)'s timeline at \(T\) and
crosses \(y\)'s timeline at \(T'\).ExampleIn the figure below, agent \(A\) at point \(T\) knows that agent \(C\)
holds no tokens. At a later point \(T'\) agent \(C\) holds a
token. What must happen between points \(T\) and \(T'\)?>Fig. 4. What must happen between T and T'?There must be a path in the timeline diagram from point \(T\) on agent
\(A\)'s timeline to point \(T'\) on agent \(C\)'s timeline. This path
is represented by edges that show time elapsing on a timeline and
message edges between timelines.>Fig. 5. There must be a path from A at T to C at T'?Consequence of the TheoremSuppose you and your friend communicate only by means of messages
that are delayed by arbitrary (finite) amounts. Consider a situation
where your friend knows
that you are in the library at 9 pm. Then, from our definition of
knowledge, because agents only know truth, you must be in the library
at 9 pm. Moreover, you can't leave the library until
you receive a message from your friend; this message may go through
intermediate agents.In one of the exercises we'll look at knowledge when agents have
clocks that may drift from each other but are not more than a some
constant \(M\) units apart. If your friend knows that you will be in
the library till her watch reads 9:00 pm, and your watches may drift apart
by a minute, then you can leave the library at 9:01 pm. Clocks are
useful even if they aren't perfect. More about clocks later.Theorem: Communication to learn about
ChangeThis theorem is similar to the "knowledge is control" theorem.Let \(x\) and \(y\) be agents in a system, and let \(P\) be a
predicate on the states of \(y\).
Let \(T\) and \(T'\) be instants in a trajectory with \(T < T'\). If
\(\neg P\) holds at \(T\) and \(x\) knows \(P\) at \(T'\) then there
is a path in the timeline diagram from point \(T\) on \(y\)'s timeline
to point \(T'\) on \(x\)'s timeline.ExampleIn the figure below, agent \(C\) holds a token at point \(T\).
At a later point \(T'\) agent \(A\) knows that agent \(C\) holds no
tokens. What must happen between points \(T\) and \(T'\)?>Fig. 6. What must happen between T and T'?There must be a path in the timeline diagram from point \(T\) on agent
\(C\)'s timeline to point \(T'\) on agent \(A\)'s timeline. This path
is represented by edges that show time elapsing on a timeline and
message edges between timelines.>Fig. 7. There must be a path from C at T to A at T'?What Agents Know about Channel StatesNext, let's look at systems in which messages are acknowledged.
For a pair of agents \(x, y\), let \(ms\) and \(mr\)
be the number of messages that \(x\) has sent to \(y\), and  the number 
of messages that \(y\) has received from \(x\), respectively.
Let \(as\) and \(ar\) be
the number of acknowledgements that \(y\) has sent to \(x\), and
the number of acknowledgements that \(x\) has received from \(y\),
respectively.
The following is an invariant:\(ms \geq mr \geq as \geq ar \)The number of messages in the channel from \(x\) to \(y\) is \(ms -
mr\).
Because \(ms\) and \(ar\) are variables of agent \(x\),
agent \(x\)knowsan upper bound,\(\; ms - ar\), on the number of
messages in channel \((x, y)\).
So, \(x\) knows that the channel is empty when \(\; ms = ar\).What an agent cannot knowAn agent cannot know that there are
exactly \(n\) messages in a channel, for \(n > 0\).
You can prove this result using the concept of consistent
cuts. Intuitively, the agent cannot know whether a message is in the
channel or has been received.Chains of KnowledgeLet \(x, y, z\) be agents of a system and \(P\) be a predicate on
states of the system. Then the following are all predicates:\(z\) knows \(P\)\(y\) knows that \(z\) knows \(P\)\(x\) knows that \(y\) knows that \(z\) knows \(P\)The theorems given earlier apply to any predicate. For example, if
\(x\) knows that \(y\) knows that \(z\) knows \(P\) at a point \(t\) in a
trajectory, and \(\neg P\) holds at a later point \(t'\)
then there must be path in the timeline diagram from point \(t\) on
\(x\)'s timeline to point \(t'\) on \(z\)'s timeline.Concurrent Systems with Shared VariablesThe theorems
and proofs given in this module apply to systems with shared
variables, and indeed any system with trajectories that are
representable by timeline diagrams and with consistent cuts.SummaryMany people working on distributed systems use the phrase "an agent
knows." This module gives a definition of the concept that is
consistent with intuitive definitions of knowledge.
The central idea in this module is the
relationship between what agents know and consistent cuts of
timelines. We presented several theorems about agent knowledge which
are intuitive when applied to human agents. The proofs are
straightforward and are all based on consistent cuts of timelines.ReviewIs the following true? Why?(\(z\) knows \(P\)) and (\(z\) knows \(Q\)) \(\equiv\) \(z\) knows
  \(\(P \wedge Q\))Is the following true? Why?(\(z\) knows \(P\)) or (\(z\) knows \(Q\)) \(\equiv\) \(z\) knows
  \(\( P \vee Q\))Suppose you and your friend communicate only using messages sent in
  channels, just as agents do in our model of distributed
  systems. When youknowthat your friend is wearing a cap does
  that mean that (a) your friend is wearing a cap, and (b) your friend
  can't take the cap off until the friend hears from you?K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology



Figure Link: https://kmchandy.github.io/Knowledge/KnowledgeDiagrams/Slide02.jpg

Caption: Fig. 2. Agent A knows P in all consistent cuts that cross
    point T


We
    define the predicate "\(x\)knows\(P\)" as follows.\(x\) knows \(P\) is the weakest local predicate \(Q\) of
    \(x\) such that:\(
    [init \; \Rightarrow \; always([Q \;
    \Rightarrow \; P])]
    \)\(x\) knows \(P\) holds in a local state \(s_{x}\) of agent \(x\)
exactly when \(P\) holds in all states of all trajectories
    (that start from an initial state) when the
local state of agent \(x\) is \(s_{x}\).ExplanationFor any predicate \(R\):\(
[init  \; \Rightarrow \; always(R)]
\)means that \(R\) holds in every state of every trajectory that starts
in an initial state. So,\(
[init  \; \Rightarrow \; always([Q \Rightarrow P])]
\)means that in \([Q \Rightarrow P]\) holds in every state of every
trajectory that starts in an initial state.
So, if local predicate \(Q\) of agent \(x\) holds in any
state of any trajectory that starts from an initial state then global predicate \(P\)
also holds in that state.ExampleA system consisting of agents \(0, \ldots,  N\) has two indivisible
    tokens which are not created or destroyed."Agent \(0\) knows no other agent holds a token" is a predicate on the states of agent
\(0\); this predicate holds for a local state \(s_{0}\) of agent \(0\)
if and only if no other agent holds a token when agent \(0\) is in
state \(s_{0}\).So, "agent \(0\) knows P" holds exactly when agent \(0\)
holds both tokens.ExampleThis example deals with a system consisting of two agents \(x\) and
\(y\) and channels in both directions between the agents. The system
has a single indivisible token that is not created or destroyed.
Let
\(P\) be the predicate: "\(y\) doesnothold the token."
Agent \(x\) knows \(P\) when \(x\) holds the token.\(\neg\) (\(x\) knows \(P\)) is a predicate too, and sometimes
programmers refer to this predicate as "\(x\) does not know \(P\)."ExampleIn the previous example, let \(Q\) be the predicate: "\(y\) holds the token."
In what local states of agent \(x\) does \(x\) know that "\(y\) holds the token?"There are no local states of agent \(x\) in which \(x\) knows that
"\(y\) holds the token." Even when \(x\) does not hold the token,
\(x\) does not know that \(y\) holds the token because
the token could be in a channel.When \(x\) does not hold the token, \(x\) does not know that \(y\) holds
the token and \(x\) does not know that \(y\) does not hold the token."NOT(x knows Q)" AND "NOT(x knows NOT Q)"is a predicate which holds in the
state which \(x\) does not hold the token.Notation"\(x\) knows \(P\) at a point \(T\)" in a timeline means that at
    point \(T\) agent \(x\) is in a state where the predicate "\(x\)
knows \(P\)" holds.Theorem: Knowledge and Consistent CutsLet \(P\) be a predicate on the states of a system.If  \(x\) knows \(P\) at a point \(T\) in \(x\)'s timeline
then \(P\) holds in every consistent cut through that point.ProofThe states corresponding to all consistent cuts that pass through the same
point on \(x\)'s timeline have the same common value for \(x\)'s local
state.Example>Fig. 2. Agent A knows P in all consistent cuts that cross
    point TThe top figure in both diagrams above show a time T at which agent A knows
that P holds. This implies that P holds in all consistent cuts through
point T. The lower figures in the diagrams show consistent cuts which
passes point T on A's timeline; the theorem says that P holds for the
state at these cuts too.Theorem: A Silent Agent retains
KnowledgeAn agent that sends no information between a point \(T\) and a later
point \(T'\) retains all the knowledge it has at \(T\) at \(T'\).Let \(x\) be an agent in a system, and let \(P\) be a
predicate on a subsystem that does not include \(x\).
Let \(T\) and \(T'\) be points on \(x\)'s timeline with \(T < T'\). If
\(x\) knows \(P\) at point \(T\) and \(x\) sends no messages in the
interval \([T, T']\) then \(x\) knows \(P\) at \(T'\).ProofLet \(c'\) be any consistent cut through point \(T'\) on \(x\)'s
timeline. We will prove that \(P\) holds for the state at cut \(c'\).Let \(c\) be the
cut that is identical to \(c'\) except that it passes through point \(T\) on
\(x\)'s timeline. \(c\) is consistent because there are no
outgoing edges from \(x\)'s timeline between cuts \(c\) and
\(c'\).Because \(x\) knows \(P\) at \(T\), \(P\) holds at \(c\). Since \(c\) and \(c'\)
are identical except for the intersection with \(x\)'s timeline,
it follows that \(P\) holds \(c'\).Example>Fig. 3. Illustration of Proof of Silent AgentsConsequence of the TheoremAn agent doesn't lose knowledge by getting information from other
agents.An agent can only lose knowledge by sending information to
other agents. This seems counterintuitive; we'll look at the reasoning
underlying this in a later theorem.Theorem: Agents who don't listen remain
IgnorantAn agent that receives no information between a point \(T\) and a later
point \(T'\) learns no new knowledge between \(T\) and \(T'\).Let \(x\) be an agent in a system, and let \(P\) be a
predicate on a subsystem that does not include \(x\).
Let \(T\) and \(T'\) be points on \(x\)'s timeline with \(T < T'\). If
\(x\) knows \(P\) at point \(T'\) and \(x\) received no messages in the
interval \([T, T']\) then \(x\) knows \(P\) at \(T\).The proof has exactly the same structure as the proof of the
previous theorem.Consequence of the Theorem\(x\) didn't learn anything in the interval  \([T, T']\); everything
\(x\) knows at the later point \(T'\) is knowledge it already had at
the earlier point \(T\).The only way for an agent to gain knowledge is
to receive messages. An agent cannot learn about other agents by only
sending messages or making internal state transitions.Theorem: Knowledge implies ControlLet \(x\) and \(y\) be agents in a system, and let \(P\) be a
predicate on the states of \(y\).
Let \(T\) and \(T'\) be instants in a trajectory with \(T < T'\). If
\(x\) knows \(P\) at  \(T\), and \(\neg P\) holds at \(T'\), then there
is a path in the timeline diagram from point \(T\) on \(x\)'s timeline
to point \(T'\) on \(y\)'s timeline.Proof: If there is no path from point \(T\) on \(x\)'s timeline
to point \(T'\) on \(y\)'s timeline then there exists a consistent cut
which crosses \(x\)'s timeline at \(T\) and
crosses \(y\)'s timeline at \(T'\).ExampleIn the figure below, agent \(A\) at point \(T\) knows that agent \(C\)
holds no tokens. At a later point \(T'\) agent \(C\) holds a
token. What must happen between points \(T\) and \(T'\)?>Fig. 4. What must happen between T and T'?There must be a path in the timeline diagram from point \(T\) on agent
\(A\)'s timeline to point \(T'\) on agent \(C\)'s timeline. This path
is represented by edges that show time elapsing on a timeline and
message edges between timelines.>Fig. 5. There must be a path from A at T to C at T'?Consequence of the TheoremSuppose you and your friend communicate only by means of messages
that are delayed by arbitrary (finite) amounts. Consider a situation
where your friend knows
that you are in the library at 9 pm. Then, from our definition of
knowledge, because agents only know truth, you must be in the library
at 9 pm. Moreover, you can't leave the library until
you receive a message from your friend; this message may go through
intermediate agents.In one of the exercises we'll look at knowledge when agents have
clocks that may drift from each other but are not more than a some
constant \(M\) units apart. If your friend knows that you will be in
the library till her watch reads 9:00 pm, and your watches may drift apart
by a minute, then you can leave the library at 9:01 pm. Clocks are
useful even if they aren't perfect. More about clocks later.Theorem: Communication to learn about
ChangeThis theorem is similar to the "knowledge is control" theorem.Let \(x\) and \(y\) be agents in a system, and let \(P\) be a
predicate on the states of \(y\).
Let \(T\) and \(T'\) be instants in a trajectory with \(T < T'\). If
\(\neg P\) holds at \(T\) and \(x\) knows \(P\) at \(T'\) then there
is a path in the timeline diagram from point \(T\) on \(y\)'s timeline
to point \(T'\) on \(x\)'s timeline.ExampleIn the figure below, agent \(C\) holds a token at point \(T\).
At a later point \(T'\) agent \(A\) knows that agent \(C\) holds no
tokens. What must happen between points \(T\) and \(T'\)?>Fig. 6. What must happen between T and T'?There must be a path in the timeline diagram from point \(T\) on agent
\(C\)'s timeline to point \(T'\) on agent \(A\)'s timeline. This path
is represented by edges that show time elapsing on a timeline and
message edges between timelines.>Fig. 7. There must be a path from C at T to A at T'?What Agents Know about Channel StatesNext, let's look at systems in which messages are acknowledged.
For a pair of agents \(x, y\), let \(ms\) and \(mr\)
be the number of messages that \(x\) has sent to \(y\), and  the number 
of messages that \(y\) has received from \(x\), respectively.
Let \(as\) and \(ar\) be
the number of acknowledgements that \(y\) has sent to \(x\), and
the number of acknowledgements that \(x\) has received from \(y\),
respectively.
The following is an invariant:\(ms \geq mr \geq as \geq ar \)The number of messages in the channel from \(x\) to \(y\) is \(ms -
mr\).
Because \(ms\) and \(ar\) are variables of agent \(x\),
agent \(x\)knowsan upper bound,\(\; ms - ar\), on the number of
messages in channel \((x, y)\).
So, \(x\) knows that the channel is empty when \(\; ms = ar\).What an agent cannot knowAn agent cannot know that there are
exactly \(n\) messages in a channel, for \(n > 0\).
You can prove this result using the concept of consistent
cuts. Intuitively, the agent cannot know whether a message is in the
channel or has been received.Chains of KnowledgeLet \(x, y, z\) be agents of a system and \(P\) be a predicate on
states of the system. Then the following are all predicates:\(z\) knows \(P\)\(y\) knows that \(z\) knows \(P\)\(x\) knows that \(y\) knows that \(z\) knows \(P\)The theorems given earlier apply to any predicate. For example, if
\(x\) knows that \(y\) knows that \(z\) knows \(P\) at a point \(t\) in a
trajectory, and \(\neg P\) holds at a later point \(t'\)
then there must be path in the timeline diagram from point \(t\) on
\(x\)'s timeline to point \(t'\) on \(z\)'s timeline.Concurrent Systems with Shared VariablesThe theorems
and proofs given in this module apply to systems with shared
variables, and indeed any system with trajectories that are
representable by timeline diagrams and with consistent cuts.SummaryMany people working on distributed systems use the phrase "an agent
knows." This module gives a definition of the concept that is
consistent with intuitive definitions of knowledge.
The central idea in this module is the
relationship between what agents know and consistent cuts of
timelines. We presented several theorems about agent knowledge which
are intuitive when applied to human agents. The proofs are
straightforward and are all based on consistent cuts of timelines.ReviewIs the following true? Why?(\(z\) knows \(P\)) and (\(z\) knows \(Q\)) \(\equiv\) \(z\) knows
  \(\(P \wedge Q\))Is the following true? Why?(\(z\) knows \(P\)) or (\(z\) knows \(Q\)) \(\equiv\) \(z\) knows
  \(\( P \vee Q\))Suppose you and your friend communicate only using messages sent in
  channels, just as agents do in our model of distributed
  systems. When youknowthat your friend is wearing a cap does
  that mean that (a) your friend is wearing a cap, and (b) your friend
  can't take the cap off until the friend hears from you?K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology



Figure Link: https://kmchandy.github.io/Knowledge/KnowledgeDiagrams/Slide02.jpg

Caption: Fig. 2. Agent A knows P in all consistent cuts that cross
    point T


\(x\) knows \(P\) is the weakest local predicate \(Q\) of
    \(x\) such that:\(
    [init \; \Rightarrow \; always([Q \;
    \Rightarrow \; P])]
    \)



Figure Link: https://kmchandy.github.io/Knowledge/KnowledgeDiagrams/Slide02.jpg

Caption: Fig. 2. Agent A knows P in all consistent cuts that cross
    point T


\(
    [init \; \Rightarrow \; always([Q \;
    \Rightarrow \; P])]
    \)



Figure Link: https://kmchandy.github.io/Knowledge/KnowledgeDiagrams/Slide02.jpg

Caption: Fig. 2. Agent A knows P in all consistent cuts that cross
    point T






Figure Link: https://kmchandy.github.io/Knowledge/KnowledgeDiagrams/Slide02.jpg

Caption: Fig. 2. Agent A knows P in all consistent cuts that cross
    point T


\(x\) knows \(P\) holds in a local state \(s_{x}\) of agent \(x\)
exactly when \(P\) holds in all states of all trajectories
    (that start from an initial state) when the
local state of agent \(x\) is \(s_{x}\).ExplanationFor any predicate \(R\):\(
[init  \; \Rightarrow \; always(R)]
\)means that \(R\) holds in every state of every trajectory that starts
in an initial state. So,\(
[init  \; \Rightarrow \; always([Q \Rightarrow P])]
\)means that in \([Q \Rightarrow P]\) holds in every state of every
trajectory that starts in an initial state.
So, if local predicate \(Q\) of agent \(x\) holds in any
state of any trajectory that starts from an initial state then global predicate \(P\)
also holds in that state.ExampleA system consisting of agents \(0, \ldots,  N\) has two indivisible
    tokens which are not created or destroyed."Agent \(0\) knows no other agent holds a token" is a predicate on the states of agent
\(0\); this predicate holds for a local state \(s_{0}\) of agent \(0\)
if and only if no other agent holds a token when agent \(0\) is in
state \(s_{0}\).So, "agent \(0\) knows P" holds exactly when agent \(0\)
holds both tokens.ExampleThis example deals with a system consisting of two agents \(x\) and
\(y\) and channels in both directions between the agents. The system
has a single indivisible token that is not created or destroyed.
Let
\(P\) be the predicate: "\(y\) doesnothold the token."
Agent \(x\) knows \(P\) when \(x\) holds the token.\(\neg\) (\(x\) knows \(P\)) is a predicate too, and sometimes
programmers refer to this predicate as "\(x\) does not know \(P\)."ExampleIn the previous example, let \(Q\) be the predicate: "\(y\) holds the token."
In what local states of agent \(x\) does \(x\) know that "\(y\) holds the token?"There are no local states of agent \(x\) in which \(x\) knows that
"\(y\) holds the token." Even when \(x\) does not hold the token,
\(x\) does not know that \(y\) holds the token because
the token could be in a channel.When \(x\) does not hold the token, \(x\) does not know that \(y\) holds
the token and \(x\) does not know that \(y\) does not hold the token."NOT(x knows Q)" AND "NOT(x knows NOT Q)"is a predicate which holds in the
state which \(x\) does not hold the token.Notation"\(x\) knows \(P\) at a point \(T\)" in a timeline means that at
    point \(T\) agent \(x\) is in a state where the predicate "\(x\)
knows \(P\)" holds.Theorem: Knowledge and Consistent CutsLet \(P\) be a predicate on the states of a system.If  \(x\) knows \(P\) at a point \(T\) in \(x\)'s timeline
then \(P\) holds in every consistent cut through that point.ProofThe states corresponding to all consistent cuts that pass through the same
point on \(x\)'s timeline have the same common value for \(x\)'s local
state.Example>Fig. 2. Agent A knows P in all consistent cuts that cross
    point TThe top figure in both diagrams above show a time T at which agent A knows
that P holds. This implies that P holds in all consistent cuts through
point T. The lower figures in the diagrams show consistent cuts which
passes point T on A's timeline; the theorem says that P holds for the
state at these cuts too.Theorem: A Silent Agent retains
KnowledgeAn agent that sends no information between a point \(T\) and a later
point \(T'\) retains all the knowledge it has at \(T\) at \(T'\).Let \(x\) be an agent in a system, and let \(P\) be a
predicate on a subsystem that does not include \(x\).
Let \(T\) and \(T'\) be points on \(x\)'s timeline with \(T < T'\). If
\(x\) knows \(P\) at point \(T\) and \(x\) sends no messages in the
interval \([T, T']\) then \(x\) knows \(P\) at \(T'\).ProofLet \(c'\) be any consistent cut through point \(T'\) on \(x\)'s
timeline. We will prove that \(P\) holds for the state at cut \(c'\).Let \(c\) be the
cut that is identical to \(c'\) except that it passes through point \(T\) on
\(x\)'s timeline. \(c\) is consistent because there are no
outgoing edges from \(x\)'s timeline between cuts \(c\) and
\(c'\).Because \(x\) knows \(P\) at \(T\), \(P\) holds at \(c\). Since \(c\) and \(c'\)
are identical except for the intersection with \(x\)'s timeline,
it follows that \(P\) holds \(c'\).Example>Fig. 3. Illustration of Proof of Silent AgentsConsequence of the TheoremAn agent doesn't lose knowledge by getting information from other
agents.An agent can only lose knowledge by sending information to
other agents. This seems counterintuitive; we'll look at the reasoning
underlying this in a later theorem.Theorem: Agents who don't listen remain
IgnorantAn agent that receives no information between a point \(T\) and a later
point \(T'\) learns no new knowledge between \(T\) and \(T'\).Let \(x\) be an agent in a system, and let \(P\) be a
predicate on a subsystem that does not include \(x\).
Let \(T\) and \(T'\) be points on \(x\)'s timeline with \(T < T'\). If
\(x\) knows \(P\) at point \(T'\) and \(x\) received no messages in the
interval \([T, T']\) then \(x\) knows \(P\) at \(T\).The proof has exactly the same structure as the proof of the
previous theorem.Consequence of the Theorem\(x\) didn't learn anything in the interval  \([T, T']\); everything
\(x\) knows at the later point \(T'\) is knowledge it already had at
the earlier point \(T\).The only way for an agent to gain knowledge is
to receive messages. An agent cannot learn about other agents by only
sending messages or making internal state transitions.Theorem: Knowledge implies ControlLet \(x\) and \(y\) be agents in a system, and let \(P\) be a
predicate on the states of \(y\).
Let \(T\) and \(T'\) be instants in a trajectory with \(T < T'\). If
\(x\) knows \(P\) at  \(T\), and \(\neg P\) holds at \(T'\), then there
is a path in the timeline diagram from point \(T\) on \(x\)'s timeline
to point \(T'\) on \(y\)'s timeline.Proof: If there is no path from point \(T\) on \(x\)'s timeline
to point \(T'\) on \(y\)'s timeline then there exists a consistent cut
which crosses \(x\)'s timeline at \(T\) and
crosses \(y\)'s timeline at \(T'\).ExampleIn the figure below, agent \(A\) at point \(T\) knows that agent \(C\)
holds no tokens. At a later point \(T'\) agent \(C\) holds a
token. What must happen between points \(T\) and \(T'\)?>Fig. 4. What must happen between T and T'?There must be a path in the timeline diagram from point \(T\) on agent
\(A\)'s timeline to point \(T'\) on agent \(C\)'s timeline. This path
is represented by edges that show time elapsing on a timeline and
message edges between timelines.>Fig. 5. There must be a path from A at T to C at T'?Consequence of the TheoremSuppose you and your friend communicate only by means of messages
that are delayed by arbitrary (finite) amounts. Consider a situation
where your friend knows
that you are in the library at 9 pm. Then, from our definition of
knowledge, because agents only know truth, you must be in the library
at 9 pm. Moreover, you can't leave the library until
you receive a message from your friend; this message may go through
intermediate agents.In one of the exercises we'll look at knowledge when agents have
clocks that may drift from each other but are not more than a some
constant \(M\) units apart. If your friend knows that you will be in
the library till her watch reads 9:00 pm, and your watches may drift apart
by a minute, then you can leave the library at 9:01 pm. Clocks are
useful even if they aren't perfect. More about clocks later.Theorem: Communication to learn about
ChangeThis theorem is similar to the "knowledge is control" theorem.Let \(x\) and \(y\) be agents in a system, and let \(P\) be a
predicate on the states of \(y\).
Let \(T\) and \(T'\) be instants in a trajectory with \(T < T'\). If
\(\neg P\) holds at \(T\) and \(x\) knows \(P\) at \(T'\) then there
is a path in the timeline diagram from point \(T\) on \(y\)'s timeline
to point \(T'\) on \(x\)'s timeline.ExampleIn the figure below, agent \(C\) holds a token at point \(T\).
At a later point \(T'\) agent \(A\) knows that agent \(C\) holds no
tokens. What must happen between points \(T\) and \(T'\)?>Fig. 6. What must happen between T and T'?There must be a path in the timeline diagram from point \(T\) on agent
\(C\)'s timeline to point \(T'\) on agent \(A\)'s timeline. This path
is represented by edges that show time elapsing on a timeline and
message edges between timelines.>Fig. 7. There must be a path from C at T to A at T'?What Agents Know about Channel StatesNext, let's look at systems in which messages are acknowledged.
For a pair of agents \(x, y\), let \(ms\) and \(mr\)
be the number of messages that \(x\) has sent to \(y\), and  the number 
of messages that \(y\) has received from \(x\), respectively.
Let \(as\) and \(ar\) be
the number of acknowledgements that \(y\) has sent to \(x\), and
the number of acknowledgements that \(x\) has received from \(y\),
respectively.
The following is an invariant:\(ms \geq mr \geq as \geq ar \)The number of messages in the channel from \(x\) to \(y\) is \(ms -
mr\).
Because \(ms\) and \(ar\) are variables of agent \(x\),
agent \(x\)knowsan upper bound,\(\; ms - ar\), on the number of
messages in channel \((x, y)\).
So, \(x\) knows that the channel is empty when \(\; ms = ar\).What an agent cannot knowAn agent cannot know that there are
exactly \(n\) messages in a channel, for \(n > 0\).
You can prove this result using the concept of consistent
cuts. Intuitively, the agent cannot know whether a message is in the
channel or has been received.Chains of KnowledgeLet \(x, y, z\) be agents of a system and \(P\) be a predicate on
states of the system. Then the following are all predicates:\(z\) knows \(P\)\(y\) knows that \(z\) knows \(P\)\(x\) knows that \(y\) knows that \(z\) knows \(P\)The theorems given earlier apply to any predicate. For example, if
\(x\) knows that \(y\) knows that \(z\) knows \(P\) at a point \(t\) in a
trajectory, and \(\neg P\) holds at a later point \(t'\)
then there must be path in the timeline diagram from point \(t\) on
\(x\)'s timeline to point \(t'\) on \(z\)'s timeline.Concurrent Systems with Shared VariablesThe theorems
and proofs given in this module apply to systems with shared
variables, and indeed any system with trajectories that are
representable by timeline diagrams and with consistent cuts.SummaryMany people working on distributed systems use the phrase "an agent
knows." This module gives a definition of the concept that is
consistent with intuitive definitions of knowledge.
The central idea in this module is the
relationship between what agents know and consistent cuts of
timelines. We presented several theorems about agent knowledge which
are intuitive when applied to human agents. The proofs are
straightforward and are all based on consistent cuts of timelines.ReviewIs the following true? Why?(\(z\) knows \(P\)) and (\(z\) knows \(Q\)) \(\equiv\) \(z\) knows
  \(\(P \wedge Q\))Is the following true? Why?(\(z\) knows \(P\)) or (\(z\) knows \(Q\)) \(\equiv\) \(z\) knows
  \(\( P \vee Q\))Suppose you and your friend communicate only using messages sent in
  channels, just as agents do in our model of distributed
  systems. When youknowthat your friend is wearing a cap does
  that mean that (a) your friend is wearing a cap, and (b) your friend
  can't take the cap off until the friend hears from you?K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology



Figure Link: https://kmchandy.github.io/Knowledge/KnowledgeDiagrams/Slide02.jpg

Caption: Fig. 2. Agent A knows P in all consistent cuts that cross
    point T


\(
[init  \; \Rightarrow \; always(R)]
\)means that \(R\) holds in every state of every trajectory that starts
in an initial state. So,\(
[init  \; \Rightarrow \; always([Q \Rightarrow P])]
\)means that in \([Q \Rightarrow P]\) holds in every state of every
trajectory that starts in an initial state.
So, if local predicate \(Q\) of agent \(x\) holds in any
state of any trajectory that starts from an initial state then global predicate \(P\)
also holds in that state.ExampleA system consisting of agents \(0, \ldots,  N\) has two indivisible
    tokens which are not created or destroyed."Agent \(0\) knows no other agent holds a token" is a predicate on the states of agent
\(0\); this predicate holds for a local state \(s_{0}\) of agent \(0\)
if and only if no other agent holds a token when agent \(0\) is in
state \(s_{0}\).So, "agent \(0\) knows P" holds exactly when agent \(0\)
holds both tokens.ExampleThis example deals with a system consisting of two agents \(x\) and
\(y\) and channels in both directions between the agents. The system
has a single indivisible token that is not created or destroyed.
Let
\(P\) be the predicate: "\(y\) doesnothold the token."
Agent \(x\) knows \(P\) when \(x\) holds the token.\(\neg\) (\(x\) knows \(P\)) is a predicate too, and sometimes
programmers refer to this predicate as "\(x\) does not know \(P\)."ExampleIn the previous example, let \(Q\) be the predicate: "\(y\) holds the token."
In what local states of agent \(x\) does \(x\) know that "\(y\) holds the token?"There are no local states of agent \(x\) in which \(x\) knows that
"\(y\) holds the token." Even when \(x\) does not hold the token,
\(x\) does not know that \(y\) holds the token because
the token could be in a channel.When \(x\) does not hold the token, \(x\) does not know that \(y\) holds
the token and \(x\) does not know that \(y\) does not hold the token."NOT(x knows Q)" AND "NOT(x knows NOT Q)"is a predicate which holds in the
state which \(x\) does not hold the token.Notation"\(x\) knows \(P\) at a point \(T\)" in a timeline means that at
    point \(T\) agent \(x\) is in a state where the predicate "\(x\)
knows \(P\)" holds.Theorem: Knowledge and Consistent CutsLet \(P\) be a predicate on the states of a system.If  \(x\) knows \(P\) at a point \(T\) in \(x\)'s timeline
then \(P\) holds in every consistent cut through that point.ProofThe states corresponding to all consistent cuts that pass through the same
point on \(x\)'s timeline have the same common value for \(x\)'s local
state.Example>Fig. 2. Agent A knows P in all consistent cuts that cross
    point TThe top figure in both diagrams above show a time T at which agent A knows
that P holds. This implies that P holds in all consistent cuts through
point T. The lower figures in the diagrams show consistent cuts which
passes point T on A's timeline; the theorem says that P holds for the
state at these cuts too.Theorem: A Silent Agent retains
KnowledgeAn agent that sends no information between a point \(T\) and a later
point \(T'\) retains all the knowledge it has at \(T\) at \(T'\).Let \(x\) be an agent in a system, and let \(P\) be a
predicate on a subsystem that does not include \(x\).
Let \(T\) and \(T'\) be points on \(x\)'s timeline with \(T < T'\). If
\(x\) knows \(P\) at point \(T\) and \(x\) sends no messages in the
interval \([T, T']\) then \(x\) knows \(P\) at \(T'\).ProofLet \(c'\) be any consistent cut through point \(T'\) on \(x\)'s
timeline. We will prove that \(P\) holds for the state at cut \(c'\).Let \(c\) be the
cut that is identical to \(c'\) except that it passes through point \(T\) on
\(x\)'s timeline. \(c\) is consistent because there are no
outgoing edges from \(x\)'s timeline between cuts \(c\) and
\(c'\).Because \(x\) knows \(P\) at \(T\), \(P\) holds at \(c\). Since \(c\) and \(c'\)
are identical except for the intersection with \(x\)'s timeline,
it follows that \(P\) holds \(c'\).Example>Fig. 3. Illustration of Proof of Silent AgentsConsequence of the TheoremAn agent doesn't lose knowledge by getting information from other
agents.An agent can only lose knowledge by sending information to
other agents. This seems counterintuitive; we'll look at the reasoning
underlying this in a later theorem.Theorem: Agents who don't listen remain
IgnorantAn agent that receives no information between a point \(T\) and a later
point \(T'\) learns no new knowledge between \(T\) and \(T'\).Let \(x\) be an agent in a system, and let \(P\) be a
predicate on a subsystem that does not include \(x\).
Let \(T\) and \(T'\) be points on \(x\)'s timeline with \(T < T'\). If
\(x\) knows \(P\) at point \(T'\) and \(x\) received no messages in the
interval \([T, T']\) then \(x\) knows \(P\) at \(T\).The proof has exactly the same structure as the proof of the
previous theorem.Consequence of the Theorem\(x\) didn't learn anything in the interval  \([T, T']\); everything
\(x\) knows at the later point \(T'\) is knowledge it already had at
the earlier point \(T\).The only way for an agent to gain knowledge is
to receive messages. An agent cannot learn about other agents by only
sending messages or making internal state transitions.Theorem: Knowledge implies ControlLet \(x\) and \(y\) be agents in a system, and let \(P\) be a
predicate on the states of \(y\).
Let \(T\) and \(T'\) be instants in a trajectory with \(T < T'\). If
\(x\) knows \(P\) at  \(T\), and \(\neg P\) holds at \(T'\), then there
is a path in the timeline diagram from point \(T\) on \(x\)'s timeline
to point \(T'\) on \(y\)'s timeline.Proof: If there is no path from point \(T\) on \(x\)'s timeline
to point \(T'\) on \(y\)'s timeline then there exists a consistent cut
which crosses \(x\)'s timeline at \(T\) and
crosses \(y\)'s timeline at \(T'\).ExampleIn the figure below, agent \(A\) at point \(T\) knows that agent \(C\)
holds no tokens. At a later point \(T'\) agent \(C\) holds a
token. What must happen between points \(T\) and \(T'\)?>Fig. 4. What must happen between T and T'?There must be a path in the timeline diagram from point \(T\) on agent
\(A\)'s timeline to point \(T'\) on agent \(C\)'s timeline. This path
is represented by edges that show time elapsing on a timeline and
message edges between timelines.>Fig. 5. There must be a path from A at T to C at T'?Consequence of the TheoremSuppose you and your friend communicate only by means of messages
that are delayed by arbitrary (finite) amounts. Consider a situation
where your friend knows
that you are in the library at 9 pm. Then, from our definition of
knowledge, because agents only know truth, you must be in the library
at 9 pm. Moreover, you can't leave the library until
you receive a message from your friend; this message may go through
intermediate agents.In one of the exercises we'll look at knowledge when agents have
clocks that may drift from each other but are not more than a some
constant \(M\) units apart. If your friend knows that you will be in
the library till her watch reads 9:00 pm, and your watches may drift apart
by a minute, then you can leave the library at 9:01 pm. Clocks are
useful even if they aren't perfect. More about clocks later.Theorem: Communication to learn about
ChangeThis theorem is similar to the "knowledge is control" theorem.Let \(x\) and \(y\) be agents in a system, and let \(P\) be a
predicate on the states of \(y\).
Let \(T\) and \(T'\) be instants in a trajectory with \(T < T'\). If
\(\neg P\) holds at \(T\) and \(x\) knows \(P\) at \(T'\) then there
is a path in the timeline diagram from point \(T\) on \(y\)'s timeline
to point \(T'\) on \(x\)'s timeline.ExampleIn the figure below, agent \(C\) holds a token at point \(T\).
At a later point \(T'\) agent \(A\) knows that agent \(C\) holds no
tokens. What must happen between points \(T\) and \(T'\)?>Fig. 6. What must happen between T and T'?There must be a path in the timeline diagram from point \(T\) on agent
\(C\)'s timeline to point \(T'\) on agent \(A\)'s timeline. This path
is represented by edges that show time elapsing on a timeline and
message edges between timelines.>Fig. 7. There must be a path from C at T to A at T'?What Agents Know about Channel StatesNext, let's look at systems in which messages are acknowledged.
For a pair of agents \(x, y\), let \(ms\) and \(mr\)
be the number of messages that \(x\) has sent to \(y\), and  the number 
of messages that \(y\) has received from \(x\), respectively.
Let \(as\) and \(ar\) be
the number of acknowledgements that \(y\) has sent to \(x\), and
the number of acknowledgements that \(x\) has received from \(y\),
respectively.
The following is an invariant:\(ms \geq mr \geq as \geq ar \)The number of messages in the channel from \(x\) to \(y\) is \(ms -
mr\).
Because \(ms\) and \(ar\) are variables of agent \(x\),
agent \(x\)knowsan upper bound,\(\; ms - ar\), on the number of
messages in channel \((x, y)\).
So, \(x\) knows that the channel is empty when \(\; ms = ar\).What an agent cannot knowAn agent cannot know that there are
exactly \(n\) messages in a channel, for \(n > 0\).
You can prove this result using the concept of consistent
cuts. Intuitively, the agent cannot know whether a message is in the
channel or has been received.Chains of KnowledgeLet \(x, y, z\) be agents of a system and \(P\) be a predicate on
states of the system. Then the following are all predicates:\(z\) knows \(P\)\(y\) knows that \(z\) knows \(P\)\(x\) knows that \(y\) knows that \(z\) knows \(P\)The theorems given earlier apply to any predicate. For example, if
\(x\) knows that \(y\) knows that \(z\) knows \(P\) at a point \(t\) in a
trajectory, and \(\neg P\) holds at a later point \(t'\)
then there must be path in the timeline diagram from point \(t\) on
\(x\)'s timeline to point \(t'\) on \(z\)'s timeline.Concurrent Systems with Shared VariablesThe theorems
and proofs given in this module apply to systems with shared
variables, and indeed any system with trajectories that are
representable by timeline diagrams and with consistent cuts.SummaryMany people working on distributed systems use the phrase "an agent
knows." This module gives a definition of the concept that is
consistent with intuitive definitions of knowledge.
The central idea in this module is the
relationship between what agents know and consistent cuts of
timelines. We presented several theorems about agent knowledge which
are intuitive when applied to human agents. The proofs are
straightforward and are all based on consistent cuts of timelines.ReviewIs the following true? Why?(\(z\) knows \(P\)) and (\(z\) knows \(Q\)) \(\equiv\) \(z\) knows
  \(\(P \wedge Q\))Is the following true? Why?(\(z\) knows \(P\)) or (\(z\) knows \(Q\)) \(\equiv\) \(z\) knows
  \(\( P \vee Q\))Suppose you and your friend communicate only using messages sent in
  channels, just as agents do in our model of distributed
  systems. When youknowthat your friend is wearing a cap does
  that mean that (a) your friend is wearing a cap, and (b) your friend
  can't take the cap off until the friend hears from you?K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology



Figure Link: https://kmchandy.github.io/Knowledge/KnowledgeDiagrams/Slide02.jpg

Caption: Fig. 2. Agent A knows P in all consistent cuts that cross
    point T


means that \(R\) holds in every state of every trajectory that starts
in an initial state. So,\(
[init  \; \Rightarrow \; always([Q \Rightarrow P])]
\)means that in \([Q \Rightarrow P]\) holds in every state of every
trajectory that starts in an initial state.
So, if local predicate \(Q\) of agent \(x\) holds in any
state of any trajectory that starts from an initial state then global predicate \(P\)
also holds in that state.ExampleA system consisting of agents \(0, \ldots,  N\) has two indivisible
    tokens which are not created or destroyed."Agent \(0\) knows no other agent holds a token" is a predicate on the states of agent
\(0\); this predicate holds for a local state \(s_{0}\) of agent \(0\)
if and only if no other agent holds a token when agent \(0\) is in
state \(s_{0}\).So, "agent \(0\) knows P" holds exactly when agent \(0\)
holds both tokens.ExampleThis example deals with a system consisting of two agents \(x\) and
\(y\) and channels in both directions between the agents. The system
has a single indivisible token that is not created or destroyed.
Let
\(P\) be the predicate: "\(y\) doesnothold the token."
Agent \(x\) knows \(P\) when \(x\) holds the token.\(\neg\) (\(x\) knows \(P\)) is a predicate too, and sometimes
programmers refer to this predicate as "\(x\) does not know \(P\)."ExampleIn the previous example, let \(Q\) be the predicate: "\(y\) holds the token."
In what local states of agent \(x\) does \(x\) know that "\(y\) holds the token?"There are no local states of agent \(x\) in which \(x\) knows that
"\(y\) holds the token." Even when \(x\) does not hold the token,
\(x\) does not know that \(y\) holds the token because
the token could be in a channel.When \(x\) does not hold the token, \(x\) does not know that \(y\) holds
the token and \(x\) does not know that \(y\) does not hold the token."NOT(x knows Q)" AND "NOT(x knows NOT Q)"is a predicate which holds in the
state which \(x\) does not hold the token.Notation"\(x\) knows \(P\) at a point \(T\)" in a timeline means that at
    point \(T\) agent \(x\) is in a state where the predicate "\(x\)
knows \(P\)" holds.Theorem: Knowledge and Consistent CutsLet \(P\) be a predicate on the states of a system.If  \(x\) knows \(P\) at a point \(T\) in \(x\)'s timeline
then \(P\) holds in every consistent cut through that point.ProofThe states corresponding to all consistent cuts that pass through the same
point on \(x\)'s timeline have the same common value for \(x\)'s local
state.Example>Fig. 2. Agent A knows P in all consistent cuts that cross
    point TThe top figure in both diagrams above show a time T at which agent A knows
that P holds. This implies that P holds in all consistent cuts through
point T. The lower figures in the diagrams show consistent cuts which
passes point T on A's timeline; the theorem says that P holds for the
state at these cuts too.Theorem: A Silent Agent retains
KnowledgeAn agent that sends no information between a point \(T\) and a later
point \(T'\) retains all the knowledge it has at \(T\) at \(T'\).Let \(x\) be an agent in a system, and let \(P\) be a
predicate on a subsystem that does not include \(x\).
Let \(T\) and \(T'\) be points on \(x\)'s timeline with \(T < T'\). If
\(x\) knows \(P\) at point \(T\) and \(x\) sends no messages in the
interval \([T, T']\) then \(x\) knows \(P\) at \(T'\).ProofLet \(c'\) be any consistent cut through point \(T'\) on \(x\)'s
timeline. We will prove that \(P\) holds for the state at cut \(c'\).Let \(c\) be the
cut that is identical to \(c'\) except that it passes through point \(T\) on
\(x\)'s timeline. \(c\) is consistent because there are no
outgoing edges from \(x\)'s timeline between cuts \(c\) and
\(c'\).Because \(x\) knows \(P\) at \(T\), \(P\) holds at \(c\). Since \(c\) and \(c'\)
are identical except for the intersection with \(x\)'s timeline,
it follows that \(P\) holds \(c'\).Example>Fig. 3. Illustration of Proof of Silent AgentsConsequence of the TheoremAn agent doesn't lose knowledge by getting information from other
agents.An agent can only lose knowledge by sending information to
other agents. This seems counterintuitive; we'll look at the reasoning
underlying this in a later theorem.Theorem: Agents who don't listen remain
IgnorantAn agent that receives no information between a point \(T\) and a later
point \(T'\) learns no new knowledge between \(T\) and \(T'\).Let \(x\) be an agent in a system, and let \(P\) be a
predicate on a subsystem that does not include \(x\).
Let \(T\) and \(T'\) be points on \(x\)'s timeline with \(T < T'\). If
\(x\) knows \(P\) at point \(T'\) and \(x\) received no messages in the
interval \([T, T']\) then \(x\) knows \(P\) at \(T\).The proof has exactly the same structure as the proof of the
previous theorem.Consequence of the Theorem\(x\) didn't learn anything in the interval  \([T, T']\); everything
\(x\) knows at the later point \(T'\) is knowledge it already had at
the earlier point \(T\).The only way for an agent to gain knowledge is
to receive messages. An agent cannot learn about other agents by only
sending messages or making internal state transitions.Theorem: Knowledge implies ControlLet \(x\) and \(y\) be agents in a system, and let \(P\) be a
predicate on the states of \(y\).
Let \(T\) and \(T'\) be instants in a trajectory with \(T < T'\). If
\(x\) knows \(P\) at  \(T\), and \(\neg P\) holds at \(T'\), then there
is a path in the timeline diagram from point \(T\) on \(x\)'s timeline
to point \(T'\) on \(y\)'s timeline.Proof: If there is no path from point \(T\) on \(x\)'s timeline
to point \(T'\) on \(y\)'s timeline then there exists a consistent cut
which crosses \(x\)'s timeline at \(T\) and
crosses \(y\)'s timeline at \(T'\).ExampleIn the figure below, agent \(A\) at point \(T\) knows that agent \(C\)
holds no tokens. At a later point \(T'\) agent \(C\) holds a
token. What must happen between points \(T\) and \(T'\)?>Fig. 4. What must happen between T and T'?There must be a path in the timeline diagram from point \(T\) on agent
\(A\)'s timeline to point \(T'\) on agent \(C\)'s timeline. This path
is represented by edges that show time elapsing on a timeline and
message edges between timelines.>Fig. 5. There must be a path from A at T to C at T'?Consequence of the TheoremSuppose you and your friend communicate only by means of messages
that are delayed by arbitrary (finite) amounts. Consider a situation
where your friend knows
that you are in the library at 9 pm. Then, from our definition of
knowledge, because agents only know truth, you must be in the library
at 9 pm. Moreover, you can't leave the library until
you receive a message from your friend; this message may go through
intermediate agents.In one of the exercises we'll look at knowledge when agents have
clocks that may drift from each other but are not more than a some
constant \(M\) units apart. If your friend knows that you will be in
the library till her watch reads 9:00 pm, and your watches may drift apart
by a minute, then you can leave the library at 9:01 pm. Clocks are
useful even if they aren't perfect. More about clocks later.Theorem: Communication to learn about
ChangeThis theorem is similar to the "knowledge is control" theorem.Let \(x\) and \(y\) be agents in a system, and let \(P\) be a
predicate on the states of \(y\).
Let \(T\) and \(T'\) be instants in a trajectory with \(T < T'\). If
\(\neg P\) holds at \(T\) and \(x\) knows \(P\) at \(T'\) then there
is a path in the timeline diagram from point \(T\) on \(y\)'s timeline
to point \(T'\) on \(x\)'s timeline.ExampleIn the figure below, agent \(C\) holds a token at point \(T\).
At a later point \(T'\) agent \(A\) knows that agent \(C\) holds no
tokens. What must happen between points \(T\) and \(T'\)?>Fig. 6. What must happen between T and T'?There must be a path in the timeline diagram from point \(T\) on agent
\(C\)'s timeline to point \(T'\) on agent \(A\)'s timeline. This path
is represented by edges that show time elapsing on a timeline and
message edges between timelines.>Fig. 7. There must be a path from C at T to A at T'?What Agents Know about Channel StatesNext, let's look at systems in which messages are acknowledged.
For a pair of agents \(x, y\), let \(ms\) and \(mr\)
be the number of messages that \(x\) has sent to \(y\), and  the number 
of messages that \(y\) has received from \(x\), respectively.
Let \(as\) and \(ar\) be
the number of acknowledgements that \(y\) has sent to \(x\), and
the number of acknowledgements that \(x\) has received from \(y\),
respectively.
The following is an invariant:\(ms \geq mr \geq as \geq ar \)The number of messages in the channel from \(x\) to \(y\) is \(ms -
mr\).
Because \(ms\) and \(ar\) are variables of agent \(x\),
agent \(x\)knowsan upper bound,\(\; ms - ar\), on the number of
messages in channel \((x, y)\).
So, \(x\) knows that the channel is empty when \(\; ms = ar\).What an agent cannot knowAn agent cannot know that there are
exactly \(n\) messages in a channel, for \(n > 0\).
You can prove this result using the concept of consistent
cuts. Intuitively, the agent cannot know whether a message is in the
channel or has been received.Chains of KnowledgeLet \(x, y, z\) be agents of a system and \(P\) be a predicate on
states of the system. Then the following are all predicates:\(z\) knows \(P\)\(y\) knows that \(z\) knows \(P\)\(x\) knows that \(y\) knows that \(z\) knows \(P\)The theorems given earlier apply to any predicate. For example, if
\(x\) knows that \(y\) knows that \(z\) knows \(P\) at a point \(t\) in a
trajectory, and \(\neg P\) holds at a later point \(t'\)
then there must be path in the timeline diagram from point \(t\) on
\(x\)'s timeline to point \(t'\) on \(z\)'s timeline.Concurrent Systems with Shared VariablesThe theorems
and proofs given in this module apply to systems with shared
variables, and indeed any system with trajectories that are
representable by timeline diagrams and with consistent cuts.SummaryMany people working on distributed systems use the phrase "an agent
knows." This module gives a definition of the concept that is
consistent with intuitive definitions of knowledge.
The central idea in this module is the
relationship between what agents know and consistent cuts of
timelines. We presented several theorems about agent knowledge which
are intuitive when applied to human agents. The proofs are
straightforward and are all based on consistent cuts of timelines.ReviewIs the following true? Why?(\(z\) knows \(P\)) and (\(z\) knows \(Q\)) \(\equiv\) \(z\) knows
  \(\(P \wedge Q\))Is the following true? Why?(\(z\) knows \(P\)) or (\(z\) knows \(Q\)) \(\equiv\) \(z\) knows
  \(\( P \vee Q\))Suppose you and your friend communicate only using messages sent in
  channels, just as agents do in our model of distributed
  systems. When youknowthat your friend is wearing a cap does
  that mean that (a) your friend is wearing a cap, and (b) your friend
  can't take the cap off until the friend hears from you?K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology



Figure Link: https://kmchandy.github.io/Knowledge/KnowledgeDiagrams/Slide02.jpg

Caption: Fig. 2. Agent A knows P in all consistent cuts that cross
    point T


\(
[init  \; \Rightarrow \; always([Q \Rightarrow P])]
\)means that in \([Q \Rightarrow P]\) holds in every state of every
trajectory that starts in an initial state.
So, if local predicate \(Q\) of agent \(x\) holds in any
state of any trajectory that starts from an initial state then global predicate \(P\)
also holds in that state.ExampleA system consisting of agents \(0, \ldots,  N\) has two indivisible
    tokens which are not created or destroyed."Agent \(0\) knows no other agent holds a token" is a predicate on the states of agent
\(0\); this predicate holds for a local state \(s_{0}\) of agent \(0\)
if and only if no other agent holds a token when agent \(0\) is in
state \(s_{0}\).So, "agent \(0\) knows P" holds exactly when agent \(0\)
holds both tokens.ExampleThis example deals with a system consisting of two agents \(x\) and
\(y\) and channels in both directions between the agents. The system
has a single indivisible token that is not created or destroyed.
Let
\(P\) be the predicate: "\(y\) doesnothold the token."
Agent \(x\) knows \(P\) when \(x\) holds the token.\(\neg\) (\(x\) knows \(P\)) is a predicate too, and sometimes
programmers refer to this predicate as "\(x\) does not know \(P\)."ExampleIn the previous example, let \(Q\) be the predicate: "\(y\) holds the token."
In what local states of agent \(x\) does \(x\) know that "\(y\) holds the token?"There are no local states of agent \(x\) in which \(x\) knows that
"\(y\) holds the token." Even when \(x\) does not hold the token,
\(x\) does not know that \(y\) holds the token because
the token could be in a channel.When \(x\) does not hold the token, \(x\) does not know that \(y\) holds
the token and \(x\) does not know that \(y\) does not hold the token."NOT(x knows Q)" AND "NOT(x knows NOT Q)"is a predicate which holds in the
state which \(x\) does not hold the token.Notation"\(x\) knows \(P\) at a point \(T\)" in a timeline means that at
    point \(T\) agent \(x\) is in a state where the predicate "\(x\)
knows \(P\)" holds.Theorem: Knowledge and Consistent CutsLet \(P\) be a predicate on the states of a system.If  \(x\) knows \(P\) at a point \(T\) in \(x\)'s timeline
then \(P\) holds in every consistent cut through that point.ProofThe states corresponding to all consistent cuts that pass through the same
point on \(x\)'s timeline have the same common value for \(x\)'s local
state.Example>Fig. 2. Agent A knows P in all consistent cuts that cross
    point TThe top figure in both diagrams above show a time T at which agent A knows
that P holds. This implies that P holds in all consistent cuts through
point T. The lower figures in the diagrams show consistent cuts which
passes point T on A's timeline; the theorem says that P holds for the
state at these cuts too.Theorem: A Silent Agent retains
KnowledgeAn agent that sends no information between a point \(T\) and a later
point \(T'\) retains all the knowledge it has at \(T\) at \(T'\).Let \(x\) be an agent in a system, and let \(P\) be a
predicate on a subsystem that does not include \(x\).
Let \(T\) and \(T'\) be points on \(x\)'s timeline with \(T < T'\). If
\(x\) knows \(P\) at point \(T\) and \(x\) sends no messages in the
interval \([T, T']\) then \(x\) knows \(P\) at \(T'\).ProofLet \(c'\) be any consistent cut through point \(T'\) on \(x\)'s
timeline. We will prove that \(P\) holds for the state at cut \(c'\).Let \(c\) be the
cut that is identical to \(c'\) except that it passes through point \(T\) on
\(x\)'s timeline. \(c\) is consistent because there are no
outgoing edges from \(x\)'s timeline between cuts \(c\) and
\(c'\).Because \(x\) knows \(P\) at \(T\), \(P\) holds at \(c\). Since \(c\) and \(c'\)
are identical except for the intersection with \(x\)'s timeline,
it follows that \(P\) holds \(c'\).Example>Fig. 3. Illustration of Proof of Silent AgentsConsequence of the TheoremAn agent doesn't lose knowledge by getting information from other
agents.An agent can only lose knowledge by sending information to
other agents. This seems counterintuitive; we'll look at the reasoning
underlying this in a later theorem.Theorem: Agents who don't listen remain
IgnorantAn agent that receives no information between a point \(T\) and a later
point \(T'\) learns no new knowledge between \(T\) and \(T'\).Let \(x\) be an agent in a system, and let \(P\) be a
predicate on a subsystem that does not include \(x\).
Let \(T\) and \(T'\) be points on \(x\)'s timeline with \(T < T'\). If
\(x\) knows \(P\) at point \(T'\) and \(x\) received no messages in the
interval \([T, T']\) then \(x\) knows \(P\) at \(T\).The proof has exactly the same structure as the proof of the
previous theorem.Consequence of the Theorem\(x\) didn't learn anything in the interval  \([T, T']\); everything
\(x\) knows at the later point \(T'\) is knowledge it already had at
the earlier point \(T\).The only way for an agent to gain knowledge is
to receive messages. An agent cannot learn about other agents by only
sending messages or making internal state transitions.Theorem: Knowledge implies ControlLet \(x\) and \(y\) be agents in a system, and let \(P\) be a
predicate on the states of \(y\).
Let \(T\) and \(T'\) be instants in a trajectory with \(T < T'\). If
\(x\) knows \(P\) at  \(T\), and \(\neg P\) holds at \(T'\), then there
is a path in the timeline diagram from point \(T\) on \(x\)'s timeline
to point \(T'\) on \(y\)'s timeline.Proof: If there is no path from point \(T\) on \(x\)'s timeline
to point \(T'\) on \(y\)'s timeline then there exists a consistent cut
which crosses \(x\)'s timeline at \(T\) and
crosses \(y\)'s timeline at \(T'\).ExampleIn the figure below, agent \(A\) at point \(T\) knows that agent \(C\)
holds no tokens. At a later point \(T'\) agent \(C\) holds a
token. What must happen between points \(T\) and \(T'\)?>Fig. 4. What must happen between T and T'?There must be a path in the timeline diagram from point \(T\) on agent
\(A\)'s timeline to point \(T'\) on agent \(C\)'s timeline. This path
is represented by edges that show time elapsing on a timeline and
message edges between timelines.>Fig. 5. There must be a path from A at T to C at T'?Consequence of the TheoremSuppose you and your friend communicate only by means of messages
that are delayed by arbitrary (finite) amounts. Consider a situation
where your friend knows
that you are in the library at 9 pm. Then, from our definition of
knowledge, because agents only know truth, you must be in the library
at 9 pm. Moreover, you can't leave the library until
you receive a message from your friend; this message may go through
intermediate agents.In one of the exercises we'll look at knowledge when agents have
clocks that may drift from each other but are not more than a some
constant \(M\) units apart. If your friend knows that you will be in
the library till her watch reads 9:00 pm, and your watches may drift apart
by a minute, then you can leave the library at 9:01 pm. Clocks are
useful even if they aren't perfect. More about clocks later.Theorem: Communication to learn about
ChangeThis theorem is similar to the "knowledge is control" theorem.Let \(x\) and \(y\) be agents in a system, and let \(P\) be a
predicate on the states of \(y\).
Let \(T\) and \(T'\) be instants in a trajectory with \(T < T'\). If
\(\neg P\) holds at \(T\) and \(x\) knows \(P\) at \(T'\) then there
is a path in the timeline diagram from point \(T\) on \(y\)'s timeline
to point \(T'\) on \(x\)'s timeline.ExampleIn the figure below, agent \(C\) holds a token at point \(T\).
At a later point \(T'\) agent \(A\) knows that agent \(C\) holds no
tokens. What must happen between points \(T\) and \(T'\)?>Fig. 6. What must happen between T and T'?There must be a path in the timeline diagram from point \(T\) on agent
\(C\)'s timeline to point \(T'\) on agent \(A\)'s timeline. This path
is represented by edges that show time elapsing on a timeline and
message edges between timelines.>Fig. 7. There must be a path from C at T to A at T'?What Agents Know about Channel StatesNext, let's look at systems in which messages are acknowledged.
For a pair of agents \(x, y\), let \(ms\) and \(mr\)
be the number of messages that \(x\) has sent to \(y\), and  the number 
of messages that \(y\) has received from \(x\), respectively.
Let \(as\) and \(ar\) be
the number of acknowledgements that \(y\) has sent to \(x\), and
the number of acknowledgements that \(x\) has received from \(y\),
respectively.
The following is an invariant:\(ms \geq mr \geq as \geq ar \)The number of messages in the channel from \(x\) to \(y\) is \(ms -
mr\).
Because \(ms\) and \(ar\) are variables of agent \(x\),
agent \(x\)knowsan upper bound,\(\; ms - ar\), on the number of
messages in channel \((x, y)\).
So, \(x\) knows that the channel is empty when \(\; ms = ar\).What an agent cannot knowAn agent cannot know that there are
exactly \(n\) messages in a channel, for \(n > 0\).
You can prove this result using the concept of consistent
cuts. Intuitively, the agent cannot know whether a message is in the
channel or has been received.Chains of KnowledgeLet \(x, y, z\) be agents of a system and \(P\) be a predicate on
states of the system. Then the following are all predicates:\(z\) knows \(P\)\(y\) knows that \(z\) knows \(P\)\(x\) knows that \(y\) knows that \(z\) knows \(P\)The theorems given earlier apply to any predicate. For example, if
\(x\) knows that \(y\) knows that \(z\) knows \(P\) at a point \(t\) in a
trajectory, and \(\neg P\) holds at a later point \(t'\)
then there must be path in the timeline diagram from point \(t\) on
\(x\)'s timeline to point \(t'\) on \(z\)'s timeline.Concurrent Systems with Shared VariablesThe theorems
and proofs given in this module apply to systems with shared
variables, and indeed any system with trajectories that are
representable by timeline diagrams and with consistent cuts.SummaryMany people working on distributed systems use the phrase "an agent
knows." This module gives a definition of the concept that is
consistent with intuitive definitions of knowledge.
The central idea in this module is the
relationship between what agents know and consistent cuts of
timelines. We presented several theorems about agent knowledge which
are intuitive when applied to human agents. The proofs are
straightforward and are all based on consistent cuts of timelines.ReviewIs the following true? Why?(\(z\) knows \(P\)) and (\(z\) knows \(Q\)) \(\equiv\) \(z\) knows
  \(\(P \wedge Q\))Is the following true? Why?(\(z\) knows \(P\)) or (\(z\) knows \(Q\)) \(\equiv\) \(z\) knows
  \(\( P \vee Q\))Suppose you and your friend communicate only using messages sent in
  channels, just as agents do in our model of distributed
  systems. When youknowthat your friend is wearing a cap does
  that mean that (a) your friend is wearing a cap, and (b) your friend
  can't take the cap off until the friend hears from you?K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology



Figure Link: https://kmchandy.github.io/Knowledge/KnowledgeDiagrams/Slide02.jpg

Caption: Fig. 2. Agent A knows P in all consistent cuts that cross
    point T


means that in \([Q \Rightarrow P]\) holds in every state of every
trajectory that starts in an initial state.
So, if local predicate \(Q\) of agent \(x\) holds in any
state of any trajectory that starts from an initial state then global predicate \(P\)
also holds in that state.ExampleA system consisting of agents \(0, \ldots,  N\) has two indivisible
    tokens which are not created or destroyed."Agent \(0\) knows no other agent holds a token" is a predicate on the states of agent
\(0\); this predicate holds for a local state \(s_{0}\) of agent \(0\)
if and only if no other agent holds a token when agent \(0\) is in
state \(s_{0}\).So, "agent \(0\) knows P" holds exactly when agent \(0\)
holds both tokens.ExampleThis example deals with a system consisting of two agents \(x\) and
\(y\) and channels in both directions between the agents. The system
has a single indivisible token that is not created or destroyed.
Let
\(P\) be the predicate: "\(y\) doesnothold the token."
Agent \(x\) knows \(P\) when \(x\) holds the token.\(\neg\) (\(x\) knows \(P\)) is a predicate too, and sometimes
programmers refer to this predicate as "\(x\) does not know \(P\)."ExampleIn the previous example, let \(Q\) be the predicate: "\(y\) holds the token."
In what local states of agent \(x\) does \(x\) know that "\(y\) holds the token?"There are no local states of agent \(x\) in which \(x\) knows that
"\(y\) holds the token." Even when \(x\) does not hold the token,
\(x\) does not know that \(y\) holds the token because
the token could be in a channel.When \(x\) does not hold the token, \(x\) does not know that \(y\) holds
the token and \(x\) does not know that \(y\) does not hold the token."NOT(x knows Q)" AND "NOT(x knows NOT Q)"is a predicate which holds in the
state which \(x\) does not hold the token.Notation"\(x\) knows \(P\) at a point \(T\)" in a timeline means that at
    point \(T\) agent \(x\) is in a state where the predicate "\(x\)
knows \(P\)" holds.Theorem: Knowledge and Consistent CutsLet \(P\) be a predicate on the states of a system.If  \(x\) knows \(P\) at a point \(T\) in \(x\)'s timeline
then \(P\) holds in every consistent cut through that point.ProofThe states corresponding to all consistent cuts that pass through the same
point on \(x\)'s timeline have the same common value for \(x\)'s local
state.Example>Fig. 2. Agent A knows P in all consistent cuts that cross
    point TThe top figure in both diagrams above show a time T at which agent A knows
that P holds. This implies that P holds in all consistent cuts through
point T. The lower figures in the diagrams show consistent cuts which
passes point T on A's timeline; the theorem says that P holds for the
state at these cuts too.Theorem: A Silent Agent retains
KnowledgeAn agent that sends no information between a point \(T\) and a later
point \(T'\) retains all the knowledge it has at \(T\) at \(T'\).Let \(x\) be an agent in a system, and let \(P\) be a
predicate on a subsystem that does not include \(x\).
Let \(T\) and \(T'\) be points on \(x\)'s timeline with \(T < T'\). If
\(x\) knows \(P\) at point \(T\) and \(x\) sends no messages in the
interval \([T, T']\) then \(x\) knows \(P\) at \(T'\).ProofLet \(c'\) be any consistent cut through point \(T'\) on \(x\)'s
timeline. We will prove that \(P\) holds for the state at cut \(c'\).Let \(c\) be the
cut that is identical to \(c'\) except that it passes through point \(T\) on
\(x\)'s timeline. \(c\) is consistent because there are no
outgoing edges from \(x\)'s timeline between cuts \(c\) and
\(c'\).Because \(x\) knows \(P\) at \(T\), \(P\) holds at \(c\). Since \(c\) and \(c'\)
are identical except for the intersection with \(x\)'s timeline,
it follows that \(P\) holds \(c'\).Example>Fig. 3. Illustration of Proof of Silent AgentsConsequence of the TheoremAn agent doesn't lose knowledge by getting information from other
agents.An agent can only lose knowledge by sending information to
other agents. This seems counterintuitive; we'll look at the reasoning
underlying this in a later theorem.Theorem: Agents who don't listen remain
IgnorantAn agent that receives no information between a point \(T\) and a later
point \(T'\) learns no new knowledge between \(T\) and \(T'\).Let \(x\) be an agent in a system, and let \(P\) be a
predicate on a subsystem that does not include \(x\).
Let \(T\) and \(T'\) be points on \(x\)'s timeline with \(T < T'\). If
\(x\) knows \(P\) at point \(T'\) and \(x\) received no messages in the
interval \([T, T']\) then \(x\) knows \(P\) at \(T\).The proof has exactly the same structure as the proof of the
previous theorem.Consequence of the Theorem\(x\) didn't learn anything in the interval  \([T, T']\); everything
\(x\) knows at the later point \(T'\) is knowledge it already had at
the earlier point \(T\).The only way for an agent to gain knowledge is
to receive messages. An agent cannot learn about other agents by only
sending messages or making internal state transitions.Theorem: Knowledge implies ControlLet \(x\) and \(y\) be agents in a system, and let \(P\) be a
predicate on the states of \(y\).
Let \(T\) and \(T'\) be instants in a trajectory with \(T < T'\). If
\(x\) knows \(P\) at  \(T\), and \(\neg P\) holds at \(T'\), then there
is a path in the timeline diagram from point \(T\) on \(x\)'s timeline
to point \(T'\) on \(y\)'s timeline.Proof: If there is no path from point \(T\) on \(x\)'s timeline
to point \(T'\) on \(y\)'s timeline then there exists a consistent cut
which crosses \(x\)'s timeline at \(T\) and
crosses \(y\)'s timeline at \(T'\).ExampleIn the figure below, agent \(A\) at point \(T\) knows that agent \(C\)
holds no tokens. At a later point \(T'\) agent \(C\) holds a
token. What must happen between points \(T\) and \(T'\)?>Fig. 4. What must happen between T and T'?There must be a path in the timeline diagram from point \(T\) on agent
\(A\)'s timeline to point \(T'\) on agent \(C\)'s timeline. This path
is represented by edges that show time elapsing on a timeline and
message edges between timelines.>Fig. 5. There must be a path from A at T to C at T'?Consequence of the TheoremSuppose you and your friend communicate only by means of messages
that are delayed by arbitrary (finite) amounts. Consider a situation
where your friend knows
that you are in the library at 9 pm. Then, from our definition of
knowledge, because agents only know truth, you must be in the library
at 9 pm. Moreover, you can't leave the library until
you receive a message from your friend; this message may go through
intermediate agents.In one of the exercises we'll look at knowledge when agents have
clocks that may drift from each other but are not more than a some
constant \(M\) units apart. If your friend knows that you will be in
the library till her watch reads 9:00 pm, and your watches may drift apart
by a minute, then you can leave the library at 9:01 pm. Clocks are
useful even if they aren't perfect. More about clocks later.Theorem: Communication to learn about
ChangeThis theorem is similar to the "knowledge is control" theorem.Let \(x\) and \(y\) be agents in a system, and let \(P\) be a
predicate on the states of \(y\).
Let \(T\) and \(T'\) be instants in a trajectory with \(T < T'\). If
\(\neg P\) holds at \(T\) and \(x\) knows \(P\) at \(T'\) then there
is a path in the timeline diagram from point \(T\) on \(y\)'s timeline
to point \(T'\) on \(x\)'s timeline.ExampleIn the figure below, agent \(C\) holds a token at point \(T\).
At a later point \(T'\) agent \(A\) knows that agent \(C\) holds no
tokens. What must happen between points \(T\) and \(T'\)?>Fig. 6. What must happen between T and T'?There must be a path in the timeline diagram from point \(T\) on agent
\(C\)'s timeline to point \(T'\) on agent \(A\)'s timeline. This path
is represented by edges that show time elapsing on a timeline and
message edges between timelines.>Fig. 7. There must be a path from C at T to A at T'?What Agents Know about Channel StatesNext, let's look at systems in which messages are acknowledged.
For a pair of agents \(x, y\), let \(ms\) and \(mr\)
be the number of messages that \(x\) has sent to \(y\), and  the number 
of messages that \(y\) has received from \(x\), respectively.
Let \(as\) and \(ar\) be
the number of acknowledgements that \(y\) has sent to \(x\), and
the number of acknowledgements that \(x\) has received from \(y\),
respectively.
The following is an invariant:\(ms \geq mr \geq as \geq ar \)The number of messages in the channel from \(x\) to \(y\) is \(ms -
mr\).
Because \(ms\) and \(ar\) are variables of agent \(x\),
agent \(x\)knowsan upper bound,\(\; ms - ar\), on the number of
messages in channel \((x, y)\).
So, \(x\) knows that the channel is empty when \(\; ms = ar\).What an agent cannot knowAn agent cannot know that there are
exactly \(n\) messages in a channel, for \(n > 0\).
You can prove this result using the concept of consistent
cuts. Intuitively, the agent cannot know whether a message is in the
channel or has been received.Chains of KnowledgeLet \(x, y, z\) be agents of a system and \(P\) be a predicate on
states of the system. Then the following are all predicates:\(z\) knows \(P\)\(y\) knows that \(z\) knows \(P\)\(x\) knows that \(y\) knows that \(z\) knows \(P\)The theorems given earlier apply to any predicate. For example, if
\(x\) knows that \(y\) knows that \(z\) knows \(P\) at a point \(t\) in a
trajectory, and \(\neg P\) holds at a later point \(t'\)
then there must be path in the timeline diagram from point \(t\) on
\(x\)'s timeline to point \(t'\) on \(z\)'s timeline.Concurrent Systems with Shared VariablesThe theorems
and proofs given in this module apply to systems with shared
variables, and indeed any system with trajectories that are
representable by timeline diagrams and with consistent cuts.SummaryMany people working on distributed systems use the phrase "an agent
knows." This module gives a definition of the concept that is
consistent with intuitive definitions of knowledge.
The central idea in this module is the
relationship between what agents know and consistent cuts of
timelines. We presented several theorems about agent knowledge which
are intuitive when applied to human agents. The proofs are
straightforward and are all based on consistent cuts of timelines.ReviewIs the following true? Why?(\(z\) knows \(P\)) and (\(z\) knows \(Q\)) \(\equiv\) \(z\) knows
  \(\(P \wedge Q\))Is the following true? Why?(\(z\) knows \(P\)) or (\(z\) knows \(Q\)) \(\equiv\) \(z\) knows
  \(\( P \vee Q\))Suppose you and your friend communicate only using messages sent in
  channels, just as agents do in our model of distributed
  systems. When youknowthat your friend is wearing a cap does
  that mean that (a) your friend is wearing a cap, and (b) your friend
  can't take the cap off until the friend hears from you?K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology



Figure Link: https://kmchandy.github.io/Knowledge/KnowledgeDiagrams/Slide02.jpg

Caption: Fig. 2. Agent A knows P in all consistent cuts that cross
    point T


ExampleA system consisting of agents \(0, \ldots,  N\) has two indivisible
    tokens which are not created or destroyed."Agent \(0\) knows no other agent holds a token" is a predicate on the states of agent
\(0\); this predicate holds for a local state \(s_{0}\) of agent \(0\)
if and only if no other agent holds a token when agent \(0\) is in
state \(s_{0}\).So, "agent \(0\) knows P" holds exactly when agent \(0\)
holds both tokens.ExampleThis example deals with a system consisting of two agents \(x\) and
\(y\) and channels in both directions between the agents. The system
has a single indivisible token that is not created or destroyed.
Let
\(P\) be the predicate: "\(y\) doesnothold the token."
Agent \(x\) knows \(P\) when \(x\) holds the token.\(\neg\) (\(x\) knows \(P\)) is a predicate too, and sometimes
programmers refer to this predicate as "\(x\) does not know \(P\)."ExampleIn the previous example, let \(Q\) be the predicate: "\(y\) holds the token."
In what local states of agent \(x\) does \(x\) know that "\(y\) holds the token?"There are no local states of agent \(x\) in which \(x\) knows that
"\(y\) holds the token." Even when \(x\) does not hold the token,
\(x\) does not know that \(y\) holds the token because
the token could be in a channel.When \(x\) does not hold the token, \(x\) does not know that \(y\) holds
the token and \(x\) does not know that \(y\) does not hold the token."NOT(x knows Q)" AND "NOT(x knows NOT Q)"is a predicate which holds in the
state which \(x\) does not hold the token.Notation"\(x\) knows \(P\) at a point \(T\)" in a timeline means that at
    point \(T\) agent \(x\) is in a state where the predicate "\(x\)
knows \(P\)" holds.Theorem: Knowledge and Consistent CutsLet \(P\) be a predicate on the states of a system.If  \(x\) knows \(P\) at a point \(T\) in \(x\)'s timeline
then \(P\) holds in every consistent cut through that point.ProofThe states corresponding to all consistent cuts that pass through the same
point on \(x\)'s timeline have the same common value for \(x\)'s local
state.Example>Fig. 2. Agent A knows P in all consistent cuts that cross
    point TThe top figure in both diagrams above show a time T at which agent A knows
that P holds. This implies that P holds in all consistent cuts through
point T. The lower figures in the diagrams show consistent cuts which
passes point T on A's timeline; the theorem says that P holds for the
state at these cuts too.Theorem: A Silent Agent retains
KnowledgeAn agent that sends no information between a point \(T\) and a later
point \(T'\) retains all the knowledge it has at \(T\) at \(T'\).Let \(x\) be an agent in a system, and let \(P\) be a
predicate on a subsystem that does not include \(x\).
Let \(T\) and \(T'\) be points on \(x\)'s timeline with \(T < T'\). If
\(x\) knows \(P\) at point \(T\) and \(x\) sends no messages in the
interval \([T, T']\) then \(x\) knows \(P\) at \(T'\).ProofLet \(c'\) be any consistent cut through point \(T'\) on \(x\)'s
timeline. We will prove that \(P\) holds for the state at cut \(c'\).Let \(c\) be the
cut that is identical to \(c'\) except that it passes through point \(T\) on
\(x\)'s timeline. \(c\) is consistent because there are no
outgoing edges from \(x\)'s timeline between cuts \(c\) and
\(c'\).Because \(x\) knows \(P\) at \(T\), \(P\) holds at \(c\). Since \(c\) and \(c'\)
are identical except for the intersection with \(x\)'s timeline,
it follows that \(P\) holds \(c'\).Example>Fig. 3. Illustration of Proof of Silent AgentsConsequence of the TheoremAn agent doesn't lose knowledge by getting information from other
agents.An agent can only lose knowledge by sending information to
other agents. This seems counterintuitive; we'll look at the reasoning
underlying this in a later theorem.Theorem: Agents who don't listen remain
IgnorantAn agent that receives no information between a point \(T\) and a later
point \(T'\) learns no new knowledge between \(T\) and \(T'\).Let \(x\) be an agent in a system, and let \(P\) be a
predicate on a subsystem that does not include \(x\).
Let \(T\) and \(T'\) be points on \(x\)'s timeline with \(T < T'\). If
\(x\) knows \(P\) at point \(T'\) and \(x\) received no messages in the
interval \([T, T']\) then \(x\) knows \(P\) at \(T\).The proof has exactly the same structure as the proof of the
previous theorem.Consequence of the Theorem\(x\) didn't learn anything in the interval  \([T, T']\); everything
\(x\) knows at the later point \(T'\) is knowledge it already had at
the earlier point \(T\).The only way for an agent to gain knowledge is
to receive messages. An agent cannot learn about other agents by only
sending messages or making internal state transitions.Theorem: Knowledge implies ControlLet \(x\) and \(y\) be agents in a system, and let \(P\) be a
predicate on the states of \(y\).
Let \(T\) and \(T'\) be instants in a trajectory with \(T < T'\). If
\(x\) knows \(P\) at  \(T\), and \(\neg P\) holds at \(T'\), then there
is a path in the timeline diagram from point \(T\) on \(x\)'s timeline
to point \(T'\) on \(y\)'s timeline.Proof: If there is no path from point \(T\) on \(x\)'s timeline
to point \(T'\) on \(y\)'s timeline then there exists a consistent cut
which crosses \(x\)'s timeline at \(T\) and
crosses \(y\)'s timeline at \(T'\).ExampleIn the figure below, agent \(A\) at point \(T\) knows that agent \(C\)
holds no tokens. At a later point \(T'\) agent \(C\) holds a
token. What must happen between points \(T\) and \(T'\)?>Fig. 4. What must happen between T and T'?There must be a path in the timeline diagram from point \(T\) on agent
\(A\)'s timeline to point \(T'\) on agent \(C\)'s timeline. This path
is represented by edges that show time elapsing on a timeline and
message edges between timelines.>Fig. 5. There must be a path from A at T to C at T'?Consequence of the TheoremSuppose you and your friend communicate only by means of messages
that are delayed by arbitrary (finite) amounts. Consider a situation
where your friend knows
that you are in the library at 9 pm. Then, from our definition of
knowledge, because agents only know truth, you must be in the library
at 9 pm. Moreover, you can't leave the library until
you receive a message from your friend; this message may go through
intermediate agents.In one of the exercises we'll look at knowledge when agents have
clocks that may drift from each other but are not more than a some
constant \(M\) units apart. If your friend knows that you will be in
the library till her watch reads 9:00 pm, and your watches may drift apart
by a minute, then you can leave the library at 9:01 pm. Clocks are
useful even if they aren't perfect. More about clocks later.Theorem: Communication to learn about
ChangeThis theorem is similar to the "knowledge is control" theorem.Let \(x\) and \(y\) be agents in a system, and let \(P\) be a
predicate on the states of \(y\).
Let \(T\) and \(T'\) be instants in a trajectory with \(T < T'\). If
\(\neg P\) holds at \(T\) and \(x\) knows \(P\) at \(T'\) then there
is a path in the timeline diagram from point \(T\) on \(y\)'s timeline
to point \(T'\) on \(x\)'s timeline.ExampleIn the figure below, agent \(C\) holds a token at point \(T\).
At a later point \(T'\) agent \(A\) knows that agent \(C\) holds no
tokens. What must happen between points \(T\) and \(T'\)?>Fig. 6. What must happen between T and T'?There must be a path in the timeline diagram from point \(T\) on agent
\(C\)'s timeline to point \(T'\) on agent \(A\)'s timeline. This path
is represented by edges that show time elapsing on a timeline and
message edges between timelines.>Fig. 7. There must be a path from C at T to A at T'?What Agents Know about Channel StatesNext, let's look at systems in which messages are acknowledged.
For a pair of agents \(x, y\), let \(ms\) and \(mr\)
be the number of messages that \(x\) has sent to \(y\), and  the number 
of messages that \(y\) has received from \(x\), respectively.
Let \(as\) and \(ar\) be
the number of acknowledgements that \(y\) has sent to \(x\), and
the number of acknowledgements that \(x\) has received from \(y\),
respectively.
The following is an invariant:\(ms \geq mr \geq as \geq ar \)The number of messages in the channel from \(x\) to \(y\) is \(ms -
mr\).
Because \(ms\) and \(ar\) are variables of agent \(x\),
agent \(x\)knowsan upper bound,\(\; ms - ar\), on the number of
messages in channel \((x, y)\).
So, \(x\) knows that the channel is empty when \(\; ms = ar\).What an agent cannot knowAn agent cannot know that there are
exactly \(n\) messages in a channel, for \(n > 0\).
You can prove this result using the concept of consistent
cuts. Intuitively, the agent cannot know whether a message is in the
channel or has been received.Chains of KnowledgeLet \(x, y, z\) be agents of a system and \(P\) be a predicate on
states of the system. Then the following are all predicates:\(z\) knows \(P\)\(y\) knows that \(z\) knows \(P\)\(x\) knows that \(y\) knows that \(z\) knows \(P\)The theorems given earlier apply to any predicate. For example, if
\(x\) knows that \(y\) knows that \(z\) knows \(P\) at a point \(t\) in a
trajectory, and \(\neg P\) holds at a later point \(t'\)
then there must be path in the timeline diagram from point \(t\) on
\(x\)'s timeline to point \(t'\) on \(z\)'s timeline.Concurrent Systems with Shared VariablesThe theorems
and proofs given in this module apply to systems with shared
variables, and indeed any system with trajectories that are
representable by timeline diagrams and with consistent cuts.SummaryMany people working on distributed systems use the phrase "an agent
knows." This module gives a definition of the concept that is
consistent with intuitive definitions of knowledge.
The central idea in this module is the
relationship between what agents know and consistent cuts of
timelines. We presented several theorems about agent knowledge which
are intuitive when applied to human agents. The proofs are
straightforward and are all based on consistent cuts of timelines.ReviewIs the following true? Why?(\(z\) knows \(P\)) and (\(z\) knows \(Q\)) \(\equiv\) \(z\) knows
  \(\(P \wedge Q\))Is the following true? Why?(\(z\) knows \(P\)) or (\(z\) knows \(Q\)) \(\equiv\) \(z\) knows
  \(\( P \vee Q\))Suppose you and your friend communicate only using messages sent in
  channels, just as agents do in our model of distributed
  systems. When youknowthat your friend is wearing a cap does
  that mean that (a) your friend is wearing a cap, and (b) your friend
  can't take the cap off until the friend hears from you?K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology



Figure Link: https://kmchandy.github.io/Knowledge/KnowledgeDiagrams/Slide02.jpg

Caption: Fig. 2. Agent A knows P in all consistent cuts that cross
    point T


"Agent \(0\) knows no other agent holds a token" is a predicate on the states of agent
\(0\); this predicate holds for a local state \(s_{0}\) of agent \(0\)
if and only if no other agent holds a token when agent \(0\) is in
state \(s_{0}\).So, "agent \(0\) knows P" holds exactly when agent \(0\)
holds both tokens.ExampleThis example deals with a system consisting of two agents \(x\) and
\(y\) and channels in both directions between the agents. The system
has a single indivisible token that is not created or destroyed.
Let
\(P\) be the predicate: "\(y\) doesnothold the token."
Agent \(x\) knows \(P\) when \(x\) holds the token.\(\neg\) (\(x\) knows \(P\)) is a predicate too, and sometimes
programmers refer to this predicate as "\(x\) does not know \(P\)."ExampleIn the previous example, let \(Q\) be the predicate: "\(y\) holds the token."
In what local states of agent \(x\) does \(x\) know that "\(y\) holds the token?"There are no local states of agent \(x\) in which \(x\) knows that
"\(y\) holds the token." Even when \(x\) does not hold the token,
\(x\) does not know that \(y\) holds the token because
the token could be in a channel.When \(x\) does not hold the token, \(x\) does not know that \(y\) holds
the token and \(x\) does not know that \(y\) does not hold the token."NOT(x knows Q)" AND "NOT(x knows NOT Q)"is a predicate which holds in the
state which \(x\) does not hold the token.Notation"\(x\) knows \(P\) at a point \(T\)" in a timeline means that at
    point \(T\) agent \(x\) is in a state where the predicate "\(x\)
knows \(P\)" holds.Theorem: Knowledge and Consistent CutsLet \(P\) be a predicate on the states of a system.If  \(x\) knows \(P\) at a point \(T\) in \(x\)'s timeline
then \(P\) holds in every consistent cut through that point.ProofThe states corresponding to all consistent cuts that pass through the same
point on \(x\)'s timeline have the same common value for \(x\)'s local
state.Example>Fig. 2. Agent A knows P in all consistent cuts that cross
    point TThe top figure in both diagrams above show a time T at which agent A knows
that P holds. This implies that P holds in all consistent cuts through
point T. The lower figures in the diagrams show consistent cuts which
passes point T on A's timeline; the theorem says that P holds for the
state at these cuts too.Theorem: A Silent Agent retains
KnowledgeAn agent that sends no information between a point \(T\) and a later
point \(T'\) retains all the knowledge it has at \(T\) at \(T'\).Let \(x\) be an agent in a system, and let \(P\) be a
predicate on a subsystem that does not include \(x\).
Let \(T\) and \(T'\) be points on \(x\)'s timeline with \(T < T'\). If
\(x\) knows \(P\) at point \(T\) and \(x\) sends no messages in the
interval \([T, T']\) then \(x\) knows \(P\) at \(T'\).ProofLet \(c'\) be any consistent cut through point \(T'\) on \(x\)'s
timeline. We will prove that \(P\) holds for the state at cut \(c'\).Let \(c\) be the
cut that is identical to \(c'\) except that it passes through point \(T\) on
\(x\)'s timeline. \(c\) is consistent because there are no
outgoing edges from \(x\)'s timeline between cuts \(c\) and
\(c'\).Because \(x\) knows \(P\) at \(T\), \(P\) holds at \(c\). Since \(c\) and \(c'\)
are identical except for the intersection with \(x\)'s timeline,
it follows that \(P\) holds \(c'\).Example>Fig. 3. Illustration of Proof of Silent AgentsConsequence of the TheoremAn agent doesn't lose knowledge by getting information from other
agents.An agent can only lose knowledge by sending information to
other agents. This seems counterintuitive; we'll look at the reasoning
underlying this in a later theorem.Theorem: Agents who don't listen remain
IgnorantAn agent that receives no information between a point \(T\) and a later
point \(T'\) learns no new knowledge between \(T\) and \(T'\).Let \(x\) be an agent in a system, and let \(P\) be a
predicate on a subsystem that does not include \(x\).
Let \(T\) and \(T'\) be points on \(x\)'s timeline with \(T < T'\). If
\(x\) knows \(P\) at point \(T'\) and \(x\) received no messages in the
interval \([T, T']\) then \(x\) knows \(P\) at \(T\).The proof has exactly the same structure as the proof of the
previous theorem.Consequence of the Theorem\(x\) didn't learn anything in the interval  \([T, T']\); everything
\(x\) knows at the later point \(T'\) is knowledge it already had at
the earlier point \(T\).The only way for an agent to gain knowledge is
to receive messages. An agent cannot learn about other agents by only
sending messages or making internal state transitions.Theorem: Knowledge implies ControlLet \(x\) and \(y\) be agents in a system, and let \(P\) be a
predicate on the states of \(y\).
Let \(T\) and \(T'\) be instants in a trajectory with \(T < T'\). If
\(x\) knows \(P\) at  \(T\), and \(\neg P\) holds at \(T'\), then there
is a path in the timeline diagram from point \(T\) on \(x\)'s timeline
to point \(T'\) on \(y\)'s timeline.Proof: If there is no path from point \(T\) on \(x\)'s timeline
to point \(T'\) on \(y\)'s timeline then there exists a consistent cut
which crosses \(x\)'s timeline at \(T\) and
crosses \(y\)'s timeline at \(T'\).ExampleIn the figure below, agent \(A\) at point \(T\) knows that agent \(C\)
holds no tokens. At a later point \(T'\) agent \(C\) holds a
token. What must happen between points \(T\) and \(T'\)?>Fig. 4. What must happen between T and T'?There must be a path in the timeline diagram from point \(T\) on agent
\(A\)'s timeline to point \(T'\) on agent \(C\)'s timeline. This path
is represented by edges that show time elapsing on a timeline and
message edges between timelines.>Fig. 5. There must be a path from A at T to C at T'?Consequence of the TheoremSuppose you and your friend communicate only by means of messages
that are delayed by arbitrary (finite) amounts. Consider a situation
where your friend knows
that you are in the library at 9 pm. Then, from our definition of
knowledge, because agents only know truth, you must be in the library
at 9 pm. Moreover, you can't leave the library until
you receive a message from your friend; this message may go through
intermediate agents.In one of the exercises we'll look at knowledge when agents have
clocks that may drift from each other but are not more than a some
constant \(M\) units apart. If your friend knows that you will be in
the library till her watch reads 9:00 pm, and your watches may drift apart
by a minute, then you can leave the library at 9:01 pm. Clocks are
useful even if they aren't perfect. More about clocks later.Theorem: Communication to learn about
ChangeThis theorem is similar to the "knowledge is control" theorem.Let \(x\) and \(y\) be agents in a system, and let \(P\) be a
predicate on the states of \(y\).
Let \(T\) and \(T'\) be instants in a trajectory with \(T < T'\). If
\(\neg P\) holds at \(T\) and \(x\) knows \(P\) at \(T'\) then there
is a path in the timeline diagram from point \(T\) on \(y\)'s timeline
to point \(T'\) on \(x\)'s timeline.ExampleIn the figure below, agent \(C\) holds a token at point \(T\).
At a later point \(T'\) agent \(A\) knows that agent \(C\) holds no
tokens. What must happen between points \(T\) and \(T'\)?>Fig. 6. What must happen between T and T'?There must be a path in the timeline diagram from point \(T\) on agent
\(C\)'s timeline to point \(T'\) on agent \(A\)'s timeline. This path
is represented by edges that show time elapsing on a timeline and
message edges between timelines.>Fig. 7. There must be a path from C at T to A at T'?What Agents Know about Channel StatesNext, let's look at systems in which messages are acknowledged.
For a pair of agents \(x, y\), let \(ms\) and \(mr\)
be the number of messages that \(x\) has sent to \(y\), and  the number 
of messages that \(y\) has received from \(x\), respectively.
Let \(as\) and \(ar\) be
the number of acknowledgements that \(y\) has sent to \(x\), and
the number of acknowledgements that \(x\) has received from \(y\),
respectively.
The following is an invariant:\(ms \geq mr \geq as \geq ar \)The number of messages in the channel from \(x\) to \(y\) is \(ms -
mr\).
Because \(ms\) and \(ar\) are variables of agent \(x\),
agent \(x\)knowsan upper bound,\(\; ms - ar\), on the number of
messages in channel \((x, y)\).
So, \(x\) knows that the channel is empty when \(\; ms = ar\).What an agent cannot knowAn agent cannot know that there are
exactly \(n\) messages in a channel, for \(n > 0\).
You can prove this result using the concept of consistent
cuts. Intuitively, the agent cannot know whether a message is in the
channel or has been received.Chains of KnowledgeLet \(x, y, z\) be agents of a system and \(P\) be a predicate on
states of the system. Then the following are all predicates:\(z\) knows \(P\)\(y\) knows that \(z\) knows \(P\)\(x\) knows that \(y\) knows that \(z\) knows \(P\)The theorems given earlier apply to any predicate. For example, if
\(x\) knows that \(y\) knows that \(z\) knows \(P\) at a point \(t\) in a
trajectory, and \(\neg P\) holds at a later point \(t'\)
then there must be path in the timeline diagram from point \(t\) on
\(x\)'s timeline to point \(t'\) on \(z\)'s timeline.Concurrent Systems with Shared VariablesThe theorems
and proofs given in this module apply to systems with shared
variables, and indeed any system with trajectories that are
representable by timeline diagrams and with consistent cuts.SummaryMany people working on distributed systems use the phrase "an agent
knows." This module gives a definition of the concept that is
consistent with intuitive definitions of knowledge.
The central idea in this module is the
relationship between what agents know and consistent cuts of
timelines. We presented several theorems about agent knowledge which
are intuitive when applied to human agents. The proofs are
straightforward and are all based on consistent cuts of timelines.ReviewIs the following true? Why?(\(z\) knows \(P\)) and (\(z\) knows \(Q\)) \(\equiv\) \(z\) knows
  \(\(P \wedge Q\))Is the following true? Why?(\(z\) knows \(P\)) or (\(z\) knows \(Q\)) \(\equiv\) \(z\) knows
  \(\( P \vee Q\))Suppose you and your friend communicate only using messages sent in
  channels, just as agents do in our model of distributed
  systems. When youknowthat your friend is wearing a cap does
  that mean that (a) your friend is wearing a cap, and (b) your friend
  can't take the cap off until the friend hears from you?K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology



Figure Link: https://kmchandy.github.io/Knowledge/KnowledgeDiagrams/Slide02.jpg

Caption: Fig. 2. Agent A knows P in all consistent cuts that cross
    point T


So, "agent \(0\) knows P" holds exactly when agent \(0\)
holds both tokens.ExampleThis example deals with a system consisting of two agents \(x\) and
\(y\) and channels in both directions between the agents. The system
has a single indivisible token that is not created or destroyed.
Let
\(P\) be the predicate: "\(y\) doesnothold the token."
Agent \(x\) knows \(P\) when \(x\) holds the token.\(\neg\) (\(x\) knows \(P\)) is a predicate too, and sometimes
programmers refer to this predicate as "\(x\) does not know \(P\)."ExampleIn the previous example, let \(Q\) be the predicate: "\(y\) holds the token."
In what local states of agent \(x\) does \(x\) know that "\(y\) holds the token?"There are no local states of agent \(x\) in which \(x\) knows that
"\(y\) holds the token." Even when \(x\) does not hold the token,
\(x\) does not know that \(y\) holds the token because
the token could be in a channel.When \(x\) does not hold the token, \(x\) does not know that \(y\) holds
the token and \(x\) does not know that \(y\) does not hold the token."NOT(x knows Q)" AND "NOT(x knows NOT Q)"is a predicate which holds in the
state which \(x\) does not hold the token.Notation"\(x\) knows \(P\) at a point \(T\)" in a timeline means that at
    point \(T\) agent \(x\) is in a state where the predicate "\(x\)
knows \(P\)" holds.Theorem: Knowledge and Consistent CutsLet \(P\) be a predicate on the states of a system.If  \(x\) knows \(P\) at a point \(T\) in \(x\)'s timeline
then \(P\) holds in every consistent cut through that point.ProofThe states corresponding to all consistent cuts that pass through the same
point on \(x\)'s timeline have the same common value for \(x\)'s local
state.Example>Fig. 2. Agent A knows P in all consistent cuts that cross
    point TThe top figure in both diagrams above show a time T at which agent A knows
that P holds. This implies that P holds in all consistent cuts through
point T. The lower figures in the diagrams show consistent cuts which
passes point T on A's timeline; the theorem says that P holds for the
state at these cuts too.Theorem: A Silent Agent retains
KnowledgeAn agent that sends no information between a point \(T\) and a later
point \(T'\) retains all the knowledge it has at \(T\) at \(T'\).Let \(x\) be an agent in a system, and let \(P\) be a
predicate on a subsystem that does not include \(x\).
Let \(T\) and \(T'\) be points on \(x\)'s timeline with \(T < T'\). If
\(x\) knows \(P\) at point \(T\) and \(x\) sends no messages in the
interval \([T, T']\) then \(x\) knows \(P\) at \(T'\).ProofLet \(c'\) be any consistent cut through point \(T'\) on \(x\)'s
timeline. We will prove that \(P\) holds for the state at cut \(c'\).Let \(c\) be the
cut that is identical to \(c'\) except that it passes through point \(T\) on
\(x\)'s timeline. \(c\) is consistent because there are no
outgoing edges from \(x\)'s timeline between cuts \(c\) and
\(c'\).Because \(x\) knows \(P\) at \(T\), \(P\) holds at \(c\). Since \(c\) and \(c'\)
are identical except for the intersection with \(x\)'s timeline,
it follows that \(P\) holds \(c'\).Example>Fig. 3. Illustration of Proof of Silent AgentsConsequence of the TheoremAn agent doesn't lose knowledge by getting information from other
agents.An agent can only lose knowledge by sending information to
other agents. This seems counterintuitive; we'll look at the reasoning
underlying this in a later theorem.Theorem: Agents who don't listen remain
IgnorantAn agent that receives no information between a point \(T\) and a later
point \(T'\) learns no new knowledge between \(T\) and \(T'\).Let \(x\) be an agent in a system, and let \(P\) be a
predicate on a subsystem that does not include \(x\).
Let \(T\) and \(T'\) be points on \(x\)'s timeline with \(T < T'\). If
\(x\) knows \(P\) at point \(T'\) and \(x\) received no messages in the
interval \([T, T']\) then \(x\) knows \(P\) at \(T\).The proof has exactly the same structure as the proof of the
previous theorem.Consequence of the Theorem\(x\) didn't learn anything in the interval  \([T, T']\); everything
\(x\) knows at the later point \(T'\) is knowledge it already had at
the earlier point \(T\).The only way for an agent to gain knowledge is
to receive messages. An agent cannot learn about other agents by only
sending messages or making internal state transitions.Theorem: Knowledge implies ControlLet \(x\) and \(y\) be agents in a system, and let \(P\) be a
predicate on the states of \(y\).
Let \(T\) and \(T'\) be instants in a trajectory with \(T < T'\). If
\(x\) knows \(P\) at  \(T\), and \(\neg P\) holds at \(T'\), then there
is a path in the timeline diagram from point \(T\) on \(x\)'s timeline
to point \(T'\) on \(y\)'s timeline.Proof: If there is no path from point \(T\) on \(x\)'s timeline
to point \(T'\) on \(y\)'s timeline then there exists a consistent cut
which crosses \(x\)'s timeline at \(T\) and
crosses \(y\)'s timeline at \(T'\).ExampleIn the figure below, agent \(A\) at point \(T\) knows that agent \(C\)
holds no tokens. At a later point \(T'\) agent \(C\) holds a
token. What must happen between points \(T\) and \(T'\)?>Fig. 4. What must happen between T and T'?There must be a path in the timeline diagram from point \(T\) on agent
\(A\)'s timeline to point \(T'\) on agent \(C\)'s timeline. This path
is represented by edges that show time elapsing on a timeline and
message edges between timelines.>Fig. 5. There must be a path from A at T to C at T'?Consequence of the TheoremSuppose you and your friend communicate only by means of messages
that are delayed by arbitrary (finite) amounts. Consider a situation
where your friend knows
that you are in the library at 9 pm. Then, from our definition of
knowledge, because agents only know truth, you must be in the library
at 9 pm. Moreover, you can't leave the library until
you receive a message from your friend; this message may go through
intermediate agents.In one of the exercises we'll look at knowledge when agents have
clocks that may drift from each other but are not more than a some
constant \(M\) units apart. If your friend knows that you will be in
the library till her watch reads 9:00 pm, and your watches may drift apart
by a minute, then you can leave the library at 9:01 pm. Clocks are
useful even if they aren't perfect. More about clocks later.Theorem: Communication to learn about
ChangeThis theorem is similar to the "knowledge is control" theorem.Let \(x\) and \(y\) be agents in a system, and let \(P\) be a
predicate on the states of \(y\).
Let \(T\) and \(T'\) be instants in a trajectory with \(T < T'\). If
\(\neg P\) holds at \(T\) and \(x\) knows \(P\) at \(T'\) then there
is a path in the timeline diagram from point \(T\) on \(y\)'s timeline
to point \(T'\) on \(x\)'s timeline.ExampleIn the figure below, agent \(C\) holds a token at point \(T\).
At a later point \(T'\) agent \(A\) knows that agent \(C\) holds no
tokens. What must happen between points \(T\) and \(T'\)?>Fig. 6. What must happen between T and T'?There must be a path in the timeline diagram from point \(T\) on agent
\(C\)'s timeline to point \(T'\) on agent \(A\)'s timeline. This path
is represented by edges that show time elapsing on a timeline and
message edges between timelines.>Fig. 7. There must be a path from C at T to A at T'?What Agents Know about Channel StatesNext, let's look at systems in which messages are acknowledged.
For a pair of agents \(x, y\), let \(ms\) and \(mr\)
be the number of messages that \(x\) has sent to \(y\), and  the number 
of messages that \(y\) has received from \(x\), respectively.
Let \(as\) and \(ar\) be
the number of acknowledgements that \(y\) has sent to \(x\), and
the number of acknowledgements that \(x\) has received from \(y\),
respectively.
The following is an invariant:\(ms \geq mr \geq as \geq ar \)The number of messages in the channel from \(x\) to \(y\) is \(ms -
mr\).
Because \(ms\) and \(ar\) are variables of agent \(x\),
agent \(x\)knowsan upper bound,\(\; ms - ar\), on the number of
messages in channel \((x, y)\).
So, \(x\) knows that the channel is empty when \(\; ms = ar\).What an agent cannot knowAn agent cannot know that there are
exactly \(n\) messages in a channel, for \(n > 0\).
You can prove this result using the concept of consistent
cuts. Intuitively, the agent cannot know whether a message is in the
channel or has been received.Chains of KnowledgeLet \(x, y, z\) be agents of a system and \(P\) be a predicate on
states of the system. Then the following are all predicates:\(z\) knows \(P\)\(y\) knows that \(z\) knows \(P\)\(x\) knows that \(y\) knows that \(z\) knows \(P\)The theorems given earlier apply to any predicate. For example, if
\(x\) knows that \(y\) knows that \(z\) knows \(P\) at a point \(t\) in a
trajectory, and \(\neg P\) holds at a later point \(t'\)
then there must be path in the timeline diagram from point \(t\) on
\(x\)'s timeline to point \(t'\) on \(z\)'s timeline.Concurrent Systems with Shared VariablesThe theorems
and proofs given in this module apply to systems with shared
variables, and indeed any system with trajectories that are
representable by timeline diagrams and with consistent cuts.SummaryMany people working on distributed systems use the phrase "an agent
knows." This module gives a definition of the concept that is
consistent with intuitive definitions of knowledge.
The central idea in this module is the
relationship between what agents know and consistent cuts of
timelines. We presented several theorems about agent knowledge which
are intuitive when applied to human agents. The proofs are
straightforward and are all based on consistent cuts of timelines.ReviewIs the following true? Why?(\(z\) knows \(P\)) and (\(z\) knows \(Q\)) \(\equiv\) \(z\) knows
  \(\(P \wedge Q\))Is the following true? Why?(\(z\) knows \(P\)) or (\(z\) knows \(Q\)) \(\equiv\) \(z\) knows
  \(\( P \vee Q\))Suppose you and your friend communicate only using messages sent in
  channels, just as agents do in our model of distributed
  systems. When youknowthat your friend is wearing a cap does
  that mean that (a) your friend is wearing a cap, and (b) your friend
  can't take the cap off until the friend hears from you?K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology



Figure Link: https://kmchandy.github.io/Knowledge/KnowledgeDiagrams/Slide02.jpg

Caption: Fig. 2. Agent A knows P in all consistent cuts that cross
    point T


Example



Figure Link: https://kmchandy.github.io/Knowledge/KnowledgeDiagrams/Slide02.jpg

Caption: Fig. 2. Agent A knows P in all consistent cuts that cross
    point T


\(\neg\) (\(x\) knows \(P\)) is a predicate too, and sometimes
programmers refer to this predicate as "\(x\) does not know \(P\)."ExampleIn the previous example, let \(Q\) be the predicate: "\(y\) holds the token."
In what local states of agent \(x\) does \(x\) know that "\(y\) holds the token?"There are no local states of agent \(x\) in which \(x\) knows that
"\(y\) holds the token." Even when \(x\) does not hold the token,
\(x\) does not know that \(y\) holds the token because
the token could be in a channel.When \(x\) does not hold the token, \(x\) does not know that \(y\) holds
the token and \(x\) does not know that \(y\) does not hold the token."NOT(x knows Q)" AND "NOT(x knows NOT Q)"is a predicate which holds in the
state which \(x\) does not hold the token.Notation"\(x\) knows \(P\) at a point \(T\)" in a timeline means that at
    point \(T\) agent \(x\) is in a state where the predicate "\(x\)
knows \(P\)" holds.Theorem: Knowledge and Consistent CutsLet \(P\) be a predicate on the states of a system.If  \(x\) knows \(P\) at a point \(T\) in \(x\)'s timeline
then \(P\) holds in every consistent cut through that point.ProofThe states corresponding to all consistent cuts that pass through the same
point on \(x\)'s timeline have the same common value for \(x\)'s local
state.Example>Fig. 2. Agent A knows P in all consistent cuts that cross
    point TThe top figure in both diagrams above show a time T at which agent A knows
that P holds. This implies that P holds in all consistent cuts through
point T. The lower figures in the diagrams show consistent cuts which
passes point T on A's timeline; the theorem says that P holds for the
state at these cuts too.Theorem: A Silent Agent retains
KnowledgeAn agent that sends no information between a point \(T\) and a later
point \(T'\) retains all the knowledge it has at \(T\) at \(T'\).Let \(x\) be an agent in a system, and let \(P\) be a
predicate on a subsystem that does not include \(x\).
Let \(T\) and \(T'\) be points on \(x\)'s timeline with \(T < T'\). If
\(x\) knows \(P\) at point \(T\) and \(x\) sends no messages in the
interval \([T, T']\) then \(x\) knows \(P\) at \(T'\).ProofLet \(c'\) be any consistent cut through point \(T'\) on \(x\)'s
timeline. We will prove that \(P\) holds for the state at cut \(c'\).Let \(c\) be the
cut that is identical to \(c'\) except that it passes through point \(T\) on
\(x\)'s timeline. \(c\) is consistent because there are no
outgoing edges from \(x\)'s timeline between cuts \(c\) and
\(c'\).Because \(x\) knows \(P\) at \(T\), \(P\) holds at \(c\). Since \(c\) and \(c'\)
are identical except for the intersection with \(x\)'s timeline,
it follows that \(P\) holds \(c'\).Example>Fig. 3. Illustration of Proof of Silent AgentsConsequence of the TheoremAn agent doesn't lose knowledge by getting information from other
agents.An agent can only lose knowledge by sending information to
other agents. This seems counterintuitive; we'll look at the reasoning
underlying this in a later theorem.Theorem: Agents who don't listen remain
IgnorantAn agent that receives no information between a point \(T\) and a later
point \(T'\) learns no new knowledge between \(T\) and \(T'\).Let \(x\) be an agent in a system, and let \(P\) be a
predicate on a subsystem that does not include \(x\).
Let \(T\) and \(T'\) be points on \(x\)'s timeline with \(T < T'\). If
\(x\) knows \(P\) at point \(T'\) and \(x\) received no messages in the
interval \([T, T']\) then \(x\) knows \(P\) at \(T\).The proof has exactly the same structure as the proof of the
previous theorem.Consequence of the Theorem\(x\) didn't learn anything in the interval  \([T, T']\); everything
\(x\) knows at the later point \(T'\) is knowledge it already had at
the earlier point \(T\).The only way for an agent to gain knowledge is
to receive messages. An agent cannot learn about other agents by only
sending messages or making internal state transitions.Theorem: Knowledge implies ControlLet \(x\) and \(y\) be agents in a system, and let \(P\) be a
predicate on the states of \(y\).
Let \(T\) and \(T'\) be instants in a trajectory with \(T < T'\). If
\(x\) knows \(P\) at  \(T\), and \(\neg P\) holds at \(T'\), then there
is a path in the timeline diagram from point \(T\) on \(x\)'s timeline
to point \(T'\) on \(y\)'s timeline.Proof: If there is no path from point \(T\) on \(x\)'s timeline
to point \(T'\) on \(y\)'s timeline then there exists a consistent cut
which crosses \(x\)'s timeline at \(T\) and
crosses \(y\)'s timeline at \(T'\).ExampleIn the figure below, agent \(A\) at point \(T\) knows that agent \(C\)
holds no tokens. At a later point \(T'\) agent \(C\) holds a
token. What must happen between points \(T\) and \(T'\)?>Fig. 4. What must happen between T and T'?There must be a path in the timeline diagram from point \(T\) on agent
\(A\)'s timeline to point \(T'\) on agent \(C\)'s timeline. This path
is represented by edges that show time elapsing on a timeline and
message edges between timelines.>Fig. 5. There must be a path from A at T to C at T'?Consequence of the TheoremSuppose you and your friend communicate only by means of messages
that are delayed by arbitrary (finite) amounts. Consider a situation
where your friend knows
that you are in the library at 9 pm. Then, from our definition of
knowledge, because agents only know truth, you must be in the library
at 9 pm. Moreover, you can't leave the library until
you receive a message from your friend; this message may go through
intermediate agents.In one of the exercises we'll look at knowledge when agents have
clocks that may drift from each other but are not more than a some
constant \(M\) units apart. If your friend knows that you will be in
the library till her watch reads 9:00 pm, and your watches may drift apart
by a minute, then you can leave the library at 9:01 pm. Clocks are
useful even if they aren't perfect. More about clocks later.Theorem: Communication to learn about
ChangeThis theorem is similar to the "knowledge is control" theorem.Let \(x\) and \(y\) be agents in a system, and let \(P\) be a
predicate on the states of \(y\).
Let \(T\) and \(T'\) be instants in a trajectory with \(T < T'\). If
\(\neg P\) holds at \(T\) and \(x\) knows \(P\) at \(T'\) then there
is a path in the timeline diagram from point \(T\) on \(y\)'s timeline
to point \(T'\) on \(x\)'s timeline.ExampleIn the figure below, agent \(C\) holds a token at point \(T\).
At a later point \(T'\) agent \(A\) knows that agent \(C\) holds no
tokens. What must happen between points \(T\) and \(T'\)?>Fig. 6. What must happen between T and T'?There must be a path in the timeline diagram from point \(T\) on agent
\(C\)'s timeline to point \(T'\) on agent \(A\)'s timeline. This path
is represented by edges that show time elapsing on a timeline and
message edges between timelines.>Fig. 7. There must be a path from C at T to A at T'?What Agents Know about Channel StatesNext, let's look at systems in which messages are acknowledged.
For a pair of agents \(x, y\), let \(ms\) and \(mr\)
be the number of messages that \(x\) has sent to \(y\), and  the number 
of messages that \(y\) has received from \(x\), respectively.
Let \(as\) and \(ar\) be
the number of acknowledgements that \(y\) has sent to \(x\), and
the number of acknowledgements that \(x\) has received from \(y\),
respectively.
The following is an invariant:\(ms \geq mr \geq as \geq ar \)The number of messages in the channel from \(x\) to \(y\) is \(ms -
mr\).
Because \(ms\) and \(ar\) are variables of agent \(x\),
agent \(x\)knowsan upper bound,\(\; ms - ar\), on the number of
messages in channel \((x, y)\).
So, \(x\) knows that the channel is empty when \(\; ms = ar\).What an agent cannot knowAn agent cannot know that there are
exactly \(n\) messages in a channel, for \(n > 0\).
You can prove this result using the concept of consistent
cuts. Intuitively, the agent cannot know whether a message is in the
channel or has been received.Chains of KnowledgeLet \(x, y, z\) be agents of a system and \(P\) be a predicate on
states of the system. Then the following are all predicates:\(z\) knows \(P\)\(y\) knows that \(z\) knows \(P\)\(x\) knows that \(y\) knows that \(z\) knows \(P\)The theorems given earlier apply to any predicate. For example, if
\(x\) knows that \(y\) knows that \(z\) knows \(P\) at a point \(t\) in a
trajectory, and \(\neg P\) holds at a later point \(t'\)
then there must be path in the timeline diagram from point \(t\) on
\(x\)'s timeline to point \(t'\) on \(z\)'s timeline.Concurrent Systems with Shared VariablesThe theorems
and proofs given in this module apply to systems with shared
variables, and indeed any system with trajectories that are
representable by timeline diagrams and with consistent cuts.SummaryMany people working on distributed systems use the phrase "an agent
knows." This module gives a definition of the concept that is
consistent with intuitive definitions of knowledge.
The central idea in this module is the
relationship between what agents know and consistent cuts of
timelines. We presented several theorems about agent knowledge which
are intuitive when applied to human agents. The proofs are
straightforward and are all based on consistent cuts of timelines.ReviewIs the following true? Why?(\(z\) knows \(P\)) and (\(z\) knows \(Q\)) \(\equiv\) \(z\) knows
  \(\(P \wedge Q\))Is the following true? Why?(\(z\) knows \(P\)) or (\(z\) knows \(Q\)) \(\equiv\) \(z\) knows
  \(\( P \vee Q\))Suppose you and your friend communicate only using messages sent in
  channels, just as agents do in our model of distributed
  systems. When youknowthat your friend is wearing a cap does
  that mean that (a) your friend is wearing a cap, and (b) your friend
  can't take the cap off until the friend hears from you?K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology



Figure Link: https://kmchandy.github.io/Knowledge/KnowledgeDiagrams/Slide02.jpg

Caption: Fig. 2. Agent A knows P in all consistent cuts that cross
    point T


Example



Figure Link: https://kmchandy.github.io/Knowledge/KnowledgeDiagrams/Slide02.jpg

Caption: Fig. 2. Agent A knows P in all consistent cuts that cross
    point T


There are no local states of agent \(x\) in which \(x\) knows that
"\(y\) holds the token." Even when \(x\) does not hold the token,
\(x\) does not know that \(y\) holds the token because
the token could be in a channel.When \(x\) does not hold the token, \(x\) does not know that \(y\) holds
the token and \(x\) does not know that \(y\) does not hold the token."NOT(x knows Q)" AND "NOT(x knows NOT Q)"is a predicate which holds in the
state which \(x\) does not hold the token.Notation"\(x\) knows \(P\) at a point \(T\)" in a timeline means that at
    point \(T\) agent \(x\) is in a state where the predicate "\(x\)
knows \(P\)" holds.Theorem: Knowledge and Consistent CutsLet \(P\) be a predicate on the states of a system.If  \(x\) knows \(P\) at a point \(T\) in \(x\)'s timeline
then \(P\) holds in every consistent cut through that point.ProofThe states corresponding to all consistent cuts that pass through the same
point on \(x\)'s timeline have the same common value for \(x\)'s local
state.Example>Fig. 2. Agent A knows P in all consistent cuts that cross
    point TThe top figure in both diagrams above show a time T at which agent A knows
that P holds. This implies that P holds in all consistent cuts through
point T. The lower figures in the diagrams show consistent cuts which
passes point T on A's timeline; the theorem says that P holds for the
state at these cuts too.Theorem: A Silent Agent retains
KnowledgeAn agent that sends no information between a point \(T\) and a later
point \(T'\) retains all the knowledge it has at \(T\) at \(T'\).Let \(x\) be an agent in a system, and let \(P\) be a
predicate on a subsystem that does not include \(x\).
Let \(T\) and \(T'\) be points on \(x\)'s timeline with \(T < T'\). If
\(x\) knows \(P\) at point \(T\) and \(x\) sends no messages in the
interval \([T, T']\) then \(x\) knows \(P\) at \(T'\).ProofLet \(c'\) be any consistent cut through point \(T'\) on \(x\)'s
timeline. We will prove that \(P\) holds for the state at cut \(c'\).Let \(c\) be the
cut that is identical to \(c'\) except that it passes through point \(T\) on
\(x\)'s timeline. \(c\) is consistent because there are no
outgoing edges from \(x\)'s timeline between cuts \(c\) and
\(c'\).Because \(x\) knows \(P\) at \(T\), \(P\) holds at \(c\). Since \(c\) and \(c'\)
are identical except for the intersection with \(x\)'s timeline,
it follows that \(P\) holds \(c'\).Example>Fig. 3. Illustration of Proof of Silent AgentsConsequence of the TheoremAn agent doesn't lose knowledge by getting information from other
agents.An agent can only lose knowledge by sending information to
other agents. This seems counterintuitive; we'll look at the reasoning
underlying this in a later theorem.Theorem: Agents who don't listen remain
IgnorantAn agent that receives no information between a point \(T\) and a later
point \(T'\) learns no new knowledge between \(T\) and \(T'\).Let \(x\) be an agent in a system, and let \(P\) be a
predicate on a subsystem that does not include \(x\).
Let \(T\) and \(T'\) be points on \(x\)'s timeline with \(T < T'\). If
\(x\) knows \(P\) at point \(T'\) and \(x\) received no messages in the
interval \([T, T']\) then \(x\) knows \(P\) at \(T\).The proof has exactly the same structure as the proof of the
previous theorem.Consequence of the Theorem\(x\) didn't learn anything in the interval  \([T, T']\); everything
\(x\) knows at the later point \(T'\) is knowledge it already had at
the earlier point \(T\).The only way for an agent to gain knowledge is
to receive messages. An agent cannot learn about other agents by only
sending messages or making internal state transitions.Theorem: Knowledge implies ControlLet \(x\) and \(y\) be agents in a system, and let \(P\) be a
predicate on the states of \(y\).
Let \(T\) and \(T'\) be instants in a trajectory with \(T < T'\). If
\(x\) knows \(P\) at  \(T\), and \(\neg P\) holds at \(T'\), then there
is a path in the timeline diagram from point \(T\) on \(x\)'s timeline
to point \(T'\) on \(y\)'s timeline.Proof: If there is no path from point \(T\) on \(x\)'s timeline
to point \(T'\) on \(y\)'s timeline then there exists a consistent cut
which crosses \(x\)'s timeline at \(T\) and
crosses \(y\)'s timeline at \(T'\).ExampleIn the figure below, agent \(A\) at point \(T\) knows that agent \(C\)
holds no tokens. At a later point \(T'\) agent \(C\) holds a
token. What must happen between points \(T\) and \(T'\)?>Fig. 4. What must happen between T and T'?There must be a path in the timeline diagram from point \(T\) on agent
\(A\)'s timeline to point \(T'\) on agent \(C\)'s timeline. This path
is represented by edges that show time elapsing on a timeline and
message edges between timelines.>Fig. 5. There must be a path from A at T to C at T'?Consequence of the TheoremSuppose you and your friend communicate only by means of messages
that are delayed by arbitrary (finite) amounts. Consider a situation
where your friend knows
that you are in the library at 9 pm. Then, from our definition of
knowledge, because agents only know truth, you must be in the library
at 9 pm. Moreover, you can't leave the library until
you receive a message from your friend; this message may go through
intermediate agents.In one of the exercises we'll look at knowledge when agents have
clocks that may drift from each other but are not more than a some
constant \(M\) units apart. If your friend knows that you will be in
the library till her watch reads 9:00 pm, and your watches may drift apart
by a minute, then you can leave the library at 9:01 pm. Clocks are
useful even if they aren't perfect. More about clocks later.Theorem: Communication to learn about
ChangeThis theorem is similar to the "knowledge is control" theorem.Let \(x\) and \(y\) be agents in a system, and let \(P\) be a
predicate on the states of \(y\).
Let \(T\) and \(T'\) be instants in a trajectory with \(T < T'\). If
\(\neg P\) holds at \(T\) and \(x\) knows \(P\) at \(T'\) then there
is a path in the timeline diagram from point \(T\) on \(y\)'s timeline
to point \(T'\) on \(x\)'s timeline.ExampleIn the figure below, agent \(C\) holds a token at point \(T\).
At a later point \(T'\) agent \(A\) knows that agent \(C\) holds no
tokens. What must happen between points \(T\) and \(T'\)?>Fig. 6. What must happen between T and T'?There must be a path in the timeline diagram from point \(T\) on agent
\(C\)'s timeline to point \(T'\) on agent \(A\)'s timeline. This path
is represented by edges that show time elapsing on a timeline and
message edges between timelines.>Fig. 7. There must be a path from C at T to A at T'?What Agents Know about Channel StatesNext, let's look at systems in which messages are acknowledged.
For a pair of agents \(x, y\), let \(ms\) and \(mr\)
be the number of messages that \(x\) has sent to \(y\), and  the number 
of messages that \(y\) has received from \(x\), respectively.
Let \(as\) and \(ar\) be
the number of acknowledgements that \(y\) has sent to \(x\), and
the number of acknowledgements that \(x\) has received from \(y\),
respectively.
The following is an invariant:\(ms \geq mr \geq as \geq ar \)The number of messages in the channel from \(x\) to \(y\) is \(ms -
mr\).
Because \(ms\) and \(ar\) are variables of agent \(x\),
agent \(x\)knowsan upper bound,\(\; ms - ar\), on the number of
messages in channel \((x, y)\).
So, \(x\) knows that the channel is empty when \(\; ms = ar\).What an agent cannot knowAn agent cannot know that there are
exactly \(n\) messages in a channel, for \(n > 0\).
You can prove this result using the concept of consistent
cuts. Intuitively, the agent cannot know whether a message is in the
channel or has been received.Chains of KnowledgeLet \(x, y, z\) be agents of a system and \(P\) be a predicate on
states of the system. Then the following are all predicates:\(z\) knows \(P\)\(y\) knows that \(z\) knows \(P\)\(x\) knows that \(y\) knows that \(z\) knows \(P\)The theorems given earlier apply to any predicate. For example, if
\(x\) knows that \(y\) knows that \(z\) knows \(P\) at a point \(t\) in a
trajectory, and \(\neg P\) holds at a later point \(t'\)
then there must be path in the timeline diagram from point \(t\) on
\(x\)'s timeline to point \(t'\) on \(z\)'s timeline.Concurrent Systems with Shared VariablesThe theorems
and proofs given in this module apply to systems with shared
variables, and indeed any system with trajectories that are
representable by timeline diagrams and with consistent cuts.SummaryMany people working on distributed systems use the phrase "an agent
knows." This module gives a definition of the concept that is
consistent with intuitive definitions of knowledge.
The central idea in this module is the
relationship between what agents know and consistent cuts of
timelines. We presented several theorems about agent knowledge which
are intuitive when applied to human agents. The proofs are
straightforward and are all based on consistent cuts of timelines.ReviewIs the following true? Why?(\(z\) knows \(P\)) and (\(z\) knows \(Q\)) \(\equiv\) \(z\) knows
  \(\(P \wedge Q\))Is the following true? Why?(\(z\) knows \(P\)) or (\(z\) knows \(Q\)) \(\equiv\) \(z\) knows
  \(\( P \vee Q\))Suppose you and your friend communicate only using messages sent in
  channels, just as agents do in our model of distributed
  systems. When youknowthat your friend is wearing a cap does
  that mean that (a) your friend is wearing a cap, and (b) your friend
  can't take the cap off until the friend hears from you?K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology



Figure Link: https://kmchandy.github.io/Knowledge/KnowledgeDiagrams/Slide02.jpg

Caption: Fig. 2. Agent A knows P in all consistent cuts that cross
    point T


When \(x\) does not hold the token, \(x\) does not know that \(y\) holds
the token and \(x\) does not know that \(y\) does not hold the token."NOT(x knows Q)" AND "NOT(x knows NOT Q)"is a predicate which holds in the
state which \(x\) does not hold the token.Notation"\(x\) knows \(P\) at a point \(T\)" in a timeline means that at
    point \(T\) agent \(x\) is in a state where the predicate "\(x\)
knows \(P\)" holds.Theorem: Knowledge and Consistent CutsLet \(P\) be a predicate on the states of a system.If  \(x\) knows \(P\) at a point \(T\) in \(x\)'s timeline
then \(P\) holds in every consistent cut through that point.ProofThe states corresponding to all consistent cuts that pass through the same
point on \(x\)'s timeline have the same common value for \(x\)'s local
state.Example>Fig. 2. Agent A knows P in all consistent cuts that cross
    point TThe top figure in both diagrams above show a time T at which agent A knows
that P holds. This implies that P holds in all consistent cuts through
point T. The lower figures in the diagrams show consistent cuts which
passes point T on A's timeline; the theorem says that P holds for the
state at these cuts too.Theorem: A Silent Agent retains
KnowledgeAn agent that sends no information between a point \(T\) and a later
point \(T'\) retains all the knowledge it has at \(T\) at \(T'\).Let \(x\) be an agent in a system, and let \(P\) be a
predicate on a subsystem that does not include \(x\).
Let \(T\) and \(T'\) be points on \(x\)'s timeline with \(T < T'\). If
\(x\) knows \(P\) at point \(T\) and \(x\) sends no messages in the
interval \([T, T']\) then \(x\) knows \(P\) at \(T'\).ProofLet \(c'\) be any consistent cut through point \(T'\) on \(x\)'s
timeline. We will prove that \(P\) holds for the state at cut \(c'\).Let \(c\) be the
cut that is identical to \(c'\) except that it passes through point \(T\) on
\(x\)'s timeline. \(c\) is consistent because there are no
outgoing edges from \(x\)'s timeline between cuts \(c\) and
\(c'\).Because \(x\) knows \(P\) at \(T\), \(P\) holds at \(c\). Since \(c\) and \(c'\)
are identical except for the intersection with \(x\)'s timeline,
it follows that \(P\) holds \(c'\).Example>Fig. 3. Illustration of Proof of Silent AgentsConsequence of the TheoremAn agent doesn't lose knowledge by getting information from other
agents.An agent can only lose knowledge by sending information to
other agents. This seems counterintuitive; we'll look at the reasoning
underlying this in a later theorem.Theorem: Agents who don't listen remain
IgnorantAn agent that receives no information between a point \(T\) and a later
point \(T'\) learns no new knowledge between \(T\) and \(T'\).Let \(x\) be an agent in a system, and let \(P\) be a
predicate on a subsystem that does not include \(x\).
Let \(T\) and \(T'\) be points on \(x\)'s timeline with \(T < T'\). If
\(x\) knows \(P\) at point \(T'\) and \(x\) received no messages in the
interval \([T, T']\) then \(x\) knows \(P\) at \(T\).The proof has exactly the same structure as the proof of the
previous theorem.Consequence of the Theorem\(x\) didn't learn anything in the interval  \([T, T']\); everything
\(x\) knows at the later point \(T'\) is knowledge it already had at
the earlier point \(T\).The only way for an agent to gain knowledge is
to receive messages. An agent cannot learn about other agents by only
sending messages or making internal state transitions.Theorem: Knowledge implies ControlLet \(x\) and \(y\) be agents in a system, and let \(P\) be a
predicate on the states of \(y\).
Let \(T\) and \(T'\) be instants in a trajectory with \(T < T'\). If
\(x\) knows \(P\) at  \(T\), and \(\neg P\) holds at \(T'\), then there
is a path in the timeline diagram from point \(T\) on \(x\)'s timeline
to point \(T'\) on \(y\)'s timeline.Proof: If there is no path from point \(T\) on \(x\)'s timeline
to point \(T'\) on \(y\)'s timeline then there exists a consistent cut
which crosses \(x\)'s timeline at \(T\) and
crosses \(y\)'s timeline at \(T'\).ExampleIn the figure below, agent \(A\) at point \(T\) knows that agent \(C\)
holds no tokens. At a later point \(T'\) agent \(C\) holds a
token. What must happen between points \(T\) and \(T'\)?>Fig. 4. What must happen between T and T'?There must be a path in the timeline diagram from point \(T\) on agent
\(A\)'s timeline to point \(T'\) on agent \(C\)'s timeline. This path
is represented by edges that show time elapsing on a timeline and
message edges between timelines.>Fig. 5. There must be a path from A at T to C at T'?Consequence of the TheoremSuppose you and your friend communicate only by means of messages
that are delayed by arbitrary (finite) amounts. Consider a situation
where your friend knows
that you are in the library at 9 pm. Then, from our definition of
knowledge, because agents only know truth, you must be in the library
at 9 pm. Moreover, you can't leave the library until
you receive a message from your friend; this message may go through
intermediate agents.In one of the exercises we'll look at knowledge when agents have
clocks that may drift from each other but are not more than a some
constant \(M\) units apart. If your friend knows that you will be in
the library till her watch reads 9:00 pm, and your watches may drift apart
by a minute, then you can leave the library at 9:01 pm. Clocks are
useful even if they aren't perfect. More about clocks later.Theorem: Communication to learn about
ChangeThis theorem is similar to the "knowledge is control" theorem.Let \(x\) and \(y\) be agents in a system, and let \(P\) be a
predicate on the states of \(y\).
Let \(T\) and \(T'\) be instants in a trajectory with \(T < T'\). If
\(\neg P\) holds at \(T\) and \(x\) knows \(P\) at \(T'\) then there
is a path in the timeline diagram from point \(T\) on \(y\)'s timeline
to point \(T'\) on \(x\)'s timeline.ExampleIn the figure below, agent \(C\) holds a token at point \(T\).
At a later point \(T'\) agent \(A\) knows that agent \(C\) holds no
tokens. What must happen between points \(T\) and \(T'\)?>Fig. 6. What must happen between T and T'?There must be a path in the timeline diagram from point \(T\) on agent
\(C\)'s timeline to point \(T'\) on agent \(A\)'s timeline. This path
is represented by edges that show time elapsing on a timeline and
message edges between timelines.>Fig. 7. There must be a path from C at T to A at T'?What Agents Know about Channel StatesNext, let's look at systems in which messages are acknowledged.
For a pair of agents \(x, y\), let \(ms\) and \(mr\)
be the number of messages that \(x\) has sent to \(y\), and  the number 
of messages that \(y\) has received from \(x\), respectively.
Let \(as\) and \(ar\) be
the number of acknowledgements that \(y\) has sent to \(x\), and
the number of acknowledgements that \(x\) has received from \(y\),
respectively.
The following is an invariant:\(ms \geq mr \geq as \geq ar \)The number of messages in the channel from \(x\) to \(y\) is \(ms -
mr\).
Because \(ms\) and \(ar\) are variables of agent \(x\),
agent \(x\)knowsan upper bound,\(\; ms - ar\), on the number of
messages in channel \((x, y)\).
So, \(x\) knows that the channel is empty when \(\; ms = ar\).What an agent cannot knowAn agent cannot know that there are
exactly \(n\) messages in a channel, for \(n > 0\).
You can prove this result using the concept of consistent
cuts. Intuitively, the agent cannot know whether a message is in the
channel or has been received.Chains of KnowledgeLet \(x, y, z\) be agents of a system and \(P\) be a predicate on
states of the system. Then the following are all predicates:\(z\) knows \(P\)\(y\) knows that \(z\) knows \(P\)\(x\) knows that \(y\) knows that \(z\) knows \(P\)The theorems given earlier apply to any predicate. For example, if
\(x\) knows that \(y\) knows that \(z\) knows \(P\) at a point \(t\) in a
trajectory, and \(\neg P\) holds at a later point \(t'\)
then there must be path in the timeline diagram from point \(t\) on
\(x\)'s timeline to point \(t'\) on \(z\)'s timeline.Concurrent Systems with Shared VariablesThe theorems
and proofs given in this module apply to systems with shared
variables, and indeed any system with trajectories that are
representable by timeline diagrams and with consistent cuts.SummaryMany people working on distributed systems use the phrase "an agent
knows." This module gives a definition of the concept that is
consistent with intuitive definitions of knowledge.
The central idea in this module is the
relationship between what agents know and consistent cuts of
timelines. We presented several theorems about agent knowledge which
are intuitive when applied to human agents. The proofs are
straightforward and are all based on consistent cuts of timelines.ReviewIs the following true? Why?(\(z\) knows \(P\)) and (\(z\) knows \(Q\)) \(\equiv\) \(z\) knows
  \(\(P \wedge Q\))Is the following true? Why?(\(z\) knows \(P\)) or (\(z\) knows \(Q\)) \(\equiv\) \(z\) knows
  \(\( P \vee Q\))Suppose you and your friend communicate only using messages sent in
  channels, just as agents do in our model of distributed
  systems. When youknowthat your friend is wearing a cap does
  that mean that (a) your friend is wearing a cap, and (b) your friend
  can't take the cap off until the friend hears from you?K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology



Figure Link: https://kmchandy.github.io/Knowledge/KnowledgeDiagrams/Slide02.jpg

Caption: Fig. 2. Agent A knows P in all consistent cuts that cross
    point T


"NOT(x knows Q)" AND "NOT(x knows NOT Q)"is a predicate which holds in the
state which \(x\) does not hold the token.Notation"\(x\) knows \(P\) at a point \(T\)" in a timeline means that at
    point \(T\) agent \(x\) is in a state where the predicate "\(x\)
knows \(P\)" holds.Theorem: Knowledge and Consistent CutsLet \(P\) be a predicate on the states of a system.If  \(x\) knows \(P\) at a point \(T\) in \(x\)'s timeline
then \(P\) holds in every consistent cut through that point.ProofThe states corresponding to all consistent cuts that pass through the same
point on \(x\)'s timeline have the same common value for \(x\)'s local
state.Example>Fig. 2. Agent A knows P in all consistent cuts that cross
    point TThe top figure in both diagrams above show a time T at which agent A knows
that P holds. This implies that P holds in all consistent cuts through
point T. The lower figures in the diagrams show consistent cuts which
passes point T on A's timeline; the theorem says that P holds for the
state at these cuts too.Theorem: A Silent Agent retains
KnowledgeAn agent that sends no information between a point \(T\) and a later
point \(T'\) retains all the knowledge it has at \(T\) at \(T'\).Let \(x\) be an agent in a system, and let \(P\) be a
predicate on a subsystem that does not include \(x\).
Let \(T\) and \(T'\) be points on \(x\)'s timeline with \(T < T'\). If
\(x\) knows \(P\) at point \(T\) and \(x\) sends no messages in the
interval \([T, T']\) then \(x\) knows \(P\) at \(T'\).ProofLet \(c'\) be any consistent cut through point \(T'\) on \(x\)'s
timeline. We will prove that \(P\) holds for the state at cut \(c'\).Let \(c\) be the
cut that is identical to \(c'\) except that it passes through point \(T\) on
\(x\)'s timeline. \(c\) is consistent because there are no
outgoing edges from \(x\)'s timeline between cuts \(c\) and
\(c'\).Because \(x\) knows \(P\) at \(T\), \(P\) holds at \(c\). Since \(c\) and \(c'\)
are identical except for the intersection with \(x\)'s timeline,
it follows that \(P\) holds \(c'\).Example>Fig. 3. Illustration of Proof of Silent AgentsConsequence of the TheoremAn agent doesn't lose knowledge by getting information from other
agents.An agent can only lose knowledge by sending information to
other agents. This seems counterintuitive; we'll look at the reasoning
underlying this in a later theorem.Theorem: Agents who don't listen remain
IgnorantAn agent that receives no information between a point \(T\) and a later
point \(T'\) learns no new knowledge between \(T\) and \(T'\).Let \(x\) be an agent in a system, and let \(P\) be a
predicate on a subsystem that does not include \(x\).
Let \(T\) and \(T'\) be points on \(x\)'s timeline with \(T < T'\). If
\(x\) knows \(P\) at point \(T'\) and \(x\) received no messages in the
interval \([T, T']\) then \(x\) knows \(P\) at \(T\).The proof has exactly the same structure as the proof of the
previous theorem.Consequence of the Theorem\(x\) didn't learn anything in the interval  \([T, T']\); everything
\(x\) knows at the later point \(T'\) is knowledge it already had at
the earlier point \(T\).The only way for an agent to gain knowledge is
to receive messages. An agent cannot learn about other agents by only
sending messages or making internal state transitions.Theorem: Knowledge implies ControlLet \(x\) and \(y\) be agents in a system, and let \(P\) be a
predicate on the states of \(y\).
Let \(T\) and \(T'\) be instants in a trajectory with \(T < T'\). If
\(x\) knows \(P\) at  \(T\), and \(\neg P\) holds at \(T'\), then there
is a path in the timeline diagram from point \(T\) on \(x\)'s timeline
to point \(T'\) on \(y\)'s timeline.Proof: If there is no path from point \(T\) on \(x\)'s timeline
to point \(T'\) on \(y\)'s timeline then there exists a consistent cut
which crosses \(x\)'s timeline at \(T\) and
crosses \(y\)'s timeline at \(T'\).ExampleIn the figure below, agent \(A\) at point \(T\) knows that agent \(C\)
holds no tokens. At a later point \(T'\) agent \(C\) holds a
token. What must happen between points \(T\) and \(T'\)?>Fig. 4. What must happen between T and T'?There must be a path in the timeline diagram from point \(T\) on agent
\(A\)'s timeline to point \(T'\) on agent \(C\)'s timeline. This path
is represented by edges that show time elapsing on a timeline and
message edges between timelines.>Fig. 5. There must be a path from A at T to C at T'?Consequence of the TheoremSuppose you and your friend communicate only by means of messages
that are delayed by arbitrary (finite) amounts. Consider a situation
where your friend knows
that you are in the library at 9 pm. Then, from our definition of
knowledge, because agents only know truth, you must be in the library
at 9 pm. Moreover, you can't leave the library until
you receive a message from your friend; this message may go through
intermediate agents.In one of the exercises we'll look at knowledge when agents have
clocks that may drift from each other but are not more than a some
constant \(M\) units apart. If your friend knows that you will be in
the library till her watch reads 9:00 pm, and your watches may drift apart
by a minute, then you can leave the library at 9:01 pm. Clocks are
useful even if they aren't perfect. More about clocks later.Theorem: Communication to learn about
ChangeThis theorem is similar to the "knowledge is control" theorem.Let \(x\) and \(y\) be agents in a system, and let \(P\) be a
predicate on the states of \(y\).
Let \(T\) and \(T'\) be instants in a trajectory with \(T < T'\). If
\(\neg P\) holds at \(T\) and \(x\) knows \(P\) at \(T'\) then there
is a path in the timeline diagram from point \(T\) on \(y\)'s timeline
to point \(T'\) on \(x\)'s timeline.ExampleIn the figure below, agent \(C\) holds a token at point \(T\).
At a later point \(T'\) agent \(A\) knows that agent \(C\) holds no
tokens. What must happen between points \(T\) and \(T'\)?>Fig. 6. What must happen between T and T'?There must be a path in the timeline diagram from point \(T\) on agent
\(C\)'s timeline to point \(T'\) on agent \(A\)'s timeline. This path
is represented by edges that show time elapsing on a timeline and
message edges between timelines.>Fig. 7. There must be a path from C at T to A at T'?What Agents Know about Channel StatesNext, let's look at systems in which messages are acknowledged.
For a pair of agents \(x, y\), let \(ms\) and \(mr\)
be the number of messages that \(x\) has sent to \(y\), and  the number 
of messages that \(y\) has received from \(x\), respectively.
Let \(as\) and \(ar\) be
the number of acknowledgements that \(y\) has sent to \(x\), and
the number of acknowledgements that \(x\) has received from \(y\),
respectively.
The following is an invariant:\(ms \geq mr \geq as \geq ar \)The number of messages in the channel from \(x\) to \(y\) is \(ms -
mr\).
Because \(ms\) and \(ar\) are variables of agent \(x\),
agent \(x\)knowsan upper bound,\(\; ms - ar\), on the number of
messages in channel \((x, y)\).
So, \(x\) knows that the channel is empty when \(\; ms = ar\).What an agent cannot knowAn agent cannot know that there are
exactly \(n\) messages in a channel, for \(n > 0\).
You can prove this result using the concept of consistent
cuts. Intuitively, the agent cannot know whether a message is in the
channel or has been received.Chains of KnowledgeLet \(x, y, z\) be agents of a system and \(P\) be a predicate on
states of the system. Then the following are all predicates:\(z\) knows \(P\)\(y\) knows that \(z\) knows \(P\)\(x\) knows that \(y\) knows that \(z\) knows \(P\)The theorems given earlier apply to any predicate. For example, if
\(x\) knows that \(y\) knows that \(z\) knows \(P\) at a point \(t\) in a
trajectory, and \(\neg P\) holds at a later point \(t'\)
then there must be path in the timeline diagram from point \(t\) on
\(x\)'s timeline to point \(t'\) on \(z\)'s timeline.Concurrent Systems with Shared VariablesThe theorems
and proofs given in this module apply to systems with shared
variables, and indeed any system with trajectories that are
representable by timeline diagrams and with consistent cuts.SummaryMany people working on distributed systems use the phrase "an agent
knows." This module gives a definition of the concept that is
consistent with intuitive definitions of knowledge.
The central idea in this module is the
relationship between what agents know and consistent cuts of
timelines. We presented several theorems about agent knowledge which
are intuitive when applied to human agents. The proofs are
straightforward and are all based on consistent cuts of timelines.ReviewIs the following true? Why?(\(z\) knows \(P\)) and (\(z\) knows \(Q\)) \(\equiv\) \(z\) knows
  \(\(P \wedge Q\))Is the following true? Why?(\(z\) knows \(P\)) or (\(z\) knows \(Q\)) \(\equiv\) \(z\) knows
  \(\( P \vee Q\))Suppose you and your friend communicate only using messages sent in
  channels, just as agents do in our model of distributed
  systems. When youknowthat your friend is wearing a cap does
  that mean that (a) your friend is wearing a cap, and (b) your friend
  can't take the cap off until the friend hears from you?K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology



Figure Link: https://kmchandy.github.io/Knowledge/KnowledgeDiagrams/Slide02.jpg

Caption: Fig. 2. Agent A knows P in all consistent cuts that cross
    point T


If  \(x\) knows \(P\) at a point \(T\) in \(x\)'s timeline
then \(P\) holds in every consistent cut through that point.ProofThe states corresponding to all consistent cuts that pass through the same
point on \(x\)'s timeline have the same common value for \(x\)'s local
state.Example>Fig. 2. Agent A knows P in all consistent cuts that cross
    point TThe top figure in both diagrams above show a time T at which agent A knows
that P holds. This implies that P holds in all consistent cuts through
point T. The lower figures in the diagrams show consistent cuts which
passes point T on A's timeline; the theorem says that P holds for the
state at these cuts too.Theorem: A Silent Agent retains
KnowledgeAn agent that sends no information between a point \(T\) and a later
point \(T'\) retains all the knowledge it has at \(T\) at \(T'\).Let \(x\) be an agent in a system, and let \(P\) be a
predicate on a subsystem that does not include \(x\).
Let \(T\) and \(T'\) be points on \(x\)'s timeline with \(T < T'\). If
\(x\) knows \(P\) at point \(T\) and \(x\) sends no messages in the
interval \([T, T']\) then \(x\) knows \(P\) at \(T'\).ProofLet \(c'\) be any consistent cut through point \(T'\) on \(x\)'s
timeline. We will prove that \(P\) holds for the state at cut \(c'\).Let \(c\) be the
cut that is identical to \(c'\) except that it passes through point \(T\) on
\(x\)'s timeline. \(c\) is consistent because there are no
outgoing edges from \(x\)'s timeline between cuts \(c\) and
\(c'\).Because \(x\) knows \(P\) at \(T\), \(P\) holds at \(c\). Since \(c\) and \(c'\)
are identical except for the intersection with \(x\)'s timeline,
it follows that \(P\) holds \(c'\).Example>Fig. 3. Illustration of Proof of Silent AgentsConsequence of the TheoremAn agent doesn't lose knowledge by getting information from other
agents.An agent can only lose knowledge by sending information to
other agents. This seems counterintuitive; we'll look at the reasoning
underlying this in a later theorem.Theorem: Agents who don't listen remain
IgnorantAn agent that receives no information between a point \(T\) and a later
point \(T'\) learns no new knowledge between \(T\) and \(T'\).Let \(x\) be an agent in a system, and let \(P\) be a
predicate on a subsystem that does not include \(x\).
Let \(T\) and \(T'\) be points on \(x\)'s timeline with \(T < T'\). If
\(x\) knows \(P\) at point \(T'\) and \(x\) received no messages in the
interval \([T, T']\) then \(x\) knows \(P\) at \(T\).The proof has exactly the same structure as the proof of the
previous theorem.Consequence of the Theorem\(x\) didn't learn anything in the interval  \([T, T']\); everything
\(x\) knows at the later point \(T'\) is knowledge it already had at
the earlier point \(T\).The only way for an agent to gain knowledge is
to receive messages. An agent cannot learn about other agents by only
sending messages or making internal state transitions.Theorem: Knowledge implies ControlLet \(x\) and \(y\) be agents in a system, and let \(P\) be a
predicate on the states of \(y\).
Let \(T\) and \(T'\) be instants in a trajectory with \(T < T'\). If
\(x\) knows \(P\) at  \(T\), and \(\neg P\) holds at \(T'\), then there
is a path in the timeline diagram from point \(T\) on \(x\)'s timeline
to point \(T'\) on \(y\)'s timeline.Proof: If there is no path from point \(T\) on \(x\)'s timeline
to point \(T'\) on \(y\)'s timeline then there exists a consistent cut
which crosses \(x\)'s timeline at \(T\) and
crosses \(y\)'s timeline at \(T'\).ExampleIn the figure below, agent \(A\) at point \(T\) knows that agent \(C\)
holds no tokens. At a later point \(T'\) agent \(C\) holds a
token. What must happen between points \(T\) and \(T'\)?>Fig. 4. What must happen between T and T'?There must be a path in the timeline diagram from point \(T\) on agent
\(A\)'s timeline to point \(T'\) on agent \(C\)'s timeline. This path
is represented by edges that show time elapsing on a timeline and
message edges between timelines.>Fig. 5. There must be a path from A at T to C at T'?Consequence of the TheoremSuppose you and your friend communicate only by means of messages
that are delayed by arbitrary (finite) amounts. Consider a situation
where your friend knows
that you are in the library at 9 pm. Then, from our definition of
knowledge, because agents only know truth, you must be in the library
at 9 pm. Moreover, you can't leave the library until
you receive a message from your friend; this message may go through
intermediate agents.In one of the exercises we'll look at knowledge when agents have
clocks that may drift from each other but are not more than a some
constant \(M\) units apart. If your friend knows that you will be in
the library till her watch reads 9:00 pm, and your watches may drift apart
by a minute, then you can leave the library at 9:01 pm. Clocks are
useful even if they aren't perfect. More about clocks later.Theorem: Communication to learn about
ChangeThis theorem is similar to the "knowledge is control" theorem.Let \(x\) and \(y\) be agents in a system, and let \(P\) be a
predicate on the states of \(y\).
Let \(T\) and \(T'\) be instants in a trajectory with \(T < T'\). If
\(\neg P\) holds at \(T\) and \(x\) knows \(P\) at \(T'\) then there
is a path in the timeline diagram from point \(T\) on \(y\)'s timeline
to point \(T'\) on \(x\)'s timeline.ExampleIn the figure below, agent \(C\) holds a token at point \(T\).
At a later point \(T'\) agent \(A\) knows that agent \(C\) holds no
tokens. What must happen between points \(T\) and \(T'\)?>Fig. 6. What must happen between T and T'?There must be a path in the timeline diagram from point \(T\) on agent
\(C\)'s timeline to point \(T'\) on agent \(A\)'s timeline. This path
is represented by edges that show time elapsing on a timeline and
message edges between timelines.>Fig. 7. There must be a path from C at T to A at T'?What Agents Know about Channel StatesNext, let's look at systems in which messages are acknowledged.
For a pair of agents \(x, y\), let \(ms\) and \(mr\)
be the number of messages that \(x\) has sent to \(y\), and  the number 
of messages that \(y\) has received from \(x\), respectively.
Let \(as\) and \(ar\) be
the number of acknowledgements that \(y\) has sent to \(x\), and
the number of acknowledgements that \(x\) has received from \(y\),
respectively.
The following is an invariant:\(ms \geq mr \geq as \geq ar \)The number of messages in the channel from \(x\) to \(y\) is \(ms -
mr\).
Because \(ms\) and \(ar\) are variables of agent \(x\),
agent \(x\)knowsan upper bound,\(\; ms - ar\), on the number of
messages in channel \((x, y)\).
So, \(x\) knows that the channel is empty when \(\; ms = ar\).What an agent cannot knowAn agent cannot know that there are
exactly \(n\) messages in a channel, for \(n > 0\).
You can prove this result using the concept of consistent
cuts. Intuitively, the agent cannot know whether a message is in the
channel or has been received.Chains of KnowledgeLet \(x, y, z\) be agents of a system and \(P\) be a predicate on
states of the system. Then the following are all predicates:\(z\) knows \(P\)\(y\) knows that \(z\) knows \(P\)\(x\) knows that \(y\) knows that \(z\) knows \(P\)The theorems given earlier apply to any predicate. For example, if
\(x\) knows that \(y\) knows that \(z\) knows \(P\) at a point \(t\) in a
trajectory, and \(\neg P\) holds at a later point \(t'\)
then there must be path in the timeline diagram from point \(t\) on
\(x\)'s timeline to point \(t'\) on \(z\)'s timeline.Concurrent Systems with Shared VariablesThe theorems
and proofs given in this module apply to systems with shared
variables, and indeed any system with trajectories that are
representable by timeline diagrams and with consistent cuts.SummaryMany people working on distributed systems use the phrase "an agent
knows." This module gives a definition of the concept that is
consistent with intuitive definitions of knowledge.
The central idea in this module is the
relationship between what agents know and consistent cuts of
timelines. We presented several theorems about agent knowledge which
are intuitive when applied to human agents. The proofs are
straightforward and are all based on consistent cuts of timelines.ReviewIs the following true? Why?(\(z\) knows \(P\)) and (\(z\) knows \(Q\)) \(\equiv\) \(z\) knows
  \(\(P \wedge Q\))Is the following true? Why?(\(z\) knows \(P\)) or (\(z\) knows \(Q\)) \(\equiv\) \(z\) knows
  \(\( P \vee Q\))Suppose you and your friend communicate only using messages sent in
  channels, just as agents do in our model of distributed
  systems. When youknowthat your friend is wearing a cap does
  that mean that (a) your friend is wearing a cap, and (b) your friend
  can't take the cap off until the friend hears from you?K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology



Figure Link: https://kmchandy.github.io/Knowledge/KnowledgeDiagrams/Slide02.jpg

Caption: Fig. 2. Agent A knows P in all consistent cuts that cross
    point T


Proof



Figure Link: https://kmchandy.github.io/Knowledge/KnowledgeDiagrams/Slide02.jpg

Caption: Fig. 2. Agent A knows P in all consistent cuts that cross
    point T


The top figure in both diagrams above show a time T at which agent A knows
that P holds. This implies that P holds in all consistent cuts through
point T. The lower figures in the diagrams show consistent cuts which
passes point T on A's timeline; the theorem says that P holds for the
state at these cuts too.Theorem: A Silent Agent retains
KnowledgeAn agent that sends no information between a point \(T\) and a later
point \(T'\) retains all the knowledge it has at \(T\) at \(T'\).Let \(x\) be an agent in a system, and let \(P\) be a
predicate on a subsystem that does not include \(x\).
Let \(T\) and \(T'\) be points on \(x\)'s timeline with \(T < T'\). If
\(x\) knows \(P\) at point \(T\) and \(x\) sends no messages in the
interval \([T, T']\) then \(x\) knows \(P\) at \(T'\).ProofLet \(c'\) be any consistent cut through point \(T'\) on \(x\)'s
timeline. We will prove that \(P\) holds for the state at cut \(c'\).Let \(c\) be the
cut that is identical to \(c'\) except that it passes through point \(T\) on
\(x\)'s timeline. \(c\) is consistent because there are no
outgoing edges from \(x\)'s timeline between cuts \(c\) and
\(c'\).Because \(x\) knows \(P\) at \(T\), \(P\) holds at \(c\). Since \(c\) and \(c'\)
are identical except for the intersection with \(x\)'s timeline,
it follows that \(P\) holds \(c'\).Example>Fig. 3. Illustration of Proof of Silent AgentsConsequence of the TheoremAn agent doesn't lose knowledge by getting information from other
agents.An agent can only lose knowledge by sending information to
other agents. This seems counterintuitive; we'll look at the reasoning
underlying this in a later theorem.Theorem: Agents who don't listen remain
IgnorantAn agent that receives no information between a point \(T\) and a later
point \(T'\) learns no new knowledge between \(T\) and \(T'\).Let \(x\) be an agent in a system, and let \(P\) be a
predicate on a subsystem that does not include \(x\).
Let \(T\) and \(T'\) be points on \(x\)'s timeline with \(T < T'\). If
\(x\) knows \(P\) at point \(T'\) and \(x\) received no messages in the
interval \([T, T']\) then \(x\) knows \(P\) at \(T\).The proof has exactly the same structure as the proof of the
previous theorem.Consequence of the Theorem\(x\) didn't learn anything in the interval  \([T, T']\); everything
\(x\) knows at the later point \(T'\) is knowledge it already had at
the earlier point \(T\).The only way for an agent to gain knowledge is
to receive messages. An agent cannot learn about other agents by only
sending messages or making internal state transitions.Theorem: Knowledge implies ControlLet \(x\) and \(y\) be agents in a system, and let \(P\) be a
predicate on the states of \(y\).
Let \(T\) and \(T'\) be instants in a trajectory with \(T < T'\). If
\(x\) knows \(P\) at  \(T\), and \(\neg P\) holds at \(T'\), then there
is a path in the timeline diagram from point \(T\) on \(x\)'s timeline
to point \(T'\) on \(y\)'s timeline.Proof: If there is no path from point \(T\) on \(x\)'s timeline
to point \(T'\) on \(y\)'s timeline then there exists a consistent cut
which crosses \(x\)'s timeline at \(T\) and
crosses \(y\)'s timeline at \(T'\).ExampleIn the figure below, agent \(A\) at point \(T\) knows that agent \(C\)
holds no tokens. At a later point \(T'\) agent \(C\) holds a
token. What must happen between points \(T\) and \(T'\)?>Fig. 4. What must happen between T and T'?There must be a path in the timeline diagram from point \(T\) on agent
\(A\)'s timeline to point \(T'\) on agent \(C\)'s timeline. This path
is represented by edges that show time elapsing on a timeline and
message edges between timelines.>Fig. 5. There must be a path from A at T to C at T'?Consequence of the TheoremSuppose you and your friend communicate only by means of messages
that are delayed by arbitrary (finite) amounts. Consider a situation
where your friend knows
that you are in the library at 9 pm. Then, from our definition of
knowledge, because agents only know truth, you must be in the library
at 9 pm. Moreover, you can't leave the library until
you receive a message from your friend; this message may go through
intermediate agents.In one of the exercises we'll look at knowledge when agents have
clocks that may drift from each other but are not more than a some
constant \(M\) units apart. If your friend knows that you will be in
the library till her watch reads 9:00 pm, and your watches may drift apart
by a minute, then you can leave the library at 9:01 pm. Clocks are
useful even if they aren't perfect. More about clocks later.Theorem: Communication to learn about
ChangeThis theorem is similar to the "knowledge is control" theorem.Let \(x\) and \(y\) be agents in a system, and let \(P\) be a
predicate on the states of \(y\).
Let \(T\) and \(T'\) be instants in a trajectory with \(T < T'\). If
\(\neg P\) holds at \(T\) and \(x\) knows \(P\) at \(T'\) then there
is a path in the timeline diagram from point \(T\) on \(y\)'s timeline
to point \(T'\) on \(x\)'s timeline.ExampleIn the figure below, agent \(C\) holds a token at point \(T\).
At a later point \(T'\) agent \(A\) knows that agent \(C\) holds no
tokens. What must happen between points \(T\) and \(T'\)?>Fig. 6. What must happen between T and T'?There must be a path in the timeline diagram from point \(T\) on agent
\(C\)'s timeline to point \(T'\) on agent \(A\)'s timeline. This path
is represented by edges that show time elapsing on a timeline and
message edges between timelines.>Fig. 7. There must be a path from C at T to A at T'?What Agents Know about Channel StatesNext, let's look at systems in which messages are acknowledged.
For a pair of agents \(x, y\), let \(ms\) and \(mr\)
be the number of messages that \(x\) has sent to \(y\), and  the number 
of messages that \(y\) has received from \(x\), respectively.
Let \(as\) and \(ar\) be
the number of acknowledgements that \(y\) has sent to \(x\), and
the number of acknowledgements that \(x\) has received from \(y\),
respectively.
The following is an invariant:\(ms \geq mr \geq as \geq ar \)The number of messages in the channel from \(x\) to \(y\) is \(ms -
mr\).
Because \(ms\) and \(ar\) are variables of agent \(x\),
agent \(x\)knowsan upper bound,\(\; ms - ar\), on the number of
messages in channel \((x, y)\).
So, \(x\) knows that the channel is empty when \(\; ms = ar\).What an agent cannot knowAn agent cannot know that there are
exactly \(n\) messages in a channel, for \(n > 0\).
You can prove this result using the concept of consistent
cuts. Intuitively, the agent cannot know whether a message is in the
channel or has been received.Chains of KnowledgeLet \(x, y, z\) be agents of a system and \(P\) be a predicate on
states of the system. Then the following are all predicates:\(z\) knows \(P\)\(y\) knows that \(z\) knows \(P\)\(x\) knows that \(y\) knows that \(z\) knows \(P\)The theorems given earlier apply to any predicate. For example, if
\(x\) knows that \(y\) knows that \(z\) knows \(P\) at a point \(t\) in a
trajectory, and \(\neg P\) holds at a later point \(t'\)
then there must be path in the timeline diagram from point \(t\) on
\(x\)'s timeline to point \(t'\) on \(z\)'s timeline.Concurrent Systems with Shared VariablesThe theorems
and proofs given in this module apply to systems with shared
variables, and indeed any system with trajectories that are
representable by timeline diagrams and with consistent cuts.SummaryMany people working on distributed systems use the phrase "an agent
knows." This module gives a definition of the concept that is
consistent with intuitive definitions of knowledge.
The central idea in this module is the
relationship between what agents know and consistent cuts of
timelines. We presented several theorems about agent knowledge which
are intuitive when applied to human agents. The proofs are
straightforward and are all based on consistent cuts of timelines.ReviewIs the following true? Why?(\(z\) knows \(P\)) and (\(z\) knows \(Q\)) \(\equiv\) \(z\) knows
  \(\(P \wedge Q\))Is the following true? Why?(\(z\) knows \(P\)) or (\(z\) knows \(Q\)) \(\equiv\) \(z\) knows
  \(\( P \vee Q\))Suppose you and your friend communicate only using messages sent in
  channels, just as agents do in our model of distributed
  systems. When youknowthat your friend is wearing a cap does
  that mean that (a) your friend is wearing a cap, and (b) your friend
  can't take the cap off until the friend hears from you?K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology



Figure Link: https://kmchandy.github.io/Knowledge/KnowledgeDiagrams/Slide04.jpg

Caption: Fig. 3. Illustration of Proof of Silent Agents


Proof



Figure Link: https://kmchandy.github.io/Knowledge/KnowledgeDiagrams/Slide04.jpg

Caption: Fig. 3. Illustration of Proof of Silent Agents


Let \(c\) be the
cut that is identical to \(c'\) except that it passes through point \(T\) on
\(x\)'s timeline. \(c\) is consistent because there are no
outgoing edges from \(x\)'s timeline between cuts \(c\) and
\(c'\).Because \(x\) knows \(P\) at \(T\), \(P\) holds at \(c\). Since \(c\) and \(c'\)
are identical except for the intersection with \(x\)'s timeline,
it follows that \(P\) holds \(c'\).Example>Fig. 3. Illustration of Proof of Silent AgentsConsequence of the TheoremAn agent doesn't lose knowledge by getting information from other
agents.An agent can only lose knowledge by sending information to
other agents. This seems counterintuitive; we'll look at the reasoning
underlying this in a later theorem.Theorem: Agents who don't listen remain
IgnorantAn agent that receives no information between a point \(T\) and a later
point \(T'\) learns no new knowledge between \(T\) and \(T'\).Let \(x\) be an agent in a system, and let \(P\) be a
predicate on a subsystem that does not include \(x\).
Let \(T\) and \(T'\) be points on \(x\)'s timeline with \(T < T'\). If
\(x\) knows \(P\) at point \(T'\) and \(x\) received no messages in the
interval \([T, T']\) then \(x\) knows \(P\) at \(T\).The proof has exactly the same structure as the proof of the
previous theorem.Consequence of the Theorem\(x\) didn't learn anything in the interval  \([T, T']\); everything
\(x\) knows at the later point \(T'\) is knowledge it already had at
the earlier point \(T\).The only way for an agent to gain knowledge is
to receive messages. An agent cannot learn about other agents by only
sending messages or making internal state transitions.Theorem: Knowledge implies ControlLet \(x\) and \(y\) be agents in a system, and let \(P\) be a
predicate on the states of \(y\).
Let \(T\) and \(T'\) be instants in a trajectory with \(T < T'\). If
\(x\) knows \(P\) at  \(T\), and \(\neg P\) holds at \(T'\), then there
is a path in the timeline diagram from point \(T\) on \(x\)'s timeline
to point \(T'\) on \(y\)'s timeline.Proof: If there is no path from point \(T\) on \(x\)'s timeline
to point \(T'\) on \(y\)'s timeline then there exists a consistent cut
which crosses \(x\)'s timeline at \(T\) and
crosses \(y\)'s timeline at \(T'\).ExampleIn the figure below, agent \(A\) at point \(T\) knows that agent \(C\)
holds no tokens. At a later point \(T'\) agent \(C\) holds a
token. What must happen between points \(T\) and \(T'\)?>Fig. 4. What must happen between T and T'?There must be a path in the timeline diagram from point \(T\) on agent
\(A\)'s timeline to point \(T'\) on agent \(C\)'s timeline. This path
is represented by edges that show time elapsing on a timeline and
message edges between timelines.>Fig. 5. There must be a path from A at T to C at T'?Consequence of the TheoremSuppose you and your friend communicate only by means of messages
that are delayed by arbitrary (finite) amounts. Consider a situation
where your friend knows
that you are in the library at 9 pm. Then, from our definition of
knowledge, because agents only know truth, you must be in the library
at 9 pm. Moreover, you can't leave the library until
you receive a message from your friend; this message may go through
intermediate agents.In one of the exercises we'll look at knowledge when agents have
clocks that may drift from each other but are not more than a some
constant \(M\) units apart. If your friend knows that you will be in
the library till her watch reads 9:00 pm, and your watches may drift apart
by a minute, then you can leave the library at 9:01 pm. Clocks are
useful even if they aren't perfect. More about clocks later.Theorem: Communication to learn about
ChangeThis theorem is similar to the "knowledge is control" theorem.Let \(x\) and \(y\) be agents in a system, and let \(P\) be a
predicate on the states of \(y\).
Let \(T\) and \(T'\) be instants in a trajectory with \(T < T'\). If
\(\neg P\) holds at \(T\) and \(x\) knows \(P\) at \(T'\) then there
is a path in the timeline diagram from point \(T\) on \(y\)'s timeline
to point \(T'\) on \(x\)'s timeline.ExampleIn the figure below, agent \(C\) holds a token at point \(T\).
At a later point \(T'\) agent \(A\) knows that agent \(C\) holds no
tokens. What must happen between points \(T\) and \(T'\)?>Fig. 6. What must happen between T and T'?There must be a path in the timeline diagram from point \(T\) on agent
\(C\)'s timeline to point \(T'\) on agent \(A\)'s timeline. This path
is represented by edges that show time elapsing on a timeline and
message edges between timelines.>Fig. 7. There must be a path from C at T to A at T'?What Agents Know about Channel StatesNext, let's look at systems in which messages are acknowledged.
For a pair of agents \(x, y\), let \(ms\) and \(mr\)
be the number of messages that \(x\) has sent to \(y\), and  the number 
of messages that \(y\) has received from \(x\), respectively.
Let \(as\) and \(ar\) be
the number of acknowledgements that \(y\) has sent to \(x\), and
the number of acknowledgements that \(x\) has received from \(y\),
respectively.
The following is an invariant:\(ms \geq mr \geq as \geq ar \)The number of messages in the channel from \(x\) to \(y\) is \(ms -
mr\).
Because \(ms\) and \(ar\) are variables of agent \(x\),
agent \(x\)knowsan upper bound,\(\; ms - ar\), on the number of
messages in channel \((x, y)\).
So, \(x\) knows that the channel is empty when \(\; ms = ar\).What an agent cannot knowAn agent cannot know that there are
exactly \(n\) messages in a channel, for \(n > 0\).
You can prove this result using the concept of consistent
cuts. Intuitively, the agent cannot know whether a message is in the
channel or has been received.Chains of KnowledgeLet \(x, y, z\) be agents of a system and \(P\) be a predicate on
states of the system. Then the following are all predicates:\(z\) knows \(P\)\(y\) knows that \(z\) knows \(P\)\(x\) knows that \(y\) knows that \(z\) knows \(P\)The theorems given earlier apply to any predicate. For example, if
\(x\) knows that \(y\) knows that \(z\) knows \(P\) at a point \(t\) in a
trajectory, and \(\neg P\) holds at a later point \(t'\)
then there must be path in the timeline diagram from point \(t\) on
\(x\)'s timeline to point \(t'\) on \(z\)'s timeline.Concurrent Systems with Shared VariablesThe theorems
and proofs given in this module apply to systems with shared
variables, and indeed any system with trajectories that are
representable by timeline diagrams and with consistent cuts.SummaryMany people working on distributed systems use the phrase "an agent
knows." This module gives a definition of the concept that is
consistent with intuitive definitions of knowledge.
The central idea in this module is the
relationship between what agents know and consistent cuts of
timelines. We presented several theorems about agent knowledge which
are intuitive when applied to human agents. The proofs are
straightforward and are all based on consistent cuts of timelines.ReviewIs the following true? Why?(\(z\) knows \(P\)) and (\(z\) knows \(Q\)) \(\equiv\) \(z\) knows
  \(\(P \wedge Q\))Is the following true? Why?(\(z\) knows \(P\)) or (\(z\) knows \(Q\)) \(\equiv\) \(z\) knows
  \(\( P \vee Q\))Suppose you and your friend communicate only using messages sent in
  channels, just as agents do in our model of distributed
  systems. When youknowthat your friend is wearing a cap does
  that mean that (a) your friend is wearing a cap, and (b) your friend
  can't take the cap off until the friend hears from you?K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology



Figure Link: https://kmchandy.github.io/Knowledge/KnowledgeDiagrams/Slide04.jpg

Caption: Fig. 3. Illustration of Proof of Silent Agents


Because \(x\) knows \(P\) at \(T\), \(P\) holds at \(c\). Since \(c\) and \(c'\)
are identical except for the intersection with \(x\)'s timeline,
it follows that \(P\) holds \(c'\).Example>Fig. 3. Illustration of Proof of Silent AgentsConsequence of the TheoremAn agent doesn't lose knowledge by getting information from other
agents.An agent can only lose knowledge by sending information to
other agents. This seems counterintuitive; we'll look at the reasoning
underlying this in a later theorem.Theorem: Agents who don't listen remain
IgnorantAn agent that receives no information between a point \(T\) and a later
point \(T'\) learns no new knowledge between \(T\) and \(T'\).Let \(x\) be an agent in a system, and let \(P\) be a
predicate on a subsystem that does not include \(x\).
Let \(T\) and \(T'\) be points on \(x\)'s timeline with \(T < T'\). If
\(x\) knows \(P\) at point \(T'\) and \(x\) received no messages in the
interval \([T, T']\) then \(x\) knows \(P\) at \(T\).The proof has exactly the same structure as the proof of the
previous theorem.Consequence of the Theorem\(x\) didn't learn anything in the interval  \([T, T']\); everything
\(x\) knows at the later point \(T'\) is knowledge it already had at
the earlier point \(T\).The only way for an agent to gain knowledge is
to receive messages. An agent cannot learn about other agents by only
sending messages or making internal state transitions.Theorem: Knowledge implies ControlLet \(x\) and \(y\) be agents in a system, and let \(P\) be a
predicate on the states of \(y\).
Let \(T\) and \(T'\) be instants in a trajectory with \(T < T'\). If
\(x\) knows \(P\) at  \(T\), and \(\neg P\) holds at \(T'\), then there
is a path in the timeline diagram from point \(T\) on \(x\)'s timeline
to point \(T'\) on \(y\)'s timeline.Proof: If there is no path from point \(T\) on \(x\)'s timeline
to point \(T'\) on \(y\)'s timeline then there exists a consistent cut
which crosses \(x\)'s timeline at \(T\) and
crosses \(y\)'s timeline at \(T'\).ExampleIn the figure below, agent \(A\) at point \(T\) knows that agent \(C\)
holds no tokens. At a later point \(T'\) agent \(C\) holds a
token. What must happen between points \(T\) and \(T'\)?>Fig. 4. What must happen between T and T'?There must be a path in the timeline diagram from point \(T\) on agent
\(A\)'s timeline to point \(T'\) on agent \(C\)'s timeline. This path
is represented by edges that show time elapsing on a timeline and
message edges between timelines.>Fig. 5. There must be a path from A at T to C at T'?Consequence of the TheoremSuppose you and your friend communicate only by means of messages
that are delayed by arbitrary (finite) amounts. Consider a situation
where your friend knows
that you are in the library at 9 pm. Then, from our definition of
knowledge, because agents only know truth, you must be in the library
at 9 pm. Moreover, you can't leave the library until
you receive a message from your friend; this message may go through
intermediate agents.In one of the exercises we'll look at knowledge when agents have
clocks that may drift from each other but are not more than a some
constant \(M\) units apart. If your friend knows that you will be in
the library till her watch reads 9:00 pm, and your watches may drift apart
by a minute, then you can leave the library at 9:01 pm. Clocks are
useful even if they aren't perfect. More about clocks later.Theorem: Communication to learn about
ChangeThis theorem is similar to the "knowledge is control" theorem.Let \(x\) and \(y\) be agents in a system, and let \(P\) be a
predicate on the states of \(y\).
Let \(T\) and \(T'\) be instants in a trajectory with \(T < T'\). If
\(\neg P\) holds at \(T\) and \(x\) knows \(P\) at \(T'\) then there
is a path in the timeline diagram from point \(T\) on \(y\)'s timeline
to point \(T'\) on \(x\)'s timeline.ExampleIn the figure below, agent \(C\) holds a token at point \(T\).
At a later point \(T'\) agent \(A\) knows that agent \(C\) holds no
tokens. What must happen between points \(T\) and \(T'\)?>Fig. 6. What must happen between T and T'?There must be a path in the timeline diagram from point \(T\) on agent
\(C\)'s timeline to point \(T'\) on agent \(A\)'s timeline. This path
is represented by edges that show time elapsing on a timeline and
message edges between timelines.>Fig. 7. There must be a path from C at T to A at T'?What Agents Know about Channel StatesNext, let's look at systems in which messages are acknowledged.
For a pair of agents \(x, y\), let \(ms\) and \(mr\)
be the number of messages that \(x\) has sent to \(y\), and  the number 
of messages that \(y\) has received from \(x\), respectively.
Let \(as\) and \(ar\) be
the number of acknowledgements that \(y\) has sent to \(x\), and
the number of acknowledgements that \(x\) has received from \(y\),
respectively.
The following is an invariant:\(ms \geq mr \geq as \geq ar \)The number of messages in the channel from \(x\) to \(y\) is \(ms -
mr\).
Because \(ms\) and \(ar\) are variables of agent \(x\),
agent \(x\)knowsan upper bound,\(\; ms - ar\), on the number of
messages in channel \((x, y)\).
So, \(x\) knows that the channel is empty when \(\; ms = ar\).What an agent cannot knowAn agent cannot know that there are
exactly \(n\) messages in a channel, for \(n > 0\).
You can prove this result using the concept of consistent
cuts. Intuitively, the agent cannot know whether a message is in the
channel or has been received.Chains of KnowledgeLet \(x, y, z\) be agents of a system and \(P\) be a predicate on
states of the system. Then the following are all predicates:\(z\) knows \(P\)\(y\) knows that \(z\) knows \(P\)\(x\) knows that \(y\) knows that \(z\) knows \(P\)The theorems given earlier apply to any predicate. For example, if
\(x\) knows that \(y\) knows that \(z\) knows \(P\) at a point \(t\) in a
trajectory, and \(\neg P\) holds at a later point \(t'\)
then there must be path in the timeline diagram from point \(t\) on
\(x\)'s timeline to point \(t'\) on \(z\)'s timeline.Concurrent Systems with Shared VariablesThe theorems
and proofs given in this module apply to systems with shared
variables, and indeed any system with trajectories that are
representable by timeline diagrams and with consistent cuts.SummaryMany people working on distributed systems use the phrase "an agent
knows." This module gives a definition of the concept that is
consistent with intuitive definitions of knowledge.
The central idea in this module is the
relationship between what agents know and consistent cuts of
timelines. We presented several theorems about agent knowledge which
are intuitive when applied to human agents. The proofs are
straightforward and are all based on consistent cuts of timelines.ReviewIs the following true? Why?(\(z\) knows \(P\)) and (\(z\) knows \(Q\)) \(\equiv\) \(z\) knows
  \(\(P \wedge Q\))Is the following true? Why?(\(z\) knows \(P\)) or (\(z\) knows \(Q\)) \(\equiv\) \(z\) knows
  \(\( P \vee Q\))Suppose you and your friend communicate only using messages sent in
  channels, just as agents do in our model of distributed
  systems. When youknowthat your friend is wearing a cap does
  that mean that (a) your friend is wearing a cap, and (b) your friend
  can't take the cap off until the friend hears from you?K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology



Figure Link: https://kmchandy.github.io/Knowledge/KnowledgeDiagrams/Slide04.jpg

Caption: Fig. 3. Illustration of Proof of Silent Agents


An agent can only lose knowledge by sending information to
other agents. This seems counterintuitive; we'll look at the reasoning
underlying this in a later theorem.Theorem: Agents who don't listen remain
IgnorantAn agent that receives no information between a point \(T\) and a later
point \(T'\) learns no new knowledge between \(T\) and \(T'\).Let \(x\) be an agent in a system, and let \(P\) be a
predicate on a subsystem that does not include \(x\).
Let \(T\) and \(T'\) be points on \(x\)'s timeline with \(T < T'\). If
\(x\) knows \(P\) at point \(T'\) and \(x\) received no messages in the
interval \([T, T']\) then \(x\) knows \(P\) at \(T\).The proof has exactly the same structure as the proof of the
previous theorem.Consequence of the Theorem\(x\) didn't learn anything in the interval  \([T, T']\); everything
\(x\) knows at the later point \(T'\) is knowledge it already had at
the earlier point \(T\).The only way for an agent to gain knowledge is
to receive messages. An agent cannot learn about other agents by only
sending messages or making internal state transitions.Theorem: Knowledge implies ControlLet \(x\) and \(y\) be agents in a system, and let \(P\) be a
predicate on the states of \(y\).
Let \(T\) and \(T'\) be instants in a trajectory with \(T < T'\). If
\(x\) knows \(P\) at  \(T\), and \(\neg P\) holds at \(T'\), then there
is a path in the timeline diagram from point \(T\) on \(x\)'s timeline
to point \(T'\) on \(y\)'s timeline.Proof: If there is no path from point \(T\) on \(x\)'s timeline
to point \(T'\) on \(y\)'s timeline then there exists a consistent cut
which crosses \(x\)'s timeline at \(T\) and
crosses \(y\)'s timeline at \(T'\).ExampleIn the figure below, agent \(A\) at point \(T\) knows that agent \(C\)
holds no tokens. At a later point \(T'\) agent \(C\) holds a
token. What must happen between points \(T\) and \(T'\)?>Fig. 4. What must happen between T and T'?There must be a path in the timeline diagram from point \(T\) on agent
\(A\)'s timeline to point \(T'\) on agent \(C\)'s timeline. This path
is represented by edges that show time elapsing on a timeline and
message edges between timelines.>Fig. 5. There must be a path from A at T to C at T'?Consequence of the TheoremSuppose you and your friend communicate only by means of messages
that are delayed by arbitrary (finite) amounts. Consider a situation
where your friend knows
that you are in the library at 9 pm. Then, from our definition of
knowledge, because agents only know truth, you must be in the library
at 9 pm. Moreover, you can't leave the library until
you receive a message from your friend; this message may go through
intermediate agents.In one of the exercises we'll look at knowledge when agents have
clocks that may drift from each other but are not more than a some
constant \(M\) units apart. If your friend knows that you will be in
the library till her watch reads 9:00 pm, and your watches may drift apart
by a minute, then you can leave the library at 9:01 pm. Clocks are
useful even if they aren't perfect. More about clocks later.Theorem: Communication to learn about
ChangeThis theorem is similar to the "knowledge is control" theorem.Let \(x\) and \(y\) be agents in a system, and let \(P\) be a
predicate on the states of \(y\).
Let \(T\) and \(T'\) be instants in a trajectory with \(T < T'\). If
\(\neg P\) holds at \(T\) and \(x\) knows \(P\) at \(T'\) then there
is a path in the timeline diagram from point \(T\) on \(y\)'s timeline
to point \(T'\) on \(x\)'s timeline.ExampleIn the figure below, agent \(C\) holds a token at point \(T\).
At a later point \(T'\) agent \(A\) knows that agent \(C\) holds no
tokens. What must happen between points \(T\) and \(T'\)?>Fig. 6. What must happen between T and T'?There must be a path in the timeline diagram from point \(T\) on agent
\(C\)'s timeline to point \(T'\) on agent \(A\)'s timeline. This path
is represented by edges that show time elapsing on a timeline and
message edges between timelines.>Fig. 7. There must be a path from C at T to A at T'?What Agents Know about Channel StatesNext, let's look at systems in which messages are acknowledged.
For a pair of agents \(x, y\), let \(ms\) and \(mr\)
be the number of messages that \(x\) has sent to \(y\), and  the number 
of messages that \(y\) has received from \(x\), respectively.
Let \(as\) and \(ar\) be
the number of acknowledgements that \(y\) has sent to \(x\), and
the number of acknowledgements that \(x\) has received from \(y\),
respectively.
The following is an invariant:\(ms \geq mr \geq as \geq ar \)The number of messages in the channel from \(x\) to \(y\) is \(ms -
mr\).
Because \(ms\) and \(ar\) are variables of agent \(x\),
agent \(x\)knowsan upper bound,\(\; ms - ar\), on the number of
messages in channel \((x, y)\).
So, \(x\) knows that the channel is empty when \(\; ms = ar\).What an agent cannot knowAn agent cannot know that there are
exactly \(n\) messages in a channel, for \(n > 0\).
You can prove this result using the concept of consistent
cuts. Intuitively, the agent cannot know whether a message is in the
channel or has been received.Chains of KnowledgeLet \(x, y, z\) be agents of a system and \(P\) be a predicate on
states of the system. Then the following are all predicates:\(z\) knows \(P\)\(y\) knows that \(z\) knows \(P\)\(x\) knows that \(y\) knows that \(z\) knows \(P\)The theorems given earlier apply to any predicate. For example, if
\(x\) knows that \(y\) knows that \(z\) knows \(P\) at a point \(t\) in a
trajectory, and \(\neg P\) holds at a later point \(t'\)
then there must be path in the timeline diagram from point \(t\) on
\(x\)'s timeline to point \(t'\) on \(z\)'s timeline.Concurrent Systems with Shared VariablesThe theorems
and proofs given in this module apply to systems with shared
variables, and indeed any system with trajectories that are
representable by timeline diagrams and with consistent cuts.SummaryMany people working on distributed systems use the phrase "an agent
knows." This module gives a definition of the concept that is
consistent with intuitive definitions of knowledge.
The central idea in this module is the
relationship between what agents know and consistent cuts of
timelines. We presented several theorems about agent knowledge which
are intuitive when applied to human agents. The proofs are
straightforward and are all based on consistent cuts of timelines.ReviewIs the following true? Why?(\(z\) knows \(P\)) and (\(z\) knows \(Q\)) \(\equiv\) \(z\) knows
  \(\(P \wedge Q\))Is the following true? Why?(\(z\) knows \(P\)) or (\(z\) knows \(Q\)) \(\equiv\) \(z\) knows
  \(\( P \vee Q\))Suppose you and your friend communicate only using messages sent in
  channels, just as agents do in our model of distributed
  systems. When youknowthat your friend is wearing a cap does
  that mean that (a) your friend is wearing a cap, and (b) your friend
  can't take the cap off until the friend hears from you?K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology



Figure Link: https://kmchandy.github.io/Knowledge/KnowledgeDiagrams/Slide08.jpg

Caption: Fig. 4. What must happen between T and T'?


The proof has exactly the same structure as the proof of the
previous theorem.Consequence of the Theorem\(x\) didn't learn anything in the interval  \([T, T']\); everything
\(x\) knows at the later point \(T'\) is knowledge it already had at
the earlier point \(T\).The only way for an agent to gain knowledge is
to receive messages. An agent cannot learn about other agents by only
sending messages or making internal state transitions.Theorem: Knowledge implies ControlLet \(x\) and \(y\) be agents in a system, and let \(P\) be a
predicate on the states of \(y\).
Let \(T\) and \(T'\) be instants in a trajectory with \(T < T'\). If
\(x\) knows \(P\) at  \(T\), and \(\neg P\) holds at \(T'\), then there
is a path in the timeline diagram from point \(T\) on \(x\)'s timeline
to point \(T'\) on \(y\)'s timeline.Proof: If there is no path from point \(T\) on \(x\)'s timeline
to point \(T'\) on \(y\)'s timeline then there exists a consistent cut
which crosses \(x\)'s timeline at \(T\) and
crosses \(y\)'s timeline at \(T'\).ExampleIn the figure below, agent \(A\) at point \(T\) knows that agent \(C\)
holds no tokens. At a later point \(T'\) agent \(C\) holds a
token. What must happen between points \(T\) and \(T'\)?>Fig. 4. What must happen between T and T'?There must be a path in the timeline diagram from point \(T\) on agent
\(A\)'s timeline to point \(T'\) on agent \(C\)'s timeline. This path
is represented by edges that show time elapsing on a timeline and
message edges between timelines.>Fig. 5. There must be a path from A at T to C at T'?Consequence of the TheoremSuppose you and your friend communicate only by means of messages
that are delayed by arbitrary (finite) amounts. Consider a situation
where your friend knows
that you are in the library at 9 pm. Then, from our definition of
knowledge, because agents only know truth, you must be in the library
at 9 pm. Moreover, you can't leave the library until
you receive a message from your friend; this message may go through
intermediate agents.In one of the exercises we'll look at knowledge when agents have
clocks that may drift from each other but are not more than a some
constant \(M\) units apart. If your friend knows that you will be in
the library till her watch reads 9:00 pm, and your watches may drift apart
by a minute, then you can leave the library at 9:01 pm. Clocks are
useful even if they aren't perfect. More about clocks later.Theorem: Communication to learn about
ChangeThis theorem is similar to the "knowledge is control" theorem.Let \(x\) and \(y\) be agents in a system, and let \(P\) be a
predicate on the states of \(y\).
Let \(T\) and \(T'\) be instants in a trajectory with \(T < T'\). If
\(\neg P\) holds at \(T\) and \(x\) knows \(P\) at \(T'\) then there
is a path in the timeline diagram from point \(T\) on \(y\)'s timeline
to point \(T'\) on \(x\)'s timeline.ExampleIn the figure below, agent \(C\) holds a token at point \(T\).
At a later point \(T'\) agent \(A\) knows that agent \(C\) holds no
tokens. What must happen between points \(T\) and \(T'\)?>Fig. 6. What must happen between T and T'?There must be a path in the timeline diagram from point \(T\) on agent
\(C\)'s timeline to point \(T'\) on agent \(A\)'s timeline. This path
is represented by edges that show time elapsing on a timeline and
message edges between timelines.>Fig. 7. There must be a path from C at T to A at T'?What Agents Know about Channel StatesNext, let's look at systems in which messages are acknowledged.
For a pair of agents \(x, y\), let \(ms\) and \(mr\)
be the number of messages that \(x\) has sent to \(y\), and  the number 
of messages that \(y\) has received from \(x\), respectively.
Let \(as\) and \(ar\) be
the number of acknowledgements that \(y\) has sent to \(x\), and
the number of acknowledgements that \(x\) has received from \(y\),
respectively.
The following is an invariant:\(ms \geq mr \geq as \geq ar \)The number of messages in the channel from \(x\) to \(y\) is \(ms -
mr\).
Because \(ms\) and \(ar\) are variables of agent \(x\),
agent \(x\)knowsan upper bound,\(\; ms - ar\), on the number of
messages in channel \((x, y)\).
So, \(x\) knows that the channel is empty when \(\; ms = ar\).What an agent cannot knowAn agent cannot know that there are
exactly \(n\) messages in a channel, for \(n > 0\).
You can prove this result using the concept of consistent
cuts. Intuitively, the agent cannot know whether a message is in the
channel or has been received.Chains of KnowledgeLet \(x, y, z\) be agents of a system and \(P\) be a predicate on
states of the system. Then the following are all predicates:\(z\) knows \(P\)\(y\) knows that \(z\) knows \(P\)\(x\) knows that \(y\) knows that \(z\) knows \(P\)The theorems given earlier apply to any predicate. For example, if
\(x\) knows that \(y\) knows that \(z\) knows \(P\) at a point \(t\) in a
trajectory, and \(\neg P\) holds at a later point \(t'\)
then there must be path in the timeline diagram from point \(t\) on
\(x\)'s timeline to point \(t'\) on \(z\)'s timeline.Concurrent Systems with Shared VariablesThe theorems
and proofs given in this module apply to systems with shared
variables, and indeed any system with trajectories that are
representable by timeline diagrams and with consistent cuts.SummaryMany people working on distributed systems use the phrase "an agent
knows." This module gives a definition of the concept that is
consistent with intuitive definitions of knowledge.
The central idea in this module is the
relationship between what agents know and consistent cuts of
timelines. We presented several theorems about agent knowledge which
are intuitive when applied to human agents. The proofs are
straightforward and are all based on consistent cuts of timelines.ReviewIs the following true? Why?(\(z\) knows \(P\)) and (\(z\) knows \(Q\)) \(\equiv\) \(z\) knows
  \(\(P \wedge Q\))Is the following true? Why?(\(z\) knows \(P\)) or (\(z\) knows \(Q\)) \(\equiv\) \(z\) knows
  \(\( P \vee Q\))Suppose you and your friend communicate only using messages sent in
  channels, just as agents do in our model of distributed
  systems. When youknowthat your friend is wearing a cap does
  that mean that (a) your friend is wearing a cap, and (b) your friend
  can't take the cap off until the friend hears from you?K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology



Figure Link: https://kmchandy.github.io/Knowledge/KnowledgeDiagrams/Slide08.jpg

Caption: Fig. 4. What must happen between T and T'?


The only way for an agent to gain knowledge is
to receive messages. An agent cannot learn about other agents by only
sending messages or making internal state transitions.Theorem: Knowledge implies ControlLet \(x\) and \(y\) be agents in a system, and let \(P\) be a
predicate on the states of \(y\).
Let \(T\) and \(T'\) be instants in a trajectory with \(T < T'\). If
\(x\) knows \(P\) at  \(T\), and \(\neg P\) holds at \(T'\), then there
is a path in the timeline diagram from point \(T\) on \(x\)'s timeline
to point \(T'\) on \(y\)'s timeline.Proof: If there is no path from point \(T\) on \(x\)'s timeline
to point \(T'\) on \(y\)'s timeline then there exists a consistent cut
which crosses \(x\)'s timeline at \(T\) and
crosses \(y\)'s timeline at \(T'\).ExampleIn the figure below, agent \(A\) at point \(T\) knows that agent \(C\)
holds no tokens. At a later point \(T'\) agent \(C\) holds a
token. What must happen between points \(T\) and \(T'\)?>Fig. 4. What must happen between T and T'?There must be a path in the timeline diagram from point \(T\) on agent
\(A\)'s timeline to point \(T'\) on agent \(C\)'s timeline. This path
is represented by edges that show time elapsing on a timeline and
message edges between timelines.>Fig. 5. There must be a path from A at T to C at T'?Consequence of the TheoremSuppose you and your friend communicate only by means of messages
that are delayed by arbitrary (finite) amounts. Consider a situation
where your friend knows
that you are in the library at 9 pm. Then, from our definition of
knowledge, because agents only know truth, you must be in the library
at 9 pm. Moreover, you can't leave the library until
you receive a message from your friend; this message may go through
intermediate agents.In one of the exercises we'll look at knowledge when agents have
clocks that may drift from each other but are not more than a some
constant \(M\) units apart. If your friend knows that you will be in
the library till her watch reads 9:00 pm, and your watches may drift apart
by a minute, then you can leave the library at 9:01 pm. Clocks are
useful even if they aren't perfect. More about clocks later.Theorem: Communication to learn about
ChangeThis theorem is similar to the "knowledge is control" theorem.Let \(x\) and \(y\) be agents in a system, and let \(P\) be a
predicate on the states of \(y\).
Let \(T\) and \(T'\) be instants in a trajectory with \(T < T'\). If
\(\neg P\) holds at \(T\) and \(x\) knows \(P\) at \(T'\) then there
is a path in the timeline diagram from point \(T\) on \(y\)'s timeline
to point \(T'\) on \(x\)'s timeline.ExampleIn the figure below, agent \(C\) holds a token at point \(T\).
At a later point \(T'\) agent \(A\) knows that agent \(C\) holds no
tokens. What must happen between points \(T\) and \(T'\)?>Fig. 6. What must happen between T and T'?There must be a path in the timeline diagram from point \(T\) on agent
\(C\)'s timeline to point \(T'\) on agent \(A\)'s timeline. This path
is represented by edges that show time elapsing on a timeline and
message edges between timelines.>Fig. 7. There must be a path from C at T to A at T'?What Agents Know about Channel StatesNext, let's look at systems in which messages are acknowledged.
For a pair of agents \(x, y\), let \(ms\) and \(mr\)
be the number of messages that \(x\) has sent to \(y\), and  the number 
of messages that \(y\) has received from \(x\), respectively.
Let \(as\) and \(ar\) be
the number of acknowledgements that \(y\) has sent to \(x\), and
the number of acknowledgements that \(x\) has received from \(y\),
respectively.
The following is an invariant:\(ms \geq mr \geq as \geq ar \)The number of messages in the channel from \(x\) to \(y\) is \(ms -
mr\).
Because \(ms\) and \(ar\) are variables of agent \(x\),
agent \(x\)knowsan upper bound,\(\; ms - ar\), on the number of
messages in channel \((x, y)\).
So, \(x\) knows that the channel is empty when \(\; ms = ar\).What an agent cannot knowAn agent cannot know that there are
exactly \(n\) messages in a channel, for \(n > 0\).
You can prove this result using the concept of consistent
cuts. Intuitively, the agent cannot know whether a message is in the
channel or has been received.Chains of KnowledgeLet \(x, y, z\) be agents of a system and \(P\) be a predicate on
states of the system. Then the following are all predicates:\(z\) knows \(P\)\(y\) knows that \(z\) knows \(P\)\(x\) knows that \(y\) knows that \(z\) knows \(P\)The theorems given earlier apply to any predicate. For example, if
\(x\) knows that \(y\) knows that \(z\) knows \(P\) at a point \(t\) in a
trajectory, and \(\neg P\) holds at a later point \(t'\)
then there must be path in the timeline diagram from point \(t\) on
\(x\)'s timeline to point \(t'\) on \(z\)'s timeline.Concurrent Systems with Shared VariablesThe theorems
and proofs given in this module apply to systems with shared
variables, and indeed any system with trajectories that are
representable by timeline diagrams and with consistent cuts.SummaryMany people working on distributed systems use the phrase "an agent
knows." This module gives a definition of the concept that is
consistent with intuitive definitions of knowledge.
The central idea in this module is the
relationship between what agents know and consistent cuts of
timelines. We presented several theorems about agent knowledge which
are intuitive when applied to human agents. The proofs are
straightforward and are all based on consistent cuts of timelines.ReviewIs the following true? Why?(\(z\) knows \(P\)) and (\(z\) knows \(Q\)) \(\equiv\) \(z\) knows
  \(\(P \wedge Q\))Is the following true? Why?(\(z\) knows \(P\)) or (\(z\) knows \(Q\)) \(\equiv\) \(z\) knows
  \(\( P \vee Q\))Suppose you and your friend communicate only using messages sent in
  channels, just as agents do in our model of distributed
  systems. When youknowthat your friend is wearing a cap does
  that mean that (a) your friend is wearing a cap, and (b) your friend
  can't take the cap off until the friend hears from you?K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology



Figure Link: https://kmchandy.github.io/Knowledge/KnowledgeDiagrams/Slide08.jpg

Caption: Fig. 4. What must happen between T and T'?


Proof: If there is no path from point \(T\) on \(x\)'s timeline
to point \(T'\) on \(y\)'s timeline then there exists a consistent cut
which crosses \(x\)'s timeline at \(T\) and
crosses \(y\)'s timeline at \(T'\).ExampleIn the figure below, agent \(A\) at point \(T\) knows that agent \(C\)
holds no tokens. At a later point \(T'\) agent \(C\) holds a
token. What must happen between points \(T\) and \(T'\)?>Fig. 4. What must happen between T and T'?There must be a path in the timeline diagram from point \(T\) on agent
\(A\)'s timeline to point \(T'\) on agent \(C\)'s timeline. This path
is represented by edges that show time elapsing on a timeline and
message edges between timelines.>Fig. 5. There must be a path from A at T to C at T'?Consequence of the TheoremSuppose you and your friend communicate only by means of messages
that are delayed by arbitrary (finite) amounts. Consider a situation
where your friend knows
that you are in the library at 9 pm. Then, from our definition of
knowledge, because agents only know truth, you must be in the library
at 9 pm. Moreover, you can't leave the library until
you receive a message from your friend; this message may go through
intermediate agents.In one of the exercises we'll look at knowledge when agents have
clocks that may drift from each other but are not more than a some
constant \(M\) units apart. If your friend knows that you will be in
the library till her watch reads 9:00 pm, and your watches may drift apart
by a minute, then you can leave the library at 9:01 pm. Clocks are
useful even if they aren't perfect. More about clocks later.Theorem: Communication to learn about
ChangeThis theorem is similar to the "knowledge is control" theorem.Let \(x\) and \(y\) be agents in a system, and let \(P\) be a
predicate on the states of \(y\).
Let \(T\) and \(T'\) be instants in a trajectory with \(T < T'\). If
\(\neg P\) holds at \(T\) and \(x\) knows \(P\) at \(T'\) then there
is a path in the timeline diagram from point \(T\) on \(y\)'s timeline
to point \(T'\) on \(x\)'s timeline.ExampleIn the figure below, agent \(C\) holds a token at point \(T\).
At a later point \(T'\) agent \(A\) knows that agent \(C\) holds no
tokens. What must happen between points \(T\) and \(T'\)?>Fig. 6. What must happen between T and T'?There must be a path in the timeline diagram from point \(T\) on agent
\(C\)'s timeline to point \(T'\) on agent \(A\)'s timeline. This path
is represented by edges that show time elapsing on a timeline and
message edges between timelines.>Fig. 7. There must be a path from C at T to A at T'?What Agents Know about Channel StatesNext, let's look at systems in which messages are acknowledged.
For a pair of agents \(x, y\), let \(ms\) and \(mr\)
be the number of messages that \(x\) has sent to \(y\), and  the number 
of messages that \(y\) has received from \(x\), respectively.
Let \(as\) and \(ar\) be
the number of acknowledgements that \(y\) has sent to \(x\), and
the number of acknowledgements that \(x\) has received from \(y\),
respectively.
The following is an invariant:\(ms \geq mr \geq as \geq ar \)The number of messages in the channel from \(x\) to \(y\) is \(ms -
mr\).
Because \(ms\) and \(ar\) are variables of agent \(x\),
agent \(x\)knowsan upper bound,\(\; ms - ar\), on the number of
messages in channel \((x, y)\).
So, \(x\) knows that the channel is empty when \(\; ms = ar\).What an agent cannot knowAn agent cannot know that there are
exactly \(n\) messages in a channel, for \(n > 0\).
You can prove this result using the concept of consistent
cuts. Intuitively, the agent cannot know whether a message is in the
channel or has been received.Chains of KnowledgeLet \(x, y, z\) be agents of a system and \(P\) be a predicate on
states of the system. Then the following are all predicates:\(z\) knows \(P\)\(y\) knows that \(z\) knows \(P\)\(x\) knows that \(y\) knows that \(z\) knows \(P\)The theorems given earlier apply to any predicate. For example, if
\(x\) knows that \(y\) knows that \(z\) knows \(P\) at a point \(t\) in a
trajectory, and \(\neg P\) holds at a later point \(t'\)
then there must be path in the timeline diagram from point \(t\) on
\(x\)'s timeline to point \(t'\) on \(z\)'s timeline.Concurrent Systems with Shared VariablesThe theorems
and proofs given in this module apply to systems with shared
variables, and indeed any system with trajectories that are
representable by timeline diagrams and with consistent cuts.SummaryMany people working on distributed systems use the phrase "an agent
knows." This module gives a definition of the concept that is
consistent with intuitive definitions of knowledge.
The central idea in this module is the
relationship between what agents know and consistent cuts of
timelines. We presented several theorems about agent knowledge which
are intuitive when applied to human agents. The proofs are
straightforward and are all based on consistent cuts of timelines.ReviewIs the following true? Why?(\(z\) knows \(P\)) and (\(z\) knows \(Q\)) \(\equiv\) \(z\) knows
  \(\(P \wedge Q\))Is the following true? Why?(\(z\) knows \(P\)) or (\(z\) knows \(Q\)) \(\equiv\) \(z\) knows
  \(\( P \vee Q\))Suppose you and your friend communicate only using messages sent in
  channels, just as agents do in our model of distributed
  systems. When youknowthat your friend is wearing a cap does
  that mean that (a) your friend is wearing a cap, and (b) your friend
  can't take the cap off until the friend hears from you?K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology



Figure Link: https://kmchandy.github.io/Knowledge/KnowledgeDiagrams/Slide08.jpg

Caption: Fig. 4. What must happen between T and T'?


In one of the exercises we'll look at knowledge when agents have
clocks that may drift from each other but are not more than a some
constant \(M\) units apart. If your friend knows that you will be in
the library till her watch reads 9:00 pm, and your watches may drift apart
by a minute, then you can leave the library at 9:01 pm. Clocks are
useful even if they aren't perfect. More about clocks later.Theorem: Communication to learn about
ChangeThis theorem is similar to the "knowledge is control" theorem.Let \(x\) and \(y\) be agents in a system, and let \(P\) be a
predicate on the states of \(y\).
Let \(T\) and \(T'\) be instants in a trajectory with \(T < T'\). If
\(\neg P\) holds at \(T\) and \(x\) knows \(P\) at \(T'\) then there
is a path in the timeline diagram from point \(T\) on \(y\)'s timeline
to point \(T'\) on \(x\)'s timeline.ExampleIn the figure below, agent \(C\) holds a token at point \(T\).
At a later point \(T'\) agent \(A\) knows that agent \(C\) holds no
tokens. What must happen between points \(T\) and \(T'\)?>Fig. 6. What must happen between T and T'?There must be a path in the timeline diagram from point \(T\) on agent
\(C\)'s timeline to point \(T'\) on agent \(A\)'s timeline. This path
is represented by edges that show time elapsing on a timeline and
message edges between timelines.>Fig. 7. There must be a path from C at T to A at T'?What Agents Know about Channel StatesNext, let's look at systems in which messages are acknowledged.
For a pair of agents \(x, y\), let \(ms\) and \(mr\)
be the number of messages that \(x\) has sent to \(y\), and  the number 
of messages that \(y\) has received from \(x\), respectively.
Let \(as\) and \(ar\) be
the number of acknowledgements that \(y\) has sent to \(x\), and
the number of acknowledgements that \(x\) has received from \(y\),
respectively.
The following is an invariant:\(ms \geq mr \geq as \geq ar \)The number of messages in the channel from \(x\) to \(y\) is \(ms -
mr\).
Because \(ms\) and \(ar\) are variables of agent \(x\),
agent \(x\)knowsan upper bound,\(\; ms - ar\), on the number of
messages in channel \((x, y)\).
So, \(x\) knows that the channel is empty when \(\; ms = ar\).What an agent cannot knowAn agent cannot know that there are
exactly \(n\) messages in a channel, for \(n > 0\).
You can prove this result using the concept of consistent
cuts. Intuitively, the agent cannot know whether a message is in the
channel or has been received.Chains of KnowledgeLet \(x, y, z\) be agents of a system and \(P\) be a predicate on
states of the system. Then the following are all predicates:\(z\) knows \(P\)\(y\) knows that \(z\) knows \(P\)\(x\) knows that \(y\) knows that \(z\) knows \(P\)The theorems given earlier apply to any predicate. For example, if
\(x\) knows that \(y\) knows that \(z\) knows \(P\) at a point \(t\) in a
trajectory, and \(\neg P\) holds at a later point \(t'\)
then there must be path in the timeline diagram from point \(t\) on
\(x\)'s timeline to point \(t'\) on \(z\)'s timeline.Concurrent Systems with Shared VariablesThe theorems
and proofs given in this module apply to systems with shared
variables, and indeed any system with trajectories that are
representable by timeline diagrams and with consistent cuts.SummaryMany people working on distributed systems use the phrase "an agent
knows." This module gives a definition of the concept that is
consistent with intuitive definitions of knowledge.
The central idea in this module is the
relationship between what agents know and consistent cuts of
timelines. We presented several theorems about agent knowledge which
are intuitive when applied to human agents. The proofs are
straightforward and are all based on consistent cuts of timelines.ReviewIs the following true? Why?(\(z\) knows \(P\)) and (\(z\) knows \(Q\)) \(\equiv\) \(z\) knows
  \(\(P \wedge Q\))Is the following true? Why?(\(z\) knows \(P\)) or (\(z\) knows \(Q\)) \(\equiv\) \(z\) knows
  \(\( P \vee Q\))Suppose you and your friend communicate only using messages sent in
  channels, just as agents do in our model of distributed
  systems. When youknowthat your friend is wearing a cap does
  that mean that (a) your friend is wearing a cap, and (b) your friend
  can't take the cap off until the friend hears from you?K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology



Figure Link: https://kmchandy.github.io/Knowledge/KnowledgeDiagrams/Slide10.jpg

Caption: Fig. 6. What must happen between T and T'?


Let \(x\) and \(y\) be agents in a system, and let \(P\) be a
predicate on the states of \(y\).
Let \(T\) and \(T'\) be instants in a trajectory with \(T < T'\). If
\(\neg P\) holds at \(T\) and \(x\) knows \(P\) at \(T'\) then there
is a path in the timeline diagram from point \(T\) on \(y\)'s timeline
to point \(T'\) on \(x\)'s timeline.ExampleIn the figure below, agent \(C\) holds a token at point \(T\).
At a later point \(T'\) agent \(A\) knows that agent \(C\) holds no
tokens. What must happen between points \(T\) and \(T'\)?>Fig. 6. What must happen between T and T'?There must be a path in the timeline diagram from point \(T\) on agent
\(C\)'s timeline to point \(T'\) on agent \(A\)'s timeline. This path
is represented by edges that show time elapsing on a timeline and
message edges between timelines.>Fig. 7. There must be a path from C at T to A at T'?What Agents Know about Channel StatesNext, let's look at systems in which messages are acknowledged.
For a pair of agents \(x, y\), let \(ms\) and \(mr\)
be the number of messages that \(x\) has sent to \(y\), and  the number 
of messages that \(y\) has received from \(x\), respectively.
Let \(as\) and \(ar\) be
the number of acknowledgements that \(y\) has sent to \(x\), and
the number of acknowledgements that \(x\) has received from \(y\),
respectively.
The following is an invariant:\(ms \geq mr \geq as \geq ar \)The number of messages in the channel from \(x\) to \(y\) is \(ms -
mr\).
Because \(ms\) and \(ar\) are variables of agent \(x\),
agent \(x\)knowsan upper bound,\(\; ms - ar\), on the number of
messages in channel \((x, y)\).
So, \(x\) knows that the channel is empty when \(\; ms = ar\).What an agent cannot knowAn agent cannot know that there are
exactly \(n\) messages in a channel, for \(n > 0\).
You can prove this result using the concept of consistent
cuts. Intuitively, the agent cannot know whether a message is in the
channel or has been received.Chains of KnowledgeLet \(x, y, z\) be agents of a system and \(P\) be a predicate on
states of the system. Then the following are all predicates:\(z\) knows \(P\)\(y\) knows that \(z\) knows \(P\)\(x\) knows that \(y\) knows that \(z\) knows \(P\)The theorems given earlier apply to any predicate. For example, if
\(x\) knows that \(y\) knows that \(z\) knows \(P\) at a point \(t\) in a
trajectory, and \(\neg P\) holds at a later point \(t'\)
then there must be path in the timeline diagram from point \(t\) on
\(x\)'s timeline to point \(t'\) on \(z\)'s timeline.Concurrent Systems with Shared VariablesThe theorems
and proofs given in this module apply to systems with shared
variables, and indeed any system with trajectories that are
representable by timeline diagrams and with consistent cuts.SummaryMany people working on distributed systems use the phrase "an agent
knows." This module gives a definition of the concept that is
consistent with intuitive definitions of knowledge.
The central idea in this module is the
relationship between what agents know and consistent cuts of
timelines. We presented several theorems about agent knowledge which
are intuitive when applied to human agents. The proofs are
straightforward and are all based on consistent cuts of timelines.ReviewIs the following true? Why?(\(z\) knows \(P\)) and (\(z\) knows \(Q\)) \(\equiv\) \(z\) knows
  \(\(P \wedge Q\))Is the following true? Why?(\(z\) knows \(P\)) or (\(z\) knows \(Q\)) \(\equiv\) \(z\) knows
  \(\( P \vee Q\))Suppose you and your friend communicate only using messages sent in
  channels, just as agents do in our model of distributed
  systems. When youknowthat your friend is wearing a cap does
  that mean that (a) your friend is wearing a cap, and (b) your friend
  can't take the cap off until the friend hears from you?K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology



Figure Link: https://kmchandy.github.io/Knowledge/KnowledgeDiagrams/Slide10.jpg

Caption: Fig. 6. What must happen between T and T'?


\(ms \geq mr \geq as \geq ar \)The number of messages in the channel from \(x\) to \(y\) is \(ms -
mr\).
Because \(ms\) and \(ar\) are variables of agent \(x\),
agent \(x\)knowsan upper bound,\(\; ms - ar\), on the number of
messages in channel \((x, y)\).
So, \(x\) knows that the channel is empty when \(\; ms = ar\).What an agent cannot knowAn agent cannot know that there are
exactly \(n\) messages in a channel, for \(n > 0\).
You can prove this result using the concept of consistent
cuts. Intuitively, the agent cannot know whether a message is in the
channel or has been received.Chains of KnowledgeLet \(x, y, z\) be agents of a system and \(P\) be a predicate on
states of the system. Then the following are all predicates:\(z\) knows \(P\)\(y\) knows that \(z\) knows \(P\)\(x\) knows that \(y\) knows that \(z\) knows \(P\)The theorems given earlier apply to any predicate. For example, if
\(x\) knows that \(y\) knows that \(z\) knows \(P\) at a point \(t\) in a
trajectory, and \(\neg P\) holds at a later point \(t'\)
then there must be path in the timeline diagram from point \(t\) on
\(x\)'s timeline to point \(t'\) on \(z\)'s timeline.Concurrent Systems with Shared VariablesThe theorems
and proofs given in this module apply to systems with shared
variables, and indeed any system with trajectories that are
representable by timeline diagrams and with consistent cuts.SummaryMany people working on distributed systems use the phrase "an agent
knows." This module gives a definition of the concept that is
consistent with intuitive definitions of knowledge.
The central idea in this module is the
relationship between what agents know and consistent cuts of
timelines. We presented several theorems about agent knowledge which
are intuitive when applied to human agents. The proofs are
straightforward and are all based on consistent cuts of timelines.ReviewIs the following true? Why?(\(z\) knows \(P\)) and (\(z\) knows \(Q\)) \(\equiv\) \(z\) knows
  \(\(P \wedge Q\))Is the following true? Why?(\(z\) knows \(P\)) or (\(z\) knows \(Q\)) \(\equiv\) \(z\) knows
  \(\( P \vee Q\))Suppose you and your friend communicate only using messages sent in
  channels, just as agents do in our model of distributed
  systems. When youknowthat your friend is wearing a cap does
  that mean that (a) your friend is wearing a cap, and (b) your friend
  can't take the cap off until the friend hears from you?K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

The number of messages in the channel from \(x\) to \(y\) is \(ms -
mr\).
Because \(ms\) and \(ar\) are variables of agent \(x\),
agent \(x\)knowsan upper bound,\(\; ms - ar\), on the number of
messages in channel \((x, y)\).
So, \(x\) knows that the channel is empty when \(\; ms = ar\).What an agent cannot knowAn agent cannot know that there are
exactly \(n\) messages in a channel, for \(n > 0\).
You can prove this result using the concept of consistent
cuts. Intuitively, the agent cannot know whether a message is in the
channel or has been received.Chains of KnowledgeLet \(x, y, z\) be agents of a system and \(P\) be a predicate on
states of the system. Then the following are all predicates:\(z\) knows \(P\)\(y\) knows that \(z\) knows \(P\)\(x\) knows that \(y\) knows that \(z\) knows \(P\)The theorems given earlier apply to any predicate. For example, if
\(x\) knows that \(y\) knows that \(z\) knows \(P\) at a point \(t\) in a
trajectory, and \(\neg P\) holds at a later point \(t'\)
then there must be path in the timeline diagram from point \(t\) on
\(x\)'s timeline to point \(t'\) on \(z\)'s timeline.Concurrent Systems with Shared VariablesThe theorems
and proofs given in this module apply to systems with shared
variables, and indeed any system with trajectories that are
representable by timeline diagrams and with consistent cuts.SummaryMany people working on distributed systems use the phrase "an agent
knows." This module gives a definition of the concept that is
consistent with intuitive definitions of knowledge.
The central idea in this module is the
relationship between what agents know and consistent cuts of
timelines. We presented several theorems about agent knowledge which
are intuitive when applied to human agents. The proofs are
straightforward and are all based on consistent cuts of timelines.ReviewIs the following true? Why?(\(z\) knows \(P\)) and (\(z\) knows \(Q\)) \(\equiv\) \(z\) knows
  \(\(P \wedge Q\))Is the following true? Why?(\(z\) knows \(P\)) or (\(z\) knows \(Q\)) \(\equiv\) \(z\) knows
  \(\( P \vee Q\))Suppose you and your friend communicate only using messages sent in
  channels, just as agents do in our model of distributed
  systems. When youknowthat your friend is wearing a cap does
  that mean that (a) your friend is wearing a cap, and (b) your friend
  can't take the cap off until the friend hears from you?K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

(\(z\) knows \(P\)) and (\(z\) knows \(Q\)) \(\equiv\) \(z\) knows
  \(\(P \wedge Q\))

(\(z\) knows \(P\)) or (\(z\) knows \(Q\)) \(\equiv\) \(z\) knows
  \(\( P \vee Q\))

K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology
--- END Content from https://kmchandy.github.io/Knowledge/Knowledge.html ---


--- START Content from https://kmchandy.github.io/DiffusingComputations/DiffusingComputations.html ---
TITLE: DiffusingComputations/DiffusingComputations.html

Data Structures in Distributed Algorithms

Nondeterministic Iteration in Sequential and
Distributed Algorithms

Initially, the system has a single active agent. This agent is
    calledthe initiator. Initially all channels are empty.
    The computation terminates exactly when all agents
    are idle and all channels are empty.The computation may never
    terminate. Our first problem is to design an algorithm that
    enables the initiator to determine 
    that the computation has terminated if it terminates. Later, we
    will extend this algorithm to enable the initiator to learn about
    the network.In this system, for every channel from an agent \(x\) to an agent
    \(y\), there is a channel from \(y\) to \(x\). For any pair \(x,
    y\) of agents there exists at most one channel from \(x\) to
    \(y\), and at most one channel from \(y\) to \(x\).An agent \(y\) sends anack(acknowledgment) along channel
\((y, x)\) after receiving a message along channel \((x, y)\).
An ack is different from a message; so acks aren't acked.Initially all channels are empty: there are no messages or acks in
transit along channels.
Let
    \(x.num\_unacked\) be the number of \(x\)'s unacknowledge
    messages, i.e, the number of messages that \(x\) has sent
    minus the number of acks that \(x\) has received. 
We can prove the invariant
that there are no messages in any of \(x\)'s outgoing channels when
\(x.num\_unacked = 0\).A Rooted TreeFor an agent to become active there
 must be a chain of messages from the initiator to the
agent.
A data structure
with paths between the initiator and active agents is a tree,
rooted at the initiator, and which spans active agents.
 For each agent \(x\) let \(x.parent\) be either \(null\) or an agent
which is
\(x\)'s parent in the tree. Agent \(x\) is not on the tree exactly
when \(x.parent = null\).We will prove the following invariants.InvariantsThe tree is rooted at the initiator, i.e.
  if \(x.parent \neq null\) then the initiator is anancestorof \(x\).An agent is off the tree exactly when the agent is idle and the
  agent has no unacknowledged messages:\(
  (x.parent = null) \quad \equiv \quad x.idle \wedge
  (x.num\_unacked = 0)
  \)If \(x\)'s parent is not \(null\) then \(x\)'s parent has at least
  one unacknowledged message.\(
  x.parent \neq null \quad \Rightarrow \quad x.parent.num\_unacked > 0
  \)From invariant 3, it follows that an agent has no children if it has
no unacknowledged messages. So, from invariant 1, if the initiator has
no unacknowledged messages then all agents, apart from the initiator,
are idle and have no messages in outgoing channels. Therefore
computation has terminated when\(
initiator.idle \wedge (initiator.num\_unacked = 0)
\)So, the initiator detects that the computation has terminated when the
initiator is idle and no unacknowledged messages.
Next we give an algorithm that maintains the above invariants.Program for an agentNext we propose a program for an agent \(x\)
other than the initiator.Program0: initially:
      x.parent = null
      x.idle = True
      x.num_unacked = 0
1. When x sends a message:
      x.num_unacked = x.num_unacked + 1
2. When x receives a message from y:
      x.idle = False
      if x.parent = null:
         x.parent = y
      else
         send ack to y
3. When x receives an ack:
      x.num_unacked = x.num_unacked - 1
      if (x.num_unacked == 0) and x.idle:
              send ack to x.parent
              x.parent = null
4. When x becomes idle:
       x.idle = True
       if x.num_unacked == 0:
              send ack to x.parent
              x.parent = nullThe InitiatorThe program for the initiator is the same except that initially the
initiator is active. Also, to keep the exposition uniform for the
initiator and the other agents, we assume that the initiator has a
parent which is not one of the agents of the network. We call the initiator's parent
\(external\). This agent plays no role other than to keep the proofs
identical for the invariant and other agents.initiator.parent = external
initiator.idle = False
initiator.num_unacked = 0
external.num_unacked = 1Proof of CorrectnessSafetyThe proof that the invariants are satisfied is carried out by showing
that they hold initially and then verifying that
each of the four commands maintains the invariants. The verification
step is straightforward if a bit laborious.ProgressIf all agents are idle and there are no messages in channels then
the underlying computation has terminated.
We will prove that if the underlying computation does terminate then
the detection algorithm terminates as well, i.e. the tree
vanishes, and at that point the initiator detects that the computation has
terminated.
After the underlying computation terminates, the only action that
executes is action 3: receiving an ack.A variant function is
the following graph. A
vertex \(x\) is in the graph exactly when \(x.parent\) is not null or
there is an ack in transit along the channel from \(x\) to its parent.
Define a partial order \(<\) between graphs as \(G \leq G'\)
when \(G\) is a subgraph of \(G'\). This graph is a tree because the
only pending acks are from a vertex to its parent (rule 3).Next we prove that every execution of an action while the variant
function is not the empty graph reduces the variant function.
When all acks are delivered to \(y\), it sends an ack to its parent.
So while the tree is not empty there is an ack in some channel.
When \(y\) receives an ack from \(x\), the edge \((x, y)\) is
deleted from the tree. Therefore every
execution of action 3 reduces the variant function.ReviewShow that when an idle agent becomes active the agent is added to
  the rooted tree if it is not already part of it.The variant function used to prove progress in this module is a
  graph. Every action (execution of a guarded command with a true
  guard) must reduce the variant function. What does reducing the
  graph mean?This module says that if the underlying computation has terminated
  and the tree hasn't vanished then there exists some channel that has
  an ack in it. Why is that true?K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

The computation may never
    terminate. Our first problem is to design an algorithm that
    enables the initiator to determine 
    that the computation has terminated if it terminates. Later, we
    will extend this algorithm to enable the initiator to learn about
    the network.In this system, for every channel from an agent \(x\) to an agent
    \(y\), there is a channel from \(y\) to \(x\). For any pair \(x,
    y\) of agents there exists at most one channel from \(x\) to
    \(y\), and at most one channel from \(y\) to \(x\).An agent \(y\) sends anack(acknowledgment) along channel
\((y, x)\) after receiving a message along channel \((x, y)\).
An ack is different from a message; so acks aren't acked.Initially all channels are empty: there are no messages or acks in
transit along channels.
Let
    \(x.num\_unacked\) be the number of \(x\)'s unacknowledge
    messages, i.e, the number of messages that \(x\) has sent
    minus the number of acks that \(x\) has received. 
We can prove the invariant
that there are no messages in any of \(x\)'s outgoing channels when
\(x.num\_unacked = 0\).A Rooted TreeFor an agent to become active there
 must be a chain of messages from the initiator to the
agent.
A data structure
with paths between the initiator and active agents is a tree,
rooted at the initiator, and which spans active agents.
 For each agent \(x\) let \(x.parent\) be either \(null\) or an agent
which is
\(x\)'s parent in the tree. Agent \(x\) is not on the tree exactly
when \(x.parent = null\).We will prove the following invariants.InvariantsThe tree is rooted at the initiator, i.e.
  if \(x.parent \neq null\) then the initiator is anancestorof \(x\).An agent is off the tree exactly when the agent is idle and the
  agent has no unacknowledged messages:\(
  (x.parent = null) \quad \equiv \quad x.idle \wedge
  (x.num\_unacked = 0)
  \)If \(x\)'s parent is not \(null\) then \(x\)'s parent has at least
  one unacknowledged message.\(
  x.parent \neq null \quad \Rightarrow \quad x.parent.num\_unacked > 0
  \)From invariant 3, it follows that an agent has no children if it has
no unacknowledged messages. So, from invariant 1, if the initiator has
no unacknowledged messages then all agents, apart from the initiator,
are idle and have no messages in outgoing channels. Therefore
computation has terminated when\(
initiator.idle \wedge (initiator.num\_unacked = 0)
\)So, the initiator detects that the computation has terminated when the
initiator is idle and no unacknowledged messages.
Next we give an algorithm that maintains the above invariants.Program for an agentNext we propose a program for an agent \(x\)
other than the initiator.Program0: initially:
      x.parent = null
      x.idle = True
      x.num_unacked = 0
1. When x sends a message:
      x.num_unacked = x.num_unacked + 1
2. When x receives a message from y:
      x.idle = False
      if x.parent = null:
         x.parent = y
      else
         send ack to y
3. When x receives an ack:
      x.num_unacked = x.num_unacked - 1
      if (x.num_unacked == 0) and x.idle:
              send ack to x.parent
              x.parent = null
4. When x becomes idle:
       x.idle = True
       if x.num_unacked == 0:
              send ack to x.parent
              x.parent = nullThe InitiatorThe program for the initiator is the same except that initially the
initiator is active. Also, to keep the exposition uniform for the
initiator and the other agents, we assume that the initiator has a
parent which is not one of the agents of the network. We call the initiator's parent
\(external\). This agent plays no role other than to keep the proofs
identical for the invariant and other agents.initiator.parent = external
initiator.idle = False
initiator.num_unacked = 0
external.num_unacked = 1Proof of CorrectnessSafetyThe proof that the invariants are satisfied is carried out by showing
that they hold initially and then verifying that
each of the four commands maintains the invariants. The verification
step is straightforward if a bit laborious.ProgressIf all agents are idle and there are no messages in channels then
the underlying computation has terminated.
We will prove that if the underlying computation does terminate then
the detection algorithm terminates as well, i.e. the tree
vanishes, and at that point the initiator detects that the computation has
terminated.
After the underlying computation terminates, the only action that
executes is action 3: receiving an ack.A variant function is
the following graph. A
vertex \(x\) is in the graph exactly when \(x.parent\) is not null or
there is an ack in transit along the channel from \(x\) to its parent.
Define a partial order \(<\) between graphs as \(G \leq G'\)
when \(G\) is a subgraph of \(G'\). This graph is a tree because the
only pending acks are from a vertex to its parent (rule 3).Next we prove that every execution of an action while the variant
function is not the empty graph reduces the variant function.
When all acks are delivered to \(y\), it sends an ack to its parent.
So while the tree is not empty there is an ack in some channel.
When \(y\) receives an ack from \(x\), the edge \((x, y)\) is
deleted from the tree. Therefore every
execution of action 3 reduces the variant function.ReviewShow that when an idle agent becomes active the agent is added to
  the rooted tree if it is not already part of it.The variant function used to prove progress in this module is a
  graph. Every action (execution of a guarded command with a true
  guard) must reduce the variant function. What does reducing the
  graph mean?This module says that if the underlying computation has terminated
  and the tree hasn't vanished then there exists some channel that has
  an ack in it. Why is that true?K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

In this system, for every channel from an agent \(x\) to an agent
    \(y\), there is a channel from \(y\) to \(x\). For any pair \(x,
    y\) of agents there exists at most one channel from \(x\) to
    \(y\), and at most one channel from \(y\) to \(x\).An agent \(y\) sends anack(acknowledgment) along channel
\((y, x)\) after receiving a message along channel \((x, y)\).
An ack is different from a message; so acks aren't acked.Initially all channels are empty: there are no messages or acks in
transit along channels.
Let
    \(x.num\_unacked\) be the number of \(x\)'s unacknowledge
    messages, i.e, the number of messages that \(x\) has sent
    minus the number of acks that \(x\) has received. 
We can prove the invariant
that there are no messages in any of \(x\)'s outgoing channels when
\(x.num\_unacked = 0\).A Rooted TreeFor an agent to become active there
 must be a chain of messages from the initiator to the
agent.
A data structure
with paths between the initiator and active agents is a tree,
rooted at the initiator, and which spans active agents.
 For each agent \(x\) let \(x.parent\) be either \(null\) or an agent
which is
\(x\)'s parent in the tree. Agent \(x\) is not on the tree exactly
when \(x.parent = null\).We will prove the following invariants.InvariantsThe tree is rooted at the initiator, i.e.
  if \(x.parent \neq null\) then the initiator is anancestorof \(x\).An agent is off the tree exactly when the agent is idle and the
  agent has no unacknowledged messages:\(
  (x.parent = null) \quad \equiv \quad x.idle \wedge
  (x.num\_unacked = 0)
  \)If \(x\)'s parent is not \(null\) then \(x\)'s parent has at least
  one unacknowledged message.\(
  x.parent \neq null \quad \Rightarrow \quad x.parent.num\_unacked > 0
  \)From invariant 3, it follows that an agent has no children if it has
no unacknowledged messages. So, from invariant 1, if the initiator has
no unacknowledged messages then all agents, apart from the initiator,
are idle and have no messages in outgoing channels. Therefore
computation has terminated when\(
initiator.idle \wedge (initiator.num\_unacked = 0)
\)So, the initiator detects that the computation has terminated when the
initiator is idle and no unacknowledged messages.
Next we give an algorithm that maintains the above invariants.Program for an agentNext we propose a program for an agent \(x\)
other than the initiator.Program0: initially:
      x.parent = null
      x.idle = True
      x.num_unacked = 0
1. When x sends a message:
      x.num_unacked = x.num_unacked + 1
2. When x receives a message from y:
      x.idle = False
      if x.parent = null:
         x.parent = y
      else
         send ack to y
3. When x receives an ack:
      x.num_unacked = x.num_unacked - 1
      if (x.num_unacked == 0) and x.idle:
              send ack to x.parent
              x.parent = null
4. When x becomes idle:
       x.idle = True
       if x.num_unacked == 0:
              send ack to x.parent
              x.parent = nullThe InitiatorThe program for the initiator is the same except that initially the
initiator is active. Also, to keep the exposition uniform for the
initiator and the other agents, we assume that the initiator has a
parent which is not one of the agents of the network. We call the initiator's parent
\(external\). This agent plays no role other than to keep the proofs
identical for the invariant and other agents.initiator.parent = external
initiator.idle = False
initiator.num_unacked = 0
external.num_unacked = 1Proof of CorrectnessSafetyThe proof that the invariants are satisfied is carried out by showing
that they hold initially and then verifying that
each of the four commands maintains the invariants. The verification
step is straightforward if a bit laborious.ProgressIf all agents are idle and there are no messages in channels then
the underlying computation has terminated.
We will prove that if the underlying computation does terminate then
the detection algorithm terminates as well, i.e. the tree
vanishes, and at that point the initiator detects that the computation has
terminated.
After the underlying computation terminates, the only action that
executes is action 3: receiving an ack.A variant function is
the following graph. A
vertex \(x\) is in the graph exactly when \(x.parent\) is not null or
there is an ack in transit along the channel from \(x\) to its parent.
Define a partial order \(<\) between graphs as \(G \leq G'\)
when \(G\) is a subgraph of \(G'\). This graph is a tree because the
only pending acks are from a vertex to its parent (rule 3).Next we prove that every execution of an action while the variant
function is not the empty graph reduces the variant function.
When all acks are delivered to \(y\), it sends an ack to its parent.
So while the tree is not empty there is an ack in some channel.
When \(y\) receives an ack from \(x\), the edge \((x, y)\) is
deleted from the tree. Therefore every
execution of action 3 reduces the variant function.ReviewShow that when an idle agent becomes active the agent is added to
  the rooted tree if it is not already part of it.The variant function used to prove progress in this module is a
  graph. Every action (execution of a guarded command with a true
  guard) must reduce the variant function. What does reducing the
  graph mean?This module says that if the underlying computation has terminated
  and the tree hasn't vanished then there exists some channel that has
  an ack in it. Why is that true?K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

An agent \(y\) sends anack(acknowledgment) along channel
\((y, x)\) after receiving a message along channel \((x, y)\).
An ack is different from a message; so acks aren't acked.Initially all channels are empty: there are no messages or acks in
transit along channels.
Let
    \(x.num\_unacked\) be the number of \(x\)'s unacknowledge
    messages, i.e, the number of messages that \(x\) has sent
    minus the number of acks that \(x\) has received. 
We can prove the invariant
that there are no messages in any of \(x\)'s outgoing channels when
\(x.num\_unacked = 0\).A Rooted TreeFor an agent to become active there
 must be a chain of messages from the initiator to the
agent.
A data structure
with paths between the initiator and active agents is a tree,
rooted at the initiator, and which spans active agents.
 For each agent \(x\) let \(x.parent\) be either \(null\) or an agent
which is
\(x\)'s parent in the tree. Agent \(x\) is not on the tree exactly
when \(x.parent = null\).We will prove the following invariants.InvariantsThe tree is rooted at the initiator, i.e.
  if \(x.parent \neq null\) then the initiator is anancestorof \(x\).An agent is off the tree exactly when the agent is idle and the
  agent has no unacknowledged messages:\(
  (x.parent = null) \quad \equiv \quad x.idle \wedge
  (x.num\_unacked = 0)
  \)If \(x\)'s parent is not \(null\) then \(x\)'s parent has at least
  one unacknowledged message.\(
  x.parent \neq null \quad \Rightarrow \quad x.parent.num\_unacked > 0
  \)From invariant 3, it follows that an agent has no children if it has
no unacknowledged messages. So, from invariant 1, if the initiator has
no unacknowledged messages then all agents, apart from the initiator,
are idle and have no messages in outgoing channels. Therefore
computation has terminated when\(
initiator.idle \wedge (initiator.num\_unacked = 0)
\)So, the initiator detects that the computation has terminated when the
initiator is idle and no unacknowledged messages.
Next we give an algorithm that maintains the above invariants.Program for an agentNext we propose a program for an agent \(x\)
other than the initiator.Program0: initially:
      x.parent = null
      x.idle = True
      x.num_unacked = 0
1. When x sends a message:
      x.num_unacked = x.num_unacked + 1
2. When x receives a message from y:
      x.idle = False
      if x.parent = null:
         x.parent = y
      else
         send ack to y
3. When x receives an ack:
      x.num_unacked = x.num_unacked - 1
      if (x.num_unacked == 0) and x.idle:
              send ack to x.parent
              x.parent = null
4. When x becomes idle:
       x.idle = True
       if x.num_unacked == 0:
              send ack to x.parent
              x.parent = nullThe InitiatorThe program for the initiator is the same except that initially the
initiator is active. Also, to keep the exposition uniform for the
initiator and the other agents, we assume that the initiator has a
parent which is not one of the agents of the network. We call the initiator's parent
\(external\). This agent plays no role other than to keep the proofs
identical for the invariant and other agents.initiator.parent = external
initiator.idle = False
initiator.num_unacked = 0
external.num_unacked = 1Proof of CorrectnessSafetyThe proof that the invariants are satisfied is carried out by showing
that they hold initially and then verifying that
each of the four commands maintains the invariants. The verification
step is straightforward if a bit laborious.ProgressIf all agents are idle and there are no messages in channels then
the underlying computation has terminated.
We will prove that if the underlying computation does terminate then
the detection algorithm terminates as well, i.e. the tree
vanishes, and at that point the initiator detects that the computation has
terminated.
After the underlying computation terminates, the only action that
executes is action 3: receiving an ack.A variant function is
the following graph. A
vertex \(x\) is in the graph exactly when \(x.parent\) is not null or
there is an ack in transit along the channel from \(x\) to its parent.
Define a partial order \(<\) between graphs as \(G \leq G'\)
when \(G\) is a subgraph of \(G'\). This graph is a tree because the
only pending acks are from a vertex to its parent (rule 3).Next we prove that every execution of an action while the variant
function is not the empty graph reduces the variant function.
When all acks are delivered to \(y\), it sends an ack to its parent.
So while the tree is not empty there is an ack in some channel.
When \(y\) receives an ack from \(x\), the edge \((x, y)\) is
deleted from the tree. Therefore every
execution of action 3 reduces the variant function.ReviewShow that when an idle agent becomes active the agent is added to
  the rooted tree if it is not already part of it.The variant function used to prove progress in this module is a
  graph. Every action (execution of a guarded command with a true
  guard) must reduce the variant function. What does reducing the
  graph mean?This module says that if the underlying computation has terminated
  and the tree hasn't vanished then there exists some channel that has
  an ack in it. Why is that true?K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

Initially all channels are empty: there are no messages or acks in
transit along channels.
Let
    \(x.num\_unacked\) be the number of \(x\)'s unacknowledge
    messages, i.e, the number of messages that \(x\) has sent
    minus the number of acks that \(x\) has received. 
We can prove the invariant
that there are no messages in any of \(x\)'s outgoing channels when
\(x.num\_unacked = 0\).A Rooted TreeFor an agent to become active there
 must be a chain of messages from the initiator to the
agent.
A data structure
with paths between the initiator and active agents is a tree,
rooted at the initiator, and which spans active agents.
 For each agent \(x\) let \(x.parent\) be either \(null\) or an agent
which is
\(x\)'s parent in the tree. Agent \(x\) is not on the tree exactly
when \(x.parent = null\).We will prove the following invariants.InvariantsThe tree is rooted at the initiator, i.e.
  if \(x.parent \neq null\) then the initiator is anancestorof \(x\).An agent is off the tree exactly when the agent is idle and the
  agent has no unacknowledged messages:\(
  (x.parent = null) \quad \equiv \quad x.idle \wedge
  (x.num\_unacked = 0)
  \)If \(x\)'s parent is not \(null\) then \(x\)'s parent has at least
  one unacknowledged message.\(
  x.parent \neq null \quad \Rightarrow \quad x.parent.num\_unacked > 0
  \)From invariant 3, it follows that an agent has no children if it has
no unacknowledged messages. So, from invariant 1, if the initiator has
no unacknowledged messages then all agents, apart from the initiator,
are idle and have no messages in outgoing channels. Therefore
computation has terminated when\(
initiator.idle \wedge (initiator.num\_unacked = 0)
\)So, the initiator detects that the computation has terminated when the
initiator is idle and no unacknowledged messages.
Next we give an algorithm that maintains the above invariants.Program for an agentNext we propose a program for an agent \(x\)
other than the initiator.Program0: initially:
      x.parent = null
      x.idle = True
      x.num_unacked = 0
1. When x sends a message:
      x.num_unacked = x.num_unacked + 1
2. When x receives a message from y:
      x.idle = False
      if x.parent = null:
         x.parent = y
      else
         send ack to y
3. When x receives an ack:
      x.num_unacked = x.num_unacked - 1
      if (x.num_unacked == 0) and x.idle:
              send ack to x.parent
              x.parent = null
4. When x becomes idle:
       x.idle = True
       if x.num_unacked == 0:
              send ack to x.parent
              x.parent = nullThe InitiatorThe program for the initiator is the same except that initially the
initiator is active. Also, to keep the exposition uniform for the
initiator and the other agents, we assume that the initiator has a
parent which is not one of the agents of the network. We call the initiator's parent
\(external\). This agent plays no role other than to keep the proofs
identical for the invariant and other agents.initiator.parent = external
initiator.idle = False
initiator.num_unacked = 0
external.num_unacked = 1Proof of CorrectnessSafetyThe proof that the invariants are satisfied is carried out by showing
that they hold initially and then verifying that
each of the four commands maintains the invariants. The verification
step is straightforward if a bit laborious.ProgressIf all agents are idle and there are no messages in channels then
the underlying computation has terminated.
We will prove that if the underlying computation does terminate then
the detection algorithm terminates as well, i.e. the tree
vanishes, and at that point the initiator detects that the computation has
terminated.
After the underlying computation terminates, the only action that
executes is action 3: receiving an ack.A variant function is
the following graph. A
vertex \(x\) is in the graph exactly when \(x.parent\) is not null or
there is an ack in transit along the channel from \(x\) to its parent.
Define a partial order \(<\) between graphs as \(G \leq G'\)
when \(G\) is a subgraph of \(G'\). This graph is a tree because the
only pending acks are from a vertex to its parent (rule 3).Next we prove that every execution of an action while the variant
function is not the empty graph reduces the variant function.
When all acks are delivered to \(y\), it sends an ack to its parent.
So while the tree is not empty there is an ack in some channel.
When \(y\) receives an ack from \(x\), the edge \((x, y)\) is
deleted from the tree. Therefore every
execution of action 3 reduces the variant function.ReviewShow that when an idle agent becomes active the agent is added to
  the rooted tree if it is not already part of it.The variant function used to prove progress in this module is a
  graph. Every action (execution of a guarded command with a true
  guard) must reduce the variant function. What does reducing the
  graph mean?This module says that if the underlying computation has terminated
  and the tree hasn't vanished then there exists some channel that has
  an ack in it. Why is that true?K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

We will prove the following invariants.InvariantsThe tree is rooted at the initiator, i.e.
  if \(x.parent \neq null\) then the initiator is anancestorof \(x\).An agent is off the tree exactly when the agent is idle and the
  agent has no unacknowledged messages:\(
  (x.parent = null) \quad \equiv \quad x.idle \wedge
  (x.num\_unacked = 0)
  \)If \(x\)'s parent is not \(null\) then \(x\)'s parent has at least
  one unacknowledged message.\(
  x.parent \neq null \quad \Rightarrow \quad x.parent.num\_unacked > 0
  \)From invariant 3, it follows that an agent has no children if it has
no unacknowledged messages. So, from invariant 1, if the initiator has
no unacknowledged messages then all agents, apart from the initiator,
are idle and have no messages in outgoing channels. Therefore
computation has terminated when\(
initiator.idle \wedge (initiator.num\_unacked = 0)
\)So, the initiator detects that the computation has terminated when the
initiator is idle and no unacknowledged messages.
Next we give an algorithm that maintains the above invariants.Program for an agentNext we propose a program for an agent \(x\)
other than the initiator.Program0: initially:
      x.parent = null
      x.idle = True
      x.num_unacked = 0
1. When x sends a message:
      x.num_unacked = x.num_unacked + 1
2. When x receives a message from y:
      x.idle = False
      if x.parent = null:
         x.parent = y
      else
         send ack to y
3. When x receives an ack:
      x.num_unacked = x.num_unacked - 1
      if (x.num_unacked == 0) and x.idle:
              send ack to x.parent
              x.parent = null
4. When x becomes idle:
       x.idle = True
       if x.num_unacked == 0:
              send ack to x.parent
              x.parent = nullThe InitiatorThe program for the initiator is the same except that initially the
initiator is active. Also, to keep the exposition uniform for the
initiator and the other agents, we assume that the initiator has a
parent which is not one of the agents of the network. We call the initiator's parent
\(external\). This agent plays no role other than to keep the proofs
identical for the invariant and other agents.initiator.parent = external
initiator.idle = False
initiator.num_unacked = 0
external.num_unacked = 1Proof of CorrectnessSafetyThe proof that the invariants are satisfied is carried out by showing
that they hold initially and then verifying that
each of the four commands maintains the invariants. The verification
step is straightforward if a bit laborious.ProgressIf all agents are idle and there are no messages in channels then
the underlying computation has terminated.
We will prove that if the underlying computation does terminate then
the detection algorithm terminates as well, i.e. the tree
vanishes, and at that point the initiator detects that the computation has
terminated.
After the underlying computation terminates, the only action that
executes is action 3: receiving an ack.A variant function is
the following graph. A
vertex \(x\) is in the graph exactly when \(x.parent\) is not null or
there is an ack in transit along the channel from \(x\) to its parent.
Define a partial order \(<\) between graphs as \(G \leq G'\)
when \(G\) is a subgraph of \(G'\). This graph is a tree because the
only pending acks are from a vertex to its parent (rule 3).Next we prove that every execution of an action while the variant
function is not the empty graph reduces the variant function.
When all acks are delivered to \(y\), it sends an ack to its parent.
So while the tree is not empty there is an ack in some channel.
When \(y\) receives an ack from \(x\), the edge \((x, y)\) is
deleted from the tree. Therefore every
execution of action 3 reduces the variant function.ReviewShow that when an idle agent becomes active the agent is added to
  the rooted tree if it is not already part of it.The variant function used to prove progress in this module is a
  graph. Every action (execution of a guarded command with a true
  guard) must reduce the variant function. What does reducing the
  graph mean?This module says that if the underlying computation has terminated
  and the tree hasn't vanished then there exists some channel that has
  an ack in it. Why is that true?K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

\(
  (x.parent = null) \quad \equiv \quad x.idle \wedge
  (x.num\_unacked = 0)
  \)

\(
  x.parent \neq null \quad \Rightarrow \quad x.parent.num\_unacked > 0
  \)

\(
initiator.idle \wedge (initiator.num\_unacked = 0)
\)So, the initiator detects that the computation has terminated when the
initiator is idle and no unacknowledged messages.
Next we give an algorithm that maintains the above invariants.Program for an agentNext we propose a program for an agent \(x\)
other than the initiator.Program0: initially:
      x.parent = null
      x.idle = True
      x.num_unacked = 0
1. When x sends a message:
      x.num_unacked = x.num_unacked + 1
2. When x receives a message from y:
      x.idle = False
      if x.parent = null:
         x.parent = y
      else
         send ack to y
3. When x receives an ack:
      x.num_unacked = x.num_unacked - 1
      if (x.num_unacked == 0) and x.idle:
              send ack to x.parent
              x.parent = null
4. When x becomes idle:
       x.idle = True
       if x.num_unacked == 0:
              send ack to x.parent
              x.parent = nullThe InitiatorThe program for the initiator is the same except that initially the
initiator is active. Also, to keep the exposition uniform for the
initiator and the other agents, we assume that the initiator has a
parent which is not one of the agents of the network. We call the initiator's parent
\(external\). This agent plays no role other than to keep the proofs
identical for the invariant and other agents.initiator.parent = external
initiator.idle = False
initiator.num_unacked = 0
external.num_unacked = 1Proof of CorrectnessSafetyThe proof that the invariants are satisfied is carried out by showing
that they hold initially and then verifying that
each of the four commands maintains the invariants. The verification
step is straightforward if a bit laborious.ProgressIf all agents are idle and there are no messages in channels then
the underlying computation has terminated.
We will prove that if the underlying computation does terminate then
the detection algorithm terminates as well, i.e. the tree
vanishes, and at that point the initiator detects that the computation has
terminated.
After the underlying computation terminates, the only action that
executes is action 3: receiving an ack.A variant function is
the following graph. A
vertex \(x\) is in the graph exactly when \(x.parent\) is not null or
there is an ack in transit along the channel from \(x\) to its parent.
Define a partial order \(<\) between graphs as \(G \leq G'\)
when \(G\) is a subgraph of \(G'\). This graph is a tree because the
only pending acks are from a vertex to its parent (rule 3).Next we prove that every execution of an action while the variant
function is not the empty graph reduces the variant function.
When all acks are delivered to \(y\), it sends an ack to its parent.
So while the tree is not empty there is an ack in some channel.
When \(y\) receives an ack from \(x\), the edge \((x, y)\) is
deleted from the tree. Therefore every
execution of action 3 reduces the variant function.ReviewShow that when an idle agent becomes active the agent is added to
  the rooted tree if it is not already part of it.The variant function used to prove progress in this module is a
  graph. Every action (execution of a guarded command with a true
  guard) must reduce the variant function. What does reducing the
  graph mean?This module says that if the underlying computation has terminated
  and the tree hasn't vanished then there exists some channel that has
  an ack in it. Why is that true?K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

So, the initiator detects that the computation has terminated when the
initiator is idle and no unacknowledged messages.
Next we give an algorithm that maintains the above invariants.Program for an agentNext we propose a program for an agent \(x\)
other than the initiator.Program0: initially:
      x.parent = null
      x.idle = True
      x.num_unacked = 0
1. When x sends a message:
      x.num_unacked = x.num_unacked + 1
2. When x receives a message from y:
      x.idle = False
      if x.parent = null:
         x.parent = y
      else
         send ack to y
3. When x receives an ack:
      x.num_unacked = x.num_unacked - 1
      if (x.num_unacked == 0) and x.idle:
              send ack to x.parent
              x.parent = null
4. When x becomes idle:
       x.idle = True
       if x.num_unacked == 0:
              send ack to x.parent
              x.parent = nullThe InitiatorThe program for the initiator is the same except that initially the
initiator is active. Also, to keep the exposition uniform for the
initiator and the other agents, we assume that the initiator has a
parent which is not one of the agents of the network. We call the initiator's parent
\(external\). This agent plays no role other than to keep the proofs
identical for the invariant and other agents.initiator.parent = external
initiator.idle = False
initiator.num_unacked = 0
external.num_unacked = 1Proof of CorrectnessSafetyThe proof that the invariants are satisfied is carried out by showing
that they hold initially and then verifying that
each of the four commands maintains the invariants. The verification
step is straightforward if a bit laborious.ProgressIf all agents are idle and there are no messages in channels then
the underlying computation has terminated.
We will prove that if the underlying computation does terminate then
the detection algorithm terminates as well, i.e. the tree
vanishes, and at that point the initiator detects that the computation has
terminated.
After the underlying computation terminates, the only action that
executes is action 3: receiving an ack.A variant function is
the following graph. A
vertex \(x\) is in the graph exactly when \(x.parent\) is not null or
there is an ack in transit along the channel from \(x\) to its parent.
Define a partial order \(<\) between graphs as \(G \leq G'\)
when \(G\) is a subgraph of \(G'\). This graph is a tree because the
only pending acks are from a vertex to its parent (rule 3).Next we prove that every execution of an action while the variant
function is not the empty graph reduces the variant function.
When all acks are delivered to \(y\), it sends an ack to its parent.
So while the tree is not empty there is an ack in some channel.
When \(y\) receives an ack from \(x\), the edge \((x, y)\) is
deleted from the tree. Therefore every
execution of action 3 reduces the variant function.ReviewShow that when an idle agent becomes active the agent is added to
  the rooted tree if it is not already part of it.The variant function used to prove progress in this module is a
  graph. Every action (execution of a guarded command with a true
  guard) must reduce the variant function. What does reducing the
  graph mean?This module says that if the underlying computation has terminated
  and the tree hasn't vanished then there exists some channel that has
  an ack in it. Why is that true?K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

The Initiator

Safety

Progress

If all agents are idle and there are no messages in channels then
the underlying computation has terminated.
We will prove that if the underlying computation does terminate then
the detection algorithm terminates as well, i.e. the tree
vanishes, and at that point the initiator detects that the computation has
terminated.
After the underlying computation terminates, the only action that
executes is action 3: receiving an ack.A variant function is
the following graph. A
vertex \(x\) is in the graph exactly when \(x.parent\) is not null or
there is an ack in transit along the channel from \(x\) to its parent.
Define a partial order \(<\) between graphs as \(G \leq G'\)
when \(G\) is a subgraph of \(G'\). This graph is a tree because the
only pending acks are from a vertex to its parent (rule 3).Next we prove that every execution of an action while the variant
function is not the empty graph reduces the variant function.
When all acks are delivered to \(y\), it sends an ack to its parent.
So while the tree is not empty there is an ack in some channel.
When \(y\) receives an ack from \(x\), the edge \((x, y)\) is
deleted from the tree. Therefore every
execution of action 3 reduces the variant function.ReviewShow that when an idle agent becomes active the agent is added to
  the rooted tree if it is not already part of it.The variant function used to prove progress in this module is a
  graph. Every action (execution of a guarded command with a true
  guard) must reduce the variant function. What does reducing the
  graph mean?This module says that if the underlying computation has terminated
  and the tree hasn't vanished then there exists some channel that has
  an ack in it. Why is that true?K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

A variant function is
the following graph. A
vertex \(x\) is in the graph exactly when \(x.parent\) is not null or
there is an ack in transit along the channel from \(x\) to its parent.
Define a partial order \(<\) between graphs as \(G \leq G'\)
when \(G\) is a subgraph of \(G'\). This graph is a tree because the
only pending acks are from a vertex to its parent (rule 3).Next we prove that every execution of an action while the variant
function is not the empty graph reduces the variant function.
When all acks are delivered to \(y\), it sends an ack to its parent.
So while the tree is not empty there is an ack in some channel.
When \(y\) receives an ack from \(x\), the edge \((x, y)\) is
deleted from the tree. Therefore every
execution of action 3 reduces the variant function.ReviewShow that when an idle agent becomes active the agent is added to
  the rooted tree if it is not already part of it.The variant function used to prove progress in this module is a
  graph. Every action (execution of a guarded command with a true
  guard) must reduce the variant function. What does reducing the
  graph mean?This module says that if the underlying computation has terminated
  and the tree hasn't vanished then there exists some channel that has
  an ack in it. Why is that true?K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

Next we prove that every execution of an action while the variant
function is not the empty graph reduces the variant function.
When all acks are delivered to \(y\), it sends an ack to its parent.
So while the tree is not empty there is an ack in some channel.
When \(y\) receives an ack from \(x\), the edge \((x, y)\) is
deleted from the tree. Therefore every
execution of action 3 reduces the variant function.ReviewShow that when an idle agent becomes active the agent is added to
  the rooted tree if it is not already part of it.The variant function used to prove progress in this module is a
  graph. Every action (execution of a guarded command with a true
  guard) must reduce the variant function. What does reducing the
  graph mean?This module says that if the underlying computation has terminated
  and the tree hasn't vanished then there exists some channel that has
  an ack in it. Why is that true?K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology
--- END Content from https://kmchandy.github.io/DiffusingComputations/DiffusingComputations.html ---


--- START Content from https://kmchandy.github.io/DistributedCollaboration/DiningPhilosophers/DiningPhilosophers.html ---
TITLE: DistributedCollaboration/DiningPhilosophers/DiningPhilosophers.html

Distributed algorithms often use a good-neighbor policy to ensure
    that all agents win conflicts eventually:An agent that wins a
    conflict makes its priority lower than the priorities of all the
    agents with which it competes.3. Tokens and What Agents KnowAn agent can resolve a conflict with other agents only if it knows
    something about the states of other agents. For example, what
    agents want to enter a critical section and what are their
priorities?What agents know is defined in this module.We used the
    concept of tokens to illustrate what agents know.  A system has a
    fixed number of indivisible tokens which are neither created nor
    destroyed. An agent that holds a token knows that no other agent
    holds that token.  This knowledge is at the core of many conflict
    resolution algorithms.The ProblemAgent States in a Mutual Exclusion ProblemFig.2: States in Mutual ExclusionAn agent in a mutual exclusion problem is in one of three states:Outside critical section:
  The agent is executing outside its critical section. An agent can remain in this
  state for ever, or it may transit after finite time to the next state: waiting to
  enter its critical section.Waiting to enter critical section:
  The agent waits to enter critical section until it is given
  permission to do so by the operating system.In critical section:
  The agent executes in its critical section.  An agent does not
  remain in its critical section forever. It does so for onlyfinitetime after which it transits to the state, outside
  critical section.Clients determine transitions fromoutside critical sectiontowaiting to
enter critical section, and fromin critical sectiontooutside critical section.
The operating system determines transitions fromwaiting to
enter critical sectiontooutside critical section.Agent States in the Dining PhilosophersProblemFig.3: States in Dining PhilosophersThe name "Dining Philosophers" is an example of CS humor
(an oxymoron?).
Philosophers may think for ever, but eat for only finite time. The
algorithm must ensure that hungry philosophers don't starve --- they
get to eateventually.
The problem and its name were proposed by Edsger W. Dijkstra, a CS pioneer.The states "Thinking", "Hungry", and "Eating" correspond exactly toOutside critical section,Waiting to enter critical
section, andIn critical section, respectively.Agent Communication StructureFig.4: Communication among AgentsThe commununication structure among agents is represented by an
undirected graph in which the nodes are agents and each edge
represents two channels, one in each direction. The agents are either
OS or client agents. There is one client agent associated with each OS
agent. Clients are shown as squares and OS agents as circles.
For example client agent \(w\) is associated with OS agent
\(w\). The diagram does not show all clients so as not to make the
diagram too crowded.A pair of OS agents are neighbors when there is an edge between
them. A pair of client agents are neighbors when the OS agents with
which they are associated are neighbors. For example, in the figure, 
\(w\) and \(y\) are neighbors.SpecificationSafety: Neighbors do not eat at the same timeLet \(safe\) be the predicateNeighboring clients are
not eating, and let \(init\) be a predicate that holds initially.The safety part of the specification is that \(safe\) holds in every state in every path from
every initial state:\([init
\Rightarrow A(safe)]\)Progress: Hungry agents eat eventuallyThe progress part of the specification is that every hungry agent
transits to eating state eventually.For every agent \(v\):\(
v.state = hungry \quad \leadsto \quad v.state = eating
\)Example of SafetyFig.5: Diagrams illustrating SafetyFigure 5 shows a client eating as a red node depicting the client and
its OS agent. An uncolored node represents a client that is thinking
or hungry. The diagram on the left shows the system in a safe state:
there are no edges between red vertices. The diagram on the right
shows an unsafe state because there are edges between red vertices.The Client's ProgramWe use two tokens that move
between a client and its OS agent. The tokens are called theresource tokenand therequest token. The client's
states are represented by which tokens the client holds.Thinking State: A thinking client holds the request token but
  not the resource token.Transition: Thinking to Hungry: Send the request token to
  the OS.Hungry State: The client holds neither the request nor the
  resource token.Transition Hungry to Eating: The client transits to
  eating when it receives both the request and resource token.Eating: The client holds both the request and resource tokens.Transition from Eating to Thinking:
  The client holds sends the resource token to the OS and continues to
  hold the request token.The figure below illustrates the states of the client. The request
    token is shown as a square and the resource token as a circle.Fig.6: Client's ProgramInitially all clients are thinking; all resource tokens are with OS
agents; and all request tokens are with clients.What the OS KnowsWhile the OS agent holds the request token it knows that its client is
hungry. While the OS agent holds the resource token it knows that its client is
not eating.Fig.7: OS Agent's ProgramWhen a client transits from thinking to hungry it sends its request
token to the agent. When the OS receives the request token, the OS
also has the resource token; so the OS knows that its client is
hungry. There is an interval after the client sends the request token
and before the OS receives it during which the client is hungry but
the OS doesn't know that. An OS agent does not need to know 
what state its client is in at every point. Likewise, a client does
not need to know its OS agent's state at every point.A client is hungryleads-toits OS agent knowing that its client is
hungry. Theleads-tocaptures the fact that the OS doesn't know
the client's state at the instant that the client transitions from
thinking to hungry.Introduction of TokensWe introduce a token on each edge of the agent communication graph,
see figure 4. The token on an edge \(v, w\) is in one of four states:
held by \(v\), in the channel from \(v\) to \(w\), held by \(w\), or
in the channel from \(w\) to \(v\). Therefore while \(v\) holds this
token it knows that \(w\) doesn't hold it. Likewise, while \(w\) holds
this token it knows that \(v\) doesn't.Fig.7: Fork on each edge of the agent communication
    graphThese tokens are calledforks. (They are calledchopsticksin some papers.) An agent eats only if it holds
forks for all the edges incident on it. Therefore, while an agent eats
none of its neighbors do, and so the safety specification is satisfied.Key Question: When does a hungry agent yield
forks?An eating philosopher holds on to all its forks until it finishes
eating. A thinking philosopher can give a fork to a neighbor that
requests it. So the key question is:Under what conditions should a
hungry neighbor give a fork that it holds to a neighbor that requests
it?Suppose every hungry agent gives a fork to a neighbor that requests
it. Then we can get the scenario shown in the figure below.Fig.8: Scenario when hungry agents yield forksThe figure shows a system with 3 agents indexed 0, 1, 2. The forks are
    shown as small colored circles. The state on
    the left shows agent \(j\) holding the fork that it shares with
    agent \((j+1)\:\textrm{mod}\:3\). If each agent yields the fork to
    its neighbor we get the state on the right in which agent \(j\)
    holds the fork that it shares with 
    agent \((j-1)\:\textrm{mod}\:3\). So, if all hungry agents yield
    forks, the states can alternate for ever between the one on the
left and the one on the right. In this case hungry agents starve: they
    remain hungry forever.If hungry agents don't yield forks then the state on the left persists
for ever. In this case too, hungry agents starve.Creating a Partial Order of PrioritiesLet's assign priorities to agents so that a hungry agent \(v\)
releases a fork to a neighbor \(w\) only if \(v\) has lower priority
than \(w\). If there is a cycle of agents, all with the same priority,
then we get the situation shown in figure 8. So, we will ensure that
priorities form a partial order in all states in all
transitions. Priorities form a partial order exactly when the priority
graph is acyclic. The graph has an edge from \(v\) to \(w\) exactly
when \(v\) has higher priority over \(w\).In the figure below, the diagram on the left shows an acyclic priority
graph while the one on the right shows a graph with a cycle.Fig.9: Priority Graph must be AcyclicHow should Priorities Change?Fig.10: How should priorities change when \(v\)
    eats?How should priorities change when an agent eats so that the priority
graph remains acyclic? For example, consider
the priority graph shown in figure 10. Assume agent \(v\) has all its
forks and is about to eat. Should the directions of edges incident on
\(v\) be flipped? Or should \(v\) have lower priority than all its
neighbors, i.e. all edges incident on \(v\) point towards \(v\)?What happens if we flip the directions of the edges incident on \(v\)?
After the flip, the edges are directed from \(w\), \(x\) and \(y\) towards \(v\), and
from \(v\) to \(u\). But now we have a cycle: \(y\) to \(v\) to \(u\)
to \(w\) to \(y\). So, flipping edge directions doesn't work.What happens if agents adopt the good neighbor policy?
The winner of a conflict gives itself lower priority thanallthe agents
with which it competes. So, an agent that starts eating gives itself lower
priority than all its neighbors. All edges point towards an eating
agent.Fig.11: Winner gets lower priority than its neighborsWhen all edges incident on a vertex point towards the vertex then
there is no cycle through that vertex. So, directing all edges
incident on an eating vertex towards the eating vertex maintains
acyclicity of the graph.For example, in the figure directing all edges incident on vertex \(v\) towards
 \(v\) ensures that no new cycle is created.Agent's priority does not decrease until the agent
eats.How an Agent knows its PriorityWe assign an attributeclean / dirtyto forks.
A fork is eitherdirtyorclean.
The forks held by an eating agent aredirty. When
an agent receives a fork from another agent, the receiving agent
cleans the fork. So the receiver holds acleanfork, and the
fork remains clean until an agent next eats with it.
(This is the "hygenic solution:" Another - sad? - attempt at CS
humor.)An agent holding a dirty fork knows that it has lower priority than
the agent with which it shares the fork. Likewise,
an agent holding a clean fork knows that it has higher priority than
the agent with which it shares the fork.If an agent does not hold the fork that it shares with a neighbor then
the agent does not know its priority with respect to that neighbor.Example of a Fork's LifecycleThe diagram below shows states of a fork shared by agents \(u\) and
\(v\). The red arrow shows priority, and the black arrows show
channels. The blue dot represents the fork.In the figure on the top left, agent \(u\) is hungry and holds a clean
fork. So, \(u\) knows that it has priority over \(v\). At this point
\(v\) does not know whether \(v\) has priority over \(u\) or not.The next figure, at the top right, shows that when \(u\) transits from
hungry to eating, the fork becomes dirty, and \(u\) has lower priority
than \(v\). Agent \(u\) continues to hold the fork while it eats.The next figure, bottom right, shows the situation after \(u\) gets a
request for the fork from \(v\). Because \(u\) got the request from
\(v\) and \(u\) hasn't sent the fork to \(v\), agent \(u\) knows that
\(v\) is hungry. Since the fork is dirty, \(u\) sends
the fork to \(v\). The figure shows the fork in the channel from \(u\)
to \(v\). While the fork is in the channel it doesn't matter
whether the fork is clean or dirty; however, merely for convenience,
let's assume that \(u\), being hygenic, cleans the fork before sending
it to its partner. While the fork is in a
channel the priority doesn't change but neither \(u\) nor \(v\) knows
what the priority is.The next figure, bottom left, shows the situation when \(v\) receives
the fork.  Receiving the fork doesn't change the priority.
At this point \(v\) is hungry and the fork is
clean and so \(v\) knows that it has higher
priority. \(v\) holds on to the fork until it next eats.Fig.12: How an Agent knows its PriorityAlgorithmProperties of Reachable StatesHere is a list of some of the properties of states in all
trajectories.The priority graph is acyclic. \(u\) has priority over a neighbor
  \(v\) exactly when \(u\) holds the fork that it shares with \(v\)
  and the fork is clean, or the fork is in the channel from \(v\) to
  \(u\), or \(v\) holds the fork and the fork is dirty.Eating philosophers hold the forks for all edges incident on them,
  and these forks are dirty.All forks held by thinking philosphers are dirty.Thinking philosophers never send requests and never receive
  forks. Thinking philosophers respond to request for forks by sending
  the requested forks.Initial StatesInitially all philosophers are thinking; all forks are dirty; and all channels are
empty. The forks are placed so that the priority graph is acyclic.
The initial assignment of forks is as follows.
Given an arbitrary acyclic graph, for any edge directed from \(v\) to
\(w\), the fork shared by \(v\) and \(w\) is initially at \(w\) and
the fork is dirty.Algorithm CommandsThe algorithm is specified by the following commands.When a thinking philosophers gets a request for a fork that it holds
  it sends the fork. (A fork held by a thinking philosopher is dirty.)When a thinking philosopher transits to hungry it sends requests for
  all forks that it does not hold.When a hungry philosopher receives a fork, it records the fork as
  being clean. If the hungry philosopher holds all its forks, and if
  it has no request for any dirty fork that it holds, then it transits
  to eating, and records all the forks that it holds in the eating
  state as dirty.When a hungry philosopher receives a request for a fork that it
  holds, it sends the fork if the fork is dirty, and holds on to the
  fork if it is clean.When an eating philosopher receives a request for a fork it
  registers the request in memory, and continues eating while holding
  the fork. When an eating philosopher transits to thinking it sends
  forks for all requests that it received.What could go wrong?The proof of safety is straightforward: Neighbors aren't eating
because neighbors can't hold the same fork at the same time.Before we look at the proof of progress, let's see what may go wrong.Could a group of philosophers exchange forks with each other so that
members of the group eat repeatedly, and starve a philosopher who is
not in the group? For example, in the figure below, could philosophers
\(u, v, w\) exchange forks so that they each eat in turn, and starve
\(y\)?Fig.13: Potential Problems: What could go wrong?Could the system enter a deadlock state in which each hungry philosopher in a
group holds only some --- but not all --- of the forks that it needs
to eat, while other members of the group hold the remaining forks?Proof of CorrectnessThe algorithm is correct. We are required to prove that every hungery
philosopher eats eventually:\(
\forall v: \quad v.h \leadsto v.e
\)where for a philosopher \(v\), \(v.h\) holds exactly when \(v\) is
hungry and \(v.e\) holds exactly when \(v\) is
eating.Variant FunctionTo prove this progress property we find a variant function that
satisfies the following two conditions:Safety: The variant function does not increase while \(v\)
  remains hungry.progress: The following predicate does not hold forever: The
  variant function remains unchanged and \(v\) remains hungry.We propose a variant function which is a pair of integers
 \(nT, nH\),  which are the number of thinking and hungry philosophers,
respectively, of 
higher priority than \(v\).
In terms of the priority graph, \(nT, nH\) are the numbers of thinking
and hungry philosophers (i.e. vertices) with paths to \(v\).Example of Variant FunctionThe figure below shows a portion of the priority graph in a state of the
system. The figure only shows philosophers with higher priority than
philosopher \(v\), i.e., it only shows vertices in the graph with
paths to \(v\). Since eating philosophers have lower priority than
their neighbors, eating philosophers don't appear in this graph.A hungry philosopher is marked with an "H" and a thinking philosopher
with a "T." In the diagram, philosophers \(v, w, x, y\) are hungry and
\(u\) is thinking.
Forks are located at philosophers and are shown as small
colored circles. A dirty fork is colored red and clean one is
blue. For example the fork shared by \(v\) and \(y\) is at \(y\) and
is clean.Fig.15: A Variant Function: Numbers of higher priority
    thinking, hungry agentsExample of Changes to Variant FunctionThe next figure is an example of changes to the variant function. The
diagram on the top left shows the higher priority vertices in the
state of the previous figure. If agent \(x\) eats next the priority
graph transits to the diagram on the top right, and the variant
function \((nT, nH)\) changes from \((1, 3)\) to \((1, 2)\).Fig.16: Example of Values of the Variant FunctionIf agent \(y\) eats next the priority
graph transits to the diagram on the bottom right, and the variant
function \((nT, nH)\) changes from \((1, 2)\) to \((1, 1)\).
If agent \(w\) eats next the priority
graph transits to the diagram on the bottom left, and the variant
function \((nT, nH)\) changes from \((1, 1)\) to \((0, 0)\).Proof that the variant function does not increase
while \(v\) remains hungryIf a philosopher of higher priority than \(v\) transits from thinking
to hungry then \(nT\) decreases. Though \(nH\) increases, the variant function \((nT,
nH)\) decreases because ordering of function values is done
lexicographically.If a philosopher of higher priority than \(v\) transits from hungry
to eating then \(nH\) decreases,  and so the variant function \((nT,
nH)\) decreases.Proof that the following predicate does not
hold forever: The variant function remains unchanged and
\(v\) remains hungry.Let \(w\) be a highest-priority hungry philosopher, i.e. a philosopher with no
hungry philosopher with priority higher than \(w\). (Note: \(w\) may be
the same as \(v\).)
All philosophers with priority higher than \(w\) are thinking. In the
next paragraph we show that either \(w\) gets all its forks and then transits
from hungry to eating, or the variant function decreases.From the algorithm, a hungry philosopher \(w\) requests forks from its
neighbors. From the algorithm, \(w\) eventually gets forks from 
all its lower priority neighbors. A higher priority neighbor \(x\) of
\(w\) is thinking. So when \(x\) gets a request for a fork from \(w\)
either (1) \(x\) sends the requested fork to \(w\) or (2) \(x\) transits from
thinking to hungry in which case the variant function \((nT, nH)\)
decreases.Summary: Key Ideas of this ModuleThis module introduced the problem of distributed mutual exclusion;
showed how the good neighbor policy --- a winning agent reduces its
priority to be lower than all the agents that it competes with --
solves this conflict resolution problem; introduced tokens and what
agents know about other agents holding tokens; and showed a proof
pattern that is one of the most common patterns for proving progress.ReviewWhat is the meaning of mutual exclusion among neighboring
  philosophers?An invariant of the algorithm is that each token is in exactly one
  place: an agent or a channel. How does this invariant help in
  designing the algorithm?An invariant of the algorithm is that the relative priorities among
  agents forms a partial order --- the priority graph is acyclic. What
  can go wrong if the priority graph has cycles?An agent that has a request for a fork releases the fork if the fork
  is dirty, and holds on to the fork if the fork is clean. What could
  go wrong if an agent releases a fork when it gets a request,
  regardless of whether the fork is clean or dirty?K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology



Figure Link: https://kmchandy.github.io/DistributedCollaboration/DiningPhilosophers/Slide02.jpg

Caption: Fig.2: States in Mutual Exclusion


Clients determine transitions fromoutside critical sectiontowaiting to
enter critical section, and fromin critical sectiontooutside critical section.
The operating system determines transitions fromwaiting to
enter critical sectiontooutside critical section.Agent States in the Dining PhilosophersProblemFig.3: States in Dining PhilosophersThe name "Dining Philosophers" is an example of CS humor
(an oxymoron?).
Philosophers may think for ever, but eat for only finite time. The
algorithm must ensure that hungry philosophers don't starve --- they
get to eateventually.
The problem and its name were proposed by Edsger W. Dijkstra, a CS pioneer.The states "Thinking", "Hungry", and "Eating" correspond exactly toOutside critical section,Waiting to enter critical
section, andIn critical section, respectively.Agent Communication StructureFig.4: Communication among AgentsThe commununication structure among agents is represented by an
undirected graph in which the nodes are agents and each edge
represents two channels, one in each direction. The agents are either
OS or client agents. There is one client agent associated with each OS
agent. Clients are shown as squares and OS agents as circles.
For example client agent \(w\) is associated with OS agent
\(w\). The diagram does not show all clients so as not to make the
diagram too crowded.A pair of OS agents are neighbors when there is an edge between
them. A pair of client agents are neighbors when the OS agents with
which they are associated are neighbors. For example, in the figure, 
\(w\) and \(y\) are neighbors.SpecificationSafety: Neighbors do not eat at the same timeLet \(safe\) be the predicateNeighboring clients are
not eating, and let \(init\) be a predicate that holds initially.The safety part of the specification is that \(safe\) holds in every state in every path from
every initial state:\([init
\Rightarrow A(safe)]\)Progress: Hungry agents eat eventuallyThe progress part of the specification is that every hungry agent
transits to eating state eventually.For every agent \(v\):\(
v.state = hungry \quad \leadsto \quad v.state = eating
\)Example of SafetyFig.5: Diagrams illustrating SafetyFigure 5 shows a client eating as a red node depicting the client and
its OS agent. An uncolored node represents a client that is thinking
or hungry. The diagram on the left shows the system in a safe state:
there are no edges between red vertices. The diagram on the right
shows an unsafe state because there are edges between red vertices.The Client's ProgramWe use two tokens that move
between a client and its OS agent. The tokens are called theresource tokenand therequest token. The client's
states are represented by which tokens the client holds.Thinking State: A thinking client holds the request token but
  not the resource token.Transition: Thinking to Hungry: Send the request token to
  the OS.Hungry State: The client holds neither the request nor the
  resource token.Transition Hungry to Eating: The client transits to
  eating when it receives both the request and resource token.Eating: The client holds both the request and resource tokens.Transition from Eating to Thinking:
  The client holds sends the resource token to the OS and continues to
  hold the request token.The figure below illustrates the states of the client. The request
    token is shown as a square and the resource token as a circle.Fig.6: Client's ProgramInitially all clients are thinking; all resource tokens are with OS
agents; and all request tokens are with clients.What the OS KnowsWhile the OS agent holds the request token it knows that its client is
hungry. While the OS agent holds the resource token it knows that its client is
not eating.Fig.7: OS Agent's ProgramWhen a client transits from thinking to hungry it sends its request
token to the agent. When the OS receives the request token, the OS
also has the resource token; so the OS knows that its client is
hungry. There is an interval after the client sends the request token
and before the OS receives it during which the client is hungry but
the OS doesn't know that. An OS agent does not need to know 
what state its client is in at every point. Likewise, a client does
not need to know its OS agent's state at every point.A client is hungryleads-toits OS agent knowing that its client is
hungry. Theleads-tocaptures the fact that the OS doesn't know
the client's state at the instant that the client transitions from
thinking to hungry.Introduction of TokensWe introduce a token on each edge of the agent communication graph,
see figure 4. The token on an edge \(v, w\) is in one of four states:
held by \(v\), in the channel from \(v\) to \(w\), held by \(w\), or
in the channel from \(w\) to \(v\). Therefore while \(v\) holds this
token it knows that \(w\) doesn't hold it. Likewise, while \(w\) holds
this token it knows that \(v\) doesn't.Fig.7: Fork on each edge of the agent communication
    graphThese tokens are calledforks. (They are calledchopsticksin some papers.) An agent eats only if it holds
forks for all the edges incident on it. Therefore, while an agent eats
none of its neighbors do, and so the safety specification is satisfied.Key Question: When does a hungry agent yield
forks?An eating philosopher holds on to all its forks until it finishes
eating. A thinking philosopher can give a fork to a neighbor that
requests it. So the key question is:Under what conditions should a
hungry neighbor give a fork that it holds to a neighbor that requests
it?Suppose every hungry agent gives a fork to a neighbor that requests
it. Then we can get the scenario shown in the figure below.Fig.8: Scenario when hungry agents yield forksThe figure shows a system with 3 agents indexed 0, 1, 2. The forks are
    shown as small colored circles. The state on
    the left shows agent \(j\) holding the fork that it shares with
    agent \((j+1)\:\textrm{mod}\:3\). If each agent yields the fork to
    its neighbor we get the state on the right in which agent \(j\)
    holds the fork that it shares with 
    agent \((j-1)\:\textrm{mod}\:3\). So, if all hungry agents yield
    forks, the states can alternate for ever between the one on the
left and the one on the right. In this case hungry agents starve: they
    remain hungry forever.If hungry agents don't yield forks then the state on the left persists
for ever. In this case too, hungry agents starve.Creating a Partial Order of PrioritiesLet's assign priorities to agents so that a hungry agent \(v\)
releases a fork to a neighbor \(w\) only if \(v\) has lower priority
than \(w\). If there is a cycle of agents, all with the same priority,
then we get the situation shown in figure 8. So, we will ensure that
priorities form a partial order in all states in all
transitions. Priorities form a partial order exactly when the priority
graph is acyclic. The graph has an edge from \(v\) to \(w\) exactly
when \(v\) has higher priority over \(w\).In the figure below, the diagram on the left shows an acyclic priority
graph while the one on the right shows a graph with a cycle.Fig.9: Priority Graph must be AcyclicHow should Priorities Change?Fig.10: How should priorities change when \(v\)
    eats?How should priorities change when an agent eats so that the priority
graph remains acyclic? For example, consider
the priority graph shown in figure 10. Assume agent \(v\) has all its
forks and is about to eat. Should the directions of edges incident on
\(v\) be flipped? Or should \(v\) have lower priority than all its
neighbors, i.e. all edges incident on \(v\) point towards \(v\)?What happens if we flip the directions of the edges incident on \(v\)?
After the flip, the edges are directed from \(w\), \(x\) and \(y\) towards \(v\), and
from \(v\) to \(u\). But now we have a cycle: \(y\) to \(v\) to \(u\)
to \(w\) to \(y\). So, flipping edge directions doesn't work.What happens if agents adopt the good neighbor policy?
The winner of a conflict gives itself lower priority thanallthe agents
with which it competes. So, an agent that starts eating gives itself lower
priority than all its neighbors. All edges point towards an eating
agent.Fig.11: Winner gets lower priority than its neighborsWhen all edges incident on a vertex point towards the vertex then
there is no cycle through that vertex. So, directing all edges
incident on an eating vertex towards the eating vertex maintains
acyclicity of the graph.For example, in the figure directing all edges incident on vertex \(v\) towards
 \(v\) ensures that no new cycle is created.Agent's priority does not decrease until the agent
eats.How an Agent knows its PriorityWe assign an attributeclean / dirtyto forks.
A fork is eitherdirtyorclean.
The forks held by an eating agent aredirty. When
an agent receives a fork from another agent, the receiving agent
cleans the fork. So the receiver holds acleanfork, and the
fork remains clean until an agent next eats with it.
(This is the "hygenic solution:" Another - sad? - attempt at CS
humor.)An agent holding a dirty fork knows that it has lower priority than
the agent with which it shares the fork. Likewise,
an agent holding a clean fork knows that it has higher priority than
the agent with which it shares the fork.If an agent does not hold the fork that it shares with a neighbor then
the agent does not know its priority with respect to that neighbor.Example of a Fork's LifecycleThe diagram below shows states of a fork shared by agents \(u\) and
\(v\). The red arrow shows priority, and the black arrows show
channels. The blue dot represents the fork.In the figure on the top left, agent \(u\) is hungry and holds a clean
fork. So, \(u\) knows that it has priority over \(v\). At this point
\(v\) does not know whether \(v\) has priority over \(u\) or not.The next figure, at the top right, shows that when \(u\) transits from
hungry to eating, the fork becomes dirty, and \(u\) has lower priority
than \(v\). Agent \(u\) continues to hold the fork while it eats.The next figure, bottom right, shows the situation after \(u\) gets a
request for the fork from \(v\). Because \(u\) got the request from
\(v\) and \(u\) hasn't sent the fork to \(v\), agent \(u\) knows that
\(v\) is hungry. Since the fork is dirty, \(u\) sends
the fork to \(v\). The figure shows the fork in the channel from \(u\)
to \(v\). While the fork is in the channel it doesn't matter
whether the fork is clean or dirty; however, merely for convenience,
let's assume that \(u\), being hygenic, cleans the fork before sending
it to its partner. While the fork is in a
channel the priority doesn't change but neither \(u\) nor \(v\) knows
what the priority is.The next figure, bottom left, shows the situation when \(v\) receives
the fork.  Receiving the fork doesn't change the priority.
At this point \(v\) is hungry and the fork is
clean and so \(v\) knows that it has higher
priority. \(v\) holds on to the fork until it next eats.Fig.12: How an Agent knows its PriorityAlgorithmProperties of Reachable StatesHere is a list of some of the properties of states in all
trajectories.The priority graph is acyclic. \(u\) has priority over a neighbor
  \(v\) exactly when \(u\) holds the fork that it shares with \(v\)
  and the fork is clean, or the fork is in the channel from \(v\) to
  \(u\), or \(v\) holds the fork and the fork is dirty.Eating philosophers hold the forks for all edges incident on them,
  and these forks are dirty.All forks held by thinking philosphers are dirty.Thinking philosophers never send requests and never receive
  forks. Thinking philosophers respond to request for forks by sending
  the requested forks.Initial StatesInitially all philosophers are thinking; all forks are dirty; and all channels are
empty. The forks are placed so that the priority graph is acyclic.
The initial assignment of forks is as follows.
Given an arbitrary acyclic graph, for any edge directed from \(v\) to
\(w\), the fork shared by \(v\) and \(w\) is initially at \(w\) and
the fork is dirty.Algorithm CommandsThe algorithm is specified by the following commands.When a thinking philosophers gets a request for a fork that it holds
  it sends the fork. (A fork held by a thinking philosopher is dirty.)When a thinking philosopher transits to hungry it sends requests for
  all forks that it does not hold.When a hungry philosopher receives a fork, it records the fork as
  being clean. If the hungry philosopher holds all its forks, and if
  it has no request for any dirty fork that it holds, then it transits
  to eating, and records all the forks that it holds in the eating
  state as dirty.When a hungry philosopher receives a request for a fork that it
  holds, it sends the fork if the fork is dirty, and holds on to the
  fork if it is clean.When an eating philosopher receives a request for a fork it
  registers the request in memory, and continues eating while holding
  the fork. When an eating philosopher transits to thinking it sends
  forks for all requests that it received.What could go wrong?The proof of safety is straightforward: Neighbors aren't eating
because neighbors can't hold the same fork at the same time.Before we look at the proof of progress, let's see what may go wrong.Could a group of philosophers exchange forks with each other so that
members of the group eat repeatedly, and starve a philosopher who is
not in the group? For example, in the figure below, could philosophers
\(u, v, w\) exchange forks so that they each eat in turn, and starve
\(y\)?Fig.13: Potential Problems: What could go wrong?Could the system enter a deadlock state in which each hungry philosopher in a
group holds only some --- but not all --- of the forks that it needs
to eat, while other members of the group hold the remaining forks?Proof of CorrectnessThe algorithm is correct. We are required to prove that every hungery
philosopher eats eventually:\(
\forall v: \quad v.h \leadsto v.e
\)where for a philosopher \(v\), \(v.h\) holds exactly when \(v\) is
hungry and \(v.e\) holds exactly when \(v\) is
eating.Variant FunctionTo prove this progress property we find a variant function that
satisfies the following two conditions:Safety: The variant function does not increase while \(v\)
  remains hungry.progress: The following predicate does not hold forever: The
  variant function remains unchanged and \(v\) remains hungry.We propose a variant function which is a pair of integers
 \(nT, nH\),  which are the number of thinking and hungry philosophers,
respectively, of 
higher priority than \(v\).
In terms of the priority graph, \(nT, nH\) are the numbers of thinking
and hungry philosophers (i.e. vertices) with paths to \(v\).Example of Variant FunctionThe figure below shows a portion of the priority graph in a state of the
system. The figure only shows philosophers with higher priority than
philosopher \(v\), i.e., it only shows vertices in the graph with
paths to \(v\). Since eating philosophers have lower priority than
their neighbors, eating philosophers don't appear in this graph.A hungry philosopher is marked with an "H" and a thinking philosopher
with a "T." In the diagram, philosophers \(v, w, x, y\) are hungry and
\(u\) is thinking.
Forks are located at philosophers and are shown as small
colored circles. A dirty fork is colored red and clean one is
blue. For example the fork shared by \(v\) and \(y\) is at \(y\) and
is clean.Fig.15: A Variant Function: Numbers of higher priority
    thinking, hungry agentsExample of Changes to Variant FunctionThe next figure is an example of changes to the variant function. The
diagram on the top left shows the higher priority vertices in the
state of the previous figure. If agent \(x\) eats next the priority
graph transits to the diagram on the top right, and the variant
function \((nT, nH)\) changes from \((1, 3)\) to \((1, 2)\).Fig.16: Example of Values of the Variant FunctionIf agent \(y\) eats next the priority
graph transits to the diagram on the bottom right, and the variant
function \((nT, nH)\) changes from \((1, 2)\) to \((1, 1)\).
If agent \(w\) eats next the priority
graph transits to the diagram on the bottom left, and the variant
function \((nT, nH)\) changes from \((1, 1)\) to \((0, 0)\).Proof that the variant function does not increase
while \(v\) remains hungryIf a philosopher of higher priority than \(v\) transits from thinking
to hungry then \(nT\) decreases. Though \(nH\) increases, the variant function \((nT,
nH)\) decreases because ordering of function values is done
lexicographically.If a philosopher of higher priority than \(v\) transits from hungry
to eating then \(nH\) decreases,  and so the variant function \((nT,
nH)\) decreases.Proof that the following predicate does not
hold forever: The variant function remains unchanged and
\(v\) remains hungry.Let \(w\) be a highest-priority hungry philosopher, i.e. a philosopher with no
hungry philosopher with priority higher than \(w\). (Note: \(w\) may be
the same as \(v\).)
All philosophers with priority higher than \(w\) are thinking. In the
next paragraph we show that either \(w\) gets all its forks and then transits
from hungry to eating, or the variant function decreases.From the algorithm, a hungry philosopher \(w\) requests forks from its
neighbors. From the algorithm, \(w\) eventually gets forks from 
all its lower priority neighbors. A higher priority neighbor \(x\) of
\(w\) is thinking. So when \(x\) gets a request for a fork from \(w\)
either (1) \(x\) sends the requested fork to \(w\) or (2) \(x\) transits from
thinking to hungry in which case the variant function \((nT, nH)\)
decreases.Summary: Key Ideas of this ModuleThis module introduced the problem of distributed mutual exclusion;
showed how the good neighbor policy --- a winning agent reduces its
priority to be lower than all the agents that it competes with --
solves this conflict resolution problem; introduced tokens and what
agents know about other agents holding tokens; and showed a proof
pattern that is one of the most common patterns for proving progress.ReviewWhat is the meaning of mutual exclusion among neighboring
  philosophers?An invariant of the algorithm is that each token is in exactly one
  place: an agent or a channel. How does this invariant help in
  designing the algorithm?An invariant of the algorithm is that the relative priorities among
  agents forms a partial order --- the priority graph is acyclic. What
  can go wrong if the priority graph has cycles?An agent that has a request for a fork releases the fork if the fork
  is dirty, and holds on to the fork if the fork is clean. What could
  go wrong if an agent releases a fork when it gets a request,
  regardless of whether the fork is clean or dirty?K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology



Figure Link: https://kmchandy.github.io/DistributedCollaboration/DiningPhilosophers/Slide03.jpg

Caption: Fig.3: States in Dining Philosophers


The states "Thinking", "Hungry", and "Eating" correspond exactly toOutside critical section,Waiting to enter critical
section, andIn critical section, respectively.Agent Communication StructureFig.4: Communication among AgentsThe commununication structure among agents is represented by an
undirected graph in which the nodes are agents and each edge
represents two channels, one in each direction. The agents are either
OS or client agents. There is one client agent associated with each OS
agent. Clients are shown as squares and OS agents as circles.
For example client agent \(w\) is associated with OS agent
\(w\). The diagram does not show all clients so as not to make the
diagram too crowded.A pair of OS agents are neighbors when there is an edge between
them. A pair of client agents are neighbors when the OS agents with
which they are associated are neighbors. For example, in the figure, 
\(w\) and \(y\) are neighbors.SpecificationSafety: Neighbors do not eat at the same timeLet \(safe\) be the predicateNeighboring clients are
not eating, and let \(init\) be a predicate that holds initially.The safety part of the specification is that \(safe\) holds in every state in every path from
every initial state:\([init
\Rightarrow A(safe)]\)Progress: Hungry agents eat eventuallyThe progress part of the specification is that every hungry agent
transits to eating state eventually.For every agent \(v\):\(
v.state = hungry \quad \leadsto \quad v.state = eating
\)Example of SafetyFig.5: Diagrams illustrating SafetyFigure 5 shows a client eating as a red node depicting the client and
its OS agent. An uncolored node represents a client that is thinking
or hungry. The diagram on the left shows the system in a safe state:
there are no edges between red vertices. The diagram on the right
shows an unsafe state because there are edges between red vertices.The Client's ProgramWe use two tokens that move
between a client and its OS agent. The tokens are called theresource tokenand therequest token. The client's
states are represented by which tokens the client holds.Thinking State: A thinking client holds the request token but
  not the resource token.Transition: Thinking to Hungry: Send the request token to
  the OS.Hungry State: The client holds neither the request nor the
  resource token.Transition Hungry to Eating: The client transits to
  eating when it receives both the request and resource token.Eating: The client holds both the request and resource tokens.Transition from Eating to Thinking:
  The client holds sends the resource token to the OS and continues to
  hold the request token.The figure below illustrates the states of the client. The request
    token is shown as a square and the resource token as a circle.Fig.6: Client's ProgramInitially all clients are thinking; all resource tokens are with OS
agents; and all request tokens are with clients.What the OS KnowsWhile the OS agent holds the request token it knows that its client is
hungry. While the OS agent holds the resource token it knows that its client is
not eating.Fig.7: OS Agent's ProgramWhen a client transits from thinking to hungry it sends its request
token to the agent. When the OS receives the request token, the OS
also has the resource token; so the OS knows that its client is
hungry. There is an interval after the client sends the request token
and before the OS receives it during which the client is hungry but
the OS doesn't know that. An OS agent does not need to know 
what state its client is in at every point. Likewise, a client does
not need to know its OS agent's state at every point.A client is hungryleads-toits OS agent knowing that its client is
hungry. Theleads-tocaptures the fact that the OS doesn't know
the client's state at the instant that the client transitions from
thinking to hungry.Introduction of TokensWe introduce a token on each edge of the agent communication graph,
see figure 4. The token on an edge \(v, w\) is in one of four states:
held by \(v\), in the channel from \(v\) to \(w\), held by \(w\), or
in the channel from \(w\) to \(v\). Therefore while \(v\) holds this
token it knows that \(w\) doesn't hold it. Likewise, while \(w\) holds
this token it knows that \(v\) doesn't.Fig.7: Fork on each edge of the agent communication
    graphThese tokens are calledforks. (They are calledchopsticksin some papers.) An agent eats only if it holds
forks for all the edges incident on it. Therefore, while an agent eats
none of its neighbors do, and so the safety specification is satisfied.Key Question: When does a hungry agent yield
forks?An eating philosopher holds on to all its forks until it finishes
eating. A thinking philosopher can give a fork to a neighbor that
requests it. So the key question is:Under what conditions should a
hungry neighbor give a fork that it holds to a neighbor that requests
it?Suppose every hungry agent gives a fork to a neighbor that requests
it. Then we can get the scenario shown in the figure below.Fig.8: Scenario when hungry agents yield forksThe figure shows a system with 3 agents indexed 0, 1, 2. The forks are
    shown as small colored circles. The state on
    the left shows agent \(j\) holding the fork that it shares with
    agent \((j+1)\:\textrm{mod}\:3\). If each agent yields the fork to
    its neighbor we get the state on the right in which agent \(j\)
    holds the fork that it shares with 
    agent \((j-1)\:\textrm{mod}\:3\). So, if all hungry agents yield
    forks, the states can alternate for ever between the one on the
left and the one on the right. In this case hungry agents starve: they
    remain hungry forever.If hungry agents don't yield forks then the state on the left persists
for ever. In this case too, hungry agents starve.Creating a Partial Order of PrioritiesLet's assign priorities to agents so that a hungry agent \(v\)
releases a fork to a neighbor \(w\) only if \(v\) has lower priority
than \(w\). If there is a cycle of agents, all with the same priority,
then we get the situation shown in figure 8. So, we will ensure that
priorities form a partial order in all states in all
transitions. Priorities form a partial order exactly when the priority
graph is acyclic. The graph has an edge from \(v\) to \(w\) exactly
when \(v\) has higher priority over \(w\).In the figure below, the diagram on the left shows an acyclic priority
graph while the one on the right shows a graph with a cycle.Fig.9: Priority Graph must be AcyclicHow should Priorities Change?Fig.10: How should priorities change when \(v\)
    eats?How should priorities change when an agent eats so that the priority
graph remains acyclic? For example, consider
the priority graph shown in figure 10. Assume agent \(v\) has all its
forks and is about to eat. Should the directions of edges incident on
\(v\) be flipped? Or should \(v\) have lower priority than all its
neighbors, i.e. all edges incident on \(v\) point towards \(v\)?What happens if we flip the directions of the edges incident on \(v\)?
After the flip, the edges are directed from \(w\), \(x\) and \(y\) towards \(v\), and
from \(v\) to \(u\). But now we have a cycle: \(y\) to \(v\) to \(u\)
to \(w\) to \(y\). So, flipping edge directions doesn't work.What happens if agents adopt the good neighbor policy?
The winner of a conflict gives itself lower priority thanallthe agents
with which it competes. So, an agent that starts eating gives itself lower
priority than all its neighbors. All edges point towards an eating
agent.Fig.11: Winner gets lower priority than its neighborsWhen all edges incident on a vertex point towards the vertex then
there is no cycle through that vertex. So, directing all edges
incident on an eating vertex towards the eating vertex maintains
acyclicity of the graph.For example, in the figure directing all edges incident on vertex \(v\) towards
 \(v\) ensures that no new cycle is created.Agent's priority does not decrease until the agent
eats.How an Agent knows its PriorityWe assign an attributeclean / dirtyto forks.
A fork is eitherdirtyorclean.
The forks held by an eating agent aredirty. When
an agent receives a fork from another agent, the receiving agent
cleans the fork. So the receiver holds acleanfork, and the
fork remains clean until an agent next eats with it.
(This is the "hygenic solution:" Another - sad? - attempt at CS
humor.)An agent holding a dirty fork knows that it has lower priority than
the agent with which it shares the fork. Likewise,
an agent holding a clean fork knows that it has higher priority than
the agent with which it shares the fork.If an agent does not hold the fork that it shares with a neighbor then
the agent does not know its priority with respect to that neighbor.Example of a Fork's LifecycleThe diagram below shows states of a fork shared by agents \(u\) and
\(v\). The red arrow shows priority, and the black arrows show
channels. The blue dot represents the fork.In the figure on the top left, agent \(u\) is hungry and holds a clean
fork. So, \(u\) knows that it has priority over \(v\). At this point
\(v\) does not know whether \(v\) has priority over \(u\) or not.The next figure, at the top right, shows that when \(u\) transits from
hungry to eating, the fork becomes dirty, and \(u\) has lower priority
than \(v\). Agent \(u\) continues to hold the fork while it eats.The next figure, bottom right, shows the situation after \(u\) gets a
request for the fork from \(v\). Because \(u\) got the request from
\(v\) and \(u\) hasn't sent the fork to \(v\), agent \(u\) knows that
\(v\) is hungry. Since the fork is dirty, \(u\) sends
the fork to \(v\). The figure shows the fork in the channel from \(u\)
to \(v\). While the fork is in the channel it doesn't matter
whether the fork is clean or dirty; however, merely for convenience,
let's assume that \(u\), being hygenic, cleans the fork before sending
it to its partner. While the fork is in a
channel the priority doesn't change but neither \(u\) nor \(v\) knows
what the priority is.The next figure, bottom left, shows the situation when \(v\) receives
the fork.  Receiving the fork doesn't change the priority.
At this point \(v\) is hungry and the fork is
clean and so \(v\) knows that it has higher
priority. \(v\) holds on to the fork until it next eats.Fig.12: How an Agent knows its PriorityAlgorithmProperties of Reachable StatesHere is a list of some of the properties of states in all
trajectories.The priority graph is acyclic. \(u\) has priority over a neighbor
  \(v\) exactly when \(u\) holds the fork that it shares with \(v\)
  and the fork is clean, or the fork is in the channel from \(v\) to
  \(u\), or \(v\) holds the fork and the fork is dirty.Eating philosophers hold the forks for all edges incident on them,
  and these forks are dirty.All forks held by thinking philosphers are dirty.Thinking philosophers never send requests and never receive
  forks. Thinking philosophers respond to request for forks by sending
  the requested forks.Initial StatesInitially all philosophers are thinking; all forks are dirty; and all channels are
empty. The forks are placed so that the priority graph is acyclic.
The initial assignment of forks is as follows.
Given an arbitrary acyclic graph, for any edge directed from \(v\) to
\(w\), the fork shared by \(v\) and \(w\) is initially at \(w\) and
the fork is dirty.Algorithm CommandsThe algorithm is specified by the following commands.When a thinking philosophers gets a request for a fork that it holds
  it sends the fork. (A fork held by a thinking philosopher is dirty.)When a thinking philosopher transits to hungry it sends requests for
  all forks that it does not hold.When a hungry philosopher receives a fork, it records the fork as
  being clean. If the hungry philosopher holds all its forks, and if
  it has no request for any dirty fork that it holds, then it transits
  to eating, and records all the forks that it holds in the eating
  state as dirty.When a hungry philosopher receives a request for a fork that it
  holds, it sends the fork if the fork is dirty, and holds on to the
  fork if it is clean.When an eating philosopher receives a request for a fork it
  registers the request in memory, and continues eating while holding
  the fork. When an eating philosopher transits to thinking it sends
  forks for all requests that it received.What could go wrong?The proof of safety is straightforward: Neighbors aren't eating
because neighbors can't hold the same fork at the same time.Before we look at the proof of progress, let's see what may go wrong.Could a group of philosophers exchange forks with each other so that
members of the group eat repeatedly, and starve a philosopher who is
not in the group? For example, in the figure below, could philosophers
\(u, v, w\) exchange forks so that they each eat in turn, and starve
\(y\)?Fig.13: Potential Problems: What could go wrong?Could the system enter a deadlock state in which each hungry philosopher in a
group holds only some --- but not all --- of the forks that it needs
to eat, while other members of the group hold the remaining forks?Proof of CorrectnessThe algorithm is correct. We are required to prove that every hungery
philosopher eats eventually:\(
\forall v: \quad v.h \leadsto v.e
\)where for a philosopher \(v\), \(v.h\) holds exactly when \(v\) is
hungry and \(v.e\) holds exactly when \(v\) is
eating.Variant FunctionTo prove this progress property we find a variant function that
satisfies the following two conditions:Safety: The variant function does not increase while \(v\)
  remains hungry.progress: The following predicate does not hold forever: The
  variant function remains unchanged and \(v\) remains hungry.We propose a variant function which is a pair of integers
 \(nT, nH\),  which are the number of thinking and hungry philosophers,
respectively, of 
higher priority than \(v\).
In terms of the priority graph, \(nT, nH\) are the numbers of thinking
and hungry philosophers (i.e. vertices) with paths to \(v\).Example of Variant FunctionThe figure below shows a portion of the priority graph in a state of the
system. The figure only shows philosophers with higher priority than
philosopher \(v\), i.e., it only shows vertices in the graph with
paths to \(v\). Since eating philosophers have lower priority than
their neighbors, eating philosophers don't appear in this graph.A hungry philosopher is marked with an "H" and a thinking philosopher
with a "T." In the diagram, philosophers \(v, w, x, y\) are hungry and
\(u\) is thinking.
Forks are located at philosophers and are shown as small
colored circles. A dirty fork is colored red and clean one is
blue. For example the fork shared by \(v\) and \(y\) is at \(y\) and
is clean.Fig.15: A Variant Function: Numbers of higher priority
    thinking, hungry agentsExample of Changes to Variant FunctionThe next figure is an example of changes to the variant function. The
diagram on the top left shows the higher priority vertices in the
state of the previous figure. If agent \(x\) eats next the priority
graph transits to the diagram on the top right, and the variant
function \((nT, nH)\) changes from \((1, 3)\) to \((1, 2)\).Fig.16: Example of Values of the Variant FunctionIf agent \(y\) eats next the priority
graph transits to the diagram on the bottom right, and the variant
function \((nT, nH)\) changes from \((1, 2)\) to \((1, 1)\).
If agent \(w\) eats next the priority
graph transits to the diagram on the bottom left, and the variant
function \((nT, nH)\) changes from \((1, 1)\) to \((0, 0)\).Proof that the variant function does not increase
while \(v\) remains hungryIf a philosopher of higher priority than \(v\) transits from thinking
to hungry then \(nT\) decreases. Though \(nH\) increases, the variant function \((nT,
nH)\) decreases because ordering of function values is done
lexicographically.If a philosopher of higher priority than \(v\) transits from hungry
to eating then \(nH\) decreases,  and so the variant function \((nT,
nH)\) decreases.Proof that the following predicate does not
hold forever: The variant function remains unchanged and
\(v\) remains hungry.Let \(w\) be a highest-priority hungry philosopher, i.e. a philosopher with no
hungry philosopher with priority higher than \(w\). (Note: \(w\) may be
the same as \(v\).)
All philosophers with priority higher than \(w\) are thinking. In the
next paragraph we show that either \(w\) gets all its forks and then transits
from hungry to eating, or the variant function decreases.From the algorithm, a hungry philosopher \(w\) requests forks from its
neighbors. From the algorithm, \(w\) eventually gets forks from 
all its lower priority neighbors. A higher priority neighbor \(x\) of
\(w\) is thinking. So when \(x\) gets a request for a fork from \(w\)
either (1) \(x\) sends the requested fork to \(w\) or (2) \(x\) transits from
thinking to hungry in which case the variant function \((nT, nH)\)
decreases.Summary: Key Ideas of this ModuleThis module introduced the problem of distributed mutual exclusion;
showed how the good neighbor policy --- a winning agent reduces its
priority to be lower than all the agents that it competes with --
solves this conflict resolution problem; introduced tokens and what
agents know about other agents holding tokens; and showed a proof
pattern that is one of the most common patterns for proving progress.ReviewWhat is the meaning of mutual exclusion among neighboring
  philosophers?An invariant of the algorithm is that each token is in exactly one
  place: an agent or a channel. How does this invariant help in
  designing the algorithm?An invariant of the algorithm is that the relative priorities among
  agents forms a partial order --- the priority graph is acyclic. What
  can go wrong if the priority graph has cycles?An agent that has a request for a fork releases the fork if the fork
  is dirty, and holds on to the fork if the fork is clean. What could
  go wrong if an agent releases a fork when it gets a request,
  regardless of whether the fork is clean or dirty?K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology



Figure Link: https://kmchandy.github.io/DistributedCollaboration/DiningPhilosophers/Slide04.jpg

Caption: Fig.4: Communication among Agents


A pair of OS agents are neighbors when there is an edge between
them. A pair of client agents are neighbors when the OS agents with
which they are associated are neighbors. For example, in the figure, 
\(w\) and \(y\) are neighbors.SpecificationSafety: Neighbors do not eat at the same timeLet \(safe\) be the predicateNeighboring clients are
not eating, and let \(init\) be a predicate that holds initially.The safety part of the specification is that \(safe\) holds in every state in every path from
every initial state:\([init
\Rightarrow A(safe)]\)Progress: Hungry agents eat eventuallyThe progress part of the specification is that every hungry agent
transits to eating state eventually.For every agent \(v\):\(
v.state = hungry \quad \leadsto \quad v.state = eating
\)Example of SafetyFig.5: Diagrams illustrating SafetyFigure 5 shows a client eating as a red node depicting the client and
its OS agent. An uncolored node represents a client that is thinking
or hungry. The diagram on the left shows the system in a safe state:
there are no edges between red vertices. The diagram on the right
shows an unsafe state because there are edges between red vertices.The Client's ProgramWe use two tokens that move
between a client and its OS agent. The tokens are called theresource tokenand therequest token. The client's
states are represented by which tokens the client holds.Thinking State: A thinking client holds the request token but
  not the resource token.Transition: Thinking to Hungry: Send the request token to
  the OS.Hungry State: The client holds neither the request nor the
  resource token.Transition Hungry to Eating: The client transits to
  eating when it receives both the request and resource token.Eating: The client holds both the request and resource tokens.Transition from Eating to Thinking:
  The client holds sends the resource token to the OS and continues to
  hold the request token.The figure below illustrates the states of the client. The request
    token is shown as a square and the resource token as a circle.Fig.6: Client's ProgramInitially all clients are thinking; all resource tokens are with OS
agents; and all request tokens are with clients.What the OS KnowsWhile the OS agent holds the request token it knows that its client is
hungry. While the OS agent holds the resource token it knows that its client is
not eating.Fig.7: OS Agent's ProgramWhen a client transits from thinking to hungry it sends its request
token to the agent. When the OS receives the request token, the OS
also has the resource token; so the OS knows that its client is
hungry. There is an interval after the client sends the request token
and before the OS receives it during which the client is hungry but
the OS doesn't know that. An OS agent does not need to know 
what state its client is in at every point. Likewise, a client does
not need to know its OS agent's state at every point.A client is hungryleads-toits OS agent knowing that its client is
hungry. Theleads-tocaptures the fact that the OS doesn't know
the client's state at the instant that the client transitions from
thinking to hungry.Introduction of TokensWe introduce a token on each edge of the agent communication graph,
see figure 4. The token on an edge \(v, w\) is in one of four states:
held by \(v\), in the channel from \(v\) to \(w\), held by \(w\), or
in the channel from \(w\) to \(v\). Therefore while \(v\) holds this
token it knows that \(w\) doesn't hold it. Likewise, while \(w\) holds
this token it knows that \(v\) doesn't.Fig.7: Fork on each edge of the agent communication
    graphThese tokens are calledforks. (They are calledchopsticksin some papers.) An agent eats only if it holds
forks for all the edges incident on it. Therefore, while an agent eats
none of its neighbors do, and so the safety specification is satisfied.Key Question: When does a hungry agent yield
forks?An eating philosopher holds on to all its forks until it finishes
eating. A thinking philosopher can give a fork to a neighbor that
requests it. So the key question is:Under what conditions should a
hungry neighbor give a fork that it holds to a neighbor that requests
it?Suppose every hungry agent gives a fork to a neighbor that requests
it. Then we can get the scenario shown in the figure below.Fig.8: Scenario when hungry agents yield forksThe figure shows a system with 3 agents indexed 0, 1, 2. The forks are
    shown as small colored circles. The state on
    the left shows agent \(j\) holding the fork that it shares with
    agent \((j+1)\:\textrm{mod}\:3\). If each agent yields the fork to
    its neighbor we get the state on the right in which agent \(j\)
    holds the fork that it shares with 
    agent \((j-1)\:\textrm{mod}\:3\). So, if all hungry agents yield
    forks, the states can alternate for ever between the one on the
left and the one on the right. In this case hungry agents starve: they
    remain hungry forever.If hungry agents don't yield forks then the state on the left persists
for ever. In this case too, hungry agents starve.Creating a Partial Order of PrioritiesLet's assign priorities to agents so that a hungry agent \(v\)
releases a fork to a neighbor \(w\) only if \(v\) has lower priority
than \(w\). If there is a cycle of agents, all with the same priority,
then we get the situation shown in figure 8. So, we will ensure that
priorities form a partial order in all states in all
transitions. Priorities form a partial order exactly when the priority
graph is acyclic. The graph has an edge from \(v\) to \(w\) exactly
when \(v\) has higher priority over \(w\).In the figure below, the diagram on the left shows an acyclic priority
graph while the one on the right shows a graph with a cycle.Fig.9: Priority Graph must be AcyclicHow should Priorities Change?Fig.10: How should priorities change when \(v\)
    eats?How should priorities change when an agent eats so that the priority
graph remains acyclic? For example, consider
the priority graph shown in figure 10. Assume agent \(v\) has all its
forks and is about to eat. Should the directions of edges incident on
\(v\) be flipped? Or should \(v\) have lower priority than all its
neighbors, i.e. all edges incident on \(v\) point towards \(v\)?What happens if we flip the directions of the edges incident on \(v\)?
After the flip, the edges are directed from \(w\), \(x\) and \(y\) towards \(v\), and
from \(v\) to \(u\). But now we have a cycle: \(y\) to \(v\) to \(u\)
to \(w\) to \(y\). So, flipping edge directions doesn't work.What happens if agents adopt the good neighbor policy?
The winner of a conflict gives itself lower priority thanallthe agents
with which it competes. So, an agent that starts eating gives itself lower
priority than all its neighbors. All edges point towards an eating
agent.Fig.11: Winner gets lower priority than its neighborsWhen all edges incident on a vertex point towards the vertex then
there is no cycle through that vertex. So, directing all edges
incident on an eating vertex towards the eating vertex maintains
acyclicity of the graph.For example, in the figure directing all edges incident on vertex \(v\) towards
 \(v\) ensures that no new cycle is created.Agent's priority does not decrease until the agent
eats.How an Agent knows its PriorityWe assign an attributeclean / dirtyto forks.
A fork is eitherdirtyorclean.
The forks held by an eating agent aredirty. When
an agent receives a fork from another agent, the receiving agent
cleans the fork. So the receiver holds acleanfork, and the
fork remains clean until an agent next eats with it.
(This is the "hygenic solution:" Another - sad? - attempt at CS
humor.)An agent holding a dirty fork knows that it has lower priority than
the agent with which it shares the fork. Likewise,
an agent holding a clean fork knows that it has higher priority than
the agent with which it shares the fork.If an agent does not hold the fork that it shares with a neighbor then
the agent does not know its priority with respect to that neighbor.Example of a Fork's LifecycleThe diagram below shows states of a fork shared by agents \(u\) and
\(v\). The red arrow shows priority, and the black arrows show
channels. The blue dot represents the fork.In the figure on the top left, agent \(u\) is hungry and holds a clean
fork. So, \(u\) knows that it has priority over \(v\). At this point
\(v\) does not know whether \(v\) has priority over \(u\) or not.The next figure, at the top right, shows that when \(u\) transits from
hungry to eating, the fork becomes dirty, and \(u\) has lower priority
than \(v\). Agent \(u\) continues to hold the fork while it eats.The next figure, bottom right, shows the situation after \(u\) gets a
request for the fork from \(v\). Because \(u\) got the request from
\(v\) and \(u\) hasn't sent the fork to \(v\), agent \(u\) knows that
\(v\) is hungry. Since the fork is dirty, \(u\) sends
the fork to \(v\). The figure shows the fork in the channel from \(u\)
to \(v\). While the fork is in the channel it doesn't matter
whether the fork is clean or dirty; however, merely for convenience,
let's assume that \(u\), being hygenic, cleans the fork before sending
it to its partner. While the fork is in a
channel the priority doesn't change but neither \(u\) nor \(v\) knows
what the priority is.The next figure, bottom left, shows the situation when \(v\) receives
the fork.  Receiving the fork doesn't change the priority.
At this point \(v\) is hungry and the fork is
clean and so \(v\) knows that it has higher
priority. \(v\) holds on to the fork until it next eats.Fig.12: How an Agent knows its PriorityAlgorithmProperties of Reachable StatesHere is a list of some of the properties of states in all
trajectories.The priority graph is acyclic. \(u\) has priority over a neighbor
  \(v\) exactly when \(u\) holds the fork that it shares with \(v\)
  and the fork is clean, or the fork is in the channel from \(v\) to
  \(u\), or \(v\) holds the fork and the fork is dirty.Eating philosophers hold the forks for all edges incident on them,
  and these forks are dirty.All forks held by thinking philosphers are dirty.Thinking philosophers never send requests and never receive
  forks. Thinking philosophers respond to request for forks by sending
  the requested forks.Initial StatesInitially all philosophers are thinking; all forks are dirty; and all channels are
empty. The forks are placed so that the priority graph is acyclic.
The initial assignment of forks is as follows.
Given an arbitrary acyclic graph, for any edge directed from \(v\) to
\(w\), the fork shared by \(v\) and \(w\) is initially at \(w\) and
the fork is dirty.Algorithm CommandsThe algorithm is specified by the following commands.When a thinking philosophers gets a request for a fork that it holds
  it sends the fork. (A fork held by a thinking philosopher is dirty.)When a thinking philosopher transits to hungry it sends requests for
  all forks that it does not hold.When a hungry philosopher receives a fork, it records the fork as
  being clean. If the hungry philosopher holds all its forks, and if
  it has no request for any dirty fork that it holds, then it transits
  to eating, and records all the forks that it holds in the eating
  state as dirty.When a hungry philosopher receives a request for a fork that it
  holds, it sends the fork if the fork is dirty, and holds on to the
  fork if it is clean.When an eating philosopher receives a request for a fork it
  registers the request in memory, and continues eating while holding
  the fork. When an eating philosopher transits to thinking it sends
  forks for all requests that it received.What could go wrong?The proof of safety is straightforward: Neighbors aren't eating
because neighbors can't hold the same fork at the same time.Before we look at the proof of progress, let's see what may go wrong.Could a group of philosophers exchange forks with each other so that
members of the group eat repeatedly, and starve a philosopher who is
not in the group? For example, in the figure below, could philosophers
\(u, v, w\) exchange forks so that they each eat in turn, and starve
\(y\)?Fig.13: Potential Problems: What could go wrong?Could the system enter a deadlock state in which each hungry philosopher in a
group holds only some --- but not all --- of the forks that it needs
to eat, while other members of the group hold the remaining forks?Proof of CorrectnessThe algorithm is correct. We are required to prove that every hungery
philosopher eats eventually:\(
\forall v: \quad v.h \leadsto v.e
\)where for a philosopher \(v\), \(v.h\) holds exactly when \(v\) is
hungry and \(v.e\) holds exactly when \(v\) is
eating.Variant FunctionTo prove this progress property we find a variant function that
satisfies the following two conditions:Safety: The variant function does not increase while \(v\)
  remains hungry.progress: The following predicate does not hold forever: The
  variant function remains unchanged and \(v\) remains hungry.We propose a variant function which is a pair of integers
 \(nT, nH\),  which are the number of thinking and hungry philosophers,
respectively, of 
higher priority than \(v\).
In terms of the priority graph, \(nT, nH\) are the numbers of thinking
and hungry philosophers (i.e. vertices) with paths to \(v\).Example of Variant FunctionThe figure below shows a portion of the priority graph in a state of the
system. The figure only shows philosophers with higher priority than
philosopher \(v\), i.e., it only shows vertices in the graph with
paths to \(v\). Since eating philosophers have lower priority than
their neighbors, eating philosophers don't appear in this graph.A hungry philosopher is marked with an "H" and a thinking philosopher
with a "T." In the diagram, philosophers \(v, w, x, y\) are hungry and
\(u\) is thinking.
Forks are located at philosophers and are shown as small
colored circles. A dirty fork is colored red and clean one is
blue. For example the fork shared by \(v\) and \(y\) is at \(y\) and
is clean.Fig.15: A Variant Function: Numbers of higher priority
    thinking, hungry agentsExample of Changes to Variant FunctionThe next figure is an example of changes to the variant function. The
diagram on the top left shows the higher priority vertices in the
state of the previous figure. If agent \(x\) eats next the priority
graph transits to the diagram on the top right, and the variant
function \((nT, nH)\) changes from \((1, 3)\) to \((1, 2)\).Fig.16: Example of Values of the Variant FunctionIf agent \(y\) eats next the priority
graph transits to the diagram on the bottom right, and the variant
function \((nT, nH)\) changes from \((1, 2)\) to \((1, 1)\).
If agent \(w\) eats next the priority
graph transits to the diagram on the bottom left, and the variant
function \((nT, nH)\) changes from \((1, 1)\) to \((0, 0)\).Proof that the variant function does not increase
while \(v\) remains hungryIf a philosopher of higher priority than \(v\) transits from thinking
to hungry then \(nT\) decreases. Though \(nH\) increases, the variant function \((nT,
nH)\) decreases because ordering of function values is done
lexicographically.If a philosopher of higher priority than \(v\) transits from hungry
to eating then \(nH\) decreases,  and so the variant function \((nT,
nH)\) decreases.Proof that the following predicate does not
hold forever: The variant function remains unchanged and
\(v\) remains hungry.Let \(w\) be a highest-priority hungry philosopher, i.e. a philosopher with no
hungry philosopher with priority higher than \(w\). (Note: \(w\) may be
the same as \(v\).)
All philosophers with priority higher than \(w\) are thinking. In the
next paragraph we show that either \(w\) gets all its forks and then transits
from hungry to eating, or the variant function decreases.From the algorithm, a hungry philosopher \(w\) requests forks from its
neighbors. From the algorithm, \(w\) eventually gets forks from 
all its lower priority neighbors. A higher priority neighbor \(x\) of
\(w\) is thinking. So when \(x\) gets a request for a fork from \(w\)
either (1) \(x\) sends the requested fork to \(w\) or (2) \(x\) transits from
thinking to hungry in which case the variant function \((nT, nH)\)
decreases.Summary: Key Ideas of this ModuleThis module introduced the problem of distributed mutual exclusion;
showed how the good neighbor policy --- a winning agent reduces its
priority to be lower than all the agents that it competes with --
solves this conflict resolution problem; introduced tokens and what
agents know about other agents holding tokens; and showed a proof
pattern that is one of the most common patterns for proving progress.ReviewWhat is the meaning of mutual exclusion among neighboring
  philosophers?An invariant of the algorithm is that each token is in exactly one
  place: an agent or a channel. How does this invariant help in
  designing the algorithm?An invariant of the algorithm is that the relative priorities among
  agents forms a partial order --- the priority graph is acyclic. What
  can go wrong if the priority graph has cycles?An agent that has a request for a fork releases the fork if the fork
  is dirty, and holds on to the fork if the fork is clean. What could
  go wrong if an agent releases a fork when it gets a request,
  regardless of whether the fork is clean or dirty?K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology



Figure Link: https://kmchandy.github.io/DistributedCollaboration/DiningPhilosophers/Slide05.jpg

Caption: Fig.5: Diagrams illustrating Safety


The safety part of the specification is that \(safe\) holds in every state in every path from
every initial state:\([init
\Rightarrow A(safe)]\)Progress: Hungry agents eat eventuallyThe progress part of the specification is that every hungry agent
transits to eating state eventually.For every agent \(v\):\(
v.state = hungry \quad \leadsto \quad v.state = eating
\)Example of SafetyFig.5: Diagrams illustrating SafetyFigure 5 shows a client eating as a red node depicting the client and
its OS agent. An uncolored node represents a client that is thinking
or hungry. The diagram on the left shows the system in a safe state:
there are no edges between red vertices. The diagram on the right
shows an unsafe state because there are edges between red vertices.The Client's ProgramWe use two tokens that move
between a client and its OS agent. The tokens are called theresource tokenand therequest token. The client's
states are represented by which tokens the client holds.Thinking State: A thinking client holds the request token but
  not the resource token.Transition: Thinking to Hungry: Send the request token to
  the OS.Hungry State: The client holds neither the request nor the
  resource token.Transition Hungry to Eating: The client transits to
  eating when it receives both the request and resource token.Eating: The client holds both the request and resource tokens.Transition from Eating to Thinking:
  The client holds sends the resource token to the OS and continues to
  hold the request token.The figure below illustrates the states of the client. The request
    token is shown as a square and the resource token as a circle.Fig.6: Client's ProgramInitially all clients are thinking; all resource tokens are with OS
agents; and all request tokens are with clients.What the OS KnowsWhile the OS agent holds the request token it knows that its client is
hungry. While the OS agent holds the resource token it knows that its client is
not eating.Fig.7: OS Agent's ProgramWhen a client transits from thinking to hungry it sends its request
token to the agent. When the OS receives the request token, the OS
also has the resource token; so the OS knows that its client is
hungry. There is an interval after the client sends the request token
and before the OS receives it during which the client is hungry but
the OS doesn't know that. An OS agent does not need to know 
what state its client is in at every point. Likewise, a client does
not need to know its OS agent's state at every point.A client is hungryleads-toits OS agent knowing that its client is
hungry. Theleads-tocaptures the fact that the OS doesn't know
the client's state at the instant that the client transitions from
thinking to hungry.Introduction of TokensWe introduce a token on each edge of the agent communication graph,
see figure 4. The token on an edge \(v, w\) is in one of four states:
held by \(v\), in the channel from \(v\) to \(w\), held by \(w\), or
in the channel from \(w\) to \(v\). Therefore while \(v\) holds this
token it knows that \(w\) doesn't hold it. Likewise, while \(w\) holds
this token it knows that \(v\) doesn't.Fig.7: Fork on each edge of the agent communication
    graphThese tokens are calledforks. (They are calledchopsticksin some papers.) An agent eats only if it holds
forks for all the edges incident on it. Therefore, while an agent eats
none of its neighbors do, and so the safety specification is satisfied.Key Question: When does a hungry agent yield
forks?An eating philosopher holds on to all its forks until it finishes
eating. A thinking philosopher can give a fork to a neighbor that
requests it. So the key question is:Under what conditions should a
hungry neighbor give a fork that it holds to a neighbor that requests
it?Suppose every hungry agent gives a fork to a neighbor that requests
it. Then we can get the scenario shown in the figure below.Fig.8: Scenario when hungry agents yield forksThe figure shows a system with 3 agents indexed 0, 1, 2. The forks are
    shown as small colored circles. The state on
    the left shows agent \(j\) holding the fork that it shares with
    agent \((j+1)\:\textrm{mod}\:3\). If each agent yields the fork to
    its neighbor we get the state on the right in which agent \(j\)
    holds the fork that it shares with 
    agent \((j-1)\:\textrm{mod}\:3\). So, if all hungry agents yield
    forks, the states can alternate for ever between the one on the
left and the one on the right. In this case hungry agents starve: they
    remain hungry forever.If hungry agents don't yield forks then the state on the left persists
for ever. In this case too, hungry agents starve.Creating a Partial Order of PrioritiesLet's assign priorities to agents so that a hungry agent \(v\)
releases a fork to a neighbor \(w\) only if \(v\) has lower priority
than \(w\). If there is a cycle of agents, all with the same priority,
then we get the situation shown in figure 8. So, we will ensure that
priorities form a partial order in all states in all
transitions. Priorities form a partial order exactly when the priority
graph is acyclic. The graph has an edge from \(v\) to \(w\) exactly
when \(v\) has higher priority over \(w\).In the figure below, the diagram on the left shows an acyclic priority
graph while the one on the right shows a graph with a cycle.Fig.9: Priority Graph must be AcyclicHow should Priorities Change?Fig.10: How should priorities change when \(v\)
    eats?How should priorities change when an agent eats so that the priority
graph remains acyclic? For example, consider
the priority graph shown in figure 10. Assume agent \(v\) has all its
forks and is about to eat. Should the directions of edges incident on
\(v\) be flipped? Or should \(v\) have lower priority than all its
neighbors, i.e. all edges incident on \(v\) point towards \(v\)?What happens if we flip the directions of the edges incident on \(v\)?
After the flip, the edges are directed from \(w\), \(x\) and \(y\) towards \(v\), and
from \(v\) to \(u\). But now we have a cycle: \(y\) to \(v\) to \(u\)
to \(w\) to \(y\). So, flipping edge directions doesn't work.What happens if agents adopt the good neighbor policy?
The winner of a conflict gives itself lower priority thanallthe agents
with which it competes. So, an agent that starts eating gives itself lower
priority than all its neighbors. All edges point towards an eating
agent.Fig.11: Winner gets lower priority than its neighborsWhen all edges incident on a vertex point towards the vertex then
there is no cycle through that vertex. So, directing all edges
incident on an eating vertex towards the eating vertex maintains
acyclicity of the graph.For example, in the figure directing all edges incident on vertex \(v\) towards
 \(v\) ensures that no new cycle is created.Agent's priority does not decrease until the agent
eats.How an Agent knows its PriorityWe assign an attributeclean / dirtyto forks.
A fork is eitherdirtyorclean.
The forks held by an eating agent aredirty. When
an agent receives a fork from another agent, the receiving agent
cleans the fork. So the receiver holds acleanfork, and the
fork remains clean until an agent next eats with it.
(This is the "hygenic solution:" Another - sad? - attempt at CS
humor.)An agent holding a dirty fork knows that it has lower priority than
the agent with which it shares the fork. Likewise,
an agent holding a clean fork knows that it has higher priority than
the agent with which it shares the fork.If an agent does not hold the fork that it shares with a neighbor then
the agent does not know its priority with respect to that neighbor.Example of a Fork's LifecycleThe diagram below shows states of a fork shared by agents \(u\) and
\(v\). The red arrow shows priority, and the black arrows show
channels. The blue dot represents the fork.In the figure on the top left, agent \(u\) is hungry and holds a clean
fork. So, \(u\) knows that it has priority over \(v\). At this point
\(v\) does not know whether \(v\) has priority over \(u\) or not.The next figure, at the top right, shows that when \(u\) transits from
hungry to eating, the fork becomes dirty, and \(u\) has lower priority
than \(v\). Agent \(u\) continues to hold the fork while it eats.The next figure, bottom right, shows the situation after \(u\) gets a
request for the fork from \(v\). Because \(u\) got the request from
\(v\) and \(u\) hasn't sent the fork to \(v\), agent \(u\) knows that
\(v\) is hungry. Since the fork is dirty, \(u\) sends
the fork to \(v\). The figure shows the fork in the channel from \(u\)
to \(v\). While the fork is in the channel it doesn't matter
whether the fork is clean or dirty; however, merely for convenience,
let's assume that \(u\), being hygenic, cleans the fork before sending
it to its partner. While the fork is in a
channel the priority doesn't change but neither \(u\) nor \(v\) knows
what the priority is.The next figure, bottom left, shows the situation when \(v\) receives
the fork.  Receiving the fork doesn't change the priority.
At this point \(v\) is hungry and the fork is
clean and so \(v\) knows that it has higher
priority. \(v\) holds on to the fork until it next eats.Fig.12: How an Agent knows its PriorityAlgorithmProperties of Reachable StatesHere is a list of some of the properties of states in all
trajectories.The priority graph is acyclic. \(u\) has priority over a neighbor
  \(v\) exactly when \(u\) holds the fork that it shares with \(v\)
  and the fork is clean, or the fork is in the channel from \(v\) to
  \(u\), or \(v\) holds the fork and the fork is dirty.Eating philosophers hold the forks for all edges incident on them,
  and these forks are dirty.All forks held by thinking philosphers are dirty.Thinking philosophers never send requests and never receive
  forks. Thinking philosophers respond to request for forks by sending
  the requested forks.Initial StatesInitially all philosophers are thinking; all forks are dirty; and all channels are
empty. The forks are placed so that the priority graph is acyclic.
The initial assignment of forks is as follows.
Given an arbitrary acyclic graph, for any edge directed from \(v\) to
\(w\), the fork shared by \(v\) and \(w\) is initially at \(w\) and
the fork is dirty.Algorithm CommandsThe algorithm is specified by the following commands.When a thinking philosophers gets a request for a fork that it holds
  it sends the fork. (A fork held by a thinking philosopher is dirty.)When a thinking philosopher transits to hungry it sends requests for
  all forks that it does not hold.When a hungry philosopher receives a fork, it records the fork as
  being clean. If the hungry philosopher holds all its forks, and if
  it has no request for any dirty fork that it holds, then it transits
  to eating, and records all the forks that it holds in the eating
  state as dirty.When a hungry philosopher receives a request for a fork that it
  holds, it sends the fork if the fork is dirty, and holds on to the
  fork if it is clean.When an eating philosopher receives a request for a fork it
  registers the request in memory, and continues eating while holding
  the fork. When an eating philosopher transits to thinking it sends
  forks for all requests that it received.What could go wrong?The proof of safety is straightforward: Neighbors aren't eating
because neighbors can't hold the same fork at the same time.Before we look at the proof of progress, let's see what may go wrong.Could a group of philosophers exchange forks with each other so that
members of the group eat repeatedly, and starve a philosopher who is
not in the group? For example, in the figure below, could philosophers
\(u, v, w\) exchange forks so that they each eat in turn, and starve
\(y\)?Fig.13: Potential Problems: What could go wrong?Could the system enter a deadlock state in which each hungry philosopher in a
group holds only some --- but not all --- of the forks that it needs
to eat, while other members of the group hold the remaining forks?Proof of CorrectnessThe algorithm is correct. We are required to prove that every hungery
philosopher eats eventually:\(
\forall v: \quad v.h \leadsto v.e
\)where for a philosopher \(v\), \(v.h\) holds exactly when \(v\) is
hungry and \(v.e\) holds exactly when \(v\) is
eating.Variant FunctionTo prove this progress property we find a variant function that
satisfies the following two conditions:Safety: The variant function does not increase while \(v\)
  remains hungry.progress: The following predicate does not hold forever: The
  variant function remains unchanged and \(v\) remains hungry.We propose a variant function which is a pair of integers
 \(nT, nH\),  which are the number of thinking and hungry philosophers,
respectively, of 
higher priority than \(v\).
In terms of the priority graph, \(nT, nH\) are the numbers of thinking
and hungry philosophers (i.e. vertices) with paths to \(v\).Example of Variant FunctionThe figure below shows a portion of the priority graph in a state of the
system. The figure only shows philosophers with higher priority than
philosopher \(v\), i.e., it only shows vertices in the graph with
paths to \(v\). Since eating philosophers have lower priority than
their neighbors, eating philosophers don't appear in this graph.A hungry philosopher is marked with an "H" and a thinking philosopher
with a "T." In the diagram, philosophers \(v, w, x, y\) are hungry and
\(u\) is thinking.
Forks are located at philosophers and are shown as small
colored circles. A dirty fork is colored red and clean one is
blue. For example the fork shared by \(v\) and \(y\) is at \(y\) and
is clean.Fig.15: A Variant Function: Numbers of higher priority
    thinking, hungry agentsExample of Changes to Variant FunctionThe next figure is an example of changes to the variant function. The
diagram on the top left shows the higher priority vertices in the
state of the previous figure. If agent \(x\) eats next the priority
graph transits to the diagram on the top right, and the variant
function \((nT, nH)\) changes from \((1, 3)\) to \((1, 2)\).Fig.16: Example of Values of the Variant FunctionIf agent \(y\) eats next the priority
graph transits to the diagram on the bottom right, and the variant
function \((nT, nH)\) changes from \((1, 2)\) to \((1, 1)\).
If agent \(w\) eats next the priority
graph transits to the diagram on the bottom left, and the variant
function \((nT, nH)\) changes from \((1, 1)\) to \((0, 0)\).Proof that the variant function does not increase
while \(v\) remains hungryIf a philosopher of higher priority than \(v\) transits from thinking
to hungry then \(nT\) decreases. Though \(nH\) increases, the variant function \((nT,
nH)\) decreases because ordering of function values is done
lexicographically.If a philosopher of higher priority than \(v\) transits from hungry
to eating then \(nH\) decreases,  and so the variant function \((nT,
nH)\) decreases.Proof that the following predicate does not
hold forever: The variant function remains unchanged and
\(v\) remains hungry.Let \(w\) be a highest-priority hungry philosopher, i.e. a philosopher with no
hungry philosopher with priority higher than \(w\). (Note: \(w\) may be
the same as \(v\).)
All philosophers with priority higher than \(w\) are thinking. In the
next paragraph we show that either \(w\) gets all its forks and then transits
from hungry to eating, or the variant function decreases.From the algorithm, a hungry philosopher \(w\) requests forks from its
neighbors. From the algorithm, \(w\) eventually gets forks from 
all its lower priority neighbors. A higher priority neighbor \(x\) of
\(w\) is thinking. So when \(x\) gets a request for a fork from \(w\)
either (1) \(x\) sends the requested fork to \(w\) or (2) \(x\) transits from
thinking to hungry in which case the variant function \((nT, nH)\)
decreases.Summary: Key Ideas of this ModuleThis module introduced the problem of distributed mutual exclusion;
showed how the good neighbor policy --- a winning agent reduces its
priority to be lower than all the agents that it competes with --
solves this conflict resolution problem; introduced tokens and what
agents know about other agents holding tokens; and showed a proof
pattern that is one of the most common patterns for proving progress.ReviewWhat is the meaning of mutual exclusion among neighboring
  philosophers?An invariant of the algorithm is that each token is in exactly one
  place: an agent or a channel. How does this invariant help in
  designing the algorithm?An invariant of the algorithm is that the relative priorities among
  agents forms a partial order --- the priority graph is acyclic. What
  can go wrong if the priority graph has cycles?An agent that has a request for a fork releases the fork if the fork
  is dirty, and holds on to the fork if the fork is clean. What could
  go wrong if an agent releases a fork when it gets a request,
  regardless of whether the fork is clean or dirty?K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology



Figure Link: https://kmchandy.github.io/DistributedCollaboration/DiningPhilosophers/Slide05.jpg

Caption: Fig.5: Diagrams illustrating Safety


\([init
\Rightarrow A(safe)]\)Progress: Hungry agents eat eventuallyThe progress part of the specification is that every hungry agent
transits to eating state eventually.For every agent \(v\):\(
v.state = hungry \quad \leadsto \quad v.state = eating
\)Example of SafetyFig.5: Diagrams illustrating SafetyFigure 5 shows a client eating as a red node depicting the client and
its OS agent. An uncolored node represents a client that is thinking
or hungry. The diagram on the left shows the system in a safe state:
there are no edges between red vertices. The diagram on the right
shows an unsafe state because there are edges between red vertices.The Client's ProgramWe use two tokens that move
between a client and its OS agent. The tokens are called theresource tokenand therequest token. The client's
states are represented by which tokens the client holds.Thinking State: A thinking client holds the request token but
  not the resource token.Transition: Thinking to Hungry: Send the request token to
  the OS.Hungry State: The client holds neither the request nor the
  resource token.Transition Hungry to Eating: The client transits to
  eating when it receives both the request and resource token.Eating: The client holds both the request and resource tokens.Transition from Eating to Thinking:
  The client holds sends the resource token to the OS and continues to
  hold the request token.The figure below illustrates the states of the client. The request
    token is shown as a square and the resource token as a circle.Fig.6: Client's ProgramInitially all clients are thinking; all resource tokens are with OS
agents; and all request tokens are with clients.What the OS KnowsWhile the OS agent holds the request token it knows that its client is
hungry. While the OS agent holds the resource token it knows that its client is
not eating.Fig.7: OS Agent's ProgramWhen a client transits from thinking to hungry it sends its request
token to the agent. When the OS receives the request token, the OS
also has the resource token; so the OS knows that its client is
hungry. There is an interval after the client sends the request token
and before the OS receives it during which the client is hungry but
the OS doesn't know that. An OS agent does not need to know 
what state its client is in at every point. Likewise, a client does
not need to know its OS agent's state at every point.A client is hungryleads-toits OS agent knowing that its client is
hungry. Theleads-tocaptures the fact that the OS doesn't know
the client's state at the instant that the client transitions from
thinking to hungry.Introduction of TokensWe introduce a token on each edge of the agent communication graph,
see figure 4. The token on an edge \(v, w\) is in one of four states:
held by \(v\), in the channel from \(v\) to \(w\), held by \(w\), or
in the channel from \(w\) to \(v\). Therefore while \(v\) holds this
token it knows that \(w\) doesn't hold it. Likewise, while \(w\) holds
this token it knows that \(v\) doesn't.Fig.7: Fork on each edge of the agent communication
    graphThese tokens are calledforks. (They are calledchopsticksin some papers.) An agent eats only if it holds
forks for all the edges incident on it. Therefore, while an agent eats
none of its neighbors do, and so the safety specification is satisfied.Key Question: When does a hungry agent yield
forks?An eating philosopher holds on to all its forks until it finishes
eating. A thinking philosopher can give a fork to a neighbor that
requests it. So the key question is:Under what conditions should a
hungry neighbor give a fork that it holds to a neighbor that requests
it?Suppose every hungry agent gives a fork to a neighbor that requests
it. Then we can get the scenario shown in the figure below.Fig.8: Scenario when hungry agents yield forksThe figure shows a system with 3 agents indexed 0, 1, 2. The forks are
    shown as small colored circles. The state on
    the left shows agent \(j\) holding the fork that it shares with
    agent \((j+1)\:\textrm{mod}\:3\). If each agent yields the fork to
    its neighbor we get the state on the right in which agent \(j\)
    holds the fork that it shares with 
    agent \((j-1)\:\textrm{mod}\:3\). So, if all hungry agents yield
    forks, the states can alternate for ever between the one on the
left and the one on the right. In this case hungry agents starve: they
    remain hungry forever.If hungry agents don't yield forks then the state on the left persists
for ever. In this case too, hungry agents starve.Creating a Partial Order of PrioritiesLet's assign priorities to agents so that a hungry agent \(v\)
releases a fork to a neighbor \(w\) only if \(v\) has lower priority
than \(w\). If there is a cycle of agents, all with the same priority,
then we get the situation shown in figure 8. So, we will ensure that
priorities form a partial order in all states in all
transitions. Priorities form a partial order exactly when the priority
graph is acyclic. The graph has an edge from \(v\) to \(w\) exactly
when \(v\) has higher priority over \(w\).In the figure below, the diagram on the left shows an acyclic priority
graph while the one on the right shows a graph with a cycle.Fig.9: Priority Graph must be AcyclicHow should Priorities Change?Fig.10: How should priorities change when \(v\)
    eats?How should priorities change when an agent eats so that the priority
graph remains acyclic? For example, consider
the priority graph shown in figure 10. Assume agent \(v\) has all its
forks and is about to eat. Should the directions of edges incident on
\(v\) be flipped? Or should \(v\) have lower priority than all its
neighbors, i.e. all edges incident on \(v\) point towards \(v\)?What happens if we flip the directions of the edges incident on \(v\)?
After the flip, the edges are directed from \(w\), \(x\) and \(y\) towards \(v\), and
from \(v\) to \(u\). But now we have a cycle: \(y\) to \(v\) to \(u\)
to \(w\) to \(y\). So, flipping edge directions doesn't work.What happens if agents adopt the good neighbor policy?
The winner of a conflict gives itself lower priority thanallthe agents
with which it competes. So, an agent that starts eating gives itself lower
priority than all its neighbors. All edges point towards an eating
agent.Fig.11: Winner gets lower priority than its neighborsWhen all edges incident on a vertex point towards the vertex then
there is no cycle through that vertex. So, directing all edges
incident on an eating vertex towards the eating vertex maintains
acyclicity of the graph.For example, in the figure directing all edges incident on vertex \(v\) towards
 \(v\) ensures that no new cycle is created.Agent's priority does not decrease until the agent
eats.How an Agent knows its PriorityWe assign an attributeclean / dirtyto forks.
A fork is eitherdirtyorclean.
The forks held by an eating agent aredirty. When
an agent receives a fork from another agent, the receiving agent
cleans the fork. So the receiver holds acleanfork, and the
fork remains clean until an agent next eats with it.
(This is the "hygenic solution:" Another - sad? - attempt at CS
humor.)An agent holding a dirty fork knows that it has lower priority than
the agent with which it shares the fork. Likewise,
an agent holding a clean fork knows that it has higher priority than
the agent with which it shares the fork.If an agent does not hold the fork that it shares with a neighbor then
the agent does not know its priority with respect to that neighbor.Example of a Fork's LifecycleThe diagram below shows states of a fork shared by agents \(u\) and
\(v\). The red arrow shows priority, and the black arrows show
channels. The blue dot represents the fork.In the figure on the top left, agent \(u\) is hungry and holds a clean
fork. So, \(u\) knows that it has priority over \(v\). At this point
\(v\) does not know whether \(v\) has priority over \(u\) or not.The next figure, at the top right, shows that when \(u\) transits from
hungry to eating, the fork becomes dirty, and \(u\) has lower priority
than \(v\). Agent \(u\) continues to hold the fork while it eats.The next figure, bottom right, shows the situation after \(u\) gets a
request for the fork from \(v\). Because \(u\) got the request from
\(v\) and \(u\) hasn't sent the fork to \(v\), agent \(u\) knows that
\(v\) is hungry. Since the fork is dirty, \(u\) sends
the fork to \(v\). The figure shows the fork in the channel from \(u\)
to \(v\). While the fork is in the channel it doesn't matter
whether the fork is clean or dirty; however, merely for convenience,
let's assume that \(u\), being hygenic, cleans the fork before sending
it to its partner. While the fork is in a
channel the priority doesn't change but neither \(u\) nor \(v\) knows
what the priority is.The next figure, bottom left, shows the situation when \(v\) receives
the fork.  Receiving the fork doesn't change the priority.
At this point \(v\) is hungry and the fork is
clean and so \(v\) knows that it has higher
priority. \(v\) holds on to the fork until it next eats.Fig.12: How an Agent knows its PriorityAlgorithmProperties of Reachable StatesHere is a list of some of the properties of states in all
trajectories.The priority graph is acyclic. \(u\) has priority over a neighbor
  \(v\) exactly when \(u\) holds the fork that it shares with \(v\)
  and the fork is clean, or the fork is in the channel from \(v\) to
  \(u\), or \(v\) holds the fork and the fork is dirty.Eating philosophers hold the forks for all edges incident on them,
  and these forks are dirty.All forks held by thinking philosphers are dirty.Thinking philosophers never send requests and never receive
  forks. Thinking philosophers respond to request for forks by sending
  the requested forks.Initial StatesInitially all philosophers are thinking; all forks are dirty; and all channels are
empty. The forks are placed so that the priority graph is acyclic.
The initial assignment of forks is as follows.
Given an arbitrary acyclic graph, for any edge directed from \(v\) to
\(w\), the fork shared by \(v\) and \(w\) is initially at \(w\) and
the fork is dirty.Algorithm CommandsThe algorithm is specified by the following commands.When a thinking philosophers gets a request for a fork that it holds
  it sends the fork. (A fork held by a thinking philosopher is dirty.)When a thinking philosopher transits to hungry it sends requests for
  all forks that it does not hold.When a hungry philosopher receives a fork, it records the fork as
  being clean. If the hungry philosopher holds all its forks, and if
  it has no request for any dirty fork that it holds, then it transits
  to eating, and records all the forks that it holds in the eating
  state as dirty.When a hungry philosopher receives a request for a fork that it
  holds, it sends the fork if the fork is dirty, and holds on to the
  fork if it is clean.When an eating philosopher receives a request for a fork it
  registers the request in memory, and continues eating while holding
  the fork. When an eating philosopher transits to thinking it sends
  forks for all requests that it received.What could go wrong?The proof of safety is straightforward: Neighbors aren't eating
because neighbors can't hold the same fork at the same time.Before we look at the proof of progress, let's see what may go wrong.Could a group of philosophers exchange forks with each other so that
members of the group eat repeatedly, and starve a philosopher who is
not in the group? For example, in the figure below, could philosophers
\(u, v, w\) exchange forks so that they each eat in turn, and starve
\(y\)?Fig.13: Potential Problems: What could go wrong?Could the system enter a deadlock state in which each hungry philosopher in a
group holds only some --- but not all --- of the forks that it needs
to eat, while other members of the group hold the remaining forks?Proof of CorrectnessThe algorithm is correct. We are required to prove that every hungery
philosopher eats eventually:\(
\forall v: \quad v.h \leadsto v.e
\)where for a philosopher \(v\), \(v.h\) holds exactly when \(v\) is
hungry and \(v.e\) holds exactly when \(v\) is
eating.Variant FunctionTo prove this progress property we find a variant function that
satisfies the following two conditions:Safety: The variant function does not increase while \(v\)
  remains hungry.progress: The following predicate does not hold forever: The
  variant function remains unchanged and \(v\) remains hungry.We propose a variant function which is a pair of integers
 \(nT, nH\),  which are the number of thinking and hungry philosophers,
respectively, of 
higher priority than \(v\).
In terms of the priority graph, \(nT, nH\) are the numbers of thinking
and hungry philosophers (i.e. vertices) with paths to \(v\).Example of Variant FunctionThe figure below shows a portion of the priority graph in a state of the
system. The figure only shows philosophers with higher priority than
philosopher \(v\), i.e., it only shows vertices in the graph with
paths to \(v\). Since eating philosophers have lower priority than
their neighbors, eating philosophers don't appear in this graph.A hungry philosopher is marked with an "H" and a thinking philosopher
with a "T." In the diagram, philosophers \(v, w, x, y\) are hungry and
\(u\) is thinking.
Forks are located at philosophers and are shown as small
colored circles. A dirty fork is colored red and clean one is
blue. For example the fork shared by \(v\) and \(y\) is at \(y\) and
is clean.Fig.15: A Variant Function: Numbers of higher priority
    thinking, hungry agentsExample of Changes to Variant FunctionThe next figure is an example of changes to the variant function. The
diagram on the top left shows the higher priority vertices in the
state of the previous figure. If agent \(x\) eats next the priority
graph transits to the diagram on the top right, and the variant
function \((nT, nH)\) changes from \((1, 3)\) to \((1, 2)\).Fig.16: Example of Values of the Variant FunctionIf agent \(y\) eats next the priority
graph transits to the diagram on the bottom right, and the variant
function \((nT, nH)\) changes from \((1, 2)\) to \((1, 1)\).
If agent \(w\) eats next the priority
graph transits to the diagram on the bottom left, and the variant
function \((nT, nH)\) changes from \((1, 1)\) to \((0, 0)\).Proof that the variant function does not increase
while \(v\) remains hungryIf a philosopher of higher priority than \(v\) transits from thinking
to hungry then \(nT\) decreases. Though \(nH\) increases, the variant function \((nT,
nH)\) decreases because ordering of function values is done
lexicographically.If a philosopher of higher priority than \(v\) transits from hungry
to eating then \(nH\) decreases,  and so the variant function \((nT,
nH)\) decreases.Proof that the following predicate does not
hold forever: The variant function remains unchanged and
\(v\) remains hungry.Let \(w\) be a highest-priority hungry philosopher, i.e. a philosopher with no
hungry philosopher with priority higher than \(w\). (Note: \(w\) may be
the same as \(v\).)
All philosophers with priority higher than \(w\) are thinking. In the
next paragraph we show that either \(w\) gets all its forks and then transits
from hungry to eating, or the variant function decreases.From the algorithm, a hungry philosopher \(w\) requests forks from its
neighbors. From the algorithm, \(w\) eventually gets forks from 
all its lower priority neighbors. A higher priority neighbor \(x\) of
\(w\) is thinking. So when \(x\) gets a request for a fork from \(w\)
either (1) \(x\) sends the requested fork to \(w\) or (2) \(x\) transits from
thinking to hungry in which case the variant function \((nT, nH)\)
decreases.Summary: Key Ideas of this ModuleThis module introduced the problem of distributed mutual exclusion;
showed how the good neighbor policy --- a winning agent reduces its
priority to be lower than all the agents that it competes with --
solves this conflict resolution problem; introduced tokens and what
agents know about other agents holding tokens; and showed a proof
pattern that is one of the most common patterns for proving progress.ReviewWhat is the meaning of mutual exclusion among neighboring
  philosophers?An invariant of the algorithm is that each token is in exactly one
  place: an agent or a channel. How does this invariant help in
  designing the algorithm?An invariant of the algorithm is that the relative priorities among
  agents forms a partial order --- the priority graph is acyclic. What
  can go wrong if the priority graph has cycles?An agent that has a request for a fork releases the fork if the fork
  is dirty, and holds on to the fork if the fork is clean. What could
  go wrong if an agent releases a fork when it gets a request,
  regardless of whether the fork is clean or dirty?K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology



Figure Link: https://kmchandy.github.io/DistributedCollaboration/DiningPhilosophers/Slide05.jpg

Caption: Fig.5: Diagrams illustrating Safety


For every agent \(v\):\(
v.state = hungry \quad \leadsto \quad v.state = eating
\)Example of SafetyFig.5: Diagrams illustrating SafetyFigure 5 shows a client eating as a red node depicting the client and
its OS agent. An uncolored node represents a client that is thinking
or hungry. The diagram on the left shows the system in a safe state:
there are no edges between red vertices. The diagram on the right
shows an unsafe state because there are edges between red vertices.The Client's ProgramWe use two tokens that move
between a client and its OS agent. The tokens are called theresource tokenand therequest token. The client's
states are represented by which tokens the client holds.Thinking State: A thinking client holds the request token but
  not the resource token.Transition: Thinking to Hungry: Send the request token to
  the OS.Hungry State: The client holds neither the request nor the
  resource token.Transition Hungry to Eating: The client transits to
  eating when it receives both the request and resource token.Eating: The client holds both the request and resource tokens.Transition from Eating to Thinking:
  The client holds sends the resource token to the OS and continues to
  hold the request token.The figure below illustrates the states of the client. The request
    token is shown as a square and the resource token as a circle.Fig.6: Client's ProgramInitially all clients are thinking; all resource tokens are with OS
agents; and all request tokens are with clients.What the OS KnowsWhile the OS agent holds the request token it knows that its client is
hungry. While the OS agent holds the resource token it knows that its client is
not eating.Fig.7: OS Agent's ProgramWhen a client transits from thinking to hungry it sends its request
token to the agent. When the OS receives the request token, the OS
also has the resource token; so the OS knows that its client is
hungry. There is an interval after the client sends the request token
and before the OS receives it during which the client is hungry but
the OS doesn't know that. An OS agent does not need to know 
what state its client is in at every point. Likewise, a client does
not need to know its OS agent's state at every point.A client is hungryleads-toits OS agent knowing that its client is
hungry. Theleads-tocaptures the fact that the OS doesn't know
the client's state at the instant that the client transitions from
thinking to hungry.Introduction of TokensWe introduce a token on each edge of the agent communication graph,
see figure 4. The token on an edge \(v, w\) is in one of four states:
held by \(v\), in the channel from \(v\) to \(w\), held by \(w\), or
in the channel from \(w\) to \(v\). Therefore while \(v\) holds this
token it knows that \(w\) doesn't hold it. Likewise, while \(w\) holds
this token it knows that \(v\) doesn't.Fig.7: Fork on each edge of the agent communication
    graphThese tokens are calledforks. (They are calledchopsticksin some papers.) An agent eats only if it holds
forks for all the edges incident on it. Therefore, while an agent eats
none of its neighbors do, and so the safety specification is satisfied.Key Question: When does a hungry agent yield
forks?An eating philosopher holds on to all its forks until it finishes
eating. A thinking philosopher can give a fork to a neighbor that
requests it. So the key question is:Under what conditions should a
hungry neighbor give a fork that it holds to a neighbor that requests
it?Suppose every hungry agent gives a fork to a neighbor that requests
it. Then we can get the scenario shown in the figure below.Fig.8: Scenario when hungry agents yield forksThe figure shows a system with 3 agents indexed 0, 1, 2. The forks are
    shown as small colored circles. The state on
    the left shows agent \(j\) holding the fork that it shares with
    agent \((j+1)\:\textrm{mod}\:3\). If each agent yields the fork to
    its neighbor we get the state on the right in which agent \(j\)
    holds the fork that it shares with 
    agent \((j-1)\:\textrm{mod}\:3\). So, if all hungry agents yield
    forks, the states can alternate for ever between the one on the
left and the one on the right. In this case hungry agents starve: they
    remain hungry forever.If hungry agents don't yield forks then the state on the left persists
for ever. In this case too, hungry agents starve.Creating a Partial Order of PrioritiesLet's assign priorities to agents so that a hungry agent \(v\)
releases a fork to a neighbor \(w\) only if \(v\) has lower priority
than \(w\). If there is a cycle of agents, all with the same priority,
then we get the situation shown in figure 8. So, we will ensure that
priorities form a partial order in all states in all
transitions. Priorities form a partial order exactly when the priority
graph is acyclic. The graph has an edge from \(v\) to \(w\) exactly
when \(v\) has higher priority over \(w\).In the figure below, the diagram on the left shows an acyclic priority
graph while the one on the right shows a graph with a cycle.Fig.9: Priority Graph must be AcyclicHow should Priorities Change?Fig.10: How should priorities change when \(v\)
    eats?How should priorities change when an agent eats so that the priority
graph remains acyclic? For example, consider
the priority graph shown in figure 10. Assume agent \(v\) has all its
forks and is about to eat. Should the directions of edges incident on
\(v\) be flipped? Or should \(v\) have lower priority than all its
neighbors, i.e. all edges incident on \(v\) point towards \(v\)?What happens if we flip the directions of the edges incident on \(v\)?
After the flip, the edges are directed from \(w\), \(x\) and \(y\) towards \(v\), and
from \(v\) to \(u\). But now we have a cycle: \(y\) to \(v\) to \(u\)
to \(w\) to \(y\). So, flipping edge directions doesn't work.What happens if agents adopt the good neighbor policy?
The winner of a conflict gives itself lower priority thanallthe agents
with which it competes. So, an agent that starts eating gives itself lower
priority than all its neighbors. All edges point towards an eating
agent.Fig.11: Winner gets lower priority than its neighborsWhen all edges incident on a vertex point towards the vertex then
there is no cycle through that vertex. So, directing all edges
incident on an eating vertex towards the eating vertex maintains
acyclicity of the graph.For example, in the figure directing all edges incident on vertex \(v\) towards
 \(v\) ensures that no new cycle is created.Agent's priority does not decrease until the agent
eats.How an Agent knows its PriorityWe assign an attributeclean / dirtyto forks.
A fork is eitherdirtyorclean.
The forks held by an eating agent aredirty. When
an agent receives a fork from another agent, the receiving agent
cleans the fork. So the receiver holds acleanfork, and the
fork remains clean until an agent next eats with it.
(This is the "hygenic solution:" Another - sad? - attempt at CS
humor.)An agent holding a dirty fork knows that it has lower priority than
the agent with which it shares the fork. Likewise,
an agent holding a clean fork knows that it has higher priority than
the agent with which it shares the fork.If an agent does not hold the fork that it shares with a neighbor then
the agent does not know its priority with respect to that neighbor.Example of a Fork's LifecycleThe diagram below shows states of a fork shared by agents \(u\) and
\(v\). The red arrow shows priority, and the black arrows show
channels. The blue dot represents the fork.In the figure on the top left, agent \(u\) is hungry and holds a clean
fork. So, \(u\) knows that it has priority over \(v\). At this point
\(v\) does not know whether \(v\) has priority over \(u\) or not.The next figure, at the top right, shows that when \(u\) transits from
hungry to eating, the fork becomes dirty, and \(u\) has lower priority
than \(v\). Agent \(u\) continues to hold the fork while it eats.The next figure, bottom right, shows the situation after \(u\) gets a
request for the fork from \(v\). Because \(u\) got the request from
\(v\) and \(u\) hasn't sent the fork to \(v\), agent \(u\) knows that
\(v\) is hungry. Since the fork is dirty, \(u\) sends
the fork to \(v\). The figure shows the fork in the channel from \(u\)
to \(v\). While the fork is in the channel it doesn't matter
whether the fork is clean or dirty; however, merely for convenience,
let's assume that \(u\), being hygenic, cleans the fork before sending
it to its partner. While the fork is in a
channel the priority doesn't change but neither \(u\) nor \(v\) knows
what the priority is.The next figure, bottom left, shows the situation when \(v\) receives
the fork.  Receiving the fork doesn't change the priority.
At this point \(v\) is hungry and the fork is
clean and so \(v\) knows that it has higher
priority. \(v\) holds on to the fork until it next eats.Fig.12: How an Agent knows its PriorityAlgorithmProperties of Reachable StatesHere is a list of some of the properties of states in all
trajectories.The priority graph is acyclic. \(u\) has priority over a neighbor
  \(v\) exactly when \(u\) holds the fork that it shares with \(v\)
  and the fork is clean, or the fork is in the channel from \(v\) to
  \(u\), or \(v\) holds the fork and the fork is dirty.Eating philosophers hold the forks for all edges incident on them,
  and these forks are dirty.All forks held by thinking philosphers are dirty.Thinking philosophers never send requests and never receive
  forks. Thinking philosophers respond to request for forks by sending
  the requested forks.Initial StatesInitially all philosophers are thinking; all forks are dirty; and all channels are
empty. The forks are placed so that the priority graph is acyclic.
The initial assignment of forks is as follows.
Given an arbitrary acyclic graph, for any edge directed from \(v\) to
\(w\), the fork shared by \(v\) and \(w\) is initially at \(w\) and
the fork is dirty.Algorithm CommandsThe algorithm is specified by the following commands.When a thinking philosophers gets a request for a fork that it holds
  it sends the fork. (A fork held by a thinking philosopher is dirty.)When a thinking philosopher transits to hungry it sends requests for
  all forks that it does not hold.When a hungry philosopher receives a fork, it records the fork as
  being clean. If the hungry philosopher holds all its forks, and if
  it has no request for any dirty fork that it holds, then it transits
  to eating, and records all the forks that it holds in the eating
  state as dirty.When a hungry philosopher receives a request for a fork that it
  holds, it sends the fork if the fork is dirty, and holds on to the
  fork if it is clean.When an eating philosopher receives a request for a fork it
  registers the request in memory, and continues eating while holding
  the fork. When an eating philosopher transits to thinking it sends
  forks for all requests that it received.What could go wrong?The proof of safety is straightforward: Neighbors aren't eating
because neighbors can't hold the same fork at the same time.Before we look at the proof of progress, let's see what may go wrong.Could a group of philosophers exchange forks with each other so that
members of the group eat repeatedly, and starve a philosopher who is
not in the group? For example, in the figure below, could philosophers
\(u, v, w\) exchange forks so that they each eat in turn, and starve
\(y\)?Fig.13: Potential Problems: What could go wrong?Could the system enter a deadlock state in which each hungry philosopher in a
group holds only some --- but not all --- of the forks that it needs
to eat, while other members of the group hold the remaining forks?Proof of CorrectnessThe algorithm is correct. We are required to prove that every hungery
philosopher eats eventually:\(
\forall v: \quad v.h \leadsto v.e
\)where for a philosopher \(v\), \(v.h\) holds exactly when \(v\) is
hungry and \(v.e\) holds exactly when \(v\) is
eating.Variant FunctionTo prove this progress property we find a variant function that
satisfies the following two conditions:Safety: The variant function does not increase while \(v\)
  remains hungry.progress: The following predicate does not hold forever: The
  variant function remains unchanged and \(v\) remains hungry.We propose a variant function which is a pair of integers
 \(nT, nH\),  which are the number of thinking and hungry philosophers,
respectively, of 
higher priority than \(v\).
In terms of the priority graph, \(nT, nH\) are the numbers of thinking
and hungry philosophers (i.e. vertices) with paths to \(v\).Example of Variant FunctionThe figure below shows a portion of the priority graph in a state of the
system. The figure only shows philosophers with higher priority than
philosopher \(v\), i.e., it only shows vertices in the graph with
paths to \(v\). Since eating philosophers have lower priority than
their neighbors, eating philosophers don't appear in this graph.A hungry philosopher is marked with an "H" and a thinking philosopher
with a "T." In the diagram, philosophers \(v, w, x, y\) are hungry and
\(u\) is thinking.
Forks are located at philosophers and are shown as small
colored circles. A dirty fork is colored red and clean one is
blue. For example the fork shared by \(v\) and \(y\) is at \(y\) and
is clean.Fig.15: A Variant Function: Numbers of higher priority
    thinking, hungry agentsExample of Changes to Variant FunctionThe next figure is an example of changes to the variant function. The
diagram on the top left shows the higher priority vertices in the
state of the previous figure. If agent \(x\) eats next the priority
graph transits to the diagram on the top right, and the variant
function \((nT, nH)\) changes from \((1, 3)\) to \((1, 2)\).Fig.16: Example of Values of the Variant FunctionIf agent \(y\) eats next the priority
graph transits to the diagram on the bottom right, and the variant
function \((nT, nH)\) changes from \((1, 2)\) to \((1, 1)\).
If agent \(w\) eats next the priority
graph transits to the diagram on the bottom left, and the variant
function \((nT, nH)\) changes from \((1, 1)\) to \((0, 0)\).Proof that the variant function does not increase
while \(v\) remains hungryIf a philosopher of higher priority than \(v\) transits from thinking
to hungry then \(nT\) decreases. Though \(nH\) increases, the variant function \((nT,
nH)\) decreases because ordering of function values is done
lexicographically.If a philosopher of higher priority than \(v\) transits from hungry
to eating then \(nH\) decreases,  and so the variant function \((nT,
nH)\) decreases.Proof that the following predicate does not
hold forever: The variant function remains unchanged and
\(v\) remains hungry.Let \(w\) be a highest-priority hungry philosopher, i.e. a philosopher with no
hungry philosopher with priority higher than \(w\). (Note: \(w\) may be
the same as \(v\).)
All philosophers with priority higher than \(w\) are thinking. In the
next paragraph we show that either \(w\) gets all its forks and then transits
from hungry to eating, or the variant function decreases.From the algorithm, a hungry philosopher \(w\) requests forks from its
neighbors. From the algorithm, \(w\) eventually gets forks from 
all its lower priority neighbors. A higher priority neighbor \(x\) of
\(w\) is thinking. So when \(x\) gets a request for a fork from \(w\)
either (1) \(x\) sends the requested fork to \(w\) or (2) \(x\) transits from
thinking to hungry in which case the variant function \((nT, nH)\)
decreases.Summary: Key Ideas of this ModuleThis module introduced the problem of distributed mutual exclusion;
showed how the good neighbor policy --- a winning agent reduces its
priority to be lower than all the agents that it competes with --
solves this conflict resolution problem; introduced tokens and what
agents know about other agents holding tokens; and showed a proof
pattern that is one of the most common patterns for proving progress.ReviewWhat is the meaning of mutual exclusion among neighboring
  philosophers?An invariant of the algorithm is that each token is in exactly one
  place: an agent or a channel. How does this invariant help in
  designing the algorithm?An invariant of the algorithm is that the relative priorities among
  agents forms a partial order --- the priority graph is acyclic. What
  can go wrong if the priority graph has cycles?An agent that has a request for a fork releases the fork if the fork
  is dirty, and holds on to the fork if the fork is clean. What could
  go wrong if an agent releases a fork when it gets a request,
  regardless of whether the fork is clean or dirty?K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology



Figure Link: https://kmchandy.github.io/DistributedCollaboration/DiningPhilosophers/Slide05.jpg

Caption: Fig.5: Diagrams illustrating Safety


We use two tokens that move
between a client and its OS agent. The tokens are called theresource tokenand therequest token. The client's
states are represented by which tokens the client holds.Thinking State: A thinking client holds the request token but
  not the resource token.Transition: Thinking to Hungry: Send the request token to
  the OS.Hungry State: The client holds neither the request nor the
  resource token.Transition Hungry to Eating: The client transits to
  eating when it receives both the request and resource token.Eating: The client holds both the request and resource tokens.Transition from Eating to Thinking:
  The client holds sends the resource token to the OS and continues to
  hold the request token.The figure below illustrates the states of the client. The request
    token is shown as a square and the resource token as a circle.Fig.6: Client's ProgramInitially all clients are thinking; all resource tokens are with OS
agents; and all request tokens are with clients.What the OS KnowsWhile the OS agent holds the request token it knows that its client is
hungry. While the OS agent holds the resource token it knows that its client is
not eating.Fig.7: OS Agent's ProgramWhen a client transits from thinking to hungry it sends its request
token to the agent. When the OS receives the request token, the OS
also has the resource token; so the OS knows that its client is
hungry. There is an interval after the client sends the request token
and before the OS receives it during which the client is hungry but
the OS doesn't know that. An OS agent does not need to know 
what state its client is in at every point. Likewise, a client does
not need to know its OS agent's state at every point.A client is hungryleads-toits OS agent knowing that its client is
hungry. Theleads-tocaptures the fact that the OS doesn't know
the client's state at the instant that the client transitions from
thinking to hungry.Introduction of TokensWe introduce a token on each edge of the agent communication graph,
see figure 4. The token on an edge \(v, w\) is in one of four states:
held by \(v\), in the channel from \(v\) to \(w\), held by \(w\), or
in the channel from \(w\) to \(v\). Therefore while \(v\) holds this
token it knows that \(w\) doesn't hold it. Likewise, while \(w\) holds
this token it knows that \(v\) doesn't.Fig.7: Fork on each edge of the agent communication
    graphThese tokens are calledforks. (They are calledchopsticksin some papers.) An agent eats only if it holds
forks for all the edges incident on it. Therefore, while an agent eats
none of its neighbors do, and so the safety specification is satisfied.Key Question: When does a hungry agent yield
forks?An eating philosopher holds on to all its forks until it finishes
eating. A thinking philosopher can give a fork to a neighbor that
requests it. So the key question is:Under what conditions should a
hungry neighbor give a fork that it holds to a neighbor that requests
it?Suppose every hungry agent gives a fork to a neighbor that requests
it. Then we can get the scenario shown in the figure below.Fig.8: Scenario when hungry agents yield forksThe figure shows a system with 3 agents indexed 0, 1, 2. The forks are
    shown as small colored circles. The state on
    the left shows agent \(j\) holding the fork that it shares with
    agent \((j+1)\:\textrm{mod}\:3\). If each agent yields the fork to
    its neighbor we get the state on the right in which agent \(j\)
    holds the fork that it shares with 
    agent \((j-1)\:\textrm{mod}\:3\). So, if all hungry agents yield
    forks, the states can alternate for ever between the one on the
left and the one on the right. In this case hungry agents starve: they
    remain hungry forever.If hungry agents don't yield forks then the state on the left persists
for ever. In this case too, hungry agents starve.Creating a Partial Order of PrioritiesLet's assign priorities to agents so that a hungry agent \(v\)
releases a fork to a neighbor \(w\) only if \(v\) has lower priority
than \(w\). If there is a cycle of agents, all with the same priority,
then we get the situation shown in figure 8. So, we will ensure that
priorities form a partial order in all states in all
transitions. Priorities form a partial order exactly when the priority
graph is acyclic. The graph has an edge from \(v\) to \(w\) exactly
when \(v\) has higher priority over \(w\).In the figure below, the diagram on the left shows an acyclic priority
graph while the one on the right shows a graph with a cycle.Fig.9: Priority Graph must be AcyclicHow should Priorities Change?Fig.10: How should priorities change when \(v\)
    eats?How should priorities change when an agent eats so that the priority
graph remains acyclic? For example, consider
the priority graph shown in figure 10. Assume agent \(v\) has all its
forks and is about to eat. Should the directions of edges incident on
\(v\) be flipped? Or should \(v\) have lower priority than all its
neighbors, i.e. all edges incident on \(v\) point towards \(v\)?What happens if we flip the directions of the edges incident on \(v\)?
After the flip, the edges are directed from \(w\), \(x\) and \(y\) towards \(v\), and
from \(v\) to \(u\). But now we have a cycle: \(y\) to \(v\) to \(u\)
to \(w\) to \(y\). So, flipping edge directions doesn't work.What happens if agents adopt the good neighbor policy?
The winner of a conflict gives itself lower priority thanallthe agents
with which it competes. So, an agent that starts eating gives itself lower
priority than all its neighbors. All edges point towards an eating
agent.Fig.11: Winner gets lower priority than its neighborsWhen all edges incident on a vertex point towards the vertex then
there is no cycle through that vertex. So, directing all edges
incident on an eating vertex towards the eating vertex maintains
acyclicity of the graph.For example, in the figure directing all edges incident on vertex \(v\) towards
 \(v\) ensures that no new cycle is created.Agent's priority does not decrease until the agent
eats.How an Agent knows its PriorityWe assign an attributeclean / dirtyto forks.
A fork is eitherdirtyorclean.
The forks held by an eating agent aredirty. When
an agent receives a fork from another agent, the receiving agent
cleans the fork. So the receiver holds acleanfork, and the
fork remains clean until an agent next eats with it.
(This is the "hygenic solution:" Another - sad? - attempt at CS
humor.)An agent holding a dirty fork knows that it has lower priority than
the agent with which it shares the fork. Likewise,
an agent holding a clean fork knows that it has higher priority than
the agent with which it shares the fork.If an agent does not hold the fork that it shares with a neighbor then
the agent does not know its priority with respect to that neighbor.Example of a Fork's LifecycleThe diagram below shows states of a fork shared by agents \(u\) and
\(v\). The red arrow shows priority, and the black arrows show
channels. The blue dot represents the fork.In the figure on the top left, agent \(u\) is hungry and holds a clean
fork. So, \(u\) knows that it has priority over \(v\). At this point
\(v\) does not know whether \(v\) has priority over \(u\) or not.The next figure, at the top right, shows that when \(u\) transits from
hungry to eating, the fork becomes dirty, and \(u\) has lower priority
than \(v\). Agent \(u\) continues to hold the fork while it eats.The next figure, bottom right, shows the situation after \(u\) gets a
request for the fork from \(v\). Because \(u\) got the request from
\(v\) and \(u\) hasn't sent the fork to \(v\), agent \(u\) knows that
\(v\) is hungry. Since the fork is dirty, \(u\) sends
the fork to \(v\). The figure shows the fork in the channel from \(u\)
to \(v\). While the fork is in the channel it doesn't matter
whether the fork is clean or dirty; however, merely for convenience,
let's assume that \(u\), being hygenic, cleans the fork before sending
it to its partner. While the fork is in a
channel the priority doesn't change but neither \(u\) nor \(v\) knows
what the priority is.The next figure, bottom left, shows the situation when \(v\) receives
the fork.  Receiving the fork doesn't change the priority.
At this point \(v\) is hungry and the fork is
clean and so \(v\) knows that it has higher
priority. \(v\) holds on to the fork until it next eats.Fig.12: How an Agent knows its PriorityAlgorithmProperties of Reachable StatesHere is a list of some of the properties of states in all
trajectories.The priority graph is acyclic. \(u\) has priority over a neighbor
  \(v\) exactly when \(u\) holds the fork that it shares with \(v\)
  and the fork is clean, or the fork is in the channel from \(v\) to
  \(u\), or \(v\) holds the fork and the fork is dirty.Eating philosophers hold the forks for all edges incident on them,
  and these forks are dirty.All forks held by thinking philosphers are dirty.Thinking philosophers never send requests and never receive
  forks. Thinking philosophers respond to request for forks by sending
  the requested forks.Initial StatesInitially all philosophers are thinking; all forks are dirty; and all channels are
empty. The forks are placed so that the priority graph is acyclic.
The initial assignment of forks is as follows.
Given an arbitrary acyclic graph, for any edge directed from \(v\) to
\(w\), the fork shared by \(v\) and \(w\) is initially at \(w\) and
the fork is dirty.Algorithm CommandsThe algorithm is specified by the following commands.When a thinking philosophers gets a request for a fork that it holds
  it sends the fork. (A fork held by a thinking philosopher is dirty.)When a thinking philosopher transits to hungry it sends requests for
  all forks that it does not hold.When a hungry philosopher receives a fork, it records the fork as
  being clean. If the hungry philosopher holds all its forks, and if
  it has no request for any dirty fork that it holds, then it transits
  to eating, and records all the forks that it holds in the eating
  state as dirty.When a hungry philosopher receives a request for a fork that it
  holds, it sends the fork if the fork is dirty, and holds on to the
  fork if it is clean.When an eating philosopher receives a request for a fork it
  registers the request in memory, and continues eating while holding
  the fork. When an eating philosopher transits to thinking it sends
  forks for all requests that it received.What could go wrong?The proof of safety is straightforward: Neighbors aren't eating
because neighbors can't hold the same fork at the same time.Before we look at the proof of progress, let's see what may go wrong.Could a group of philosophers exchange forks with each other so that
members of the group eat repeatedly, and starve a philosopher who is
not in the group? For example, in the figure below, could philosophers
\(u, v, w\) exchange forks so that they each eat in turn, and starve
\(y\)?Fig.13: Potential Problems: What could go wrong?Could the system enter a deadlock state in which each hungry philosopher in a
group holds only some --- but not all --- of the forks that it needs
to eat, while other members of the group hold the remaining forks?Proof of CorrectnessThe algorithm is correct. We are required to prove that every hungery
philosopher eats eventually:\(
\forall v: \quad v.h \leadsto v.e
\)where for a philosopher \(v\), \(v.h\) holds exactly when \(v\) is
hungry and \(v.e\) holds exactly when \(v\) is
eating.Variant FunctionTo prove this progress property we find a variant function that
satisfies the following two conditions:Safety: The variant function does not increase while \(v\)
  remains hungry.progress: The following predicate does not hold forever: The
  variant function remains unchanged and \(v\) remains hungry.We propose a variant function which is a pair of integers
 \(nT, nH\),  which are the number of thinking and hungry philosophers,
respectively, of 
higher priority than \(v\).
In terms of the priority graph, \(nT, nH\) are the numbers of thinking
and hungry philosophers (i.e. vertices) with paths to \(v\).Example of Variant FunctionThe figure below shows a portion of the priority graph in a state of the
system. The figure only shows philosophers with higher priority than
philosopher \(v\), i.e., it only shows vertices in the graph with
paths to \(v\). Since eating philosophers have lower priority than
their neighbors, eating philosophers don't appear in this graph.A hungry philosopher is marked with an "H" and a thinking philosopher
with a "T." In the diagram, philosophers \(v, w, x, y\) are hungry and
\(u\) is thinking.
Forks are located at philosophers and are shown as small
colored circles. A dirty fork is colored red and clean one is
blue. For example the fork shared by \(v\) and \(y\) is at \(y\) and
is clean.Fig.15: A Variant Function: Numbers of higher priority
    thinking, hungry agentsExample of Changes to Variant FunctionThe next figure is an example of changes to the variant function. The
diagram on the top left shows the higher priority vertices in the
state of the previous figure. If agent \(x\) eats next the priority
graph transits to the diagram on the top right, and the variant
function \((nT, nH)\) changes from \((1, 3)\) to \((1, 2)\).Fig.16: Example of Values of the Variant FunctionIf agent \(y\) eats next the priority
graph transits to the diagram on the bottom right, and the variant
function \((nT, nH)\) changes from \((1, 2)\) to \((1, 1)\).
If agent \(w\) eats next the priority
graph transits to the diagram on the bottom left, and the variant
function \((nT, nH)\) changes from \((1, 1)\) to \((0, 0)\).Proof that the variant function does not increase
while \(v\) remains hungryIf a philosopher of higher priority than \(v\) transits from thinking
to hungry then \(nT\) decreases. Though \(nH\) increases, the variant function \((nT,
nH)\) decreases because ordering of function values is done
lexicographically.If a philosopher of higher priority than \(v\) transits from hungry
to eating then \(nH\) decreases,  and so the variant function \((nT,
nH)\) decreases.Proof that the following predicate does not
hold forever: The variant function remains unchanged and
\(v\) remains hungry.Let \(w\) be a highest-priority hungry philosopher, i.e. a philosopher with no
hungry philosopher with priority higher than \(w\). (Note: \(w\) may be
the same as \(v\).)
All philosophers with priority higher than \(w\) are thinking. In the
next paragraph we show that either \(w\) gets all its forks and then transits
from hungry to eating, or the variant function decreases.From the algorithm, a hungry philosopher \(w\) requests forks from its
neighbors. From the algorithm, \(w\) eventually gets forks from 
all its lower priority neighbors. A higher priority neighbor \(x\) of
\(w\) is thinking. So when \(x\) gets a request for a fork from \(w\)
either (1) \(x\) sends the requested fork to \(w\) or (2) \(x\) transits from
thinking to hungry in which case the variant function \((nT, nH)\)
decreases.Summary: Key Ideas of this ModuleThis module introduced the problem of distributed mutual exclusion;
showed how the good neighbor policy --- a winning agent reduces its
priority to be lower than all the agents that it competes with --
solves this conflict resolution problem; introduced tokens and what
agents know about other agents holding tokens; and showed a proof
pattern that is one of the most common patterns for proving progress.ReviewWhat is the meaning of mutual exclusion among neighboring
  philosophers?An invariant of the algorithm is that each token is in exactly one
  place: an agent or a channel. How does this invariant help in
  designing the algorithm?An invariant of the algorithm is that the relative priorities among
  agents forms a partial order --- the priority graph is acyclic. What
  can go wrong if the priority graph has cycles?An agent that has a request for a fork releases the fork if the fork
  is dirty, and holds on to the fork if the fork is clean. What could
  go wrong if an agent releases a fork when it gets a request,
  regardless of whether the fork is clean or dirty?K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology



Figure Link: https://kmchandy.github.io/DistributedCollaboration/DiningPhilosophers/Slide06.jpg

Caption: Fig.6: Client's Program


When a client transits from thinking to hungry it sends its request
token to the agent. When the OS receives the request token, the OS
also has the resource token; so the OS knows that its client is
hungry. There is an interval after the client sends the request token
and before the OS receives it during which the client is hungry but
the OS doesn't know that. An OS agent does not need to know 
what state its client is in at every point. Likewise, a client does
not need to know its OS agent's state at every point.A client is hungryleads-toits OS agent knowing that its client is
hungry. Theleads-tocaptures the fact that the OS doesn't know
the client's state at the instant that the client transitions from
thinking to hungry.Introduction of TokensWe introduce a token on each edge of the agent communication graph,
see figure 4. The token on an edge \(v, w\) is in one of four states:
held by \(v\), in the channel from \(v\) to \(w\), held by \(w\), or
in the channel from \(w\) to \(v\). Therefore while \(v\) holds this
token it knows that \(w\) doesn't hold it. Likewise, while \(w\) holds
this token it knows that \(v\) doesn't.Fig.7: Fork on each edge of the agent communication
    graphThese tokens are calledforks. (They are calledchopsticksin some papers.) An agent eats only if it holds
forks for all the edges incident on it. Therefore, while an agent eats
none of its neighbors do, and so the safety specification is satisfied.Key Question: When does a hungry agent yield
forks?An eating philosopher holds on to all its forks until it finishes
eating. A thinking philosopher can give a fork to a neighbor that
requests it. So the key question is:Under what conditions should a
hungry neighbor give a fork that it holds to a neighbor that requests
it?Suppose every hungry agent gives a fork to a neighbor that requests
it. Then we can get the scenario shown in the figure below.Fig.8: Scenario when hungry agents yield forksThe figure shows a system with 3 agents indexed 0, 1, 2. The forks are
    shown as small colored circles. The state on
    the left shows agent \(j\) holding the fork that it shares with
    agent \((j+1)\:\textrm{mod}\:3\). If each agent yields the fork to
    its neighbor we get the state on the right in which agent \(j\)
    holds the fork that it shares with 
    agent \((j-1)\:\textrm{mod}\:3\). So, if all hungry agents yield
    forks, the states can alternate for ever between the one on the
left and the one on the right. In this case hungry agents starve: they
    remain hungry forever.If hungry agents don't yield forks then the state on the left persists
for ever. In this case too, hungry agents starve.Creating a Partial Order of PrioritiesLet's assign priorities to agents so that a hungry agent \(v\)
releases a fork to a neighbor \(w\) only if \(v\) has lower priority
than \(w\). If there is a cycle of agents, all with the same priority,
then we get the situation shown in figure 8. So, we will ensure that
priorities form a partial order in all states in all
transitions. Priorities form a partial order exactly when the priority
graph is acyclic. The graph has an edge from \(v\) to \(w\) exactly
when \(v\) has higher priority over \(w\).In the figure below, the diagram on the left shows an acyclic priority
graph while the one on the right shows a graph with a cycle.Fig.9: Priority Graph must be AcyclicHow should Priorities Change?Fig.10: How should priorities change when \(v\)
    eats?How should priorities change when an agent eats so that the priority
graph remains acyclic? For example, consider
the priority graph shown in figure 10. Assume agent \(v\) has all its
forks and is about to eat. Should the directions of edges incident on
\(v\) be flipped? Or should \(v\) have lower priority than all its
neighbors, i.e. all edges incident on \(v\) point towards \(v\)?What happens if we flip the directions of the edges incident on \(v\)?
After the flip, the edges are directed from \(w\), \(x\) and \(y\) towards \(v\), and
from \(v\) to \(u\). But now we have a cycle: \(y\) to \(v\) to \(u\)
to \(w\) to \(y\). So, flipping edge directions doesn't work.What happens if agents adopt the good neighbor policy?
The winner of a conflict gives itself lower priority thanallthe agents
with which it competes. So, an agent that starts eating gives itself lower
priority than all its neighbors. All edges point towards an eating
agent.Fig.11: Winner gets lower priority than its neighborsWhen all edges incident on a vertex point towards the vertex then
there is no cycle through that vertex. So, directing all edges
incident on an eating vertex towards the eating vertex maintains
acyclicity of the graph.For example, in the figure directing all edges incident on vertex \(v\) towards
 \(v\) ensures that no new cycle is created.Agent's priority does not decrease until the agent
eats.How an Agent knows its PriorityWe assign an attributeclean / dirtyto forks.
A fork is eitherdirtyorclean.
The forks held by an eating agent aredirty. When
an agent receives a fork from another agent, the receiving agent
cleans the fork. So the receiver holds acleanfork, and the
fork remains clean until an agent next eats with it.
(This is the "hygenic solution:" Another - sad? - attempt at CS
humor.)An agent holding a dirty fork knows that it has lower priority than
the agent with which it shares the fork. Likewise,
an agent holding a clean fork knows that it has higher priority than
the agent with which it shares the fork.If an agent does not hold the fork that it shares with a neighbor then
the agent does not know its priority with respect to that neighbor.Example of a Fork's LifecycleThe diagram below shows states of a fork shared by agents \(u\) and
\(v\). The red arrow shows priority, and the black arrows show
channels. The blue dot represents the fork.In the figure on the top left, agent \(u\) is hungry and holds a clean
fork. So, \(u\) knows that it has priority over \(v\). At this point
\(v\) does not know whether \(v\) has priority over \(u\) or not.The next figure, at the top right, shows that when \(u\) transits from
hungry to eating, the fork becomes dirty, and \(u\) has lower priority
than \(v\). Agent \(u\) continues to hold the fork while it eats.The next figure, bottom right, shows the situation after \(u\) gets a
request for the fork from \(v\). Because \(u\) got the request from
\(v\) and \(u\) hasn't sent the fork to \(v\), agent \(u\) knows that
\(v\) is hungry. Since the fork is dirty, \(u\) sends
the fork to \(v\). The figure shows the fork in the channel from \(u\)
to \(v\). While the fork is in the channel it doesn't matter
whether the fork is clean or dirty; however, merely for convenience,
let's assume that \(u\), being hygenic, cleans the fork before sending
it to its partner. While the fork is in a
channel the priority doesn't change but neither \(u\) nor \(v\) knows
what the priority is.The next figure, bottom left, shows the situation when \(v\) receives
the fork.  Receiving the fork doesn't change the priority.
At this point \(v\) is hungry and the fork is
clean and so \(v\) knows that it has higher
priority. \(v\) holds on to the fork until it next eats.Fig.12: How an Agent knows its PriorityAlgorithmProperties of Reachable StatesHere is a list of some of the properties of states in all
trajectories.The priority graph is acyclic. \(u\) has priority over a neighbor
  \(v\) exactly when \(u\) holds the fork that it shares with \(v\)
  and the fork is clean, or the fork is in the channel from \(v\) to
  \(u\), or \(v\) holds the fork and the fork is dirty.Eating philosophers hold the forks for all edges incident on them,
  and these forks are dirty.All forks held by thinking philosphers are dirty.Thinking philosophers never send requests and never receive
  forks. Thinking philosophers respond to request for forks by sending
  the requested forks.Initial StatesInitially all philosophers are thinking; all forks are dirty; and all channels are
empty. The forks are placed so that the priority graph is acyclic.
The initial assignment of forks is as follows.
Given an arbitrary acyclic graph, for any edge directed from \(v\) to
\(w\), the fork shared by \(v\) and \(w\) is initially at \(w\) and
the fork is dirty.Algorithm CommandsThe algorithm is specified by the following commands.When a thinking philosophers gets a request for a fork that it holds
  it sends the fork. (A fork held by a thinking philosopher is dirty.)When a thinking philosopher transits to hungry it sends requests for
  all forks that it does not hold.When a hungry philosopher receives a fork, it records the fork as
  being clean. If the hungry philosopher holds all its forks, and if
  it has no request for any dirty fork that it holds, then it transits
  to eating, and records all the forks that it holds in the eating
  state as dirty.When a hungry philosopher receives a request for a fork that it
  holds, it sends the fork if the fork is dirty, and holds on to the
  fork if it is clean.When an eating philosopher receives a request for a fork it
  registers the request in memory, and continues eating while holding
  the fork. When an eating philosopher transits to thinking it sends
  forks for all requests that it received.What could go wrong?The proof of safety is straightforward: Neighbors aren't eating
because neighbors can't hold the same fork at the same time.Before we look at the proof of progress, let's see what may go wrong.Could a group of philosophers exchange forks with each other so that
members of the group eat repeatedly, and starve a philosopher who is
not in the group? For example, in the figure below, could philosophers
\(u, v, w\) exchange forks so that they each eat in turn, and starve
\(y\)?Fig.13: Potential Problems: What could go wrong?Could the system enter a deadlock state in which each hungry philosopher in a
group holds only some --- but not all --- of the forks that it needs
to eat, while other members of the group hold the remaining forks?Proof of CorrectnessThe algorithm is correct. We are required to prove that every hungery
philosopher eats eventually:\(
\forall v: \quad v.h \leadsto v.e
\)where for a philosopher \(v\), \(v.h\) holds exactly when \(v\) is
hungry and \(v.e\) holds exactly when \(v\) is
eating.Variant FunctionTo prove this progress property we find a variant function that
satisfies the following two conditions:Safety: The variant function does not increase while \(v\)
  remains hungry.progress: The following predicate does not hold forever: The
  variant function remains unchanged and \(v\) remains hungry.We propose a variant function which is a pair of integers
 \(nT, nH\),  which are the number of thinking and hungry philosophers,
respectively, of 
higher priority than \(v\).
In terms of the priority graph, \(nT, nH\) are the numbers of thinking
and hungry philosophers (i.e. vertices) with paths to \(v\).Example of Variant FunctionThe figure below shows a portion of the priority graph in a state of the
system. The figure only shows philosophers with higher priority than
philosopher \(v\), i.e., it only shows vertices in the graph with
paths to \(v\). Since eating philosophers have lower priority than
their neighbors, eating philosophers don't appear in this graph.A hungry philosopher is marked with an "H" and a thinking philosopher
with a "T." In the diagram, philosophers \(v, w, x, y\) are hungry and
\(u\) is thinking.
Forks are located at philosophers and are shown as small
colored circles. A dirty fork is colored red and clean one is
blue. For example the fork shared by \(v\) and \(y\) is at \(y\) and
is clean.Fig.15: A Variant Function: Numbers of higher priority
    thinking, hungry agentsExample of Changes to Variant FunctionThe next figure is an example of changes to the variant function. The
diagram on the top left shows the higher priority vertices in the
state of the previous figure. If agent \(x\) eats next the priority
graph transits to the diagram on the top right, and the variant
function \((nT, nH)\) changes from \((1, 3)\) to \((1, 2)\).Fig.16: Example of Values of the Variant FunctionIf agent \(y\) eats next the priority
graph transits to the diagram on the bottom right, and the variant
function \((nT, nH)\) changes from \((1, 2)\) to \((1, 1)\).
If agent \(w\) eats next the priority
graph transits to the diagram on the bottom left, and the variant
function \((nT, nH)\) changes from \((1, 1)\) to \((0, 0)\).Proof that the variant function does not increase
while \(v\) remains hungryIf a philosopher of higher priority than \(v\) transits from thinking
to hungry then \(nT\) decreases. Though \(nH\) increases, the variant function \((nT,
nH)\) decreases because ordering of function values is done
lexicographically.If a philosopher of higher priority than \(v\) transits from hungry
to eating then \(nH\) decreases,  and so the variant function \((nT,
nH)\) decreases.Proof that the following predicate does not
hold forever: The variant function remains unchanged and
\(v\) remains hungry.Let \(w\) be a highest-priority hungry philosopher, i.e. a philosopher with no
hungry philosopher with priority higher than \(w\). (Note: \(w\) may be
the same as \(v\).)
All philosophers with priority higher than \(w\) are thinking. In the
next paragraph we show that either \(w\) gets all its forks and then transits
from hungry to eating, or the variant function decreases.From the algorithm, a hungry philosopher \(w\) requests forks from its
neighbors. From the algorithm, \(w\) eventually gets forks from 
all its lower priority neighbors. A higher priority neighbor \(x\) of
\(w\) is thinking. So when \(x\) gets a request for a fork from \(w\)
either (1) \(x\) sends the requested fork to \(w\) or (2) \(x\) transits from
thinking to hungry in which case the variant function \((nT, nH)\)
decreases.Summary: Key Ideas of this ModuleThis module introduced the problem of distributed mutual exclusion;
showed how the good neighbor policy --- a winning agent reduces its
priority to be lower than all the agents that it competes with --
solves this conflict resolution problem; introduced tokens and what
agents know about other agents holding tokens; and showed a proof
pattern that is one of the most common patterns for proving progress.ReviewWhat is the meaning of mutual exclusion among neighboring
  philosophers?An invariant of the algorithm is that each token is in exactly one
  place: an agent or a channel. How does this invariant help in
  designing the algorithm?An invariant of the algorithm is that the relative priorities among
  agents forms a partial order --- the priority graph is acyclic. What
  can go wrong if the priority graph has cycles?An agent that has a request for a fork releases the fork if the fork
  is dirty, and holds on to the fork if the fork is clean. What could
  go wrong if an agent releases a fork when it gets a request,
  regardless of whether the fork is clean or dirty?K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology



Figure Link: https://kmchandy.github.io/DistributedCollaboration/DiningPhilosophers/Slide07.jpg

Caption: Fig.7: Fork on each edge of the agent communication
    graph


A client is hungryleads-toits OS agent knowing that its client is
hungry. Theleads-tocaptures the fact that the OS doesn't know
the client's state at the instant that the client transitions from
thinking to hungry.Introduction of TokensWe introduce a token on each edge of the agent communication graph,
see figure 4. The token on an edge \(v, w\) is in one of four states:
held by \(v\), in the channel from \(v\) to \(w\), held by \(w\), or
in the channel from \(w\) to \(v\). Therefore while \(v\) holds this
token it knows that \(w\) doesn't hold it. Likewise, while \(w\) holds
this token it knows that \(v\) doesn't.Fig.7: Fork on each edge of the agent communication
    graphThese tokens are calledforks. (They are calledchopsticksin some papers.) An agent eats only if it holds
forks for all the edges incident on it. Therefore, while an agent eats
none of its neighbors do, and so the safety specification is satisfied.Key Question: When does a hungry agent yield
forks?An eating philosopher holds on to all its forks until it finishes
eating. A thinking philosopher can give a fork to a neighbor that
requests it. So the key question is:Under what conditions should a
hungry neighbor give a fork that it holds to a neighbor that requests
it?Suppose every hungry agent gives a fork to a neighbor that requests
it. Then we can get the scenario shown in the figure below.Fig.8: Scenario when hungry agents yield forksThe figure shows a system with 3 agents indexed 0, 1, 2. The forks are
    shown as small colored circles. The state on
    the left shows agent \(j\) holding the fork that it shares with
    agent \((j+1)\:\textrm{mod}\:3\). If each agent yields the fork to
    its neighbor we get the state on the right in which agent \(j\)
    holds the fork that it shares with 
    agent \((j-1)\:\textrm{mod}\:3\). So, if all hungry agents yield
    forks, the states can alternate for ever between the one on the
left and the one on the right. In this case hungry agents starve: they
    remain hungry forever.If hungry agents don't yield forks then the state on the left persists
for ever. In this case too, hungry agents starve.Creating a Partial Order of PrioritiesLet's assign priorities to agents so that a hungry agent \(v\)
releases a fork to a neighbor \(w\) only if \(v\) has lower priority
than \(w\). If there is a cycle of agents, all with the same priority,
then we get the situation shown in figure 8. So, we will ensure that
priorities form a partial order in all states in all
transitions. Priorities form a partial order exactly when the priority
graph is acyclic. The graph has an edge from \(v\) to \(w\) exactly
when \(v\) has higher priority over \(w\).In the figure below, the diagram on the left shows an acyclic priority
graph while the one on the right shows a graph with a cycle.Fig.9: Priority Graph must be AcyclicHow should Priorities Change?Fig.10: How should priorities change when \(v\)
    eats?How should priorities change when an agent eats so that the priority
graph remains acyclic? For example, consider
the priority graph shown in figure 10. Assume agent \(v\) has all its
forks and is about to eat. Should the directions of edges incident on
\(v\) be flipped? Or should \(v\) have lower priority than all its
neighbors, i.e. all edges incident on \(v\) point towards \(v\)?What happens if we flip the directions of the edges incident on \(v\)?
After the flip, the edges are directed from \(w\), \(x\) and \(y\) towards \(v\), and
from \(v\) to \(u\). But now we have a cycle: \(y\) to \(v\) to \(u\)
to \(w\) to \(y\). So, flipping edge directions doesn't work.What happens if agents adopt the good neighbor policy?
The winner of a conflict gives itself lower priority thanallthe agents
with which it competes. So, an agent that starts eating gives itself lower
priority than all its neighbors. All edges point towards an eating
agent.Fig.11: Winner gets lower priority than its neighborsWhen all edges incident on a vertex point towards the vertex then
there is no cycle through that vertex. So, directing all edges
incident on an eating vertex towards the eating vertex maintains
acyclicity of the graph.For example, in the figure directing all edges incident on vertex \(v\) towards
 \(v\) ensures that no new cycle is created.Agent's priority does not decrease until the agent
eats.How an Agent knows its PriorityWe assign an attributeclean / dirtyto forks.
A fork is eitherdirtyorclean.
The forks held by an eating agent aredirty. When
an agent receives a fork from another agent, the receiving agent
cleans the fork. So the receiver holds acleanfork, and the
fork remains clean until an agent next eats with it.
(This is the "hygenic solution:" Another - sad? - attempt at CS
humor.)An agent holding a dirty fork knows that it has lower priority than
the agent with which it shares the fork. Likewise,
an agent holding a clean fork knows that it has higher priority than
the agent with which it shares the fork.If an agent does not hold the fork that it shares with a neighbor then
the agent does not know its priority with respect to that neighbor.Example of a Fork's LifecycleThe diagram below shows states of a fork shared by agents \(u\) and
\(v\). The red arrow shows priority, and the black arrows show
channels. The blue dot represents the fork.In the figure on the top left, agent \(u\) is hungry and holds a clean
fork. So, \(u\) knows that it has priority over \(v\). At this point
\(v\) does not know whether \(v\) has priority over \(u\) or not.The next figure, at the top right, shows that when \(u\) transits from
hungry to eating, the fork becomes dirty, and \(u\) has lower priority
than \(v\). Agent \(u\) continues to hold the fork while it eats.The next figure, bottom right, shows the situation after \(u\) gets a
request for the fork from \(v\). Because \(u\) got the request from
\(v\) and \(u\) hasn't sent the fork to \(v\), agent \(u\) knows that
\(v\) is hungry. Since the fork is dirty, \(u\) sends
the fork to \(v\). The figure shows the fork in the channel from \(u\)
to \(v\). While the fork is in the channel it doesn't matter
whether the fork is clean or dirty; however, merely for convenience,
let's assume that \(u\), being hygenic, cleans the fork before sending
it to its partner. While the fork is in a
channel the priority doesn't change but neither \(u\) nor \(v\) knows
what the priority is.The next figure, bottom left, shows the situation when \(v\) receives
the fork.  Receiving the fork doesn't change the priority.
At this point \(v\) is hungry and the fork is
clean and so \(v\) knows that it has higher
priority. \(v\) holds on to the fork until it next eats.Fig.12: How an Agent knows its PriorityAlgorithmProperties of Reachable StatesHere is a list of some of the properties of states in all
trajectories.The priority graph is acyclic. \(u\) has priority over a neighbor
  \(v\) exactly when \(u\) holds the fork that it shares with \(v\)
  and the fork is clean, or the fork is in the channel from \(v\) to
  \(u\), or \(v\) holds the fork and the fork is dirty.Eating philosophers hold the forks for all edges incident on them,
  and these forks are dirty.All forks held by thinking philosphers are dirty.Thinking philosophers never send requests and never receive
  forks. Thinking philosophers respond to request for forks by sending
  the requested forks.Initial StatesInitially all philosophers are thinking; all forks are dirty; and all channels are
empty. The forks are placed so that the priority graph is acyclic.
The initial assignment of forks is as follows.
Given an arbitrary acyclic graph, for any edge directed from \(v\) to
\(w\), the fork shared by \(v\) and \(w\) is initially at \(w\) and
the fork is dirty.Algorithm CommandsThe algorithm is specified by the following commands.When a thinking philosophers gets a request for a fork that it holds
  it sends the fork. (A fork held by a thinking philosopher is dirty.)When a thinking philosopher transits to hungry it sends requests for
  all forks that it does not hold.When a hungry philosopher receives a fork, it records the fork as
  being clean. If the hungry philosopher holds all its forks, and if
  it has no request for any dirty fork that it holds, then it transits
  to eating, and records all the forks that it holds in the eating
  state as dirty.When a hungry philosopher receives a request for a fork that it
  holds, it sends the fork if the fork is dirty, and holds on to the
  fork if it is clean.When an eating philosopher receives a request for a fork it
  registers the request in memory, and continues eating while holding
  the fork. When an eating philosopher transits to thinking it sends
  forks for all requests that it received.What could go wrong?The proof of safety is straightforward: Neighbors aren't eating
because neighbors can't hold the same fork at the same time.Before we look at the proof of progress, let's see what may go wrong.Could a group of philosophers exchange forks with each other so that
members of the group eat repeatedly, and starve a philosopher who is
not in the group? For example, in the figure below, could philosophers
\(u, v, w\) exchange forks so that they each eat in turn, and starve
\(y\)?Fig.13: Potential Problems: What could go wrong?Could the system enter a deadlock state in which each hungry philosopher in a
group holds only some --- but not all --- of the forks that it needs
to eat, while other members of the group hold the remaining forks?Proof of CorrectnessThe algorithm is correct. We are required to prove that every hungery
philosopher eats eventually:\(
\forall v: \quad v.h \leadsto v.e
\)where for a philosopher \(v\), \(v.h\) holds exactly when \(v\) is
hungry and \(v.e\) holds exactly when \(v\) is
eating.Variant FunctionTo prove this progress property we find a variant function that
satisfies the following two conditions:Safety: The variant function does not increase while \(v\)
  remains hungry.progress: The following predicate does not hold forever: The
  variant function remains unchanged and \(v\) remains hungry.We propose a variant function which is a pair of integers
 \(nT, nH\),  which are the number of thinking and hungry philosophers,
respectively, of 
higher priority than \(v\).
In terms of the priority graph, \(nT, nH\) are the numbers of thinking
and hungry philosophers (i.e. vertices) with paths to \(v\).Example of Variant FunctionThe figure below shows a portion of the priority graph in a state of the
system. The figure only shows philosophers with higher priority than
philosopher \(v\), i.e., it only shows vertices in the graph with
paths to \(v\). Since eating philosophers have lower priority than
their neighbors, eating philosophers don't appear in this graph.A hungry philosopher is marked with an "H" and a thinking philosopher
with a "T." In the diagram, philosophers \(v, w, x, y\) are hungry and
\(u\) is thinking.
Forks are located at philosophers and are shown as small
colored circles. A dirty fork is colored red and clean one is
blue. For example the fork shared by \(v\) and \(y\) is at \(y\) and
is clean.Fig.15: A Variant Function: Numbers of higher priority
    thinking, hungry agentsExample of Changes to Variant FunctionThe next figure is an example of changes to the variant function. The
diagram on the top left shows the higher priority vertices in the
state of the previous figure. If agent \(x\) eats next the priority
graph transits to the diagram on the top right, and the variant
function \((nT, nH)\) changes from \((1, 3)\) to \((1, 2)\).Fig.16: Example of Values of the Variant FunctionIf agent \(y\) eats next the priority
graph transits to the diagram on the bottom right, and the variant
function \((nT, nH)\) changes from \((1, 2)\) to \((1, 1)\).
If agent \(w\) eats next the priority
graph transits to the diagram on the bottom left, and the variant
function \((nT, nH)\) changes from \((1, 1)\) to \((0, 0)\).Proof that the variant function does not increase
while \(v\) remains hungryIf a philosopher of higher priority than \(v\) transits from thinking
to hungry then \(nT\) decreases. Though \(nH\) increases, the variant function \((nT,
nH)\) decreases because ordering of function values is done
lexicographically.If a philosopher of higher priority than \(v\) transits from hungry
to eating then \(nH\) decreases,  and so the variant function \((nT,
nH)\) decreases.Proof that the following predicate does not
hold forever: The variant function remains unchanged and
\(v\) remains hungry.Let \(w\) be a highest-priority hungry philosopher, i.e. a philosopher with no
hungry philosopher with priority higher than \(w\). (Note: \(w\) may be
the same as \(v\).)
All philosophers with priority higher than \(w\) are thinking. In the
next paragraph we show that either \(w\) gets all its forks and then transits
from hungry to eating, or the variant function decreases.From the algorithm, a hungry philosopher \(w\) requests forks from its
neighbors. From the algorithm, \(w\) eventually gets forks from 
all its lower priority neighbors. A higher priority neighbor \(x\) of
\(w\) is thinking. So when \(x\) gets a request for a fork from \(w\)
either (1) \(x\) sends the requested fork to \(w\) or (2) \(x\) transits from
thinking to hungry in which case the variant function \((nT, nH)\)
decreases.Summary: Key Ideas of this ModuleThis module introduced the problem of distributed mutual exclusion;
showed how the good neighbor policy --- a winning agent reduces its
priority to be lower than all the agents that it competes with --
solves this conflict resolution problem; introduced tokens and what
agents know about other agents holding tokens; and showed a proof
pattern that is one of the most common patterns for proving progress.ReviewWhat is the meaning of mutual exclusion among neighboring
  philosophers?An invariant of the algorithm is that each token is in exactly one
  place: an agent or a channel. How does this invariant help in
  designing the algorithm?An invariant of the algorithm is that the relative priorities among
  agents forms a partial order --- the priority graph is acyclic. What
  can go wrong if the priority graph has cycles?An agent that has a request for a fork releases the fork if the fork
  is dirty, and holds on to the fork if the fork is clean. What could
  go wrong if an agent releases a fork when it gets a request,
  regardless of whether the fork is clean or dirty?K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology



Figure Link: https://kmchandy.github.io/DistributedCollaboration/DiningPhilosophers/Slide07.jpg

Caption: Fig.7: Fork on each edge of the agent communication
    graph


These tokens are calledforks. (They are calledchopsticksin some papers.) An agent eats only if it holds
forks for all the edges incident on it. Therefore, while an agent eats
none of its neighbors do, and so the safety specification is satisfied.Key Question: When does a hungry agent yield
forks?An eating philosopher holds on to all its forks until it finishes
eating. A thinking philosopher can give a fork to a neighbor that
requests it. So the key question is:Under what conditions should a
hungry neighbor give a fork that it holds to a neighbor that requests
it?Suppose every hungry agent gives a fork to a neighbor that requests
it. Then we can get the scenario shown in the figure below.Fig.8: Scenario when hungry agents yield forksThe figure shows a system with 3 agents indexed 0, 1, 2. The forks are
    shown as small colored circles. The state on
    the left shows agent \(j\) holding the fork that it shares with
    agent \((j+1)\:\textrm{mod}\:3\). If each agent yields the fork to
    its neighbor we get the state on the right in which agent \(j\)
    holds the fork that it shares with 
    agent \((j-1)\:\textrm{mod}\:3\). So, if all hungry agents yield
    forks, the states can alternate for ever between the one on the
left and the one on the right. In this case hungry agents starve: they
    remain hungry forever.If hungry agents don't yield forks then the state on the left persists
for ever. In this case too, hungry agents starve.Creating a Partial Order of PrioritiesLet's assign priorities to agents so that a hungry agent \(v\)
releases a fork to a neighbor \(w\) only if \(v\) has lower priority
than \(w\). If there is a cycle of agents, all with the same priority,
then we get the situation shown in figure 8. So, we will ensure that
priorities form a partial order in all states in all
transitions. Priorities form a partial order exactly when the priority
graph is acyclic. The graph has an edge from \(v\) to \(w\) exactly
when \(v\) has higher priority over \(w\).In the figure below, the diagram on the left shows an acyclic priority
graph while the one on the right shows a graph with a cycle.Fig.9: Priority Graph must be AcyclicHow should Priorities Change?Fig.10: How should priorities change when \(v\)
    eats?How should priorities change when an agent eats so that the priority
graph remains acyclic? For example, consider
the priority graph shown in figure 10. Assume agent \(v\) has all its
forks and is about to eat. Should the directions of edges incident on
\(v\) be flipped? Or should \(v\) have lower priority than all its
neighbors, i.e. all edges incident on \(v\) point towards \(v\)?What happens if we flip the directions of the edges incident on \(v\)?
After the flip, the edges are directed from \(w\), \(x\) and \(y\) towards \(v\), and
from \(v\) to \(u\). But now we have a cycle: \(y\) to \(v\) to \(u\)
to \(w\) to \(y\). So, flipping edge directions doesn't work.What happens if agents adopt the good neighbor policy?
The winner of a conflict gives itself lower priority thanallthe agents
with which it competes. So, an agent that starts eating gives itself lower
priority than all its neighbors. All edges point towards an eating
agent.Fig.11: Winner gets lower priority than its neighborsWhen all edges incident on a vertex point towards the vertex then
there is no cycle through that vertex. So, directing all edges
incident on an eating vertex towards the eating vertex maintains
acyclicity of the graph.For example, in the figure directing all edges incident on vertex \(v\) towards
 \(v\) ensures that no new cycle is created.Agent's priority does not decrease until the agent
eats.How an Agent knows its PriorityWe assign an attributeclean / dirtyto forks.
A fork is eitherdirtyorclean.
The forks held by an eating agent aredirty. When
an agent receives a fork from another agent, the receiving agent
cleans the fork. So the receiver holds acleanfork, and the
fork remains clean until an agent next eats with it.
(This is the "hygenic solution:" Another - sad? - attempt at CS
humor.)An agent holding a dirty fork knows that it has lower priority than
the agent with which it shares the fork. Likewise,
an agent holding a clean fork knows that it has higher priority than
the agent with which it shares the fork.If an agent does not hold the fork that it shares with a neighbor then
the agent does not know its priority with respect to that neighbor.Example of a Fork's LifecycleThe diagram below shows states of a fork shared by agents \(u\) and
\(v\). The red arrow shows priority, and the black arrows show
channels. The blue dot represents the fork.In the figure on the top left, agent \(u\) is hungry and holds a clean
fork. So, \(u\) knows that it has priority over \(v\). At this point
\(v\) does not know whether \(v\) has priority over \(u\) or not.The next figure, at the top right, shows that when \(u\) transits from
hungry to eating, the fork becomes dirty, and \(u\) has lower priority
than \(v\). Agent \(u\) continues to hold the fork while it eats.The next figure, bottom right, shows the situation after \(u\) gets a
request for the fork from \(v\). Because \(u\) got the request from
\(v\) and \(u\) hasn't sent the fork to \(v\), agent \(u\) knows that
\(v\) is hungry. Since the fork is dirty, \(u\) sends
the fork to \(v\). The figure shows the fork in the channel from \(u\)
to \(v\). While the fork is in the channel it doesn't matter
whether the fork is clean or dirty; however, merely for convenience,
let's assume that \(u\), being hygenic, cleans the fork before sending
it to its partner. While the fork is in a
channel the priority doesn't change but neither \(u\) nor \(v\) knows
what the priority is.The next figure, bottom left, shows the situation when \(v\) receives
the fork.  Receiving the fork doesn't change the priority.
At this point \(v\) is hungry and the fork is
clean and so \(v\) knows that it has higher
priority. \(v\) holds on to the fork until it next eats.Fig.12: How an Agent knows its PriorityAlgorithmProperties of Reachable StatesHere is a list of some of the properties of states in all
trajectories.The priority graph is acyclic. \(u\) has priority over a neighbor
  \(v\) exactly when \(u\) holds the fork that it shares with \(v\)
  and the fork is clean, or the fork is in the channel from \(v\) to
  \(u\), or \(v\) holds the fork and the fork is dirty.Eating philosophers hold the forks for all edges incident on them,
  and these forks are dirty.All forks held by thinking philosphers are dirty.Thinking philosophers never send requests and never receive
  forks. Thinking philosophers respond to request for forks by sending
  the requested forks.Initial StatesInitially all philosophers are thinking; all forks are dirty; and all channels are
empty. The forks are placed so that the priority graph is acyclic.
The initial assignment of forks is as follows.
Given an arbitrary acyclic graph, for any edge directed from \(v\) to
\(w\), the fork shared by \(v\) and \(w\) is initially at \(w\) and
the fork is dirty.Algorithm CommandsThe algorithm is specified by the following commands.When a thinking philosophers gets a request for a fork that it holds
  it sends the fork. (A fork held by a thinking philosopher is dirty.)When a thinking philosopher transits to hungry it sends requests for
  all forks that it does not hold.When a hungry philosopher receives a fork, it records the fork as
  being clean. If the hungry philosopher holds all its forks, and if
  it has no request for any dirty fork that it holds, then it transits
  to eating, and records all the forks that it holds in the eating
  state as dirty.When a hungry philosopher receives a request for a fork that it
  holds, it sends the fork if the fork is dirty, and holds on to the
  fork if it is clean.When an eating philosopher receives a request for a fork it
  registers the request in memory, and continues eating while holding
  the fork. When an eating philosopher transits to thinking it sends
  forks for all requests that it received.What could go wrong?The proof of safety is straightforward: Neighbors aren't eating
because neighbors can't hold the same fork at the same time.Before we look at the proof of progress, let's see what may go wrong.Could a group of philosophers exchange forks with each other so that
members of the group eat repeatedly, and starve a philosopher who is
not in the group? For example, in the figure below, could philosophers
\(u, v, w\) exchange forks so that they each eat in turn, and starve
\(y\)?Fig.13: Potential Problems: What could go wrong?Could the system enter a deadlock state in which each hungry philosopher in a
group holds only some --- but not all --- of the forks that it needs
to eat, while other members of the group hold the remaining forks?Proof of CorrectnessThe algorithm is correct. We are required to prove that every hungery
philosopher eats eventually:\(
\forall v: \quad v.h \leadsto v.e
\)where for a philosopher \(v\), \(v.h\) holds exactly when \(v\) is
hungry and \(v.e\) holds exactly when \(v\) is
eating.Variant FunctionTo prove this progress property we find a variant function that
satisfies the following two conditions:Safety: The variant function does not increase while \(v\)
  remains hungry.progress: The following predicate does not hold forever: The
  variant function remains unchanged and \(v\) remains hungry.We propose a variant function which is a pair of integers
 \(nT, nH\),  which are the number of thinking and hungry philosophers,
respectively, of 
higher priority than \(v\).
In terms of the priority graph, \(nT, nH\) are the numbers of thinking
and hungry philosophers (i.e. vertices) with paths to \(v\).Example of Variant FunctionThe figure below shows a portion of the priority graph in a state of the
system. The figure only shows philosophers with higher priority than
philosopher \(v\), i.e., it only shows vertices in the graph with
paths to \(v\). Since eating philosophers have lower priority than
their neighbors, eating philosophers don't appear in this graph.A hungry philosopher is marked with an "H" and a thinking philosopher
with a "T." In the diagram, philosophers \(v, w, x, y\) are hungry and
\(u\) is thinking.
Forks are located at philosophers and are shown as small
colored circles. A dirty fork is colored red and clean one is
blue. For example the fork shared by \(v\) and \(y\) is at \(y\) and
is clean.Fig.15: A Variant Function: Numbers of higher priority
    thinking, hungry agentsExample of Changes to Variant FunctionThe next figure is an example of changes to the variant function. The
diagram on the top left shows the higher priority vertices in the
state of the previous figure. If agent \(x\) eats next the priority
graph transits to the diagram on the top right, and the variant
function \((nT, nH)\) changes from \((1, 3)\) to \((1, 2)\).Fig.16: Example of Values of the Variant FunctionIf agent \(y\) eats next the priority
graph transits to the diagram on the bottom right, and the variant
function \((nT, nH)\) changes from \((1, 2)\) to \((1, 1)\).
If agent \(w\) eats next the priority
graph transits to the diagram on the bottom left, and the variant
function \((nT, nH)\) changes from \((1, 1)\) to \((0, 0)\).Proof that the variant function does not increase
while \(v\) remains hungryIf a philosopher of higher priority than \(v\) transits from thinking
to hungry then \(nT\) decreases. Though \(nH\) increases, the variant function \((nT,
nH)\) decreases because ordering of function values is done
lexicographically.If a philosopher of higher priority than \(v\) transits from hungry
to eating then \(nH\) decreases,  and so the variant function \((nT,
nH)\) decreases.Proof that the following predicate does not
hold forever: The variant function remains unchanged and
\(v\) remains hungry.Let \(w\) be a highest-priority hungry philosopher, i.e. a philosopher with no
hungry philosopher with priority higher than \(w\). (Note: \(w\) may be
the same as \(v\).)
All philosophers with priority higher than \(w\) are thinking. In the
next paragraph we show that either \(w\) gets all its forks and then transits
from hungry to eating, or the variant function decreases.From the algorithm, a hungry philosopher \(w\) requests forks from its
neighbors. From the algorithm, \(w\) eventually gets forks from 
all its lower priority neighbors. A higher priority neighbor \(x\) of
\(w\) is thinking. So when \(x\) gets a request for a fork from \(w\)
either (1) \(x\) sends the requested fork to \(w\) or (2) \(x\) transits from
thinking to hungry in which case the variant function \((nT, nH)\)
decreases.Summary: Key Ideas of this ModuleThis module introduced the problem of distributed mutual exclusion;
showed how the good neighbor policy --- a winning agent reduces its
priority to be lower than all the agents that it competes with --
solves this conflict resolution problem; introduced tokens and what
agents know about other agents holding tokens; and showed a proof
pattern that is one of the most common patterns for proving progress.ReviewWhat is the meaning of mutual exclusion among neighboring
  philosophers?An invariant of the algorithm is that each token is in exactly one
  place: an agent or a channel. How does this invariant help in
  designing the algorithm?An invariant of the algorithm is that the relative priorities among
  agents forms a partial order --- the priority graph is acyclic. What
  can go wrong if the priority graph has cycles?An agent that has a request for a fork releases the fork if the fork
  is dirty, and holds on to the fork if the fork is clean. What could
  go wrong if an agent releases a fork when it gets a request,
  regardless of whether the fork is clean or dirty?K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology



Figure Link: https://kmchandy.github.io/DistributedCollaboration/DiningPhilosophers/Slide08.jpg

Caption: Fig.8: Scenario when hungry agents yield forks


Suppose every hungry agent gives a fork to a neighbor that requests
it. Then we can get the scenario shown in the figure below.Fig.8: Scenario when hungry agents yield forksThe figure shows a system with 3 agents indexed 0, 1, 2. The forks are
    shown as small colored circles. The state on
    the left shows agent \(j\) holding the fork that it shares with
    agent \((j+1)\:\textrm{mod}\:3\). If each agent yields the fork to
    its neighbor we get the state on the right in which agent \(j\)
    holds the fork that it shares with 
    agent \((j-1)\:\textrm{mod}\:3\). So, if all hungry agents yield
    forks, the states can alternate for ever between the one on the
left and the one on the right. In this case hungry agents starve: they
    remain hungry forever.If hungry agents don't yield forks then the state on the left persists
for ever. In this case too, hungry agents starve.Creating a Partial Order of PrioritiesLet's assign priorities to agents so that a hungry agent \(v\)
releases a fork to a neighbor \(w\) only if \(v\) has lower priority
than \(w\). If there is a cycle of agents, all with the same priority,
then we get the situation shown in figure 8. So, we will ensure that
priorities form a partial order in all states in all
transitions. Priorities form a partial order exactly when the priority
graph is acyclic. The graph has an edge from \(v\) to \(w\) exactly
when \(v\) has higher priority over \(w\).In the figure below, the diagram on the left shows an acyclic priority
graph while the one on the right shows a graph with a cycle.Fig.9: Priority Graph must be AcyclicHow should Priorities Change?Fig.10: How should priorities change when \(v\)
    eats?How should priorities change when an agent eats so that the priority
graph remains acyclic? For example, consider
the priority graph shown in figure 10. Assume agent \(v\) has all its
forks and is about to eat. Should the directions of edges incident on
\(v\) be flipped? Or should \(v\) have lower priority than all its
neighbors, i.e. all edges incident on \(v\) point towards \(v\)?What happens if we flip the directions of the edges incident on \(v\)?
After the flip, the edges are directed from \(w\), \(x\) and \(y\) towards \(v\), and
from \(v\) to \(u\). But now we have a cycle: \(y\) to \(v\) to \(u\)
to \(w\) to \(y\). So, flipping edge directions doesn't work.What happens if agents adopt the good neighbor policy?
The winner of a conflict gives itself lower priority thanallthe agents
with which it competes. So, an agent that starts eating gives itself lower
priority than all its neighbors. All edges point towards an eating
agent.Fig.11: Winner gets lower priority than its neighborsWhen all edges incident on a vertex point towards the vertex then
there is no cycle through that vertex. So, directing all edges
incident on an eating vertex towards the eating vertex maintains
acyclicity of the graph.For example, in the figure directing all edges incident on vertex \(v\) towards
 \(v\) ensures that no new cycle is created.Agent's priority does not decrease until the agent
eats.How an Agent knows its PriorityWe assign an attributeclean / dirtyto forks.
A fork is eitherdirtyorclean.
The forks held by an eating agent aredirty. When
an agent receives a fork from another agent, the receiving agent
cleans the fork. So the receiver holds acleanfork, and the
fork remains clean until an agent next eats with it.
(This is the "hygenic solution:" Another - sad? - attempt at CS
humor.)An agent holding a dirty fork knows that it has lower priority than
the agent with which it shares the fork. Likewise,
an agent holding a clean fork knows that it has higher priority than
the agent with which it shares the fork.If an agent does not hold the fork that it shares with a neighbor then
the agent does not know its priority with respect to that neighbor.Example of a Fork's LifecycleThe diagram below shows states of a fork shared by agents \(u\) and
\(v\). The red arrow shows priority, and the black arrows show
channels. The blue dot represents the fork.In the figure on the top left, agent \(u\) is hungry and holds a clean
fork. So, \(u\) knows that it has priority over \(v\). At this point
\(v\) does not know whether \(v\) has priority over \(u\) or not.The next figure, at the top right, shows that when \(u\) transits from
hungry to eating, the fork becomes dirty, and \(u\) has lower priority
than \(v\). Agent \(u\) continues to hold the fork while it eats.The next figure, bottom right, shows the situation after \(u\) gets a
request for the fork from \(v\). Because \(u\) got the request from
\(v\) and \(u\) hasn't sent the fork to \(v\), agent \(u\) knows that
\(v\) is hungry. Since the fork is dirty, \(u\) sends
the fork to \(v\). The figure shows the fork in the channel from \(u\)
to \(v\). While the fork is in the channel it doesn't matter
whether the fork is clean or dirty; however, merely for convenience,
let's assume that \(u\), being hygenic, cleans the fork before sending
it to its partner. While the fork is in a
channel the priority doesn't change but neither \(u\) nor \(v\) knows
what the priority is.The next figure, bottom left, shows the situation when \(v\) receives
the fork.  Receiving the fork doesn't change the priority.
At this point \(v\) is hungry and the fork is
clean and so \(v\) knows that it has higher
priority. \(v\) holds on to the fork until it next eats.Fig.12: How an Agent knows its PriorityAlgorithmProperties of Reachable StatesHere is a list of some of the properties of states in all
trajectories.The priority graph is acyclic. \(u\) has priority over a neighbor
  \(v\) exactly when \(u\) holds the fork that it shares with \(v\)
  and the fork is clean, or the fork is in the channel from \(v\) to
  \(u\), or \(v\) holds the fork and the fork is dirty.Eating philosophers hold the forks for all edges incident on them,
  and these forks are dirty.All forks held by thinking philosphers are dirty.Thinking philosophers never send requests and never receive
  forks. Thinking philosophers respond to request for forks by sending
  the requested forks.Initial StatesInitially all philosophers are thinking; all forks are dirty; and all channels are
empty. The forks are placed so that the priority graph is acyclic.
The initial assignment of forks is as follows.
Given an arbitrary acyclic graph, for any edge directed from \(v\) to
\(w\), the fork shared by \(v\) and \(w\) is initially at \(w\) and
the fork is dirty.Algorithm CommandsThe algorithm is specified by the following commands.When a thinking philosophers gets a request for a fork that it holds
  it sends the fork. (A fork held by a thinking philosopher is dirty.)When a thinking philosopher transits to hungry it sends requests for
  all forks that it does not hold.When a hungry philosopher receives a fork, it records the fork as
  being clean. If the hungry philosopher holds all its forks, and if
  it has no request for any dirty fork that it holds, then it transits
  to eating, and records all the forks that it holds in the eating
  state as dirty.When a hungry philosopher receives a request for a fork that it
  holds, it sends the fork if the fork is dirty, and holds on to the
  fork if it is clean.When an eating philosopher receives a request for a fork it
  registers the request in memory, and continues eating while holding
  the fork. When an eating philosopher transits to thinking it sends
  forks for all requests that it received.What could go wrong?The proof of safety is straightforward: Neighbors aren't eating
because neighbors can't hold the same fork at the same time.Before we look at the proof of progress, let's see what may go wrong.Could a group of philosophers exchange forks with each other so that
members of the group eat repeatedly, and starve a philosopher who is
not in the group? For example, in the figure below, could philosophers
\(u, v, w\) exchange forks so that they each eat in turn, and starve
\(y\)?Fig.13: Potential Problems: What could go wrong?Could the system enter a deadlock state in which each hungry philosopher in a
group holds only some --- but not all --- of the forks that it needs
to eat, while other members of the group hold the remaining forks?Proof of CorrectnessThe algorithm is correct. We are required to prove that every hungery
philosopher eats eventually:\(
\forall v: \quad v.h \leadsto v.e
\)where for a philosopher \(v\), \(v.h\) holds exactly when \(v\) is
hungry and \(v.e\) holds exactly when \(v\) is
eating.Variant FunctionTo prove this progress property we find a variant function that
satisfies the following two conditions:Safety: The variant function does not increase while \(v\)
  remains hungry.progress: The following predicate does not hold forever: The
  variant function remains unchanged and \(v\) remains hungry.We propose a variant function which is a pair of integers
 \(nT, nH\),  which are the number of thinking and hungry philosophers,
respectively, of 
higher priority than \(v\).
In terms of the priority graph, \(nT, nH\) are the numbers of thinking
and hungry philosophers (i.e. vertices) with paths to \(v\).Example of Variant FunctionThe figure below shows a portion of the priority graph in a state of the
system. The figure only shows philosophers with higher priority than
philosopher \(v\), i.e., it only shows vertices in the graph with
paths to \(v\). Since eating philosophers have lower priority than
their neighbors, eating philosophers don't appear in this graph.A hungry philosopher is marked with an "H" and a thinking philosopher
with a "T." In the diagram, philosophers \(v, w, x, y\) are hungry and
\(u\) is thinking.
Forks are located at philosophers and are shown as small
colored circles. A dirty fork is colored red and clean one is
blue. For example the fork shared by \(v\) and \(y\) is at \(y\) and
is clean.Fig.15: A Variant Function: Numbers of higher priority
    thinking, hungry agentsExample of Changes to Variant FunctionThe next figure is an example of changes to the variant function. The
diagram on the top left shows the higher priority vertices in the
state of the previous figure. If agent \(x\) eats next the priority
graph transits to the diagram on the top right, and the variant
function \((nT, nH)\) changes from \((1, 3)\) to \((1, 2)\).Fig.16: Example of Values of the Variant FunctionIf agent \(y\) eats next the priority
graph transits to the diagram on the bottom right, and the variant
function \((nT, nH)\) changes from \((1, 2)\) to \((1, 1)\).
If agent \(w\) eats next the priority
graph transits to the diagram on the bottom left, and the variant
function \((nT, nH)\) changes from \((1, 1)\) to \((0, 0)\).Proof that the variant function does not increase
while \(v\) remains hungryIf a philosopher of higher priority than \(v\) transits from thinking
to hungry then \(nT\) decreases. Though \(nH\) increases, the variant function \((nT,
nH)\) decreases because ordering of function values is done
lexicographically.If a philosopher of higher priority than \(v\) transits from hungry
to eating then \(nH\) decreases,  and so the variant function \((nT,
nH)\) decreases.Proof that the following predicate does not
hold forever: The variant function remains unchanged and
\(v\) remains hungry.Let \(w\) be a highest-priority hungry philosopher, i.e. a philosopher with no
hungry philosopher with priority higher than \(w\). (Note: \(w\) may be
the same as \(v\).)
All philosophers with priority higher than \(w\) are thinking. In the
next paragraph we show that either \(w\) gets all its forks and then transits
from hungry to eating, or the variant function decreases.From the algorithm, a hungry philosopher \(w\) requests forks from its
neighbors. From the algorithm, \(w\) eventually gets forks from 
all its lower priority neighbors. A higher priority neighbor \(x\) of
\(w\) is thinking. So when \(x\) gets a request for a fork from \(w\)
either (1) \(x\) sends the requested fork to \(w\) or (2) \(x\) transits from
thinking to hungry in which case the variant function \((nT, nH)\)
decreases.Summary: Key Ideas of this ModuleThis module introduced the problem of distributed mutual exclusion;
showed how the good neighbor policy --- a winning agent reduces its
priority to be lower than all the agents that it competes with --
solves this conflict resolution problem; introduced tokens and what
agents know about other agents holding tokens; and showed a proof
pattern that is one of the most common patterns for proving progress.ReviewWhat is the meaning of mutual exclusion among neighboring
  philosophers?An invariant of the algorithm is that each token is in exactly one
  place: an agent or a channel. How does this invariant help in
  designing the algorithm?An invariant of the algorithm is that the relative priorities among
  agents forms a partial order --- the priority graph is acyclic. What
  can go wrong if the priority graph has cycles?An agent that has a request for a fork releases the fork if the fork
  is dirty, and holds on to the fork if the fork is clean. What could
  go wrong if an agent releases a fork when it gets a request,
  regardless of whether the fork is clean or dirty?K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology



Figure Link: https://kmchandy.github.io/DistributedCollaboration/DiningPhilosophers/Slide08.jpg

Caption: Fig.8: Scenario when hungry agents yield forks


If hungry agents don't yield forks then the state on the left persists
for ever. In this case too, hungry agents starve.Creating a Partial Order of PrioritiesLet's assign priorities to agents so that a hungry agent \(v\)
releases a fork to a neighbor \(w\) only if \(v\) has lower priority
than \(w\). If there is a cycle of agents, all with the same priority,
then we get the situation shown in figure 8. So, we will ensure that
priorities form a partial order in all states in all
transitions. Priorities form a partial order exactly when the priority
graph is acyclic. The graph has an edge from \(v\) to \(w\) exactly
when \(v\) has higher priority over \(w\).In the figure below, the diagram on the left shows an acyclic priority
graph while the one on the right shows a graph with a cycle.Fig.9: Priority Graph must be AcyclicHow should Priorities Change?Fig.10: How should priorities change when \(v\)
    eats?How should priorities change when an agent eats so that the priority
graph remains acyclic? For example, consider
the priority graph shown in figure 10. Assume agent \(v\) has all its
forks and is about to eat. Should the directions of edges incident on
\(v\) be flipped? Or should \(v\) have lower priority than all its
neighbors, i.e. all edges incident on \(v\) point towards \(v\)?What happens if we flip the directions of the edges incident on \(v\)?
After the flip, the edges are directed from \(w\), \(x\) and \(y\) towards \(v\), and
from \(v\) to \(u\). But now we have a cycle: \(y\) to \(v\) to \(u\)
to \(w\) to \(y\). So, flipping edge directions doesn't work.What happens if agents adopt the good neighbor policy?
The winner of a conflict gives itself lower priority thanallthe agents
with which it competes. So, an agent that starts eating gives itself lower
priority than all its neighbors. All edges point towards an eating
agent.Fig.11: Winner gets lower priority than its neighborsWhen all edges incident on a vertex point towards the vertex then
there is no cycle through that vertex. So, directing all edges
incident on an eating vertex towards the eating vertex maintains
acyclicity of the graph.For example, in the figure directing all edges incident on vertex \(v\) towards
 \(v\) ensures that no new cycle is created.Agent's priority does not decrease until the agent
eats.How an Agent knows its PriorityWe assign an attributeclean / dirtyto forks.
A fork is eitherdirtyorclean.
The forks held by an eating agent aredirty. When
an agent receives a fork from another agent, the receiving agent
cleans the fork. So the receiver holds acleanfork, and the
fork remains clean until an agent next eats with it.
(This is the "hygenic solution:" Another - sad? - attempt at CS
humor.)An agent holding a dirty fork knows that it has lower priority than
the agent with which it shares the fork. Likewise,
an agent holding a clean fork knows that it has higher priority than
the agent with which it shares the fork.If an agent does not hold the fork that it shares with a neighbor then
the agent does not know its priority with respect to that neighbor.Example of a Fork's LifecycleThe diagram below shows states of a fork shared by agents \(u\) and
\(v\). The red arrow shows priority, and the black arrows show
channels. The blue dot represents the fork.In the figure on the top left, agent \(u\) is hungry and holds a clean
fork. So, \(u\) knows that it has priority over \(v\). At this point
\(v\) does not know whether \(v\) has priority over \(u\) or not.The next figure, at the top right, shows that when \(u\) transits from
hungry to eating, the fork becomes dirty, and \(u\) has lower priority
than \(v\). Agent \(u\) continues to hold the fork while it eats.The next figure, bottom right, shows the situation after \(u\) gets a
request for the fork from \(v\). Because \(u\) got the request from
\(v\) and \(u\) hasn't sent the fork to \(v\), agent \(u\) knows that
\(v\) is hungry. Since the fork is dirty, \(u\) sends
the fork to \(v\). The figure shows the fork in the channel from \(u\)
to \(v\). While the fork is in the channel it doesn't matter
whether the fork is clean or dirty; however, merely for convenience,
let's assume that \(u\), being hygenic, cleans the fork before sending
it to its partner. While the fork is in a
channel the priority doesn't change but neither \(u\) nor \(v\) knows
what the priority is.The next figure, bottom left, shows the situation when \(v\) receives
the fork.  Receiving the fork doesn't change the priority.
At this point \(v\) is hungry and the fork is
clean and so \(v\) knows that it has higher
priority. \(v\) holds on to the fork until it next eats.Fig.12: How an Agent knows its PriorityAlgorithmProperties of Reachable StatesHere is a list of some of the properties of states in all
trajectories.The priority graph is acyclic. \(u\) has priority over a neighbor
  \(v\) exactly when \(u\) holds the fork that it shares with \(v\)
  and the fork is clean, or the fork is in the channel from \(v\) to
  \(u\), or \(v\) holds the fork and the fork is dirty.Eating philosophers hold the forks for all edges incident on them,
  and these forks are dirty.All forks held by thinking philosphers are dirty.Thinking philosophers never send requests and never receive
  forks. Thinking philosophers respond to request for forks by sending
  the requested forks.Initial StatesInitially all philosophers are thinking; all forks are dirty; and all channels are
empty. The forks are placed so that the priority graph is acyclic.
The initial assignment of forks is as follows.
Given an arbitrary acyclic graph, for any edge directed from \(v\) to
\(w\), the fork shared by \(v\) and \(w\) is initially at \(w\) and
the fork is dirty.Algorithm CommandsThe algorithm is specified by the following commands.When a thinking philosophers gets a request for a fork that it holds
  it sends the fork. (A fork held by a thinking philosopher is dirty.)When a thinking philosopher transits to hungry it sends requests for
  all forks that it does not hold.When a hungry philosopher receives a fork, it records the fork as
  being clean. If the hungry philosopher holds all its forks, and if
  it has no request for any dirty fork that it holds, then it transits
  to eating, and records all the forks that it holds in the eating
  state as dirty.When a hungry philosopher receives a request for a fork that it
  holds, it sends the fork if the fork is dirty, and holds on to the
  fork if it is clean.When an eating philosopher receives a request for a fork it
  registers the request in memory, and continues eating while holding
  the fork. When an eating philosopher transits to thinking it sends
  forks for all requests that it received.What could go wrong?The proof of safety is straightforward: Neighbors aren't eating
because neighbors can't hold the same fork at the same time.Before we look at the proof of progress, let's see what may go wrong.Could a group of philosophers exchange forks with each other so that
members of the group eat repeatedly, and starve a philosopher who is
not in the group? For example, in the figure below, could philosophers
\(u, v, w\) exchange forks so that they each eat in turn, and starve
\(y\)?Fig.13: Potential Problems: What could go wrong?Could the system enter a deadlock state in which each hungry philosopher in a
group holds only some --- but not all --- of the forks that it needs
to eat, while other members of the group hold the remaining forks?Proof of CorrectnessThe algorithm is correct. We are required to prove that every hungery
philosopher eats eventually:\(
\forall v: \quad v.h \leadsto v.e
\)where for a philosopher \(v\), \(v.h\) holds exactly when \(v\) is
hungry and \(v.e\) holds exactly when \(v\) is
eating.Variant FunctionTo prove this progress property we find a variant function that
satisfies the following two conditions:Safety: The variant function does not increase while \(v\)
  remains hungry.progress: The following predicate does not hold forever: The
  variant function remains unchanged and \(v\) remains hungry.We propose a variant function which is a pair of integers
 \(nT, nH\),  which are the number of thinking and hungry philosophers,
respectively, of 
higher priority than \(v\).
In terms of the priority graph, \(nT, nH\) are the numbers of thinking
and hungry philosophers (i.e. vertices) with paths to \(v\).Example of Variant FunctionThe figure below shows a portion of the priority graph in a state of the
system. The figure only shows philosophers with higher priority than
philosopher \(v\), i.e., it only shows vertices in the graph with
paths to \(v\). Since eating philosophers have lower priority than
their neighbors, eating philosophers don't appear in this graph.A hungry philosopher is marked with an "H" and a thinking philosopher
with a "T." In the diagram, philosophers \(v, w, x, y\) are hungry and
\(u\) is thinking.
Forks are located at philosophers and are shown as small
colored circles. A dirty fork is colored red and clean one is
blue. For example the fork shared by \(v\) and \(y\) is at \(y\) and
is clean.Fig.15: A Variant Function: Numbers of higher priority
    thinking, hungry agentsExample of Changes to Variant FunctionThe next figure is an example of changes to the variant function. The
diagram on the top left shows the higher priority vertices in the
state of the previous figure. If agent \(x\) eats next the priority
graph transits to the diagram on the top right, and the variant
function \((nT, nH)\) changes from \((1, 3)\) to \((1, 2)\).Fig.16: Example of Values of the Variant FunctionIf agent \(y\) eats next the priority
graph transits to the diagram on the bottom right, and the variant
function \((nT, nH)\) changes from \((1, 2)\) to \((1, 1)\).
If agent \(w\) eats next the priority
graph transits to the diagram on the bottom left, and the variant
function \((nT, nH)\) changes from \((1, 1)\) to \((0, 0)\).Proof that the variant function does not increase
while \(v\) remains hungryIf a philosopher of higher priority than \(v\) transits from thinking
to hungry then \(nT\) decreases. Though \(nH\) increases, the variant function \((nT,
nH)\) decreases because ordering of function values is done
lexicographically.If a philosopher of higher priority than \(v\) transits from hungry
to eating then \(nH\) decreases,  and so the variant function \((nT,
nH)\) decreases.Proof that the following predicate does not
hold forever: The variant function remains unchanged and
\(v\) remains hungry.Let \(w\) be a highest-priority hungry philosopher, i.e. a philosopher with no
hungry philosopher with priority higher than \(w\). (Note: \(w\) may be
the same as \(v\).)
All philosophers with priority higher than \(w\) are thinking. In the
next paragraph we show that either \(w\) gets all its forks and then transits
from hungry to eating, or the variant function decreases.From the algorithm, a hungry philosopher \(w\) requests forks from its
neighbors. From the algorithm, \(w\) eventually gets forks from 
all its lower priority neighbors. A higher priority neighbor \(x\) of
\(w\) is thinking. So when \(x\) gets a request for a fork from \(w\)
either (1) \(x\) sends the requested fork to \(w\) or (2) \(x\) transits from
thinking to hungry in which case the variant function \((nT, nH)\)
decreases.Summary: Key Ideas of this ModuleThis module introduced the problem of distributed mutual exclusion;
showed how the good neighbor policy --- a winning agent reduces its
priority to be lower than all the agents that it competes with --
solves this conflict resolution problem; introduced tokens and what
agents know about other agents holding tokens; and showed a proof
pattern that is one of the most common patterns for proving progress.ReviewWhat is the meaning of mutual exclusion among neighboring
  philosophers?An invariant of the algorithm is that each token is in exactly one
  place: an agent or a channel. How does this invariant help in
  designing the algorithm?An invariant of the algorithm is that the relative priorities among
  agents forms a partial order --- the priority graph is acyclic. What
  can go wrong if the priority graph has cycles?An agent that has a request for a fork releases the fork if the fork
  is dirty, and holds on to the fork if the fork is clean. What could
  go wrong if an agent releases a fork when it gets a request,
  regardless of whether the fork is clean or dirty?K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology



Figure Link: https://kmchandy.github.io/DistributedCollaboration/DiningPhilosophers/Slide09.jpg

Caption: Fig.9: Priority Graph must be Acyclic


In the figure below, the diagram on the left shows an acyclic priority
graph while the one on the right shows a graph with a cycle.Fig.9: Priority Graph must be AcyclicHow should Priorities Change?Fig.10: How should priorities change when \(v\)
    eats?How should priorities change when an agent eats so that the priority
graph remains acyclic? For example, consider
the priority graph shown in figure 10. Assume agent \(v\) has all its
forks and is about to eat. Should the directions of edges incident on
\(v\) be flipped? Or should \(v\) have lower priority than all its
neighbors, i.e. all edges incident on \(v\) point towards \(v\)?What happens if we flip the directions of the edges incident on \(v\)?
After the flip, the edges are directed from \(w\), \(x\) and \(y\) towards \(v\), and
from \(v\) to \(u\). But now we have a cycle: \(y\) to \(v\) to \(u\)
to \(w\) to \(y\). So, flipping edge directions doesn't work.What happens if agents adopt the good neighbor policy?
The winner of a conflict gives itself lower priority thanallthe agents
with which it competes. So, an agent that starts eating gives itself lower
priority than all its neighbors. All edges point towards an eating
agent.Fig.11: Winner gets lower priority than its neighborsWhen all edges incident on a vertex point towards the vertex then
there is no cycle through that vertex. So, directing all edges
incident on an eating vertex towards the eating vertex maintains
acyclicity of the graph.For example, in the figure directing all edges incident on vertex \(v\) towards
 \(v\) ensures that no new cycle is created.Agent's priority does not decrease until the agent
eats.How an Agent knows its PriorityWe assign an attributeclean / dirtyto forks.
A fork is eitherdirtyorclean.
The forks held by an eating agent aredirty. When
an agent receives a fork from another agent, the receiving agent
cleans the fork. So the receiver holds acleanfork, and the
fork remains clean until an agent next eats with it.
(This is the "hygenic solution:" Another - sad? - attempt at CS
humor.)An agent holding a dirty fork knows that it has lower priority than
the agent with which it shares the fork. Likewise,
an agent holding a clean fork knows that it has higher priority than
the agent with which it shares the fork.If an agent does not hold the fork that it shares with a neighbor then
the agent does not know its priority with respect to that neighbor.Example of a Fork's LifecycleThe diagram below shows states of a fork shared by agents \(u\) and
\(v\). The red arrow shows priority, and the black arrows show
channels. The blue dot represents the fork.In the figure on the top left, agent \(u\) is hungry and holds a clean
fork. So, \(u\) knows that it has priority over \(v\). At this point
\(v\) does not know whether \(v\) has priority over \(u\) or not.The next figure, at the top right, shows that when \(u\) transits from
hungry to eating, the fork becomes dirty, and \(u\) has lower priority
than \(v\). Agent \(u\) continues to hold the fork while it eats.The next figure, bottom right, shows the situation after \(u\) gets a
request for the fork from \(v\). Because \(u\) got the request from
\(v\) and \(u\) hasn't sent the fork to \(v\), agent \(u\) knows that
\(v\) is hungry. Since the fork is dirty, \(u\) sends
the fork to \(v\). The figure shows the fork in the channel from \(u\)
to \(v\). While the fork is in the channel it doesn't matter
whether the fork is clean or dirty; however, merely for convenience,
let's assume that \(u\), being hygenic, cleans the fork before sending
it to its partner. While the fork is in a
channel the priority doesn't change but neither \(u\) nor \(v\) knows
what the priority is.The next figure, bottom left, shows the situation when \(v\) receives
the fork.  Receiving the fork doesn't change the priority.
At this point \(v\) is hungry and the fork is
clean and so \(v\) knows that it has higher
priority. \(v\) holds on to the fork until it next eats.Fig.12: How an Agent knows its PriorityAlgorithmProperties of Reachable StatesHere is a list of some of the properties of states in all
trajectories.The priority graph is acyclic. \(u\) has priority over a neighbor
  \(v\) exactly when \(u\) holds the fork that it shares with \(v\)
  and the fork is clean, or the fork is in the channel from \(v\) to
  \(u\), or \(v\) holds the fork and the fork is dirty.Eating philosophers hold the forks for all edges incident on them,
  and these forks are dirty.All forks held by thinking philosphers are dirty.Thinking philosophers never send requests and never receive
  forks. Thinking philosophers respond to request for forks by sending
  the requested forks.Initial StatesInitially all philosophers are thinking; all forks are dirty; and all channels are
empty. The forks are placed so that the priority graph is acyclic.
The initial assignment of forks is as follows.
Given an arbitrary acyclic graph, for any edge directed from \(v\) to
\(w\), the fork shared by \(v\) and \(w\) is initially at \(w\) and
the fork is dirty.Algorithm CommandsThe algorithm is specified by the following commands.When a thinking philosophers gets a request for a fork that it holds
  it sends the fork. (A fork held by a thinking philosopher is dirty.)When a thinking philosopher transits to hungry it sends requests for
  all forks that it does not hold.When a hungry philosopher receives a fork, it records the fork as
  being clean. If the hungry philosopher holds all its forks, and if
  it has no request for any dirty fork that it holds, then it transits
  to eating, and records all the forks that it holds in the eating
  state as dirty.When a hungry philosopher receives a request for a fork that it
  holds, it sends the fork if the fork is dirty, and holds on to the
  fork if it is clean.When an eating philosopher receives a request for a fork it
  registers the request in memory, and continues eating while holding
  the fork. When an eating philosopher transits to thinking it sends
  forks for all requests that it received.What could go wrong?The proof of safety is straightforward: Neighbors aren't eating
because neighbors can't hold the same fork at the same time.Before we look at the proof of progress, let's see what may go wrong.Could a group of philosophers exchange forks with each other so that
members of the group eat repeatedly, and starve a philosopher who is
not in the group? For example, in the figure below, could philosophers
\(u, v, w\) exchange forks so that they each eat in turn, and starve
\(y\)?Fig.13: Potential Problems: What could go wrong?Could the system enter a deadlock state in which each hungry philosopher in a
group holds only some --- but not all --- of the forks that it needs
to eat, while other members of the group hold the remaining forks?Proof of CorrectnessThe algorithm is correct. We are required to prove that every hungery
philosopher eats eventually:\(
\forall v: \quad v.h \leadsto v.e
\)where for a philosopher \(v\), \(v.h\) holds exactly when \(v\) is
hungry and \(v.e\) holds exactly when \(v\) is
eating.Variant FunctionTo prove this progress property we find a variant function that
satisfies the following two conditions:Safety: The variant function does not increase while \(v\)
  remains hungry.progress: The following predicate does not hold forever: The
  variant function remains unchanged and \(v\) remains hungry.We propose a variant function which is a pair of integers
 \(nT, nH\),  which are the number of thinking and hungry philosophers,
respectively, of 
higher priority than \(v\).
In terms of the priority graph, \(nT, nH\) are the numbers of thinking
and hungry philosophers (i.e. vertices) with paths to \(v\).Example of Variant FunctionThe figure below shows a portion of the priority graph in a state of the
system. The figure only shows philosophers with higher priority than
philosopher \(v\), i.e., it only shows vertices in the graph with
paths to \(v\). Since eating philosophers have lower priority than
their neighbors, eating philosophers don't appear in this graph.A hungry philosopher is marked with an "H" and a thinking philosopher
with a "T." In the diagram, philosophers \(v, w, x, y\) are hungry and
\(u\) is thinking.
Forks are located at philosophers and are shown as small
colored circles. A dirty fork is colored red and clean one is
blue. For example the fork shared by \(v\) and \(y\) is at \(y\) and
is clean.Fig.15: A Variant Function: Numbers of higher priority
    thinking, hungry agentsExample of Changes to Variant FunctionThe next figure is an example of changes to the variant function. The
diagram on the top left shows the higher priority vertices in the
state of the previous figure. If agent \(x\) eats next the priority
graph transits to the diagram on the top right, and the variant
function \((nT, nH)\) changes from \((1, 3)\) to \((1, 2)\).Fig.16: Example of Values of the Variant FunctionIf agent \(y\) eats next the priority
graph transits to the diagram on the bottom right, and the variant
function \((nT, nH)\) changes from \((1, 2)\) to \((1, 1)\).
If agent \(w\) eats next the priority
graph transits to the diagram on the bottom left, and the variant
function \((nT, nH)\) changes from \((1, 1)\) to \((0, 0)\).Proof that the variant function does not increase
while \(v\) remains hungryIf a philosopher of higher priority than \(v\) transits from thinking
to hungry then \(nT\) decreases. Though \(nH\) increases, the variant function \((nT,
nH)\) decreases because ordering of function values is done
lexicographically.If a philosopher of higher priority than \(v\) transits from hungry
to eating then \(nH\) decreases,  and so the variant function \((nT,
nH)\) decreases.Proof that the following predicate does not
hold forever: The variant function remains unchanged and
\(v\) remains hungry.Let \(w\) be a highest-priority hungry philosopher, i.e. a philosopher with no
hungry philosopher with priority higher than \(w\). (Note: \(w\) may be
the same as \(v\).)
All philosophers with priority higher than \(w\) are thinking. In the
next paragraph we show that either \(w\) gets all its forks and then transits
from hungry to eating, or the variant function decreases.From the algorithm, a hungry philosopher \(w\) requests forks from its
neighbors. From the algorithm, \(w\) eventually gets forks from 
all its lower priority neighbors. A higher priority neighbor \(x\) of
\(w\) is thinking. So when \(x\) gets a request for a fork from \(w\)
either (1) \(x\) sends the requested fork to \(w\) or (2) \(x\) transits from
thinking to hungry in which case the variant function \((nT, nH)\)
decreases.Summary: Key Ideas of this ModuleThis module introduced the problem of distributed mutual exclusion;
showed how the good neighbor policy --- a winning agent reduces its
priority to be lower than all the agents that it competes with --
solves this conflict resolution problem; introduced tokens and what
agents know about other agents holding tokens; and showed a proof
pattern that is one of the most common patterns for proving progress.ReviewWhat is the meaning of mutual exclusion among neighboring
  philosophers?An invariant of the algorithm is that each token is in exactly one
  place: an agent or a channel. How does this invariant help in
  designing the algorithm?An invariant of the algorithm is that the relative priorities among
  agents forms a partial order --- the priority graph is acyclic. What
  can go wrong if the priority graph has cycles?An agent that has a request for a fork releases the fork if the fork
  is dirty, and holds on to the fork if the fork is clean. What could
  go wrong if an agent releases a fork when it gets a request,
  regardless of whether the fork is clean or dirty?K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology



Figure Link: https://kmchandy.github.io/DistributedCollaboration/DiningPhilosophers/Slide09.jpg

Caption: Fig.9: Priority Graph must be Acyclic


What happens if we flip the directions of the edges incident on \(v\)?
After the flip, the edges are directed from \(w\), \(x\) and \(y\) towards \(v\), and
from \(v\) to \(u\). But now we have a cycle: \(y\) to \(v\) to \(u\)
to \(w\) to \(y\). So, flipping edge directions doesn't work.What happens if agents adopt the good neighbor policy?
The winner of a conflict gives itself lower priority thanallthe agents
with which it competes. So, an agent that starts eating gives itself lower
priority than all its neighbors. All edges point towards an eating
agent.Fig.11: Winner gets lower priority than its neighborsWhen all edges incident on a vertex point towards the vertex then
there is no cycle through that vertex. So, directing all edges
incident on an eating vertex towards the eating vertex maintains
acyclicity of the graph.For example, in the figure directing all edges incident on vertex \(v\) towards
 \(v\) ensures that no new cycle is created.Agent's priority does not decrease until the agent
eats.How an Agent knows its PriorityWe assign an attributeclean / dirtyto forks.
A fork is eitherdirtyorclean.
The forks held by an eating agent aredirty. When
an agent receives a fork from another agent, the receiving agent
cleans the fork. So the receiver holds acleanfork, and the
fork remains clean until an agent next eats with it.
(This is the "hygenic solution:" Another - sad? - attempt at CS
humor.)An agent holding a dirty fork knows that it has lower priority than
the agent with which it shares the fork. Likewise,
an agent holding a clean fork knows that it has higher priority than
the agent with which it shares the fork.If an agent does not hold the fork that it shares with a neighbor then
the agent does not know its priority with respect to that neighbor.Example of a Fork's LifecycleThe diagram below shows states of a fork shared by agents \(u\) and
\(v\). The red arrow shows priority, and the black arrows show
channels. The blue dot represents the fork.In the figure on the top left, agent \(u\) is hungry and holds a clean
fork. So, \(u\) knows that it has priority over \(v\). At this point
\(v\) does not know whether \(v\) has priority over \(u\) or not.The next figure, at the top right, shows that when \(u\) transits from
hungry to eating, the fork becomes dirty, and \(u\) has lower priority
than \(v\). Agent \(u\) continues to hold the fork while it eats.The next figure, bottom right, shows the situation after \(u\) gets a
request for the fork from \(v\). Because \(u\) got the request from
\(v\) and \(u\) hasn't sent the fork to \(v\), agent \(u\) knows that
\(v\) is hungry. Since the fork is dirty, \(u\) sends
the fork to \(v\). The figure shows the fork in the channel from \(u\)
to \(v\). While the fork is in the channel it doesn't matter
whether the fork is clean or dirty; however, merely for convenience,
let's assume that \(u\), being hygenic, cleans the fork before sending
it to its partner. While the fork is in a
channel the priority doesn't change but neither \(u\) nor \(v\) knows
what the priority is.The next figure, bottom left, shows the situation when \(v\) receives
the fork.  Receiving the fork doesn't change the priority.
At this point \(v\) is hungry and the fork is
clean and so \(v\) knows that it has higher
priority. \(v\) holds on to the fork until it next eats.Fig.12: How an Agent knows its PriorityAlgorithmProperties of Reachable StatesHere is a list of some of the properties of states in all
trajectories.The priority graph is acyclic. \(u\) has priority over a neighbor
  \(v\) exactly when \(u\) holds the fork that it shares with \(v\)
  and the fork is clean, or the fork is in the channel from \(v\) to
  \(u\), or \(v\) holds the fork and the fork is dirty.Eating philosophers hold the forks for all edges incident on them,
  and these forks are dirty.All forks held by thinking philosphers are dirty.Thinking philosophers never send requests and never receive
  forks. Thinking philosophers respond to request for forks by sending
  the requested forks.Initial StatesInitially all philosophers are thinking; all forks are dirty; and all channels are
empty. The forks are placed so that the priority graph is acyclic.
The initial assignment of forks is as follows.
Given an arbitrary acyclic graph, for any edge directed from \(v\) to
\(w\), the fork shared by \(v\) and \(w\) is initially at \(w\) and
the fork is dirty.Algorithm CommandsThe algorithm is specified by the following commands.When a thinking philosophers gets a request for a fork that it holds
  it sends the fork. (A fork held by a thinking philosopher is dirty.)When a thinking philosopher transits to hungry it sends requests for
  all forks that it does not hold.When a hungry philosopher receives a fork, it records the fork as
  being clean. If the hungry philosopher holds all its forks, and if
  it has no request for any dirty fork that it holds, then it transits
  to eating, and records all the forks that it holds in the eating
  state as dirty.When a hungry philosopher receives a request for a fork that it
  holds, it sends the fork if the fork is dirty, and holds on to the
  fork if it is clean.When an eating philosopher receives a request for a fork it
  registers the request in memory, and continues eating while holding
  the fork. When an eating philosopher transits to thinking it sends
  forks for all requests that it received.What could go wrong?The proof of safety is straightforward: Neighbors aren't eating
because neighbors can't hold the same fork at the same time.Before we look at the proof of progress, let's see what may go wrong.Could a group of philosophers exchange forks with each other so that
members of the group eat repeatedly, and starve a philosopher who is
not in the group? For example, in the figure below, could philosophers
\(u, v, w\) exchange forks so that they each eat in turn, and starve
\(y\)?Fig.13: Potential Problems: What could go wrong?Could the system enter a deadlock state in which each hungry philosopher in a
group holds only some --- but not all --- of the forks that it needs
to eat, while other members of the group hold the remaining forks?Proof of CorrectnessThe algorithm is correct. We are required to prove that every hungery
philosopher eats eventually:\(
\forall v: \quad v.h \leadsto v.e
\)where for a philosopher \(v\), \(v.h\) holds exactly when \(v\) is
hungry and \(v.e\) holds exactly when \(v\) is
eating.Variant FunctionTo prove this progress property we find a variant function that
satisfies the following two conditions:Safety: The variant function does not increase while \(v\)
  remains hungry.progress: The following predicate does not hold forever: The
  variant function remains unchanged and \(v\) remains hungry.We propose a variant function which is a pair of integers
 \(nT, nH\),  which are the number of thinking and hungry philosophers,
respectively, of 
higher priority than \(v\).
In terms of the priority graph, \(nT, nH\) are the numbers of thinking
and hungry philosophers (i.e. vertices) with paths to \(v\).Example of Variant FunctionThe figure below shows a portion of the priority graph in a state of the
system. The figure only shows philosophers with higher priority than
philosopher \(v\), i.e., it only shows vertices in the graph with
paths to \(v\). Since eating philosophers have lower priority than
their neighbors, eating philosophers don't appear in this graph.A hungry philosopher is marked with an "H" and a thinking philosopher
with a "T." In the diagram, philosophers \(v, w, x, y\) are hungry and
\(u\) is thinking.
Forks are located at philosophers and are shown as small
colored circles. A dirty fork is colored red and clean one is
blue. For example the fork shared by \(v\) and \(y\) is at \(y\) and
is clean.Fig.15: A Variant Function: Numbers of higher priority
    thinking, hungry agentsExample of Changes to Variant FunctionThe next figure is an example of changes to the variant function. The
diagram on the top left shows the higher priority vertices in the
state of the previous figure. If agent \(x\) eats next the priority
graph transits to the diagram on the top right, and the variant
function \((nT, nH)\) changes from \((1, 3)\) to \((1, 2)\).Fig.16: Example of Values of the Variant FunctionIf agent \(y\) eats next the priority
graph transits to the diagram on the bottom right, and the variant
function \((nT, nH)\) changes from \((1, 2)\) to \((1, 1)\).
If agent \(w\) eats next the priority
graph transits to the diagram on the bottom left, and the variant
function \((nT, nH)\) changes from \((1, 1)\) to \((0, 0)\).Proof that the variant function does not increase
while \(v\) remains hungryIf a philosopher of higher priority than \(v\) transits from thinking
to hungry then \(nT\) decreases. Though \(nH\) increases, the variant function \((nT,
nH)\) decreases because ordering of function values is done
lexicographically.If a philosopher of higher priority than \(v\) transits from hungry
to eating then \(nH\) decreases,  and so the variant function \((nT,
nH)\) decreases.Proof that the following predicate does not
hold forever: The variant function remains unchanged and
\(v\) remains hungry.Let \(w\) be a highest-priority hungry philosopher, i.e. a philosopher with no
hungry philosopher with priority higher than \(w\). (Note: \(w\) may be
the same as \(v\).)
All philosophers with priority higher than \(w\) are thinking. In the
next paragraph we show that either \(w\) gets all its forks and then transits
from hungry to eating, or the variant function decreases.From the algorithm, a hungry philosopher \(w\) requests forks from its
neighbors. From the algorithm, \(w\) eventually gets forks from 
all its lower priority neighbors. A higher priority neighbor \(x\) of
\(w\) is thinking. So when \(x\) gets a request for a fork from \(w\)
either (1) \(x\) sends the requested fork to \(w\) or (2) \(x\) transits from
thinking to hungry in which case the variant function \((nT, nH)\)
decreases.Summary: Key Ideas of this ModuleThis module introduced the problem of distributed mutual exclusion;
showed how the good neighbor policy --- a winning agent reduces its
priority to be lower than all the agents that it competes with --
solves this conflict resolution problem; introduced tokens and what
agents know about other agents holding tokens; and showed a proof
pattern that is one of the most common patterns for proving progress.ReviewWhat is the meaning of mutual exclusion among neighboring
  philosophers?An invariant of the algorithm is that each token is in exactly one
  place: an agent or a channel. How does this invariant help in
  designing the algorithm?An invariant of the algorithm is that the relative priorities among
  agents forms a partial order --- the priority graph is acyclic. What
  can go wrong if the priority graph has cycles?An agent that has a request for a fork releases the fork if the fork
  is dirty, and holds on to the fork if the fork is clean. What could
  go wrong if an agent releases a fork when it gets a request,
  regardless of whether the fork is clean or dirty?K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology



Figure Link: https://kmchandy.github.io/DistributedCollaboration/DiningPhilosophers/Slide11.jpg

Caption: Fig.11: Winner gets lower priority than its neighbors


What happens if agents adopt the good neighbor policy?
The winner of a conflict gives itself lower priority thanallthe agents
with which it competes. So, an agent that starts eating gives itself lower
priority than all its neighbors. All edges point towards an eating
agent.Fig.11: Winner gets lower priority than its neighborsWhen all edges incident on a vertex point towards the vertex then
there is no cycle through that vertex. So, directing all edges
incident on an eating vertex towards the eating vertex maintains
acyclicity of the graph.For example, in the figure directing all edges incident on vertex \(v\) towards
 \(v\) ensures that no new cycle is created.Agent's priority does not decrease until the agent
eats.How an Agent knows its PriorityWe assign an attributeclean / dirtyto forks.
A fork is eitherdirtyorclean.
The forks held by an eating agent aredirty. When
an agent receives a fork from another agent, the receiving agent
cleans the fork. So the receiver holds acleanfork, and the
fork remains clean until an agent next eats with it.
(This is the "hygenic solution:" Another - sad? - attempt at CS
humor.)An agent holding a dirty fork knows that it has lower priority than
the agent with which it shares the fork. Likewise,
an agent holding a clean fork knows that it has higher priority than
the agent with which it shares the fork.If an agent does not hold the fork that it shares with a neighbor then
the agent does not know its priority with respect to that neighbor.Example of a Fork's LifecycleThe diagram below shows states of a fork shared by agents \(u\) and
\(v\). The red arrow shows priority, and the black arrows show
channels. The blue dot represents the fork.In the figure on the top left, agent \(u\) is hungry and holds a clean
fork. So, \(u\) knows that it has priority over \(v\). At this point
\(v\) does not know whether \(v\) has priority over \(u\) or not.The next figure, at the top right, shows that when \(u\) transits from
hungry to eating, the fork becomes dirty, and \(u\) has lower priority
than \(v\). Agent \(u\) continues to hold the fork while it eats.The next figure, bottom right, shows the situation after \(u\) gets a
request for the fork from \(v\). Because \(u\) got the request from
\(v\) and \(u\) hasn't sent the fork to \(v\), agent \(u\) knows that
\(v\) is hungry. Since the fork is dirty, \(u\) sends
the fork to \(v\). The figure shows the fork in the channel from \(u\)
to \(v\). While the fork is in the channel it doesn't matter
whether the fork is clean or dirty; however, merely for convenience,
let's assume that \(u\), being hygenic, cleans the fork before sending
it to its partner. While the fork is in a
channel the priority doesn't change but neither \(u\) nor \(v\) knows
what the priority is.The next figure, bottom left, shows the situation when \(v\) receives
the fork.  Receiving the fork doesn't change the priority.
At this point \(v\) is hungry and the fork is
clean and so \(v\) knows that it has higher
priority. \(v\) holds on to the fork until it next eats.Fig.12: How an Agent knows its PriorityAlgorithmProperties of Reachable StatesHere is a list of some of the properties of states in all
trajectories.The priority graph is acyclic. \(u\) has priority over a neighbor
  \(v\) exactly when \(u\) holds the fork that it shares with \(v\)
  and the fork is clean, or the fork is in the channel from \(v\) to
  \(u\), or \(v\) holds the fork and the fork is dirty.Eating philosophers hold the forks for all edges incident on them,
  and these forks are dirty.All forks held by thinking philosphers are dirty.Thinking philosophers never send requests and never receive
  forks. Thinking philosophers respond to request for forks by sending
  the requested forks.Initial StatesInitially all philosophers are thinking; all forks are dirty; and all channels are
empty. The forks are placed so that the priority graph is acyclic.
The initial assignment of forks is as follows.
Given an arbitrary acyclic graph, for any edge directed from \(v\) to
\(w\), the fork shared by \(v\) and \(w\) is initially at \(w\) and
the fork is dirty.Algorithm CommandsThe algorithm is specified by the following commands.When a thinking philosophers gets a request for a fork that it holds
  it sends the fork. (A fork held by a thinking philosopher is dirty.)When a thinking philosopher transits to hungry it sends requests for
  all forks that it does not hold.When a hungry philosopher receives a fork, it records the fork as
  being clean. If the hungry philosopher holds all its forks, and if
  it has no request for any dirty fork that it holds, then it transits
  to eating, and records all the forks that it holds in the eating
  state as dirty.When a hungry philosopher receives a request for a fork that it
  holds, it sends the fork if the fork is dirty, and holds on to the
  fork if it is clean.When an eating philosopher receives a request for a fork it
  registers the request in memory, and continues eating while holding
  the fork. When an eating philosopher transits to thinking it sends
  forks for all requests that it received.What could go wrong?The proof of safety is straightforward: Neighbors aren't eating
because neighbors can't hold the same fork at the same time.Before we look at the proof of progress, let's see what may go wrong.Could a group of philosophers exchange forks with each other so that
members of the group eat repeatedly, and starve a philosopher who is
not in the group? For example, in the figure below, could philosophers
\(u, v, w\) exchange forks so that they each eat in turn, and starve
\(y\)?Fig.13: Potential Problems: What could go wrong?Could the system enter a deadlock state in which each hungry philosopher in a
group holds only some --- but not all --- of the forks that it needs
to eat, while other members of the group hold the remaining forks?Proof of CorrectnessThe algorithm is correct. We are required to prove that every hungery
philosopher eats eventually:\(
\forall v: \quad v.h \leadsto v.e
\)where for a philosopher \(v\), \(v.h\) holds exactly when \(v\) is
hungry and \(v.e\) holds exactly when \(v\) is
eating.Variant FunctionTo prove this progress property we find a variant function that
satisfies the following two conditions:Safety: The variant function does not increase while \(v\)
  remains hungry.progress: The following predicate does not hold forever: The
  variant function remains unchanged and \(v\) remains hungry.We propose a variant function which is a pair of integers
 \(nT, nH\),  which are the number of thinking and hungry philosophers,
respectively, of 
higher priority than \(v\).
In terms of the priority graph, \(nT, nH\) are the numbers of thinking
and hungry philosophers (i.e. vertices) with paths to \(v\).Example of Variant FunctionThe figure below shows a portion of the priority graph in a state of the
system. The figure only shows philosophers with higher priority than
philosopher \(v\), i.e., it only shows vertices in the graph with
paths to \(v\). Since eating philosophers have lower priority than
their neighbors, eating philosophers don't appear in this graph.A hungry philosopher is marked with an "H" and a thinking philosopher
with a "T." In the diagram, philosophers \(v, w, x, y\) are hungry and
\(u\) is thinking.
Forks are located at philosophers and are shown as small
colored circles. A dirty fork is colored red and clean one is
blue. For example the fork shared by \(v\) and \(y\) is at \(y\) and
is clean.Fig.15: A Variant Function: Numbers of higher priority
    thinking, hungry agentsExample of Changes to Variant FunctionThe next figure is an example of changes to the variant function. The
diagram on the top left shows the higher priority vertices in the
state of the previous figure. If agent \(x\) eats next the priority
graph transits to the diagram on the top right, and the variant
function \((nT, nH)\) changes from \((1, 3)\) to \((1, 2)\).Fig.16: Example of Values of the Variant FunctionIf agent \(y\) eats next the priority
graph transits to the diagram on the bottom right, and the variant
function \((nT, nH)\) changes from \((1, 2)\) to \((1, 1)\).
If agent \(w\) eats next the priority
graph transits to the diagram on the bottom left, and the variant
function \((nT, nH)\) changes from \((1, 1)\) to \((0, 0)\).Proof that the variant function does not increase
while \(v\) remains hungryIf a philosopher of higher priority than \(v\) transits from thinking
to hungry then \(nT\) decreases. Though \(nH\) increases, the variant function \((nT,
nH)\) decreases because ordering of function values is done
lexicographically.If a philosopher of higher priority than \(v\) transits from hungry
to eating then \(nH\) decreases,  and so the variant function \((nT,
nH)\) decreases.Proof that the following predicate does not
hold forever: The variant function remains unchanged and
\(v\) remains hungry.Let \(w\) be a highest-priority hungry philosopher, i.e. a philosopher with no
hungry philosopher with priority higher than \(w\). (Note: \(w\) may be
the same as \(v\).)
All philosophers with priority higher than \(w\) are thinking. In the
next paragraph we show that either \(w\) gets all its forks and then transits
from hungry to eating, or the variant function decreases.From the algorithm, a hungry philosopher \(w\) requests forks from its
neighbors. From the algorithm, \(w\) eventually gets forks from 
all its lower priority neighbors. A higher priority neighbor \(x\) of
\(w\) is thinking. So when \(x\) gets a request for a fork from \(w\)
either (1) \(x\) sends the requested fork to \(w\) or (2) \(x\) transits from
thinking to hungry in which case the variant function \((nT, nH)\)
decreases.Summary: Key Ideas of this ModuleThis module introduced the problem of distributed mutual exclusion;
showed how the good neighbor policy --- a winning agent reduces its
priority to be lower than all the agents that it competes with --
solves this conflict resolution problem; introduced tokens and what
agents know about other agents holding tokens; and showed a proof
pattern that is one of the most common patterns for proving progress.ReviewWhat is the meaning of mutual exclusion among neighboring
  philosophers?An invariant of the algorithm is that each token is in exactly one
  place: an agent or a channel. How does this invariant help in
  designing the algorithm?An invariant of the algorithm is that the relative priorities among
  agents forms a partial order --- the priority graph is acyclic. What
  can go wrong if the priority graph has cycles?An agent that has a request for a fork releases the fork if the fork
  is dirty, and holds on to the fork if the fork is clean. What could
  go wrong if an agent releases a fork when it gets a request,
  regardless of whether the fork is clean or dirty?K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology



Figure Link: https://kmchandy.github.io/DistributedCollaboration/DiningPhilosophers/Slide11.jpg

Caption: Fig.11: Winner gets lower priority than its neighbors


For example, in the figure directing all edges incident on vertex \(v\) towards
 \(v\) ensures that no new cycle is created.Agent's priority does not decrease until the agent
eats.How an Agent knows its PriorityWe assign an attributeclean / dirtyto forks.
A fork is eitherdirtyorclean.
The forks held by an eating agent aredirty. When
an agent receives a fork from another agent, the receiving agent
cleans the fork. So the receiver holds acleanfork, and the
fork remains clean until an agent next eats with it.
(This is the "hygenic solution:" Another - sad? - attempt at CS
humor.)An agent holding a dirty fork knows that it has lower priority than
the agent with which it shares the fork. Likewise,
an agent holding a clean fork knows that it has higher priority than
the agent with which it shares the fork.If an agent does not hold the fork that it shares with a neighbor then
the agent does not know its priority with respect to that neighbor.Example of a Fork's LifecycleThe diagram below shows states of a fork shared by agents \(u\) and
\(v\). The red arrow shows priority, and the black arrows show
channels. The blue dot represents the fork.In the figure on the top left, agent \(u\) is hungry and holds a clean
fork. So, \(u\) knows that it has priority over \(v\). At this point
\(v\) does not know whether \(v\) has priority over \(u\) or not.The next figure, at the top right, shows that when \(u\) transits from
hungry to eating, the fork becomes dirty, and \(u\) has lower priority
than \(v\). Agent \(u\) continues to hold the fork while it eats.The next figure, bottom right, shows the situation after \(u\) gets a
request for the fork from \(v\). Because \(u\) got the request from
\(v\) and \(u\) hasn't sent the fork to \(v\), agent \(u\) knows that
\(v\) is hungry. Since the fork is dirty, \(u\) sends
the fork to \(v\). The figure shows the fork in the channel from \(u\)
to \(v\). While the fork is in the channel it doesn't matter
whether the fork is clean or dirty; however, merely for convenience,
let's assume that \(u\), being hygenic, cleans the fork before sending
it to its partner. While the fork is in a
channel the priority doesn't change but neither \(u\) nor \(v\) knows
what the priority is.The next figure, bottom left, shows the situation when \(v\) receives
the fork.  Receiving the fork doesn't change the priority.
At this point \(v\) is hungry and the fork is
clean and so \(v\) knows that it has higher
priority. \(v\) holds on to the fork until it next eats.Fig.12: How an Agent knows its PriorityAlgorithmProperties of Reachable StatesHere is a list of some of the properties of states in all
trajectories.The priority graph is acyclic. \(u\) has priority over a neighbor
  \(v\) exactly when \(u\) holds the fork that it shares with \(v\)
  and the fork is clean, or the fork is in the channel from \(v\) to
  \(u\), or \(v\) holds the fork and the fork is dirty.Eating philosophers hold the forks for all edges incident on them,
  and these forks are dirty.All forks held by thinking philosphers are dirty.Thinking philosophers never send requests and never receive
  forks. Thinking philosophers respond to request for forks by sending
  the requested forks.Initial StatesInitially all philosophers are thinking; all forks are dirty; and all channels are
empty. The forks are placed so that the priority graph is acyclic.
The initial assignment of forks is as follows.
Given an arbitrary acyclic graph, for any edge directed from \(v\) to
\(w\), the fork shared by \(v\) and \(w\) is initially at \(w\) and
the fork is dirty.Algorithm CommandsThe algorithm is specified by the following commands.When a thinking philosophers gets a request for a fork that it holds
  it sends the fork. (A fork held by a thinking philosopher is dirty.)When a thinking philosopher transits to hungry it sends requests for
  all forks that it does not hold.When a hungry philosopher receives a fork, it records the fork as
  being clean. If the hungry philosopher holds all its forks, and if
  it has no request for any dirty fork that it holds, then it transits
  to eating, and records all the forks that it holds in the eating
  state as dirty.When a hungry philosopher receives a request for a fork that it
  holds, it sends the fork if the fork is dirty, and holds on to the
  fork if it is clean.When an eating philosopher receives a request for a fork it
  registers the request in memory, and continues eating while holding
  the fork. When an eating philosopher transits to thinking it sends
  forks for all requests that it received.What could go wrong?The proof of safety is straightforward: Neighbors aren't eating
because neighbors can't hold the same fork at the same time.Before we look at the proof of progress, let's see what may go wrong.Could a group of philosophers exchange forks with each other so that
members of the group eat repeatedly, and starve a philosopher who is
not in the group? For example, in the figure below, could philosophers
\(u, v, w\) exchange forks so that they each eat in turn, and starve
\(y\)?Fig.13: Potential Problems: What could go wrong?Could the system enter a deadlock state in which each hungry philosopher in a
group holds only some --- but not all --- of the forks that it needs
to eat, while other members of the group hold the remaining forks?Proof of CorrectnessThe algorithm is correct. We are required to prove that every hungery
philosopher eats eventually:\(
\forall v: \quad v.h \leadsto v.e
\)where for a philosopher \(v\), \(v.h\) holds exactly when \(v\) is
hungry and \(v.e\) holds exactly when \(v\) is
eating.Variant FunctionTo prove this progress property we find a variant function that
satisfies the following two conditions:Safety: The variant function does not increase while \(v\)
  remains hungry.progress: The following predicate does not hold forever: The
  variant function remains unchanged and \(v\) remains hungry.We propose a variant function which is a pair of integers
 \(nT, nH\),  which are the number of thinking and hungry philosophers,
respectively, of 
higher priority than \(v\).
In terms of the priority graph, \(nT, nH\) are the numbers of thinking
and hungry philosophers (i.e. vertices) with paths to \(v\).Example of Variant FunctionThe figure below shows a portion of the priority graph in a state of the
system. The figure only shows philosophers with higher priority than
philosopher \(v\), i.e., it only shows vertices in the graph with
paths to \(v\). Since eating philosophers have lower priority than
their neighbors, eating philosophers don't appear in this graph.A hungry philosopher is marked with an "H" and a thinking philosopher
with a "T." In the diagram, philosophers \(v, w, x, y\) are hungry and
\(u\) is thinking.
Forks are located at philosophers and are shown as small
colored circles. A dirty fork is colored red and clean one is
blue. For example the fork shared by \(v\) and \(y\) is at \(y\) and
is clean.Fig.15: A Variant Function: Numbers of higher priority
    thinking, hungry agentsExample of Changes to Variant FunctionThe next figure is an example of changes to the variant function. The
diagram on the top left shows the higher priority vertices in the
state of the previous figure. If agent \(x\) eats next the priority
graph transits to the diagram on the top right, and the variant
function \((nT, nH)\) changes from \((1, 3)\) to \((1, 2)\).Fig.16: Example of Values of the Variant FunctionIf agent \(y\) eats next the priority
graph transits to the diagram on the bottom right, and the variant
function \((nT, nH)\) changes from \((1, 2)\) to \((1, 1)\).
If agent \(w\) eats next the priority
graph transits to the diagram on the bottom left, and the variant
function \((nT, nH)\) changes from \((1, 1)\) to \((0, 0)\).Proof that the variant function does not increase
while \(v\) remains hungryIf a philosopher of higher priority than \(v\) transits from thinking
to hungry then \(nT\) decreases. Though \(nH\) increases, the variant function \((nT,
nH)\) decreases because ordering of function values is done
lexicographically.If a philosopher of higher priority than \(v\) transits from hungry
to eating then \(nH\) decreases,  and so the variant function \((nT,
nH)\) decreases.Proof that the following predicate does not
hold forever: The variant function remains unchanged and
\(v\) remains hungry.Let \(w\) be a highest-priority hungry philosopher, i.e. a philosopher with no
hungry philosopher with priority higher than \(w\). (Note: \(w\) may be
the same as \(v\).)
All philosophers with priority higher than \(w\) are thinking. In the
next paragraph we show that either \(w\) gets all its forks and then transits
from hungry to eating, or the variant function decreases.From the algorithm, a hungry philosopher \(w\) requests forks from its
neighbors. From the algorithm, \(w\) eventually gets forks from 
all its lower priority neighbors. A higher priority neighbor \(x\) of
\(w\) is thinking. So when \(x\) gets a request for a fork from \(w\)
either (1) \(x\) sends the requested fork to \(w\) or (2) \(x\) transits from
thinking to hungry in which case the variant function \((nT, nH)\)
decreases.Summary: Key Ideas of this ModuleThis module introduced the problem of distributed mutual exclusion;
showed how the good neighbor policy --- a winning agent reduces its
priority to be lower than all the agents that it competes with --
solves this conflict resolution problem; introduced tokens and what
agents know about other agents holding tokens; and showed a proof
pattern that is one of the most common patterns for proving progress.ReviewWhat is the meaning of mutual exclusion among neighboring
  philosophers?An invariant of the algorithm is that each token is in exactly one
  place: an agent or a channel. How does this invariant help in
  designing the algorithm?An invariant of the algorithm is that the relative priorities among
  agents forms a partial order --- the priority graph is acyclic. What
  can go wrong if the priority graph has cycles?An agent that has a request for a fork releases the fork if the fork
  is dirty, and holds on to the fork if the fork is clean. What could
  go wrong if an agent releases a fork when it gets a request,
  regardless of whether the fork is clean or dirty?K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology



Figure Link: https://kmchandy.github.io/DistributedCollaboration/DiningPhilosophers/Slide12.jpg

Caption: Fig.12: How an Agent knows its Priority


An agent holding a dirty fork knows that it has lower priority than
the agent with which it shares the fork. Likewise,
an agent holding a clean fork knows that it has higher priority than
the agent with which it shares the fork.If an agent does not hold the fork that it shares with a neighbor then
the agent does not know its priority with respect to that neighbor.Example of a Fork's LifecycleThe diagram below shows states of a fork shared by agents \(u\) and
\(v\). The red arrow shows priority, and the black arrows show
channels. The blue dot represents the fork.In the figure on the top left, agent \(u\) is hungry and holds a clean
fork. So, \(u\) knows that it has priority over \(v\). At this point
\(v\) does not know whether \(v\) has priority over \(u\) or not.The next figure, at the top right, shows that when \(u\) transits from
hungry to eating, the fork becomes dirty, and \(u\) has lower priority
than \(v\). Agent \(u\) continues to hold the fork while it eats.The next figure, bottom right, shows the situation after \(u\) gets a
request for the fork from \(v\). Because \(u\) got the request from
\(v\) and \(u\) hasn't sent the fork to \(v\), agent \(u\) knows that
\(v\) is hungry. Since the fork is dirty, \(u\) sends
the fork to \(v\). The figure shows the fork in the channel from \(u\)
to \(v\). While the fork is in the channel it doesn't matter
whether the fork is clean or dirty; however, merely for convenience,
let's assume that \(u\), being hygenic, cleans the fork before sending
it to its partner. While the fork is in a
channel the priority doesn't change but neither \(u\) nor \(v\) knows
what the priority is.The next figure, bottom left, shows the situation when \(v\) receives
the fork.  Receiving the fork doesn't change the priority.
At this point \(v\) is hungry and the fork is
clean and so \(v\) knows that it has higher
priority. \(v\) holds on to the fork until it next eats.Fig.12: How an Agent knows its PriorityAlgorithmProperties of Reachable StatesHere is a list of some of the properties of states in all
trajectories.The priority graph is acyclic. \(u\) has priority over a neighbor
  \(v\) exactly when \(u\) holds the fork that it shares with \(v\)
  and the fork is clean, or the fork is in the channel from \(v\) to
  \(u\), or \(v\) holds the fork and the fork is dirty.Eating philosophers hold the forks for all edges incident on them,
  and these forks are dirty.All forks held by thinking philosphers are dirty.Thinking philosophers never send requests and never receive
  forks. Thinking philosophers respond to request for forks by sending
  the requested forks.Initial StatesInitially all philosophers are thinking; all forks are dirty; and all channels are
empty. The forks are placed so that the priority graph is acyclic.
The initial assignment of forks is as follows.
Given an arbitrary acyclic graph, for any edge directed from \(v\) to
\(w\), the fork shared by \(v\) and \(w\) is initially at \(w\) and
the fork is dirty.Algorithm CommandsThe algorithm is specified by the following commands.When a thinking philosophers gets a request for a fork that it holds
  it sends the fork. (A fork held by a thinking philosopher is dirty.)When a thinking philosopher transits to hungry it sends requests for
  all forks that it does not hold.When a hungry philosopher receives a fork, it records the fork as
  being clean. If the hungry philosopher holds all its forks, and if
  it has no request for any dirty fork that it holds, then it transits
  to eating, and records all the forks that it holds in the eating
  state as dirty.When a hungry philosopher receives a request for a fork that it
  holds, it sends the fork if the fork is dirty, and holds on to the
  fork if it is clean.When an eating philosopher receives a request for a fork it
  registers the request in memory, and continues eating while holding
  the fork. When an eating philosopher transits to thinking it sends
  forks for all requests that it received.What could go wrong?The proof of safety is straightforward: Neighbors aren't eating
because neighbors can't hold the same fork at the same time.Before we look at the proof of progress, let's see what may go wrong.Could a group of philosophers exchange forks with each other so that
members of the group eat repeatedly, and starve a philosopher who is
not in the group? For example, in the figure below, could philosophers
\(u, v, w\) exchange forks so that they each eat in turn, and starve
\(y\)?Fig.13: Potential Problems: What could go wrong?Could the system enter a deadlock state in which each hungry philosopher in a
group holds only some --- but not all --- of the forks that it needs
to eat, while other members of the group hold the remaining forks?Proof of CorrectnessThe algorithm is correct. We are required to prove that every hungery
philosopher eats eventually:\(
\forall v: \quad v.h \leadsto v.e
\)where for a philosopher \(v\), \(v.h\) holds exactly when \(v\) is
hungry and \(v.e\) holds exactly when \(v\) is
eating.Variant FunctionTo prove this progress property we find a variant function that
satisfies the following two conditions:Safety: The variant function does not increase while \(v\)
  remains hungry.progress: The following predicate does not hold forever: The
  variant function remains unchanged and \(v\) remains hungry.We propose a variant function which is a pair of integers
 \(nT, nH\),  which are the number of thinking and hungry philosophers,
respectively, of 
higher priority than \(v\).
In terms of the priority graph, \(nT, nH\) are the numbers of thinking
and hungry philosophers (i.e. vertices) with paths to \(v\).Example of Variant FunctionThe figure below shows a portion of the priority graph in a state of the
system. The figure only shows philosophers with higher priority than
philosopher \(v\), i.e., it only shows vertices in the graph with
paths to \(v\). Since eating philosophers have lower priority than
their neighbors, eating philosophers don't appear in this graph.A hungry philosopher is marked with an "H" and a thinking philosopher
with a "T." In the diagram, philosophers \(v, w, x, y\) are hungry and
\(u\) is thinking.
Forks are located at philosophers and are shown as small
colored circles. A dirty fork is colored red and clean one is
blue. For example the fork shared by \(v\) and \(y\) is at \(y\) and
is clean.Fig.15: A Variant Function: Numbers of higher priority
    thinking, hungry agentsExample of Changes to Variant FunctionThe next figure is an example of changes to the variant function. The
diagram on the top left shows the higher priority vertices in the
state of the previous figure. If agent \(x\) eats next the priority
graph transits to the diagram on the top right, and the variant
function \((nT, nH)\) changes from \((1, 3)\) to \((1, 2)\).Fig.16: Example of Values of the Variant FunctionIf agent \(y\) eats next the priority
graph transits to the diagram on the bottom right, and the variant
function \((nT, nH)\) changes from \((1, 2)\) to \((1, 1)\).
If agent \(w\) eats next the priority
graph transits to the diagram on the bottom left, and the variant
function \((nT, nH)\) changes from \((1, 1)\) to \((0, 0)\).Proof that the variant function does not increase
while \(v\) remains hungryIf a philosopher of higher priority than \(v\) transits from thinking
to hungry then \(nT\) decreases. Though \(nH\) increases, the variant function \((nT,
nH)\) decreases because ordering of function values is done
lexicographically.If a philosopher of higher priority than \(v\) transits from hungry
to eating then \(nH\) decreases,  and so the variant function \((nT,
nH)\) decreases.Proof that the following predicate does not
hold forever: The variant function remains unchanged and
\(v\) remains hungry.Let \(w\) be a highest-priority hungry philosopher, i.e. a philosopher with no
hungry philosopher with priority higher than \(w\). (Note: \(w\) may be
the same as \(v\).)
All philosophers with priority higher than \(w\) are thinking. In the
next paragraph we show that either \(w\) gets all its forks and then transits
from hungry to eating, or the variant function decreases.From the algorithm, a hungry philosopher \(w\) requests forks from its
neighbors. From the algorithm, \(w\) eventually gets forks from 
all its lower priority neighbors. A higher priority neighbor \(x\) of
\(w\) is thinking. So when \(x\) gets a request for a fork from \(w\)
either (1) \(x\) sends the requested fork to \(w\) or (2) \(x\) transits from
thinking to hungry in which case the variant function \((nT, nH)\)
decreases.Summary: Key Ideas of this ModuleThis module introduced the problem of distributed mutual exclusion;
showed how the good neighbor policy --- a winning agent reduces its
priority to be lower than all the agents that it competes with --
solves this conflict resolution problem; introduced tokens and what
agents know about other agents holding tokens; and showed a proof
pattern that is one of the most common patterns for proving progress.ReviewWhat is the meaning of mutual exclusion among neighboring
  philosophers?An invariant of the algorithm is that each token is in exactly one
  place: an agent or a channel. How does this invariant help in
  designing the algorithm?An invariant of the algorithm is that the relative priorities among
  agents forms a partial order --- the priority graph is acyclic. What
  can go wrong if the priority graph has cycles?An agent that has a request for a fork releases the fork if the fork
  is dirty, and holds on to the fork if the fork is clean. What could
  go wrong if an agent releases a fork when it gets a request,
  regardless of whether the fork is clean or dirty?K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology



Figure Link: https://kmchandy.github.io/DistributedCollaboration/DiningPhilosophers/Slide12.jpg

Caption: Fig.12: How an Agent knows its Priority


If an agent does not hold the fork that it shares with a neighbor then
the agent does not know its priority with respect to that neighbor.Example of a Fork's LifecycleThe diagram below shows states of a fork shared by agents \(u\) and
\(v\). The red arrow shows priority, and the black arrows show
channels. The blue dot represents the fork.In the figure on the top left, agent \(u\) is hungry and holds a clean
fork. So, \(u\) knows that it has priority over \(v\). At this point
\(v\) does not know whether \(v\) has priority over \(u\) or not.The next figure, at the top right, shows that when \(u\) transits from
hungry to eating, the fork becomes dirty, and \(u\) has lower priority
than \(v\). Agent \(u\) continues to hold the fork while it eats.The next figure, bottom right, shows the situation after \(u\) gets a
request for the fork from \(v\). Because \(u\) got the request from
\(v\) and \(u\) hasn't sent the fork to \(v\), agent \(u\) knows that
\(v\) is hungry. Since the fork is dirty, \(u\) sends
the fork to \(v\). The figure shows the fork in the channel from \(u\)
to \(v\). While the fork is in the channel it doesn't matter
whether the fork is clean or dirty; however, merely for convenience,
let's assume that \(u\), being hygenic, cleans the fork before sending
it to its partner. While the fork is in a
channel the priority doesn't change but neither \(u\) nor \(v\) knows
what the priority is.The next figure, bottom left, shows the situation when \(v\) receives
the fork.  Receiving the fork doesn't change the priority.
At this point \(v\) is hungry and the fork is
clean and so \(v\) knows that it has higher
priority. \(v\) holds on to the fork until it next eats.Fig.12: How an Agent knows its PriorityAlgorithmProperties of Reachable StatesHere is a list of some of the properties of states in all
trajectories.The priority graph is acyclic. \(u\) has priority over a neighbor
  \(v\) exactly when \(u\) holds the fork that it shares with \(v\)
  and the fork is clean, or the fork is in the channel from \(v\) to
  \(u\), or \(v\) holds the fork and the fork is dirty.Eating philosophers hold the forks for all edges incident on them,
  and these forks are dirty.All forks held by thinking philosphers are dirty.Thinking philosophers never send requests and never receive
  forks. Thinking philosophers respond to request for forks by sending
  the requested forks.Initial StatesInitially all philosophers are thinking; all forks are dirty; and all channels are
empty. The forks are placed so that the priority graph is acyclic.
The initial assignment of forks is as follows.
Given an arbitrary acyclic graph, for any edge directed from \(v\) to
\(w\), the fork shared by \(v\) and \(w\) is initially at \(w\) and
the fork is dirty.Algorithm CommandsThe algorithm is specified by the following commands.When a thinking philosophers gets a request for a fork that it holds
  it sends the fork. (A fork held by a thinking philosopher is dirty.)When a thinking philosopher transits to hungry it sends requests for
  all forks that it does not hold.When a hungry philosopher receives a fork, it records the fork as
  being clean. If the hungry philosopher holds all its forks, and if
  it has no request for any dirty fork that it holds, then it transits
  to eating, and records all the forks that it holds in the eating
  state as dirty.When a hungry philosopher receives a request for a fork that it
  holds, it sends the fork if the fork is dirty, and holds on to the
  fork if it is clean.When an eating philosopher receives a request for a fork it
  registers the request in memory, and continues eating while holding
  the fork. When an eating philosopher transits to thinking it sends
  forks for all requests that it received.What could go wrong?The proof of safety is straightforward: Neighbors aren't eating
because neighbors can't hold the same fork at the same time.Before we look at the proof of progress, let's see what may go wrong.Could a group of philosophers exchange forks with each other so that
members of the group eat repeatedly, and starve a philosopher who is
not in the group? For example, in the figure below, could philosophers
\(u, v, w\) exchange forks so that they each eat in turn, and starve
\(y\)?Fig.13: Potential Problems: What could go wrong?Could the system enter a deadlock state in which each hungry philosopher in a
group holds only some --- but not all --- of the forks that it needs
to eat, while other members of the group hold the remaining forks?Proof of CorrectnessThe algorithm is correct. We are required to prove that every hungery
philosopher eats eventually:\(
\forall v: \quad v.h \leadsto v.e
\)where for a philosopher \(v\), \(v.h\) holds exactly when \(v\) is
hungry and \(v.e\) holds exactly when \(v\) is
eating.Variant FunctionTo prove this progress property we find a variant function that
satisfies the following two conditions:Safety: The variant function does not increase while \(v\)
  remains hungry.progress: The following predicate does not hold forever: The
  variant function remains unchanged and \(v\) remains hungry.We propose a variant function which is a pair of integers
 \(nT, nH\),  which are the number of thinking and hungry philosophers,
respectively, of 
higher priority than \(v\).
In terms of the priority graph, \(nT, nH\) are the numbers of thinking
and hungry philosophers (i.e. vertices) with paths to \(v\).Example of Variant FunctionThe figure below shows a portion of the priority graph in a state of the
system. The figure only shows philosophers with higher priority than
philosopher \(v\), i.e., it only shows vertices in the graph with
paths to \(v\). Since eating philosophers have lower priority than
their neighbors, eating philosophers don't appear in this graph.A hungry philosopher is marked with an "H" and a thinking philosopher
with a "T." In the diagram, philosophers \(v, w, x, y\) are hungry and
\(u\) is thinking.
Forks are located at philosophers and are shown as small
colored circles. A dirty fork is colored red and clean one is
blue. For example the fork shared by \(v\) and \(y\) is at \(y\) and
is clean.Fig.15: A Variant Function: Numbers of higher priority
    thinking, hungry agentsExample of Changes to Variant FunctionThe next figure is an example of changes to the variant function. The
diagram on the top left shows the higher priority vertices in the
state of the previous figure. If agent \(x\) eats next the priority
graph transits to the diagram on the top right, and the variant
function \((nT, nH)\) changes from \((1, 3)\) to \((1, 2)\).Fig.16: Example of Values of the Variant FunctionIf agent \(y\) eats next the priority
graph transits to the diagram on the bottom right, and the variant
function \((nT, nH)\) changes from \((1, 2)\) to \((1, 1)\).
If agent \(w\) eats next the priority
graph transits to the diagram on the bottom left, and the variant
function \((nT, nH)\) changes from \((1, 1)\) to \((0, 0)\).Proof that the variant function does not increase
while \(v\) remains hungryIf a philosopher of higher priority than \(v\) transits from thinking
to hungry then \(nT\) decreases. Though \(nH\) increases, the variant function \((nT,
nH)\) decreases because ordering of function values is done
lexicographically.If a philosopher of higher priority than \(v\) transits from hungry
to eating then \(nH\) decreases,  and so the variant function \((nT,
nH)\) decreases.Proof that the following predicate does not
hold forever: The variant function remains unchanged and
\(v\) remains hungry.Let \(w\) be a highest-priority hungry philosopher, i.e. a philosopher with no
hungry philosopher with priority higher than \(w\). (Note: \(w\) may be
the same as \(v\).)
All philosophers with priority higher than \(w\) are thinking. In the
next paragraph we show that either \(w\) gets all its forks and then transits
from hungry to eating, or the variant function decreases.From the algorithm, a hungry philosopher \(w\) requests forks from its
neighbors. From the algorithm, \(w\) eventually gets forks from 
all its lower priority neighbors. A higher priority neighbor \(x\) of
\(w\) is thinking. So when \(x\) gets a request for a fork from \(w\)
either (1) \(x\) sends the requested fork to \(w\) or (2) \(x\) transits from
thinking to hungry in which case the variant function \((nT, nH)\)
decreases.Summary: Key Ideas of this ModuleThis module introduced the problem of distributed mutual exclusion;
showed how the good neighbor policy --- a winning agent reduces its
priority to be lower than all the agents that it competes with --
solves this conflict resolution problem; introduced tokens and what
agents know about other agents holding tokens; and showed a proof
pattern that is one of the most common patterns for proving progress.ReviewWhat is the meaning of mutual exclusion among neighboring
  philosophers?An invariant of the algorithm is that each token is in exactly one
  place: an agent or a channel. How does this invariant help in
  designing the algorithm?An invariant of the algorithm is that the relative priorities among
  agents forms a partial order --- the priority graph is acyclic. What
  can go wrong if the priority graph has cycles?An agent that has a request for a fork releases the fork if the fork
  is dirty, and holds on to the fork if the fork is clean. What could
  go wrong if an agent releases a fork when it gets a request,
  regardless of whether the fork is clean or dirty?K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology



Figure Link: https://kmchandy.github.io/DistributedCollaboration/DiningPhilosophers/Slide12.jpg

Caption: Fig.12: How an Agent knows its Priority


In the figure on the top left, agent \(u\) is hungry and holds a clean
fork. So, \(u\) knows that it has priority over \(v\). At this point
\(v\) does not know whether \(v\) has priority over \(u\) or not.The next figure, at the top right, shows that when \(u\) transits from
hungry to eating, the fork becomes dirty, and \(u\) has lower priority
than \(v\). Agent \(u\) continues to hold the fork while it eats.The next figure, bottom right, shows the situation after \(u\) gets a
request for the fork from \(v\). Because \(u\) got the request from
\(v\) and \(u\) hasn't sent the fork to \(v\), agent \(u\) knows that
\(v\) is hungry. Since the fork is dirty, \(u\) sends
the fork to \(v\). The figure shows the fork in the channel from \(u\)
to \(v\). While the fork is in the channel it doesn't matter
whether the fork is clean or dirty; however, merely for convenience,
let's assume that \(u\), being hygenic, cleans the fork before sending
it to its partner. While the fork is in a
channel the priority doesn't change but neither \(u\) nor \(v\) knows
what the priority is.The next figure, bottom left, shows the situation when \(v\) receives
the fork.  Receiving the fork doesn't change the priority.
At this point \(v\) is hungry and the fork is
clean and so \(v\) knows that it has higher
priority. \(v\) holds on to the fork until it next eats.Fig.12: How an Agent knows its PriorityAlgorithmProperties of Reachable StatesHere is a list of some of the properties of states in all
trajectories.The priority graph is acyclic. \(u\) has priority over a neighbor
  \(v\) exactly when \(u\) holds the fork that it shares with \(v\)
  and the fork is clean, or the fork is in the channel from \(v\) to
  \(u\), or \(v\) holds the fork and the fork is dirty.Eating philosophers hold the forks for all edges incident on them,
  and these forks are dirty.All forks held by thinking philosphers are dirty.Thinking philosophers never send requests and never receive
  forks. Thinking philosophers respond to request for forks by sending
  the requested forks.Initial StatesInitially all philosophers are thinking; all forks are dirty; and all channels are
empty. The forks are placed so that the priority graph is acyclic.
The initial assignment of forks is as follows.
Given an arbitrary acyclic graph, for any edge directed from \(v\) to
\(w\), the fork shared by \(v\) and \(w\) is initially at \(w\) and
the fork is dirty.Algorithm CommandsThe algorithm is specified by the following commands.When a thinking philosophers gets a request for a fork that it holds
  it sends the fork. (A fork held by a thinking philosopher is dirty.)When a thinking philosopher transits to hungry it sends requests for
  all forks that it does not hold.When a hungry philosopher receives a fork, it records the fork as
  being clean. If the hungry philosopher holds all its forks, and if
  it has no request for any dirty fork that it holds, then it transits
  to eating, and records all the forks that it holds in the eating
  state as dirty.When a hungry philosopher receives a request for a fork that it
  holds, it sends the fork if the fork is dirty, and holds on to the
  fork if it is clean.When an eating philosopher receives a request for a fork it
  registers the request in memory, and continues eating while holding
  the fork. When an eating philosopher transits to thinking it sends
  forks for all requests that it received.What could go wrong?The proof of safety is straightforward: Neighbors aren't eating
because neighbors can't hold the same fork at the same time.Before we look at the proof of progress, let's see what may go wrong.Could a group of philosophers exchange forks with each other so that
members of the group eat repeatedly, and starve a philosopher who is
not in the group? For example, in the figure below, could philosophers
\(u, v, w\) exchange forks so that they each eat in turn, and starve
\(y\)?Fig.13: Potential Problems: What could go wrong?Could the system enter a deadlock state in which each hungry philosopher in a
group holds only some --- but not all --- of the forks that it needs
to eat, while other members of the group hold the remaining forks?Proof of CorrectnessThe algorithm is correct. We are required to prove that every hungery
philosopher eats eventually:\(
\forall v: \quad v.h \leadsto v.e
\)where for a philosopher \(v\), \(v.h\) holds exactly when \(v\) is
hungry and \(v.e\) holds exactly when \(v\) is
eating.Variant FunctionTo prove this progress property we find a variant function that
satisfies the following two conditions:Safety: The variant function does not increase while \(v\)
  remains hungry.progress: The following predicate does not hold forever: The
  variant function remains unchanged and \(v\) remains hungry.We propose a variant function which is a pair of integers
 \(nT, nH\),  which are the number of thinking and hungry philosophers,
respectively, of 
higher priority than \(v\).
In terms of the priority graph, \(nT, nH\) are the numbers of thinking
and hungry philosophers (i.e. vertices) with paths to \(v\).Example of Variant FunctionThe figure below shows a portion of the priority graph in a state of the
system. The figure only shows philosophers with higher priority than
philosopher \(v\), i.e., it only shows vertices in the graph with
paths to \(v\). Since eating philosophers have lower priority than
their neighbors, eating philosophers don't appear in this graph.A hungry philosopher is marked with an "H" and a thinking philosopher
with a "T." In the diagram, philosophers \(v, w, x, y\) are hungry and
\(u\) is thinking.
Forks are located at philosophers and are shown as small
colored circles. A dirty fork is colored red and clean one is
blue. For example the fork shared by \(v\) and \(y\) is at \(y\) and
is clean.Fig.15: A Variant Function: Numbers of higher priority
    thinking, hungry agentsExample of Changes to Variant FunctionThe next figure is an example of changes to the variant function. The
diagram on the top left shows the higher priority vertices in the
state of the previous figure. If agent \(x\) eats next the priority
graph transits to the diagram on the top right, and the variant
function \((nT, nH)\) changes from \((1, 3)\) to \((1, 2)\).Fig.16: Example of Values of the Variant FunctionIf agent \(y\) eats next the priority
graph transits to the diagram on the bottom right, and the variant
function \((nT, nH)\) changes from \((1, 2)\) to \((1, 1)\).
If agent \(w\) eats next the priority
graph transits to the diagram on the bottom left, and the variant
function \((nT, nH)\) changes from \((1, 1)\) to \((0, 0)\).Proof that the variant function does not increase
while \(v\) remains hungryIf a philosopher of higher priority than \(v\) transits from thinking
to hungry then \(nT\) decreases. Though \(nH\) increases, the variant function \((nT,
nH)\) decreases because ordering of function values is done
lexicographically.If a philosopher of higher priority than \(v\) transits from hungry
to eating then \(nH\) decreases,  and so the variant function \((nT,
nH)\) decreases.Proof that the following predicate does not
hold forever: The variant function remains unchanged and
\(v\) remains hungry.Let \(w\) be a highest-priority hungry philosopher, i.e. a philosopher with no
hungry philosopher with priority higher than \(w\). (Note: \(w\) may be
the same as \(v\).)
All philosophers with priority higher than \(w\) are thinking. In the
next paragraph we show that either \(w\) gets all its forks and then transits
from hungry to eating, or the variant function decreases.From the algorithm, a hungry philosopher \(w\) requests forks from its
neighbors. From the algorithm, \(w\) eventually gets forks from 
all its lower priority neighbors. A higher priority neighbor \(x\) of
\(w\) is thinking. So when \(x\) gets a request for a fork from \(w\)
either (1) \(x\) sends the requested fork to \(w\) or (2) \(x\) transits from
thinking to hungry in which case the variant function \((nT, nH)\)
decreases.Summary: Key Ideas of this ModuleThis module introduced the problem of distributed mutual exclusion;
showed how the good neighbor policy --- a winning agent reduces its
priority to be lower than all the agents that it competes with --
solves this conflict resolution problem; introduced tokens and what
agents know about other agents holding tokens; and showed a proof
pattern that is one of the most common patterns for proving progress.ReviewWhat is the meaning of mutual exclusion among neighboring
  philosophers?An invariant of the algorithm is that each token is in exactly one
  place: an agent or a channel. How does this invariant help in
  designing the algorithm?An invariant of the algorithm is that the relative priorities among
  agents forms a partial order --- the priority graph is acyclic. What
  can go wrong if the priority graph has cycles?An agent that has a request for a fork releases the fork if the fork
  is dirty, and holds on to the fork if the fork is clean. What could
  go wrong if an agent releases a fork when it gets a request,
  regardless of whether the fork is clean or dirty?K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology



Figure Link: https://kmchandy.github.io/DistributedCollaboration/DiningPhilosophers/Slide12.jpg

Caption: Fig.12: How an Agent knows its Priority


The next figure, at the top right, shows that when \(u\) transits from
hungry to eating, the fork becomes dirty, and \(u\) has lower priority
than \(v\). Agent \(u\) continues to hold the fork while it eats.The next figure, bottom right, shows the situation after \(u\) gets a
request for the fork from \(v\). Because \(u\) got the request from
\(v\) and \(u\) hasn't sent the fork to \(v\), agent \(u\) knows that
\(v\) is hungry. Since the fork is dirty, \(u\) sends
the fork to \(v\). The figure shows the fork in the channel from \(u\)
to \(v\). While the fork is in the channel it doesn't matter
whether the fork is clean or dirty; however, merely for convenience,
let's assume that \(u\), being hygenic, cleans the fork before sending
it to its partner. While the fork is in a
channel the priority doesn't change but neither \(u\) nor \(v\) knows
what the priority is.The next figure, bottom left, shows the situation when \(v\) receives
the fork.  Receiving the fork doesn't change the priority.
At this point \(v\) is hungry and the fork is
clean and so \(v\) knows that it has higher
priority. \(v\) holds on to the fork until it next eats.Fig.12: How an Agent knows its PriorityAlgorithmProperties of Reachable StatesHere is a list of some of the properties of states in all
trajectories.The priority graph is acyclic. \(u\) has priority over a neighbor
  \(v\) exactly when \(u\) holds the fork that it shares with \(v\)
  and the fork is clean, or the fork is in the channel from \(v\) to
  \(u\), or \(v\) holds the fork and the fork is dirty.Eating philosophers hold the forks for all edges incident on them,
  and these forks are dirty.All forks held by thinking philosphers are dirty.Thinking philosophers never send requests and never receive
  forks. Thinking philosophers respond to request for forks by sending
  the requested forks.Initial StatesInitially all philosophers are thinking; all forks are dirty; and all channels are
empty. The forks are placed so that the priority graph is acyclic.
The initial assignment of forks is as follows.
Given an arbitrary acyclic graph, for any edge directed from \(v\) to
\(w\), the fork shared by \(v\) and \(w\) is initially at \(w\) and
the fork is dirty.Algorithm CommandsThe algorithm is specified by the following commands.When a thinking philosophers gets a request for a fork that it holds
  it sends the fork. (A fork held by a thinking philosopher is dirty.)When a thinking philosopher transits to hungry it sends requests for
  all forks that it does not hold.When a hungry philosopher receives a fork, it records the fork as
  being clean. If the hungry philosopher holds all its forks, and if
  it has no request for any dirty fork that it holds, then it transits
  to eating, and records all the forks that it holds in the eating
  state as dirty.When a hungry philosopher receives a request for a fork that it
  holds, it sends the fork if the fork is dirty, and holds on to the
  fork if it is clean.When an eating philosopher receives a request for a fork it
  registers the request in memory, and continues eating while holding
  the fork. When an eating philosopher transits to thinking it sends
  forks for all requests that it received.What could go wrong?The proof of safety is straightforward: Neighbors aren't eating
because neighbors can't hold the same fork at the same time.Before we look at the proof of progress, let's see what may go wrong.Could a group of philosophers exchange forks with each other so that
members of the group eat repeatedly, and starve a philosopher who is
not in the group? For example, in the figure below, could philosophers
\(u, v, w\) exchange forks so that they each eat in turn, and starve
\(y\)?Fig.13: Potential Problems: What could go wrong?Could the system enter a deadlock state in which each hungry philosopher in a
group holds only some --- but not all --- of the forks that it needs
to eat, while other members of the group hold the remaining forks?Proof of CorrectnessThe algorithm is correct. We are required to prove that every hungery
philosopher eats eventually:\(
\forall v: \quad v.h \leadsto v.e
\)where for a philosopher \(v\), \(v.h\) holds exactly when \(v\) is
hungry and \(v.e\) holds exactly when \(v\) is
eating.Variant FunctionTo prove this progress property we find a variant function that
satisfies the following two conditions:Safety: The variant function does not increase while \(v\)
  remains hungry.progress: The following predicate does not hold forever: The
  variant function remains unchanged and \(v\) remains hungry.We propose a variant function which is a pair of integers
 \(nT, nH\),  which are the number of thinking and hungry philosophers,
respectively, of 
higher priority than \(v\).
In terms of the priority graph, \(nT, nH\) are the numbers of thinking
and hungry philosophers (i.e. vertices) with paths to \(v\).Example of Variant FunctionThe figure below shows a portion of the priority graph in a state of the
system. The figure only shows philosophers with higher priority than
philosopher \(v\), i.e., it only shows vertices in the graph with
paths to \(v\). Since eating philosophers have lower priority than
their neighbors, eating philosophers don't appear in this graph.A hungry philosopher is marked with an "H" and a thinking philosopher
with a "T." In the diagram, philosophers \(v, w, x, y\) are hungry and
\(u\) is thinking.
Forks are located at philosophers and are shown as small
colored circles. A dirty fork is colored red and clean one is
blue. For example the fork shared by \(v\) and \(y\) is at \(y\) and
is clean.Fig.15: A Variant Function: Numbers of higher priority
    thinking, hungry agentsExample of Changes to Variant FunctionThe next figure is an example of changes to the variant function. The
diagram on the top left shows the higher priority vertices in the
state of the previous figure. If agent \(x\) eats next the priority
graph transits to the diagram on the top right, and the variant
function \((nT, nH)\) changes from \((1, 3)\) to \((1, 2)\).Fig.16: Example of Values of the Variant FunctionIf agent \(y\) eats next the priority
graph transits to the diagram on the bottom right, and the variant
function \((nT, nH)\) changes from \((1, 2)\) to \((1, 1)\).
If agent \(w\) eats next the priority
graph transits to the diagram on the bottom left, and the variant
function \((nT, nH)\) changes from \((1, 1)\) to \((0, 0)\).Proof that the variant function does not increase
while \(v\) remains hungryIf a philosopher of higher priority than \(v\) transits from thinking
to hungry then \(nT\) decreases. Though \(nH\) increases, the variant function \((nT,
nH)\) decreases because ordering of function values is done
lexicographically.If a philosopher of higher priority than \(v\) transits from hungry
to eating then \(nH\) decreases,  and so the variant function \((nT,
nH)\) decreases.Proof that the following predicate does not
hold forever: The variant function remains unchanged and
\(v\) remains hungry.Let \(w\) be a highest-priority hungry philosopher, i.e. a philosopher with no
hungry philosopher with priority higher than \(w\). (Note: \(w\) may be
the same as \(v\).)
All philosophers with priority higher than \(w\) are thinking. In the
next paragraph we show that either \(w\) gets all its forks and then transits
from hungry to eating, or the variant function decreases.From the algorithm, a hungry philosopher \(w\) requests forks from its
neighbors. From the algorithm, \(w\) eventually gets forks from 
all its lower priority neighbors. A higher priority neighbor \(x\) of
\(w\) is thinking. So when \(x\) gets a request for a fork from \(w\)
either (1) \(x\) sends the requested fork to \(w\) or (2) \(x\) transits from
thinking to hungry in which case the variant function \((nT, nH)\)
decreases.Summary: Key Ideas of this ModuleThis module introduced the problem of distributed mutual exclusion;
showed how the good neighbor policy --- a winning agent reduces its
priority to be lower than all the agents that it competes with --
solves this conflict resolution problem; introduced tokens and what
agents know about other agents holding tokens; and showed a proof
pattern that is one of the most common patterns for proving progress.ReviewWhat is the meaning of mutual exclusion among neighboring
  philosophers?An invariant of the algorithm is that each token is in exactly one
  place: an agent or a channel. How does this invariant help in
  designing the algorithm?An invariant of the algorithm is that the relative priorities among
  agents forms a partial order --- the priority graph is acyclic. What
  can go wrong if the priority graph has cycles?An agent that has a request for a fork releases the fork if the fork
  is dirty, and holds on to the fork if the fork is clean. What could
  go wrong if an agent releases a fork when it gets a request,
  regardless of whether the fork is clean or dirty?K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology



Figure Link: https://kmchandy.github.io/DistributedCollaboration/DiningPhilosophers/Slide12.jpg

Caption: Fig.12: How an Agent knows its Priority


The next figure, bottom right, shows the situation after \(u\) gets a
request for the fork from \(v\). Because \(u\) got the request from
\(v\) and \(u\) hasn't sent the fork to \(v\), agent \(u\) knows that
\(v\) is hungry. Since the fork is dirty, \(u\) sends
the fork to \(v\). The figure shows the fork in the channel from \(u\)
to \(v\). While the fork is in the channel it doesn't matter
whether the fork is clean or dirty; however, merely for convenience,
let's assume that \(u\), being hygenic, cleans the fork before sending
it to its partner. While the fork is in a
channel the priority doesn't change but neither \(u\) nor \(v\) knows
what the priority is.The next figure, bottom left, shows the situation when \(v\) receives
the fork.  Receiving the fork doesn't change the priority.
At this point \(v\) is hungry and the fork is
clean and so \(v\) knows that it has higher
priority. \(v\) holds on to the fork until it next eats.Fig.12: How an Agent knows its PriorityAlgorithmProperties of Reachable StatesHere is a list of some of the properties of states in all
trajectories.The priority graph is acyclic. \(u\) has priority over a neighbor
  \(v\) exactly when \(u\) holds the fork that it shares with \(v\)
  and the fork is clean, or the fork is in the channel from \(v\) to
  \(u\), or \(v\) holds the fork and the fork is dirty.Eating philosophers hold the forks for all edges incident on them,
  and these forks are dirty.All forks held by thinking philosphers are dirty.Thinking philosophers never send requests and never receive
  forks. Thinking philosophers respond to request for forks by sending
  the requested forks.Initial StatesInitially all philosophers are thinking; all forks are dirty; and all channels are
empty. The forks are placed so that the priority graph is acyclic.
The initial assignment of forks is as follows.
Given an arbitrary acyclic graph, for any edge directed from \(v\) to
\(w\), the fork shared by \(v\) and \(w\) is initially at \(w\) and
the fork is dirty.Algorithm CommandsThe algorithm is specified by the following commands.When a thinking philosophers gets a request for a fork that it holds
  it sends the fork. (A fork held by a thinking philosopher is dirty.)When a thinking philosopher transits to hungry it sends requests for
  all forks that it does not hold.When a hungry philosopher receives a fork, it records the fork as
  being clean. If the hungry philosopher holds all its forks, and if
  it has no request for any dirty fork that it holds, then it transits
  to eating, and records all the forks that it holds in the eating
  state as dirty.When a hungry philosopher receives a request for a fork that it
  holds, it sends the fork if the fork is dirty, and holds on to the
  fork if it is clean.When an eating philosopher receives a request for a fork it
  registers the request in memory, and continues eating while holding
  the fork. When an eating philosopher transits to thinking it sends
  forks for all requests that it received.What could go wrong?The proof of safety is straightforward: Neighbors aren't eating
because neighbors can't hold the same fork at the same time.Before we look at the proof of progress, let's see what may go wrong.Could a group of philosophers exchange forks with each other so that
members of the group eat repeatedly, and starve a philosopher who is
not in the group? For example, in the figure below, could philosophers
\(u, v, w\) exchange forks so that they each eat in turn, and starve
\(y\)?Fig.13: Potential Problems: What could go wrong?Could the system enter a deadlock state in which each hungry philosopher in a
group holds only some --- but not all --- of the forks that it needs
to eat, while other members of the group hold the remaining forks?Proof of CorrectnessThe algorithm is correct. We are required to prove that every hungery
philosopher eats eventually:\(
\forall v: \quad v.h \leadsto v.e
\)where for a philosopher \(v\), \(v.h\) holds exactly when \(v\) is
hungry and \(v.e\) holds exactly when \(v\) is
eating.Variant FunctionTo prove this progress property we find a variant function that
satisfies the following two conditions:Safety: The variant function does not increase while \(v\)
  remains hungry.progress: The following predicate does not hold forever: The
  variant function remains unchanged and \(v\) remains hungry.We propose a variant function which is a pair of integers
 \(nT, nH\),  which are the number of thinking and hungry philosophers,
respectively, of 
higher priority than \(v\).
In terms of the priority graph, \(nT, nH\) are the numbers of thinking
and hungry philosophers (i.e. vertices) with paths to \(v\).Example of Variant FunctionThe figure below shows a portion of the priority graph in a state of the
system. The figure only shows philosophers with higher priority than
philosopher \(v\), i.e., it only shows vertices in the graph with
paths to \(v\). Since eating philosophers have lower priority than
their neighbors, eating philosophers don't appear in this graph.A hungry philosopher is marked with an "H" and a thinking philosopher
with a "T." In the diagram, philosophers \(v, w, x, y\) are hungry and
\(u\) is thinking.
Forks are located at philosophers and are shown as small
colored circles. A dirty fork is colored red and clean one is
blue. For example the fork shared by \(v\) and \(y\) is at \(y\) and
is clean.Fig.15: A Variant Function: Numbers of higher priority
    thinking, hungry agentsExample of Changes to Variant FunctionThe next figure is an example of changes to the variant function. The
diagram on the top left shows the higher priority vertices in the
state of the previous figure. If agent \(x\) eats next the priority
graph transits to the diagram on the top right, and the variant
function \((nT, nH)\) changes from \((1, 3)\) to \((1, 2)\).Fig.16: Example of Values of the Variant FunctionIf agent \(y\) eats next the priority
graph transits to the diagram on the bottom right, and the variant
function \((nT, nH)\) changes from \((1, 2)\) to \((1, 1)\).
If agent \(w\) eats next the priority
graph transits to the diagram on the bottom left, and the variant
function \((nT, nH)\) changes from \((1, 1)\) to \((0, 0)\).Proof that the variant function does not increase
while \(v\) remains hungryIf a philosopher of higher priority than \(v\) transits from thinking
to hungry then \(nT\) decreases. Though \(nH\) increases, the variant function \((nT,
nH)\) decreases because ordering of function values is done
lexicographically.If a philosopher of higher priority than \(v\) transits from hungry
to eating then \(nH\) decreases,  and so the variant function \((nT,
nH)\) decreases.Proof that the following predicate does not
hold forever: The variant function remains unchanged and
\(v\) remains hungry.Let \(w\) be a highest-priority hungry philosopher, i.e. a philosopher with no
hungry philosopher with priority higher than \(w\). (Note: \(w\) may be
the same as \(v\).)
All philosophers with priority higher than \(w\) are thinking. In the
next paragraph we show that either \(w\) gets all its forks and then transits
from hungry to eating, or the variant function decreases.From the algorithm, a hungry philosopher \(w\) requests forks from its
neighbors. From the algorithm, \(w\) eventually gets forks from 
all its lower priority neighbors. A higher priority neighbor \(x\) of
\(w\) is thinking. So when \(x\) gets a request for a fork from \(w\)
either (1) \(x\) sends the requested fork to \(w\) or (2) \(x\) transits from
thinking to hungry in which case the variant function \((nT, nH)\)
decreases.Summary: Key Ideas of this ModuleThis module introduced the problem of distributed mutual exclusion;
showed how the good neighbor policy --- a winning agent reduces its
priority to be lower than all the agents that it competes with --
solves this conflict resolution problem; introduced tokens and what
agents know about other agents holding tokens; and showed a proof
pattern that is one of the most common patterns for proving progress.ReviewWhat is the meaning of mutual exclusion among neighboring
  philosophers?An invariant of the algorithm is that each token is in exactly one
  place: an agent or a channel. How does this invariant help in
  designing the algorithm?An invariant of the algorithm is that the relative priorities among
  agents forms a partial order --- the priority graph is acyclic. What
  can go wrong if the priority graph has cycles?An agent that has a request for a fork releases the fork if the fork
  is dirty, and holds on to the fork if the fork is clean. What could
  go wrong if an agent releases a fork when it gets a request,
  regardless of whether the fork is clean or dirty?K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology



Figure Link: https://kmchandy.github.io/DistributedCollaboration/DiningPhilosophers/Slide12.jpg

Caption: Fig.12: How an Agent knows its Priority


The next figure, bottom left, shows the situation when \(v\) receives
the fork.  Receiving the fork doesn't change the priority.
At this point \(v\) is hungry and the fork is
clean and so \(v\) knows that it has higher
priority. \(v\) holds on to the fork until it next eats.Fig.12: How an Agent knows its PriorityAlgorithmProperties of Reachable StatesHere is a list of some of the properties of states in all
trajectories.The priority graph is acyclic. \(u\) has priority over a neighbor
  \(v\) exactly when \(u\) holds the fork that it shares with \(v\)
  and the fork is clean, or the fork is in the channel from \(v\) to
  \(u\), or \(v\) holds the fork and the fork is dirty.Eating philosophers hold the forks for all edges incident on them,
  and these forks are dirty.All forks held by thinking philosphers are dirty.Thinking philosophers never send requests and never receive
  forks. Thinking philosophers respond to request for forks by sending
  the requested forks.Initial StatesInitially all philosophers are thinking; all forks are dirty; and all channels are
empty. The forks are placed so that the priority graph is acyclic.
The initial assignment of forks is as follows.
Given an arbitrary acyclic graph, for any edge directed from \(v\) to
\(w\), the fork shared by \(v\) and \(w\) is initially at \(w\) and
the fork is dirty.Algorithm CommandsThe algorithm is specified by the following commands.When a thinking philosophers gets a request for a fork that it holds
  it sends the fork. (A fork held by a thinking philosopher is dirty.)When a thinking philosopher transits to hungry it sends requests for
  all forks that it does not hold.When a hungry philosopher receives a fork, it records the fork as
  being clean. If the hungry philosopher holds all its forks, and if
  it has no request for any dirty fork that it holds, then it transits
  to eating, and records all the forks that it holds in the eating
  state as dirty.When a hungry philosopher receives a request for a fork that it
  holds, it sends the fork if the fork is dirty, and holds on to the
  fork if it is clean.When an eating philosopher receives a request for a fork it
  registers the request in memory, and continues eating while holding
  the fork. When an eating philosopher transits to thinking it sends
  forks for all requests that it received.What could go wrong?The proof of safety is straightforward: Neighbors aren't eating
because neighbors can't hold the same fork at the same time.Before we look at the proof of progress, let's see what may go wrong.Could a group of philosophers exchange forks with each other so that
members of the group eat repeatedly, and starve a philosopher who is
not in the group? For example, in the figure below, could philosophers
\(u, v, w\) exchange forks so that they each eat in turn, and starve
\(y\)?Fig.13: Potential Problems: What could go wrong?Could the system enter a deadlock state in which each hungry philosopher in a
group holds only some --- but not all --- of the forks that it needs
to eat, while other members of the group hold the remaining forks?Proof of CorrectnessThe algorithm is correct. We are required to prove that every hungery
philosopher eats eventually:\(
\forall v: \quad v.h \leadsto v.e
\)where for a philosopher \(v\), \(v.h\) holds exactly when \(v\) is
hungry and \(v.e\) holds exactly when \(v\) is
eating.Variant FunctionTo prove this progress property we find a variant function that
satisfies the following two conditions:Safety: The variant function does not increase while \(v\)
  remains hungry.progress: The following predicate does not hold forever: The
  variant function remains unchanged and \(v\) remains hungry.We propose a variant function which is a pair of integers
 \(nT, nH\),  which are the number of thinking and hungry philosophers,
respectively, of 
higher priority than \(v\).
In terms of the priority graph, \(nT, nH\) are the numbers of thinking
and hungry philosophers (i.e. vertices) with paths to \(v\).Example of Variant FunctionThe figure below shows a portion of the priority graph in a state of the
system. The figure only shows philosophers with higher priority than
philosopher \(v\), i.e., it only shows vertices in the graph with
paths to \(v\). Since eating philosophers have lower priority than
their neighbors, eating philosophers don't appear in this graph.A hungry philosopher is marked with an "H" and a thinking philosopher
with a "T." In the diagram, philosophers \(v, w, x, y\) are hungry and
\(u\) is thinking.
Forks are located at philosophers and are shown as small
colored circles. A dirty fork is colored red and clean one is
blue. For example the fork shared by \(v\) and \(y\) is at \(y\) and
is clean.Fig.15: A Variant Function: Numbers of higher priority
    thinking, hungry agentsExample of Changes to Variant FunctionThe next figure is an example of changes to the variant function. The
diagram on the top left shows the higher priority vertices in the
state of the previous figure. If agent \(x\) eats next the priority
graph transits to the diagram on the top right, and the variant
function \((nT, nH)\) changes from \((1, 3)\) to \((1, 2)\).Fig.16: Example of Values of the Variant FunctionIf agent \(y\) eats next the priority
graph transits to the diagram on the bottom right, and the variant
function \((nT, nH)\) changes from \((1, 2)\) to \((1, 1)\).
If agent \(w\) eats next the priority
graph transits to the diagram on the bottom left, and the variant
function \((nT, nH)\) changes from \((1, 1)\) to \((0, 0)\).Proof that the variant function does not increase
while \(v\) remains hungryIf a philosopher of higher priority than \(v\) transits from thinking
to hungry then \(nT\) decreases. Though \(nH\) increases, the variant function \((nT,
nH)\) decreases because ordering of function values is done
lexicographically.If a philosopher of higher priority than \(v\) transits from hungry
to eating then \(nH\) decreases,  and so the variant function \((nT,
nH)\) decreases.Proof that the following predicate does not
hold forever: The variant function remains unchanged and
\(v\) remains hungry.Let \(w\) be a highest-priority hungry philosopher, i.e. a philosopher with no
hungry philosopher with priority higher than \(w\). (Note: \(w\) may be
the same as \(v\).)
All philosophers with priority higher than \(w\) are thinking. In the
next paragraph we show that either \(w\) gets all its forks and then transits
from hungry to eating, or the variant function decreases.From the algorithm, a hungry philosopher \(w\) requests forks from its
neighbors. From the algorithm, \(w\) eventually gets forks from 
all its lower priority neighbors. A higher priority neighbor \(x\) of
\(w\) is thinking. So when \(x\) gets a request for a fork from \(w\)
either (1) \(x\) sends the requested fork to \(w\) or (2) \(x\) transits from
thinking to hungry in which case the variant function \((nT, nH)\)
decreases.Summary: Key Ideas of this ModuleThis module introduced the problem of distributed mutual exclusion;
showed how the good neighbor policy --- a winning agent reduces its
priority to be lower than all the agents that it competes with --
solves this conflict resolution problem; introduced tokens and what
agents know about other agents holding tokens; and showed a proof
pattern that is one of the most common patterns for proving progress.ReviewWhat is the meaning of mutual exclusion among neighboring
  philosophers?An invariant of the algorithm is that each token is in exactly one
  place: an agent or a channel. How does this invariant help in
  designing the algorithm?An invariant of the algorithm is that the relative priorities among
  agents forms a partial order --- the priority graph is acyclic. What
  can go wrong if the priority graph has cycles?An agent that has a request for a fork releases the fork if the fork
  is dirty, and holds on to the fork if the fork is clean. What could
  go wrong if an agent releases a fork when it gets a request,
  regardless of whether the fork is clean or dirty?K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology



Figure Link: https://kmchandy.github.io/DistributedCollaboration/DiningPhilosophers/Slide12.jpg

Caption: Fig.12: How an Agent knows its Priority


The algorithm is specified by the following commands.When a thinking philosophers gets a request for a fork that it holds
  it sends the fork. (A fork held by a thinking philosopher is dirty.)When a thinking philosopher transits to hungry it sends requests for
  all forks that it does not hold.When a hungry philosopher receives a fork, it records the fork as
  being clean. If the hungry philosopher holds all its forks, and if
  it has no request for any dirty fork that it holds, then it transits
  to eating, and records all the forks that it holds in the eating
  state as dirty.When a hungry philosopher receives a request for a fork that it
  holds, it sends the fork if the fork is dirty, and holds on to the
  fork if it is clean.When an eating philosopher receives a request for a fork it
  registers the request in memory, and continues eating while holding
  the fork. When an eating philosopher transits to thinking it sends
  forks for all requests that it received.What could go wrong?The proof of safety is straightforward: Neighbors aren't eating
because neighbors can't hold the same fork at the same time.Before we look at the proof of progress, let's see what may go wrong.Could a group of philosophers exchange forks with each other so that
members of the group eat repeatedly, and starve a philosopher who is
not in the group? For example, in the figure below, could philosophers
\(u, v, w\) exchange forks so that they each eat in turn, and starve
\(y\)?Fig.13: Potential Problems: What could go wrong?Could the system enter a deadlock state in which each hungry philosopher in a
group holds only some --- but not all --- of the forks that it needs
to eat, while other members of the group hold the remaining forks?Proof of CorrectnessThe algorithm is correct. We are required to prove that every hungery
philosopher eats eventually:\(
\forall v: \quad v.h \leadsto v.e
\)where for a philosopher \(v\), \(v.h\) holds exactly when \(v\) is
hungry and \(v.e\) holds exactly when \(v\) is
eating.Variant FunctionTo prove this progress property we find a variant function that
satisfies the following two conditions:Safety: The variant function does not increase while \(v\)
  remains hungry.progress: The following predicate does not hold forever: The
  variant function remains unchanged and \(v\) remains hungry.We propose a variant function which is a pair of integers
 \(nT, nH\),  which are the number of thinking and hungry philosophers,
respectively, of 
higher priority than \(v\).
In terms of the priority graph, \(nT, nH\) are the numbers of thinking
and hungry philosophers (i.e. vertices) with paths to \(v\).Example of Variant FunctionThe figure below shows a portion of the priority graph in a state of the
system. The figure only shows philosophers with higher priority than
philosopher \(v\), i.e., it only shows vertices in the graph with
paths to \(v\). Since eating philosophers have lower priority than
their neighbors, eating philosophers don't appear in this graph.A hungry philosopher is marked with an "H" and a thinking philosopher
with a "T." In the diagram, philosophers \(v, w, x, y\) are hungry and
\(u\) is thinking.
Forks are located at philosophers and are shown as small
colored circles. A dirty fork is colored red and clean one is
blue. For example the fork shared by \(v\) and \(y\) is at \(y\) and
is clean.Fig.15: A Variant Function: Numbers of higher priority
    thinking, hungry agentsExample of Changes to Variant FunctionThe next figure is an example of changes to the variant function. The
diagram on the top left shows the higher priority vertices in the
state of the previous figure. If agent \(x\) eats next the priority
graph transits to the diagram on the top right, and the variant
function \((nT, nH)\) changes from \((1, 3)\) to \((1, 2)\).Fig.16: Example of Values of the Variant FunctionIf agent \(y\) eats next the priority
graph transits to the diagram on the bottom right, and the variant
function \((nT, nH)\) changes from \((1, 2)\) to \((1, 1)\).
If agent \(w\) eats next the priority
graph transits to the diagram on the bottom left, and the variant
function \((nT, nH)\) changes from \((1, 1)\) to \((0, 0)\).Proof that the variant function does not increase
while \(v\) remains hungryIf a philosopher of higher priority than \(v\) transits from thinking
to hungry then \(nT\) decreases. Though \(nH\) increases, the variant function \((nT,
nH)\) decreases because ordering of function values is done
lexicographically.If a philosopher of higher priority than \(v\) transits from hungry
to eating then \(nH\) decreases,  and so the variant function \((nT,
nH)\) decreases.Proof that the following predicate does not
hold forever: The variant function remains unchanged and
\(v\) remains hungry.Let \(w\) be a highest-priority hungry philosopher, i.e. a philosopher with no
hungry philosopher with priority higher than \(w\). (Note: \(w\) may be
the same as \(v\).)
All philosophers with priority higher than \(w\) are thinking. In the
next paragraph we show that either \(w\) gets all its forks and then transits
from hungry to eating, or the variant function decreases.From the algorithm, a hungry philosopher \(w\) requests forks from its
neighbors. From the algorithm, \(w\) eventually gets forks from 
all its lower priority neighbors. A higher priority neighbor \(x\) of
\(w\) is thinking. So when \(x\) gets a request for a fork from \(w\)
either (1) \(x\) sends the requested fork to \(w\) or (2) \(x\) transits from
thinking to hungry in which case the variant function \((nT, nH)\)
decreases.Summary: Key Ideas of this ModuleThis module introduced the problem of distributed mutual exclusion;
showed how the good neighbor policy --- a winning agent reduces its
priority to be lower than all the agents that it competes with --
solves this conflict resolution problem; introduced tokens and what
agents know about other agents holding tokens; and showed a proof
pattern that is one of the most common patterns for proving progress.ReviewWhat is the meaning of mutual exclusion among neighboring
  philosophers?An invariant of the algorithm is that each token is in exactly one
  place: an agent or a channel. How does this invariant help in
  designing the algorithm?An invariant of the algorithm is that the relative priorities among
  agents forms a partial order --- the priority graph is acyclic. What
  can go wrong if the priority graph has cycles?An agent that has a request for a fork releases the fork if the fork
  is dirty, and holds on to the fork if the fork is clean. What could
  go wrong if an agent releases a fork when it gets a request,
  regardless of whether the fork is clean or dirty?K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology



Figure Link: https://kmchandy.github.io/DistributedCollaboration/DiningPhilosophers/Slide13.jpg

Caption: Fig.13: Potential Problems: What could go wrong?


Before we look at the proof of progress, let's see what may go wrong.Could a group of philosophers exchange forks with each other so that
members of the group eat repeatedly, and starve a philosopher who is
not in the group? For example, in the figure below, could philosophers
\(u, v, w\) exchange forks so that they each eat in turn, and starve
\(y\)?Fig.13: Potential Problems: What could go wrong?Could the system enter a deadlock state in which each hungry philosopher in a
group holds only some --- but not all --- of the forks that it needs
to eat, while other members of the group hold the remaining forks?Proof of CorrectnessThe algorithm is correct. We are required to prove that every hungery
philosopher eats eventually:\(
\forall v: \quad v.h \leadsto v.e
\)where for a philosopher \(v\), \(v.h\) holds exactly when \(v\) is
hungry and \(v.e\) holds exactly when \(v\) is
eating.Variant FunctionTo prove this progress property we find a variant function that
satisfies the following two conditions:Safety: The variant function does not increase while \(v\)
  remains hungry.progress: The following predicate does not hold forever: The
  variant function remains unchanged and \(v\) remains hungry.We propose a variant function which is a pair of integers
 \(nT, nH\),  which are the number of thinking and hungry philosophers,
respectively, of 
higher priority than \(v\).
In terms of the priority graph, \(nT, nH\) are the numbers of thinking
and hungry philosophers (i.e. vertices) with paths to \(v\).Example of Variant FunctionThe figure below shows a portion of the priority graph in a state of the
system. The figure only shows philosophers with higher priority than
philosopher \(v\), i.e., it only shows vertices in the graph with
paths to \(v\). Since eating philosophers have lower priority than
their neighbors, eating philosophers don't appear in this graph.A hungry philosopher is marked with an "H" and a thinking philosopher
with a "T." In the diagram, philosophers \(v, w, x, y\) are hungry and
\(u\) is thinking.
Forks are located at philosophers and are shown as small
colored circles. A dirty fork is colored red and clean one is
blue. For example the fork shared by \(v\) and \(y\) is at \(y\) and
is clean.Fig.15: A Variant Function: Numbers of higher priority
    thinking, hungry agentsExample of Changes to Variant FunctionThe next figure is an example of changes to the variant function. The
diagram on the top left shows the higher priority vertices in the
state of the previous figure. If agent \(x\) eats next the priority
graph transits to the diagram on the top right, and the variant
function \((nT, nH)\) changes from \((1, 3)\) to \((1, 2)\).Fig.16: Example of Values of the Variant FunctionIf agent \(y\) eats next the priority
graph transits to the diagram on the bottom right, and the variant
function \((nT, nH)\) changes from \((1, 2)\) to \((1, 1)\).
If agent \(w\) eats next the priority
graph transits to the diagram on the bottom left, and the variant
function \((nT, nH)\) changes from \((1, 1)\) to \((0, 0)\).Proof that the variant function does not increase
while \(v\) remains hungryIf a philosopher of higher priority than \(v\) transits from thinking
to hungry then \(nT\) decreases. Though \(nH\) increases, the variant function \((nT,
nH)\) decreases because ordering of function values is done
lexicographically.If a philosopher of higher priority than \(v\) transits from hungry
to eating then \(nH\) decreases,  and so the variant function \((nT,
nH)\) decreases.Proof that the following predicate does not
hold forever: The variant function remains unchanged and
\(v\) remains hungry.Let \(w\) be a highest-priority hungry philosopher, i.e. a philosopher with no
hungry philosopher with priority higher than \(w\). (Note: \(w\) may be
the same as \(v\).)
All philosophers with priority higher than \(w\) are thinking. In the
next paragraph we show that either \(w\) gets all its forks and then transits
from hungry to eating, or the variant function decreases.From the algorithm, a hungry philosopher \(w\) requests forks from its
neighbors. From the algorithm, \(w\) eventually gets forks from 
all its lower priority neighbors. A higher priority neighbor \(x\) of
\(w\) is thinking. So when \(x\) gets a request for a fork from \(w\)
either (1) \(x\) sends the requested fork to \(w\) or (2) \(x\) transits from
thinking to hungry in which case the variant function \((nT, nH)\)
decreases.Summary: Key Ideas of this ModuleThis module introduced the problem of distributed mutual exclusion;
showed how the good neighbor policy --- a winning agent reduces its
priority to be lower than all the agents that it competes with --
solves this conflict resolution problem; introduced tokens and what
agents know about other agents holding tokens; and showed a proof
pattern that is one of the most common patterns for proving progress.ReviewWhat is the meaning of mutual exclusion among neighboring
  philosophers?An invariant of the algorithm is that each token is in exactly one
  place: an agent or a channel. How does this invariant help in
  designing the algorithm?An invariant of the algorithm is that the relative priorities among
  agents forms a partial order --- the priority graph is acyclic. What
  can go wrong if the priority graph has cycles?An agent that has a request for a fork releases the fork if the fork
  is dirty, and holds on to the fork if the fork is clean. What could
  go wrong if an agent releases a fork when it gets a request,
  regardless of whether the fork is clean or dirty?K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology



Figure Link: https://kmchandy.github.io/DistributedCollaboration/DiningPhilosophers/Slide13.jpg

Caption: Fig.13: Potential Problems: What could go wrong?


Could a group of philosophers exchange forks with each other so that
members of the group eat repeatedly, and starve a philosopher who is
not in the group? For example, in the figure below, could philosophers
\(u, v, w\) exchange forks so that they each eat in turn, and starve
\(y\)?Fig.13: Potential Problems: What could go wrong?Could the system enter a deadlock state in which each hungry philosopher in a
group holds only some --- but not all --- of the forks that it needs
to eat, while other members of the group hold the remaining forks?Proof of CorrectnessThe algorithm is correct. We are required to prove that every hungery
philosopher eats eventually:\(
\forall v: \quad v.h \leadsto v.e
\)where for a philosopher \(v\), \(v.h\) holds exactly when \(v\) is
hungry and \(v.e\) holds exactly when \(v\) is
eating.Variant FunctionTo prove this progress property we find a variant function that
satisfies the following two conditions:Safety: The variant function does not increase while \(v\)
  remains hungry.progress: The following predicate does not hold forever: The
  variant function remains unchanged and \(v\) remains hungry.We propose a variant function which is a pair of integers
 \(nT, nH\),  which are the number of thinking and hungry philosophers,
respectively, of 
higher priority than \(v\).
In terms of the priority graph, \(nT, nH\) are the numbers of thinking
and hungry philosophers (i.e. vertices) with paths to \(v\).Example of Variant FunctionThe figure below shows a portion of the priority graph in a state of the
system. The figure only shows philosophers with higher priority than
philosopher \(v\), i.e., it only shows vertices in the graph with
paths to \(v\). Since eating philosophers have lower priority than
their neighbors, eating philosophers don't appear in this graph.A hungry philosopher is marked with an "H" and a thinking philosopher
with a "T." In the diagram, philosophers \(v, w, x, y\) are hungry and
\(u\) is thinking.
Forks are located at philosophers and are shown as small
colored circles. A dirty fork is colored red and clean one is
blue. For example the fork shared by \(v\) and \(y\) is at \(y\) and
is clean.Fig.15: A Variant Function: Numbers of higher priority
    thinking, hungry agentsExample of Changes to Variant FunctionThe next figure is an example of changes to the variant function. The
diagram on the top left shows the higher priority vertices in the
state of the previous figure. If agent \(x\) eats next the priority
graph transits to the diagram on the top right, and the variant
function \((nT, nH)\) changes from \((1, 3)\) to \((1, 2)\).Fig.16: Example of Values of the Variant FunctionIf agent \(y\) eats next the priority
graph transits to the diagram on the bottom right, and the variant
function \((nT, nH)\) changes from \((1, 2)\) to \((1, 1)\).
If agent \(w\) eats next the priority
graph transits to the diagram on the bottom left, and the variant
function \((nT, nH)\) changes from \((1, 1)\) to \((0, 0)\).Proof that the variant function does not increase
while \(v\) remains hungryIf a philosopher of higher priority than \(v\) transits from thinking
to hungry then \(nT\) decreases. Though \(nH\) increases, the variant function \((nT,
nH)\) decreases because ordering of function values is done
lexicographically.If a philosopher of higher priority than \(v\) transits from hungry
to eating then \(nH\) decreases,  and so the variant function \((nT,
nH)\) decreases.Proof that the following predicate does not
hold forever: The variant function remains unchanged and
\(v\) remains hungry.Let \(w\) be a highest-priority hungry philosopher, i.e. a philosopher with no
hungry philosopher with priority higher than \(w\). (Note: \(w\) may be
the same as \(v\).)
All philosophers with priority higher than \(w\) are thinking. In the
next paragraph we show that either \(w\) gets all its forks and then transits
from hungry to eating, or the variant function decreases.From the algorithm, a hungry philosopher \(w\) requests forks from its
neighbors. From the algorithm, \(w\) eventually gets forks from 
all its lower priority neighbors. A higher priority neighbor \(x\) of
\(w\) is thinking. So when \(x\) gets a request for a fork from \(w\)
either (1) \(x\) sends the requested fork to \(w\) or (2) \(x\) transits from
thinking to hungry in which case the variant function \((nT, nH)\)
decreases.Summary: Key Ideas of this ModuleThis module introduced the problem of distributed mutual exclusion;
showed how the good neighbor policy --- a winning agent reduces its
priority to be lower than all the agents that it competes with --
solves this conflict resolution problem; introduced tokens and what
agents know about other agents holding tokens; and showed a proof
pattern that is one of the most common patterns for proving progress.ReviewWhat is the meaning of mutual exclusion among neighboring
  philosophers?An invariant of the algorithm is that each token is in exactly one
  place: an agent or a channel. How does this invariant help in
  designing the algorithm?An invariant of the algorithm is that the relative priorities among
  agents forms a partial order --- the priority graph is acyclic. What
  can go wrong if the priority graph has cycles?An agent that has a request for a fork releases the fork if the fork
  is dirty, and holds on to the fork if the fork is clean. What could
  go wrong if an agent releases a fork when it gets a request,
  regardless of whether the fork is clean or dirty?K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology



Figure Link: https://kmchandy.github.io/DistributedCollaboration/DiningPhilosophers/Slide13.jpg

Caption: Fig.13: Potential Problems: What could go wrong?


Could the system enter a deadlock state in which each hungry philosopher in a
group holds only some --- but not all --- of the forks that it needs
to eat, while other members of the group hold the remaining forks?Proof of CorrectnessThe algorithm is correct. We are required to prove that every hungery
philosopher eats eventually:\(
\forall v: \quad v.h \leadsto v.e
\)where for a philosopher \(v\), \(v.h\) holds exactly when \(v\) is
hungry and \(v.e\) holds exactly when \(v\) is
eating.Variant FunctionTo prove this progress property we find a variant function that
satisfies the following two conditions:Safety: The variant function does not increase while \(v\)
  remains hungry.progress: The following predicate does not hold forever: The
  variant function remains unchanged and \(v\) remains hungry.We propose a variant function which is a pair of integers
 \(nT, nH\),  which are the number of thinking and hungry philosophers,
respectively, of 
higher priority than \(v\).
In terms of the priority graph, \(nT, nH\) are the numbers of thinking
and hungry philosophers (i.e. vertices) with paths to \(v\).Example of Variant FunctionThe figure below shows a portion of the priority graph in a state of the
system. The figure only shows philosophers with higher priority than
philosopher \(v\), i.e., it only shows vertices in the graph with
paths to \(v\). Since eating philosophers have lower priority than
their neighbors, eating philosophers don't appear in this graph.A hungry philosopher is marked with an "H" and a thinking philosopher
with a "T." In the diagram, philosophers \(v, w, x, y\) are hungry and
\(u\) is thinking.
Forks are located at philosophers and are shown as small
colored circles. A dirty fork is colored red and clean one is
blue. For example the fork shared by \(v\) and \(y\) is at \(y\) and
is clean.Fig.15: A Variant Function: Numbers of higher priority
    thinking, hungry agentsExample of Changes to Variant FunctionThe next figure is an example of changes to the variant function. The
diagram on the top left shows the higher priority vertices in the
state of the previous figure. If agent \(x\) eats next the priority
graph transits to the diagram on the top right, and the variant
function \((nT, nH)\) changes from \((1, 3)\) to \((1, 2)\).Fig.16: Example of Values of the Variant FunctionIf agent \(y\) eats next the priority
graph transits to the diagram on the bottom right, and the variant
function \((nT, nH)\) changes from \((1, 2)\) to \((1, 1)\).
If agent \(w\) eats next the priority
graph transits to the diagram on the bottom left, and the variant
function \((nT, nH)\) changes from \((1, 1)\) to \((0, 0)\).Proof that the variant function does not increase
while \(v\) remains hungryIf a philosopher of higher priority than \(v\) transits from thinking
to hungry then \(nT\) decreases. Though \(nH\) increases, the variant function \((nT,
nH)\) decreases because ordering of function values is done
lexicographically.If a philosopher of higher priority than \(v\) transits from hungry
to eating then \(nH\) decreases,  and so the variant function \((nT,
nH)\) decreases.Proof that the following predicate does not
hold forever: The variant function remains unchanged and
\(v\) remains hungry.Let \(w\) be a highest-priority hungry philosopher, i.e. a philosopher with no
hungry philosopher with priority higher than \(w\). (Note: \(w\) may be
the same as \(v\).)
All philosophers with priority higher than \(w\) are thinking. In the
next paragraph we show that either \(w\) gets all its forks and then transits
from hungry to eating, or the variant function decreases.From the algorithm, a hungry philosopher \(w\) requests forks from its
neighbors. From the algorithm, \(w\) eventually gets forks from 
all its lower priority neighbors. A higher priority neighbor \(x\) of
\(w\) is thinking. So when \(x\) gets a request for a fork from \(w\)
either (1) \(x\) sends the requested fork to \(w\) or (2) \(x\) transits from
thinking to hungry in which case the variant function \((nT, nH)\)
decreases.Summary: Key Ideas of this ModuleThis module introduced the problem of distributed mutual exclusion;
showed how the good neighbor policy --- a winning agent reduces its
priority to be lower than all the agents that it competes with --
solves this conflict resolution problem; introduced tokens and what
agents know about other agents holding tokens; and showed a proof
pattern that is one of the most common patterns for proving progress.ReviewWhat is the meaning of mutual exclusion among neighboring
  philosophers?An invariant of the algorithm is that each token is in exactly one
  place: an agent or a channel. How does this invariant help in
  designing the algorithm?An invariant of the algorithm is that the relative priorities among
  agents forms a partial order --- the priority graph is acyclic. What
  can go wrong if the priority graph has cycles?An agent that has a request for a fork releases the fork if the fork
  is dirty, and holds on to the fork if the fork is clean. What could
  go wrong if an agent releases a fork when it gets a request,
  regardless of whether the fork is clean or dirty?K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology



Figure Link: https://kmchandy.github.io/DistributedCollaboration/DiningPhilosophers/Slide15.jpg

Caption: Fig.15: A Variant Function: Numbers of higher priority
    thinking, hungry agents


\(
\forall v: \quad v.h \leadsto v.e
\)where for a philosopher \(v\), \(v.h\) holds exactly when \(v\) is
hungry and \(v.e\) holds exactly when \(v\) is
eating.Variant FunctionTo prove this progress property we find a variant function that
satisfies the following two conditions:Safety: The variant function does not increase while \(v\)
  remains hungry.progress: The following predicate does not hold forever: The
  variant function remains unchanged and \(v\) remains hungry.We propose a variant function which is a pair of integers
 \(nT, nH\),  which are the number of thinking and hungry philosophers,
respectively, of 
higher priority than \(v\).
In terms of the priority graph, \(nT, nH\) are the numbers of thinking
and hungry philosophers (i.e. vertices) with paths to \(v\).Example of Variant FunctionThe figure below shows a portion of the priority graph in a state of the
system. The figure only shows philosophers with higher priority than
philosopher \(v\), i.e., it only shows vertices in the graph with
paths to \(v\). Since eating philosophers have lower priority than
their neighbors, eating philosophers don't appear in this graph.A hungry philosopher is marked with an "H" and a thinking philosopher
with a "T." In the diagram, philosophers \(v, w, x, y\) are hungry and
\(u\) is thinking.
Forks are located at philosophers and are shown as small
colored circles. A dirty fork is colored red and clean one is
blue. For example the fork shared by \(v\) and \(y\) is at \(y\) and
is clean.Fig.15: A Variant Function: Numbers of higher priority
    thinking, hungry agentsExample of Changes to Variant FunctionThe next figure is an example of changes to the variant function. The
diagram on the top left shows the higher priority vertices in the
state of the previous figure. If agent \(x\) eats next the priority
graph transits to the diagram on the top right, and the variant
function \((nT, nH)\) changes from \((1, 3)\) to \((1, 2)\).Fig.16: Example of Values of the Variant FunctionIf agent \(y\) eats next the priority
graph transits to the diagram on the bottom right, and the variant
function \((nT, nH)\) changes from \((1, 2)\) to \((1, 1)\).
If agent \(w\) eats next the priority
graph transits to the diagram on the bottom left, and the variant
function \((nT, nH)\) changes from \((1, 1)\) to \((0, 0)\).Proof that the variant function does not increase
while \(v\) remains hungryIf a philosopher of higher priority than \(v\) transits from thinking
to hungry then \(nT\) decreases. Though \(nH\) increases, the variant function \((nT,
nH)\) decreases because ordering of function values is done
lexicographically.If a philosopher of higher priority than \(v\) transits from hungry
to eating then \(nH\) decreases,  and so the variant function \((nT,
nH)\) decreases.Proof that the following predicate does not
hold forever: The variant function remains unchanged and
\(v\) remains hungry.Let \(w\) be a highest-priority hungry philosopher, i.e. a philosopher with no
hungry philosopher with priority higher than \(w\). (Note: \(w\) may be
the same as \(v\).)
All philosophers with priority higher than \(w\) are thinking. In the
next paragraph we show that either \(w\) gets all its forks and then transits
from hungry to eating, or the variant function decreases.From the algorithm, a hungry philosopher \(w\) requests forks from its
neighbors. From the algorithm, \(w\) eventually gets forks from 
all its lower priority neighbors. A higher priority neighbor \(x\) of
\(w\) is thinking. So when \(x\) gets a request for a fork from \(w\)
either (1) \(x\) sends the requested fork to \(w\) or (2) \(x\) transits from
thinking to hungry in which case the variant function \((nT, nH)\)
decreases.Summary: Key Ideas of this ModuleThis module introduced the problem of distributed mutual exclusion;
showed how the good neighbor policy --- a winning agent reduces its
priority to be lower than all the agents that it competes with --
solves this conflict resolution problem; introduced tokens and what
agents know about other agents holding tokens; and showed a proof
pattern that is one of the most common patterns for proving progress.ReviewWhat is the meaning of mutual exclusion among neighboring
  philosophers?An invariant of the algorithm is that each token is in exactly one
  place: an agent or a channel. How does this invariant help in
  designing the algorithm?An invariant of the algorithm is that the relative priorities among
  agents forms a partial order --- the priority graph is acyclic. What
  can go wrong if the priority graph has cycles?An agent that has a request for a fork releases the fork if the fork
  is dirty, and holds on to the fork if the fork is clean. What could
  go wrong if an agent releases a fork when it gets a request,
  regardless of whether the fork is clean or dirty?K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology



Figure Link: https://kmchandy.github.io/DistributedCollaboration/DiningPhilosophers/Slide15.jpg

Caption: Fig.15: A Variant Function: Numbers of higher priority
    thinking, hungry agents


where for a philosopher \(v\), \(v.h\) holds exactly when \(v\) is
hungry and \(v.e\) holds exactly when \(v\) is
eating.Variant FunctionTo prove this progress property we find a variant function that
satisfies the following two conditions:Safety: The variant function does not increase while \(v\)
  remains hungry.progress: The following predicate does not hold forever: The
  variant function remains unchanged and \(v\) remains hungry.We propose a variant function which is a pair of integers
 \(nT, nH\),  which are the number of thinking and hungry philosophers,
respectively, of 
higher priority than \(v\).
In terms of the priority graph, \(nT, nH\) are the numbers of thinking
and hungry philosophers (i.e. vertices) with paths to \(v\).Example of Variant FunctionThe figure below shows a portion of the priority graph in a state of the
system. The figure only shows philosophers with higher priority than
philosopher \(v\), i.e., it only shows vertices in the graph with
paths to \(v\). Since eating philosophers have lower priority than
their neighbors, eating philosophers don't appear in this graph.A hungry philosopher is marked with an "H" and a thinking philosopher
with a "T." In the diagram, philosophers \(v, w, x, y\) are hungry and
\(u\) is thinking.
Forks are located at philosophers and are shown as small
colored circles. A dirty fork is colored red and clean one is
blue. For example the fork shared by \(v\) and \(y\) is at \(y\) and
is clean.Fig.15: A Variant Function: Numbers of higher priority
    thinking, hungry agentsExample of Changes to Variant FunctionThe next figure is an example of changes to the variant function. The
diagram on the top left shows the higher priority vertices in the
state of the previous figure. If agent \(x\) eats next the priority
graph transits to the diagram on the top right, and the variant
function \((nT, nH)\) changes from \((1, 3)\) to \((1, 2)\).Fig.16: Example of Values of the Variant FunctionIf agent \(y\) eats next the priority
graph transits to the diagram on the bottom right, and the variant
function \((nT, nH)\) changes from \((1, 2)\) to \((1, 1)\).
If agent \(w\) eats next the priority
graph transits to the diagram on the bottom left, and the variant
function \((nT, nH)\) changes from \((1, 1)\) to \((0, 0)\).Proof that the variant function does not increase
while \(v\) remains hungryIf a philosopher of higher priority than \(v\) transits from thinking
to hungry then \(nT\) decreases. Though \(nH\) increases, the variant function \((nT,
nH)\) decreases because ordering of function values is done
lexicographically.If a philosopher of higher priority than \(v\) transits from hungry
to eating then \(nH\) decreases,  and so the variant function \((nT,
nH)\) decreases.Proof that the following predicate does not
hold forever: The variant function remains unchanged and
\(v\) remains hungry.Let \(w\) be a highest-priority hungry philosopher, i.e. a philosopher with no
hungry philosopher with priority higher than \(w\). (Note: \(w\) may be
the same as \(v\).)
All philosophers with priority higher than \(w\) are thinking. In the
next paragraph we show that either \(w\) gets all its forks and then transits
from hungry to eating, or the variant function decreases.From the algorithm, a hungry philosopher \(w\) requests forks from its
neighbors. From the algorithm, \(w\) eventually gets forks from 
all its lower priority neighbors. A higher priority neighbor \(x\) of
\(w\) is thinking. So when \(x\) gets a request for a fork from \(w\)
either (1) \(x\) sends the requested fork to \(w\) or (2) \(x\) transits from
thinking to hungry in which case the variant function \((nT, nH)\)
decreases.Summary: Key Ideas of this ModuleThis module introduced the problem of distributed mutual exclusion;
showed how the good neighbor policy --- a winning agent reduces its
priority to be lower than all the agents that it competes with --
solves this conflict resolution problem; introduced tokens and what
agents know about other agents holding tokens; and showed a proof
pattern that is one of the most common patterns for proving progress.ReviewWhat is the meaning of mutual exclusion among neighboring
  philosophers?An invariant of the algorithm is that each token is in exactly one
  place: an agent or a channel. How does this invariant help in
  designing the algorithm?An invariant of the algorithm is that the relative priorities among
  agents forms a partial order --- the priority graph is acyclic. What
  can go wrong if the priority graph has cycles?An agent that has a request for a fork releases the fork if the fork
  is dirty, and holds on to the fork if the fork is clean. What could
  go wrong if an agent releases a fork when it gets a request,
  regardless of whether the fork is clean or dirty?K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology



Figure Link: https://kmchandy.github.io/DistributedCollaboration/DiningPhilosophers/Slide15.jpg

Caption: Fig.15: A Variant Function: Numbers of higher priority
    thinking, hungry agents


We propose a variant function which is a pair of integers
 \(nT, nH\),  which are the number of thinking and hungry philosophers,
respectively, of 
higher priority than \(v\).
In terms of the priority graph, \(nT, nH\) are the numbers of thinking
and hungry philosophers (i.e. vertices) with paths to \(v\).Example of Variant FunctionThe figure below shows a portion of the priority graph in a state of the
system. The figure only shows philosophers with higher priority than
philosopher \(v\), i.e., it only shows vertices in the graph with
paths to \(v\). Since eating philosophers have lower priority than
their neighbors, eating philosophers don't appear in this graph.A hungry philosopher is marked with an "H" and a thinking philosopher
with a "T." In the diagram, philosophers \(v, w, x, y\) are hungry and
\(u\) is thinking.
Forks are located at philosophers and are shown as small
colored circles. A dirty fork is colored red and clean one is
blue. For example the fork shared by \(v\) and \(y\) is at \(y\) and
is clean.Fig.15: A Variant Function: Numbers of higher priority
    thinking, hungry agentsExample of Changes to Variant FunctionThe next figure is an example of changes to the variant function. The
diagram on the top left shows the higher priority vertices in the
state of the previous figure. If agent \(x\) eats next the priority
graph transits to the diagram on the top right, and the variant
function \((nT, nH)\) changes from \((1, 3)\) to \((1, 2)\).Fig.16: Example of Values of the Variant FunctionIf agent \(y\) eats next the priority
graph transits to the diagram on the bottom right, and the variant
function \((nT, nH)\) changes from \((1, 2)\) to \((1, 1)\).
If agent \(w\) eats next the priority
graph transits to the diagram on the bottom left, and the variant
function \((nT, nH)\) changes from \((1, 1)\) to \((0, 0)\).Proof that the variant function does not increase
while \(v\) remains hungryIf a philosopher of higher priority than \(v\) transits from thinking
to hungry then \(nT\) decreases. Though \(nH\) increases, the variant function \((nT,
nH)\) decreases because ordering of function values is done
lexicographically.If a philosopher of higher priority than \(v\) transits from hungry
to eating then \(nH\) decreases,  and so the variant function \((nT,
nH)\) decreases.Proof that the following predicate does not
hold forever: The variant function remains unchanged and
\(v\) remains hungry.Let \(w\) be a highest-priority hungry philosopher, i.e. a philosopher with no
hungry philosopher with priority higher than \(w\). (Note: \(w\) may be
the same as \(v\).)
All philosophers with priority higher than \(w\) are thinking. In the
next paragraph we show that either \(w\) gets all its forks and then transits
from hungry to eating, or the variant function decreases.From the algorithm, a hungry philosopher \(w\) requests forks from its
neighbors. From the algorithm, \(w\) eventually gets forks from 
all its lower priority neighbors. A higher priority neighbor \(x\) of
\(w\) is thinking. So when \(x\) gets a request for a fork from \(w\)
either (1) \(x\) sends the requested fork to \(w\) or (2) \(x\) transits from
thinking to hungry in which case the variant function \((nT, nH)\)
decreases.Summary: Key Ideas of this ModuleThis module introduced the problem of distributed mutual exclusion;
showed how the good neighbor policy --- a winning agent reduces its
priority to be lower than all the agents that it competes with --
solves this conflict resolution problem; introduced tokens and what
agents know about other agents holding tokens; and showed a proof
pattern that is one of the most common patterns for proving progress.ReviewWhat is the meaning of mutual exclusion among neighboring
  philosophers?An invariant of the algorithm is that each token is in exactly one
  place: an agent or a channel. How does this invariant help in
  designing the algorithm?An invariant of the algorithm is that the relative priorities among
  agents forms a partial order --- the priority graph is acyclic. What
  can go wrong if the priority graph has cycles?An agent that has a request for a fork releases the fork if the fork
  is dirty, and holds on to the fork if the fork is clean. What could
  go wrong if an agent releases a fork when it gets a request,
  regardless of whether the fork is clean or dirty?K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology



Figure Link: https://kmchandy.github.io/DistributedCollaboration/DiningPhilosophers/Slide15.jpg

Caption: Fig.15: A Variant Function: Numbers of higher priority
    thinking, hungry agents


A hungry philosopher is marked with an "H" and a thinking philosopher
with a "T." In the diagram, philosophers \(v, w, x, y\) are hungry and
\(u\) is thinking.
Forks are located at philosophers and are shown as small
colored circles. A dirty fork is colored red and clean one is
blue. For example the fork shared by \(v\) and \(y\) is at \(y\) and
is clean.Fig.15: A Variant Function: Numbers of higher priority
    thinking, hungry agentsExample of Changes to Variant FunctionThe next figure is an example of changes to the variant function. The
diagram on the top left shows the higher priority vertices in the
state of the previous figure. If agent \(x\) eats next the priority
graph transits to the diagram on the top right, and the variant
function \((nT, nH)\) changes from \((1, 3)\) to \((1, 2)\).Fig.16: Example of Values of the Variant FunctionIf agent \(y\) eats next the priority
graph transits to the diagram on the bottom right, and the variant
function \((nT, nH)\) changes from \((1, 2)\) to \((1, 1)\).
If agent \(w\) eats next the priority
graph transits to the diagram on the bottom left, and the variant
function \((nT, nH)\) changes from \((1, 1)\) to \((0, 0)\).Proof that the variant function does not increase
while \(v\) remains hungryIf a philosopher of higher priority than \(v\) transits from thinking
to hungry then \(nT\) decreases. Though \(nH\) increases, the variant function \((nT,
nH)\) decreases because ordering of function values is done
lexicographically.If a philosopher of higher priority than \(v\) transits from hungry
to eating then \(nH\) decreases,  and so the variant function \((nT,
nH)\) decreases.Proof that the following predicate does not
hold forever: The variant function remains unchanged and
\(v\) remains hungry.Let \(w\) be a highest-priority hungry philosopher, i.e. a philosopher with no
hungry philosopher with priority higher than \(w\). (Note: \(w\) may be
the same as \(v\).)
All philosophers with priority higher than \(w\) are thinking. In the
next paragraph we show that either \(w\) gets all its forks and then transits
from hungry to eating, or the variant function decreases.From the algorithm, a hungry philosopher \(w\) requests forks from its
neighbors. From the algorithm, \(w\) eventually gets forks from 
all its lower priority neighbors. A higher priority neighbor \(x\) of
\(w\) is thinking. So when \(x\) gets a request for a fork from \(w\)
either (1) \(x\) sends the requested fork to \(w\) or (2) \(x\) transits from
thinking to hungry in which case the variant function \((nT, nH)\)
decreases.Summary: Key Ideas of this ModuleThis module introduced the problem of distributed mutual exclusion;
showed how the good neighbor policy --- a winning agent reduces its
priority to be lower than all the agents that it competes with --
solves this conflict resolution problem; introduced tokens and what
agents know about other agents holding tokens; and showed a proof
pattern that is one of the most common patterns for proving progress.ReviewWhat is the meaning of mutual exclusion among neighboring
  philosophers?An invariant of the algorithm is that each token is in exactly one
  place: an agent or a channel. How does this invariant help in
  designing the algorithm?An invariant of the algorithm is that the relative priorities among
  agents forms a partial order --- the priority graph is acyclic. What
  can go wrong if the priority graph has cycles?An agent that has a request for a fork releases the fork if the fork
  is dirty, and holds on to the fork if the fork is clean. What could
  go wrong if an agent releases a fork when it gets a request,
  regardless of whether the fork is clean or dirty?K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology



Figure Link: https://kmchandy.github.io/DistributedCollaboration/DiningPhilosophers/Slide15.jpg

Caption: Fig.15: A Variant Function: Numbers of higher priority
    thinking, hungry agents


If a philosopher of higher priority than \(v\) transits from thinking
to hungry then \(nT\) decreases. Though \(nH\) increases, the variant function \((nT,
nH)\) decreases because ordering of function values is done
lexicographically.If a philosopher of higher priority than \(v\) transits from hungry
to eating then \(nH\) decreases,  and so the variant function \((nT,
nH)\) decreases.Proof that the following predicate does not
hold forever: The variant function remains unchanged and
\(v\) remains hungry.Let \(w\) be a highest-priority hungry philosopher, i.e. a philosopher with no
hungry philosopher with priority higher than \(w\). (Note: \(w\) may be
the same as \(v\).)
All philosophers with priority higher than \(w\) are thinking. In the
next paragraph we show that either \(w\) gets all its forks and then transits
from hungry to eating, or the variant function decreases.From the algorithm, a hungry philosopher \(w\) requests forks from its
neighbors. From the algorithm, \(w\) eventually gets forks from 
all its lower priority neighbors. A higher priority neighbor \(x\) of
\(w\) is thinking. So when \(x\) gets a request for a fork from \(w\)
either (1) \(x\) sends the requested fork to \(w\) or (2) \(x\) transits from
thinking to hungry in which case the variant function \((nT, nH)\)
decreases.Summary: Key Ideas of this ModuleThis module introduced the problem of distributed mutual exclusion;
showed how the good neighbor policy --- a winning agent reduces its
priority to be lower than all the agents that it competes with --
solves this conflict resolution problem; introduced tokens and what
agents know about other agents holding tokens; and showed a proof
pattern that is one of the most common patterns for proving progress.ReviewWhat is the meaning of mutual exclusion among neighboring
  philosophers?An invariant of the algorithm is that each token is in exactly one
  place: an agent or a channel. How does this invariant help in
  designing the algorithm?An invariant of the algorithm is that the relative priorities among
  agents forms a partial order --- the priority graph is acyclic. What
  can go wrong if the priority graph has cycles?An agent that has a request for a fork releases the fork if the fork
  is dirty, and holds on to the fork if the fork is clean. What could
  go wrong if an agent releases a fork when it gets a request,
  regardless of whether the fork is clean or dirty?K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

If a philosopher of higher priority than \(v\) transits from hungry
to eating then \(nH\) decreases,  and so the variant function \((nT,
nH)\) decreases.Proof that the following predicate does not
hold forever: The variant function remains unchanged and
\(v\) remains hungry.Let \(w\) be a highest-priority hungry philosopher, i.e. a philosopher with no
hungry philosopher with priority higher than \(w\). (Note: \(w\) may be
the same as \(v\).)
All philosophers with priority higher than \(w\) are thinking. In the
next paragraph we show that either \(w\) gets all its forks and then transits
from hungry to eating, or the variant function decreases.From the algorithm, a hungry philosopher \(w\) requests forks from its
neighbors. From the algorithm, \(w\) eventually gets forks from 
all its lower priority neighbors. A higher priority neighbor \(x\) of
\(w\) is thinking. So when \(x\) gets a request for a fork from \(w\)
either (1) \(x\) sends the requested fork to \(w\) or (2) \(x\) transits from
thinking to hungry in which case the variant function \((nT, nH)\)
decreases.Summary: Key Ideas of this ModuleThis module introduced the problem of distributed mutual exclusion;
showed how the good neighbor policy --- a winning agent reduces its
priority to be lower than all the agents that it competes with --
solves this conflict resolution problem; introduced tokens and what
agents know about other agents holding tokens; and showed a proof
pattern that is one of the most common patterns for proving progress.ReviewWhat is the meaning of mutual exclusion among neighboring
  philosophers?An invariant of the algorithm is that each token is in exactly one
  place: an agent or a channel. How does this invariant help in
  designing the algorithm?An invariant of the algorithm is that the relative priorities among
  agents forms a partial order --- the priority graph is acyclic. What
  can go wrong if the priority graph has cycles?An agent that has a request for a fork releases the fork if the fork
  is dirty, and holds on to the fork if the fork is clean. What could
  go wrong if an agent releases a fork when it gets a request,
  regardless of whether the fork is clean or dirty?K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

Let \(w\) be a highest-priority hungry philosopher, i.e. a philosopher with no
hungry philosopher with priority higher than \(w\). (Note: \(w\) may be
the same as \(v\).)
All philosophers with priority higher than \(w\) are thinking. In the
next paragraph we show that either \(w\) gets all its forks and then transits
from hungry to eating, or the variant function decreases.From the algorithm, a hungry philosopher \(w\) requests forks from its
neighbors. From the algorithm, \(w\) eventually gets forks from 
all its lower priority neighbors. A higher priority neighbor \(x\) of
\(w\) is thinking. So when \(x\) gets a request for a fork from \(w\)
either (1) \(x\) sends the requested fork to \(w\) or (2) \(x\) transits from
thinking to hungry in which case the variant function \((nT, nH)\)
decreases.Summary: Key Ideas of this ModuleThis module introduced the problem of distributed mutual exclusion;
showed how the good neighbor policy --- a winning agent reduces its
priority to be lower than all the agents that it competes with --
solves this conflict resolution problem; introduced tokens and what
agents know about other agents holding tokens; and showed a proof
pattern that is one of the most common patterns for proving progress.ReviewWhat is the meaning of mutual exclusion among neighboring
  philosophers?An invariant of the algorithm is that each token is in exactly one
  place: an agent or a channel. How does this invariant help in
  designing the algorithm?An invariant of the algorithm is that the relative priorities among
  agents forms a partial order --- the priority graph is acyclic. What
  can go wrong if the priority graph has cycles?An agent that has a request for a fork releases the fork if the fork
  is dirty, and holds on to the fork if the fork is clean. What could
  go wrong if an agent releases a fork when it gets a request,
  regardless of whether the fork is clean or dirty?K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

From the algorithm, a hungry philosopher \(w\) requests forks from its
neighbors. From the algorithm, \(w\) eventually gets forks from 
all its lower priority neighbors. A higher priority neighbor \(x\) of
\(w\) is thinking. So when \(x\) gets a request for a fork from \(w\)
either (1) \(x\) sends the requested fork to \(w\) or (2) \(x\) transits from
thinking to hungry in which case the variant function \((nT, nH)\)
decreases.Summary: Key Ideas of this ModuleThis module introduced the problem of distributed mutual exclusion;
showed how the good neighbor policy --- a winning agent reduces its
priority to be lower than all the agents that it competes with --
solves this conflict resolution problem; introduced tokens and what
agents know about other agents holding tokens; and showed a proof
pattern that is one of the most common patterns for proving progress.ReviewWhat is the meaning of mutual exclusion among neighboring
  philosophers?An invariant of the algorithm is that each token is in exactly one
  place: an agent or a channel. How does this invariant help in
  designing the algorithm?An invariant of the algorithm is that the relative priorities among
  agents forms a partial order --- the priority graph is acyclic. What
  can go wrong if the priority graph has cycles?An agent that has a request for a fork releases the fork if the fork
  is dirty, and holds on to the fork if the fork is clean. What could
  go wrong if an agent releases a fork when it gets a request,
  regardless of whether the fork is clean or dirty?K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology
--- END Content from https://kmchandy.github.io/DistributedCollaboration/DiningPhilosophers/DiningPhilosophers.html ---


--- START Content from https://kmchandy.github.io/DistributedCollaboration/DrinkingPhilosophers/DrinkingPhilosophers.html ---
TITLE: DistributedCollaboration/DrinkingPhilosophers/DrinkingPhilosophers.html

The description of the algorithm in this module is not as detailed
    as that given for dining philosophers. You can define a formal
    specification and carry out a detailed proof for this algorithm in
    almost exactly the same was as for dining philosophers.The ProblemA set of agents shares a set of indivisible resources. Exclusive
    access to a file is an example of an indivisible resource.The
    lifecycle of an agent is the same as in Dining Philosophers: (1)
    executing outside the critical section, (2) waiting to enter a
    critical section, and (3) executing in the critical section. The
    problem is identical to Dining Philosophers except for the
    transition to the critical section.An  agent executes outside the critical section without
    holding any resources. An agent may execute outside critical
    sections for ever, or it may start waiting to enter a critical section.
    When an agent starts waiting
    to enter a critical section it waits to get exclusive access to a
    nonempty subset of the resources. It continues waiting until it
    gets all the resources for which it waits. While it waits it does
    not change the subset of resources for which it waits.
    The agent continues to hold these resources when it executes in
    the critical section. An agent remains in a critical section for
    only a finite number of steps and then starts executing outside
    the critical section at which point it no longer needs resources.For example, an agent needs exclusive access to a set of files for
    it to execute in its next critical section, and it waits until it
    is given this access. While it is in the critical section it
    continues to have exclusive access to these files. When it is
    executing outside the critical section it does not need access to
    these files. Each time an agent starts to wait it may wait for a
different set of files.Fig.1: Agent LifecycleThe ideas in this module can be used to manage agents that require
    read or write access to files. Multiple agents can have read
    access to a file concurrently. While an agent has write access to
    a file no other agent can have access to the file. A homework
    problem deals with read/write access.Drinking PhilosophersThe drinking philosophers name is another example of an attempt at
CS humor. A philosopher is in one of three states:tranquil,thirstyordrinking. A philosopher may remain tranquil
for ever, or it may become thirsty for one or more beverages. It
remains thirsty until it gets all the beverages for which it
waits. Only when a thirsty philosopher gets all the beverages for
which it waits does it start drinking. It continues to drinking all these
beverages until it becomes tranquil again. Philosophers drink for only
finite time.A philosopher that enters the tranquil state remains in that state for
at least a constant \(\gamma\) amount of time. So, a philosopher can't
go from drinking to thirsty instantaneously or in an arbitrarily small
amount of time.Fig.1: Drinking PhilosophersA beverage can be held by at most one philosopher. Imagine there's
only one bottle of each beverage in the system, and philosophers send
bottles to each other. One philosopher can drink vodka and cola
while another philosopher drinks gin and tonic. However, one agent cannot drink
vodka and cola while another drinks vodka and orange juice.An AlgorithmThere are many algorithms for this problem; here we discuss one.
Each beverage is an indivisible unique token. A token is
exchanged between agents and amanagerof the token. We assume
that each token has its own manager --- this assumption is merely for
convenience of exposition.Messagesrequest: An agent sends a request for a beverage to a
  manager. A request is a pair \(agent\_id, request\_priority\), the
  id of the requestor and the priority of the request.beverage: A manager sends a beverage to a requesting agent, and agents
  send beverages back to managers. A beverage is uniquely identified
  by its name.demand: A manager sends a demand to an agent for the
  beverage that the manager manages and that the agent holds.Agent ActionsWhen an agent becomes thirsty it sends requests to managers for
    all the beverages that it needs to drink.If a thirsty agent gets a demand to return a beverage to a manager
    then the agent returns the beverage and sends another request for
  the beverage. The priority of this new request is the same as the
  priority of this agent's last request.If a thirsty agent gets all the beverages that it needs to drink
    then it starts drinking.When an agent finishes drinking it returns all the beverages that
    it holds to the managers of the beverages.Manager ActionsA manager has local variable \((hr, hp)\),  which is the id of the agent to which the
  manager has most recently sent 
  the beverage, and the priority of the request made by that
  agent. \(hr\) is an acronym forhandlingrequestor,
  and \(hp\) is an acronym forhandlingpriority.
  If the manager holds the beverage then this variable is empty (\(Null\)).A manager also maintains a priority queue of pending requests ordered by
  priority.The actions of a manager are as follows.If a manager gets a request \((r, p)\) while it holds a beverage then it
    sends the beverage to the requestor \(r\), and sets
    \(hr, hp  = r, p\).If a manager gets a request \((r, p)\) while it does not hold the
    beverage and \(hp > p\) then the manager inserts the request \((r,
    p)\) into the priority queue of pending requests.If a manager gets a request \((r, p)\) while it does not hold the
    beverage and \(hp < p\) then:the manager sends a demand to \(hr\) to return the
      beverage, if the manager has not already sent that demand, andthe manager inserts the request \((r, p)\) into its priority queue of
      pending requests.If a manager gets a beverage and it has no pending requests then
    the manager holds the beverage and sets the handling requestor and
    priority to empty.If a manager gets a beverage and it has pending requests then
    let \((r, p)\) be the request at the head of the priority queue
    (i.e. the request with the highest priority). It sends the
    beverage to requestor \(r\),
    removes \((r, p)\) from the queue of pending requests,
    and sets
     \(hr, hp  = r, p\).ExampleThe example shows a scenario. The system has two agents, Maya and
  Liu both of whom are tranquil in the initial state. There are three beverages: Tea,
  coffee and milk. Initially, these beverages are with their managers.The next diagram, stage 1, shows the state after Maya gets thirsty for tea and milk.
  So she sends requests to the managers of tea and milk. The priority
  of this request is 2. (We will discuss how priorities are obtained
  later.)Fig.3: Stage 1The next diagram, stage 2, shows the situation after the managers of tea and
  milk get requests from Maya, and respond by sending the beverages to
  Maya because there are no pending requests for these beverages. The
  beverages, tea and milk, are in the channel to Maya.Fig.4: Stage 2The stage 3 diagram shows a state after Liu becomes thirsty for
  coffee and milk. So she sends requests for coffee and milk to the
  managers of these beverages. The priority of the request is 5.Maya has received the milk beverage but tea is still in the
  channel. So, Maya remains thirsty.Fig.5: Stage 3The stage 4 diagram shows a state in which the coffee manager
  receives Liu's request and sends
  coffee to Liu because there are no pending requests for coffee. When
  the milk manager gets Liu's request, the manager puts the request on
  the priority queue of pending requests. At this point the queue has
  only Liu's request.
  The
  milk manager demands milk back from Maya because her request has
  priority 2 whereas Liu's request has priority 5. Maya is still
  thirsty because the tea hasn't arrived yet.Fig.6: Stage 4Next, Maya receives the demand for milk and responds by sending the
  milk back to the milk manager. Maya also resends its original
request for milk, with priority 2, to the milk manager.
Then Maya receives tea. Maya
  continues to be thirsty because she has only one of the two
  beverages that she needs to drink. Liu has coffee, but she remains
  thirsty because milk is in the channel from Maya to the milk manager.Fig.7: Stage 5In stage 6, the milk manager has received Maya's request for milk with
  priority 2 and received milk. So, the milk manager sends the milk to
  respond to the highest priority request in the priority queue; this
  request is from Liu. The milk manager puts Maya's request into the
  queue of pending requests.Fig.8: Stage 6In stage 7, Liu has received both milk and coffee, and so she is
drinking. Maya is still thirsty, holding tea, while Maya's request for
milk is in the queue of pending requests for milk.Fig.9: Stage 7How Priorities ChangeIf priorities don't change then the agents with high priorities
may continue to go rapidly through the tranquil, thirsty, drinking
cycle while other agents remain thirsty for ever.One way to assign priorities is as follows.
Associated with each request is a timestamp which is 
the time read from the requestor's local clock at the instant at which
the request is made. A request's timestamp does not change after the
request is created.A request's priority is a pair (timestamp,
requestor's id), with priorities compared lexicographically, and lower
values having higher priority. So, requests made earlier have higher
priority than requests made later. The requestor's id is used to break
ties.What are the requirement's of agents' local
clocks that ensure that all thirsty philosophers drink eventually?A Minimum Requirement on ClocksAn agent's clock must tick forwardbetween successive requests
by the agent. If the agent's clock remained stuck at the same
value for ever then the agent using that clock may get the highest
priority for ever, and go through tranquil, thirsty, drinking cycles
infinitely which makes other agents remain tranquil for ever.Assume that each clock reading is an integer: for example, the number of
picoseconds since January 1, 1900. (NTP units are \(2^{-32}\) of a
second.) The clock ticks forward
between successive requests by the same philosopher because the
philosopher remains in tranquil state for at least \(\gamma\) units of
time where \(\gamma\) is a positive constant. Treat each agent's clock
tick as an event.Local clocks of agents may drift apart, but let's assume that the
magnitude of the difference between clock readings of different agents
is bounded.Even if the
philosopher makes a new request when its clock ticks forward by one
unit, eventually the timestamp of a request from that philosopher will
exceed \(T\) for any value of \(T\). This ensures that while a
philosopher is thirsty, another philosopher cannot overtake it for
ever.Proof of CorrectnessThe proof of safety --- multiple philosophers don't hold the same
beverage at the same time --- is straightforward. It follows
because a token is at exactly one agent or in exactly one channel.Let's prove that a thirsty philosopher \(v\) with a request with timestamp
\(T\) drinks eventually. Let \(t[i]\) be the reading of philosopher
\(i\)'s clock.Each agent's clock reading eventually exceeds \(T\)We observe that each agent's clock ticks forward:\(
\forall i, \tau: (t[i] = \tau) \leadsto (t[i] \geq \tau + 1)
\)Therefore, each \(i\)'s clock eventually reads a
value greater than \(T\), using transitivity of \(\leadsto\).\(
\forall i, \tau: (t[i] = \tau) \leadsto (t[i] > T)
\)From the previous formula, and taking disjunction over all values of
\(\tau\):\(
\forall i:  \quad true \leadsto (t[i] > T)
\)Because clock readings never go back in time, \(t[i] > T\) is stable,
and therefore:\(
\forall i:  \quad true \leadsto always(t[i] > T)
\)Let \(Q\) be the predicate that all clocks read values that
exceed \(T\):\(Q = \forall i: t[i] > T\).From the above formula and because \(X \leadsto always(Y)\) and \(X
\leadsto always(Y')\) allows us to deduce \(X \leadsto always(Y \vee Y')\)\(
 true \leadsto always(Q)
\)The number of pending requests with timestamps
less than \(T\) decreasesFrom the above formula:\(
v.thirsty \; \leadsto \; (v.drinking \wedge Q) \vee (v.thirsty \wedge Q)
\)Next we will prove that if \(v\) is thirsty and all clocks read values
that exceed \(T\), then \(v\) will drink eventually:\(
 (v.thirsty \wedge Q) \leadsto v.drinking
\)Let \(req\) be the number of pending requests with timestamps less than
\(T\). We will prove\(
(v.thirsty \wedge Q \wedge (req = k) \; \leadsto \;
v.drinking \vee (v.thirsty \wedge Q \wedge (req < k))
\)This proof is straightforward: The request with the lowest timestamp
gets all the resources it needs.\(
(v.thirsty \wedge Q \wedge (req = 0) \; \leadsto \;
v.drinking 
\)The result follows using the rules
for variant functions.ReviewIn the algorithm, each manager of a resource maintains apriorityqueue of requests for the resource. Is the algorithm
  correct if managers maintain first-come-first-served queues rather
  than priority queues?Show that the proposed variant function is correct: show that it
  does not increase while an agent remains thirsty, and show that it
  decreases eventually.K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology



Figure Link: https://kmchandy.github.io/DistributedCollaboration/DrinkingPhilosophers/Slide1.jpg

Caption: Fig.1: Agent Lifecycle


The
    lifecycle of an agent is the same as in Dining Philosophers: (1)
    executing outside the critical section, (2) waiting to enter a
    critical section, and (3) executing in the critical section. The
    problem is identical to Dining Philosophers except for the
    transition to the critical section.An  agent executes outside the critical section without
    holding any resources. An agent may execute outside critical
    sections for ever, or it may start waiting to enter a critical section.
    When an agent starts waiting
    to enter a critical section it waits to get exclusive access to a
    nonempty subset of the resources. It continues waiting until it
    gets all the resources for which it waits. While it waits it does
    not change the subset of resources for which it waits.
    The agent continues to hold these resources when it executes in
    the critical section. An agent remains in a critical section for
    only a finite number of steps and then starts executing outside
    the critical section at which point it no longer needs resources.For example, an agent needs exclusive access to a set of files for
    it to execute in its next critical section, and it waits until it
    is given this access. While it is in the critical section it
    continues to have exclusive access to these files. When it is
    executing outside the critical section it does not need access to
    these files. Each time an agent starts to wait it may wait for a
different set of files.Fig.1: Agent LifecycleThe ideas in this module can be used to manage agents that require
    read or write access to files. Multiple agents can have read
    access to a file concurrently. While an agent has write access to
    a file no other agent can have access to the file. A homework
    problem deals with read/write access.Drinking PhilosophersThe drinking philosophers name is another example of an attempt at
CS humor. A philosopher is in one of three states:tranquil,thirstyordrinking. A philosopher may remain tranquil
for ever, or it may become thirsty for one or more beverages. It
remains thirsty until it gets all the beverages for which it
waits. Only when a thirsty philosopher gets all the beverages for
which it waits does it start drinking. It continues to drinking all these
beverages until it becomes tranquil again. Philosophers drink for only
finite time.A philosopher that enters the tranquil state remains in that state for
at least a constant \(\gamma\) amount of time. So, a philosopher can't
go from drinking to thirsty instantaneously or in an arbitrarily small
amount of time.Fig.1: Drinking PhilosophersA beverage can be held by at most one philosopher. Imagine there's
only one bottle of each beverage in the system, and philosophers send
bottles to each other. One philosopher can drink vodka and cola
while another philosopher drinks gin and tonic. However, one agent cannot drink
vodka and cola while another drinks vodka and orange juice.An AlgorithmThere are many algorithms for this problem; here we discuss one.
Each beverage is an indivisible unique token. A token is
exchanged between agents and amanagerof the token. We assume
that each token has its own manager --- this assumption is merely for
convenience of exposition.Messagesrequest: An agent sends a request for a beverage to a
  manager. A request is a pair \(agent\_id, request\_priority\), the
  id of the requestor and the priority of the request.beverage: A manager sends a beverage to a requesting agent, and agents
  send beverages back to managers. A beverage is uniquely identified
  by its name.demand: A manager sends a demand to an agent for the
  beverage that the manager manages and that the agent holds.Agent ActionsWhen an agent becomes thirsty it sends requests to managers for
    all the beverages that it needs to drink.If a thirsty agent gets a demand to return a beverage to a manager
    then the agent returns the beverage and sends another request for
  the beverage. The priority of this new request is the same as the
  priority of this agent's last request.If a thirsty agent gets all the beverages that it needs to drink
    then it starts drinking.When an agent finishes drinking it returns all the beverages that
    it holds to the managers of the beverages.Manager ActionsA manager has local variable \((hr, hp)\),  which is the id of the agent to which the
  manager has most recently sent 
  the beverage, and the priority of the request made by that
  agent. \(hr\) is an acronym forhandlingrequestor,
  and \(hp\) is an acronym forhandlingpriority.
  If the manager holds the beverage then this variable is empty (\(Null\)).A manager also maintains a priority queue of pending requests ordered by
  priority.The actions of a manager are as follows.If a manager gets a request \((r, p)\) while it holds a beverage then it
    sends the beverage to the requestor \(r\), and sets
    \(hr, hp  = r, p\).If a manager gets a request \((r, p)\) while it does not hold the
    beverage and \(hp > p\) then the manager inserts the request \((r,
    p)\) into the priority queue of pending requests.If a manager gets a request \((r, p)\) while it does not hold the
    beverage and \(hp < p\) then:the manager sends a demand to \(hr\) to return the
      beverage, if the manager has not already sent that demand, andthe manager inserts the request \((r, p)\) into its priority queue of
      pending requests.If a manager gets a beverage and it has no pending requests then
    the manager holds the beverage and sets the handling requestor and
    priority to empty.If a manager gets a beverage and it has pending requests then
    let \((r, p)\) be the request at the head of the priority queue
    (i.e. the request with the highest priority). It sends the
    beverage to requestor \(r\),
    removes \((r, p)\) from the queue of pending requests,
    and sets
     \(hr, hp  = r, p\).ExampleThe example shows a scenario. The system has two agents, Maya and
  Liu both of whom are tranquil in the initial state. There are three beverages: Tea,
  coffee and milk. Initially, these beverages are with their managers.The next diagram, stage 1, shows the state after Maya gets thirsty for tea and milk.
  So she sends requests to the managers of tea and milk. The priority
  of this request is 2. (We will discuss how priorities are obtained
  later.)Fig.3: Stage 1The next diagram, stage 2, shows the situation after the managers of tea and
  milk get requests from Maya, and respond by sending the beverages to
  Maya because there are no pending requests for these beverages. The
  beverages, tea and milk, are in the channel to Maya.Fig.4: Stage 2The stage 3 diagram shows a state after Liu becomes thirsty for
  coffee and milk. So she sends requests for coffee and milk to the
  managers of these beverages. The priority of the request is 5.Maya has received the milk beverage but tea is still in the
  channel. So, Maya remains thirsty.Fig.5: Stage 3The stage 4 diagram shows a state in which the coffee manager
  receives Liu's request and sends
  coffee to Liu because there are no pending requests for coffee. When
  the milk manager gets Liu's request, the manager puts the request on
  the priority queue of pending requests. At this point the queue has
  only Liu's request.
  The
  milk manager demands milk back from Maya because her request has
  priority 2 whereas Liu's request has priority 5. Maya is still
  thirsty because the tea hasn't arrived yet.Fig.6: Stage 4Next, Maya receives the demand for milk and responds by sending the
  milk back to the milk manager. Maya also resends its original
request for milk, with priority 2, to the milk manager.
Then Maya receives tea. Maya
  continues to be thirsty because she has only one of the two
  beverages that she needs to drink. Liu has coffee, but she remains
  thirsty because milk is in the channel from Maya to the milk manager.Fig.7: Stage 5In stage 6, the milk manager has received Maya's request for milk with
  priority 2 and received milk. So, the milk manager sends the milk to
  respond to the highest priority request in the priority queue; this
  request is from Liu. The milk manager puts Maya's request into the
  queue of pending requests.Fig.8: Stage 6In stage 7, Liu has received both milk and coffee, and so she is
drinking. Maya is still thirsty, holding tea, while Maya's request for
milk is in the queue of pending requests for milk.Fig.9: Stage 7How Priorities ChangeIf priorities don't change then the agents with high priorities
may continue to go rapidly through the tranquil, thirsty, drinking
cycle while other agents remain thirsty for ever.One way to assign priorities is as follows.
Associated with each request is a timestamp which is 
the time read from the requestor's local clock at the instant at which
the request is made. A request's timestamp does not change after the
request is created.A request's priority is a pair (timestamp,
requestor's id), with priorities compared lexicographically, and lower
values having higher priority. So, requests made earlier have higher
priority than requests made later. The requestor's id is used to break
ties.What are the requirement's of agents' local
clocks that ensure that all thirsty philosophers drink eventually?A Minimum Requirement on ClocksAn agent's clock must tick forwardbetween successive requests
by the agent. If the agent's clock remained stuck at the same
value for ever then the agent using that clock may get the highest
priority for ever, and go through tranquil, thirsty, drinking cycles
infinitely which makes other agents remain tranquil for ever.Assume that each clock reading is an integer: for example, the number of
picoseconds since January 1, 1900. (NTP units are \(2^{-32}\) of a
second.) The clock ticks forward
between successive requests by the same philosopher because the
philosopher remains in tranquil state for at least \(\gamma\) units of
time where \(\gamma\) is a positive constant. Treat each agent's clock
tick as an event.Local clocks of agents may drift apart, but let's assume that the
magnitude of the difference between clock readings of different agents
is bounded.Even if the
philosopher makes a new request when its clock ticks forward by one
unit, eventually the timestamp of a request from that philosopher will
exceed \(T\) for any value of \(T\). This ensures that while a
philosopher is thirsty, another philosopher cannot overtake it for
ever.Proof of CorrectnessThe proof of safety --- multiple philosophers don't hold the same
beverage at the same time --- is straightforward. It follows
because a token is at exactly one agent or in exactly one channel.Let's prove that a thirsty philosopher \(v\) with a request with timestamp
\(T\) drinks eventually. Let \(t[i]\) be the reading of philosopher
\(i\)'s clock.Each agent's clock reading eventually exceeds \(T\)We observe that each agent's clock ticks forward:\(
\forall i, \tau: (t[i] = \tau) \leadsto (t[i] \geq \tau + 1)
\)Therefore, each \(i\)'s clock eventually reads a
value greater than \(T\), using transitivity of \(\leadsto\).\(
\forall i, \tau: (t[i] = \tau) \leadsto (t[i] > T)
\)From the previous formula, and taking disjunction over all values of
\(\tau\):\(
\forall i:  \quad true \leadsto (t[i] > T)
\)Because clock readings never go back in time, \(t[i] > T\) is stable,
and therefore:\(
\forall i:  \quad true \leadsto always(t[i] > T)
\)Let \(Q\) be the predicate that all clocks read values that
exceed \(T\):\(Q = \forall i: t[i] > T\).From the above formula and because \(X \leadsto always(Y)\) and \(X
\leadsto always(Y')\) allows us to deduce \(X \leadsto always(Y \vee Y')\)\(
 true \leadsto always(Q)
\)The number of pending requests with timestamps
less than \(T\) decreasesFrom the above formula:\(
v.thirsty \; \leadsto \; (v.drinking \wedge Q) \vee (v.thirsty \wedge Q)
\)Next we will prove that if \(v\) is thirsty and all clocks read values
that exceed \(T\), then \(v\) will drink eventually:\(
 (v.thirsty \wedge Q) \leadsto v.drinking
\)Let \(req\) be the number of pending requests with timestamps less than
\(T\). We will prove\(
(v.thirsty \wedge Q \wedge (req = k) \; \leadsto \;
v.drinking \vee (v.thirsty \wedge Q \wedge (req < k))
\)This proof is straightforward: The request with the lowest timestamp
gets all the resources it needs.\(
(v.thirsty \wedge Q \wedge (req = 0) \; \leadsto \;
v.drinking 
\)The result follows using the rules
for variant functions.ReviewIn the algorithm, each manager of a resource maintains apriorityqueue of requests for the resource. Is the algorithm
  correct if managers maintain first-come-first-served queues rather
  than priority queues?Show that the proposed variant function is correct: show that it
  does not increase while an agent remains thirsty, and show that it
  decreases eventually.K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology



Figure Link: https://kmchandy.github.io/DistributedCollaboration/DrinkingPhilosophers/Slide1.jpg

Caption: Fig.1: Agent Lifecycle


An  agent executes outside the critical section without
    holding any resources. An agent may execute outside critical
    sections for ever, or it may start waiting to enter a critical section.
    When an agent starts waiting
    to enter a critical section it waits to get exclusive access to a
    nonempty subset of the resources. It continues waiting until it
    gets all the resources for which it waits. While it waits it does
    not change the subset of resources for which it waits.
    The agent continues to hold these resources when it executes in
    the critical section. An agent remains in a critical section for
    only a finite number of steps and then starts executing outside
    the critical section at which point it no longer needs resources.For example, an agent needs exclusive access to a set of files for
    it to execute in its next critical section, and it waits until it
    is given this access. While it is in the critical section it
    continues to have exclusive access to these files. When it is
    executing outside the critical section it does not need access to
    these files. Each time an agent starts to wait it may wait for a
different set of files.Fig.1: Agent LifecycleThe ideas in this module can be used to manage agents that require
    read or write access to files. Multiple agents can have read
    access to a file concurrently. While an agent has write access to
    a file no other agent can have access to the file. A homework
    problem deals with read/write access.Drinking PhilosophersThe drinking philosophers name is another example of an attempt at
CS humor. A philosopher is in one of three states:tranquil,thirstyordrinking. A philosopher may remain tranquil
for ever, or it may become thirsty for one or more beverages. It
remains thirsty until it gets all the beverages for which it
waits. Only when a thirsty philosopher gets all the beverages for
which it waits does it start drinking. It continues to drinking all these
beverages until it becomes tranquil again. Philosophers drink for only
finite time.A philosopher that enters the tranquil state remains in that state for
at least a constant \(\gamma\) amount of time. So, a philosopher can't
go from drinking to thirsty instantaneously or in an arbitrarily small
amount of time.Fig.1: Drinking PhilosophersA beverage can be held by at most one philosopher. Imagine there's
only one bottle of each beverage in the system, and philosophers send
bottles to each other. One philosopher can drink vodka and cola
while another philosopher drinks gin and tonic. However, one agent cannot drink
vodka and cola while another drinks vodka and orange juice.An AlgorithmThere are many algorithms for this problem; here we discuss one.
Each beverage is an indivisible unique token. A token is
exchanged between agents and amanagerof the token. We assume
that each token has its own manager --- this assumption is merely for
convenience of exposition.Messagesrequest: An agent sends a request for a beverage to a
  manager. A request is a pair \(agent\_id, request\_priority\), the
  id of the requestor and the priority of the request.beverage: A manager sends a beverage to a requesting agent, and agents
  send beverages back to managers. A beverage is uniquely identified
  by its name.demand: A manager sends a demand to an agent for the
  beverage that the manager manages and that the agent holds.Agent ActionsWhen an agent becomes thirsty it sends requests to managers for
    all the beverages that it needs to drink.If a thirsty agent gets a demand to return a beverage to a manager
    then the agent returns the beverage and sends another request for
  the beverage. The priority of this new request is the same as the
  priority of this agent's last request.If a thirsty agent gets all the beverages that it needs to drink
    then it starts drinking.When an agent finishes drinking it returns all the beverages that
    it holds to the managers of the beverages.Manager ActionsA manager has local variable \((hr, hp)\),  which is the id of the agent to which the
  manager has most recently sent 
  the beverage, and the priority of the request made by that
  agent. \(hr\) is an acronym forhandlingrequestor,
  and \(hp\) is an acronym forhandlingpriority.
  If the manager holds the beverage then this variable is empty (\(Null\)).A manager also maintains a priority queue of pending requests ordered by
  priority.The actions of a manager are as follows.If a manager gets a request \((r, p)\) while it holds a beverage then it
    sends the beverage to the requestor \(r\), and sets
    \(hr, hp  = r, p\).If a manager gets a request \((r, p)\) while it does not hold the
    beverage and \(hp > p\) then the manager inserts the request \((r,
    p)\) into the priority queue of pending requests.If a manager gets a request \((r, p)\) while it does not hold the
    beverage and \(hp < p\) then:the manager sends a demand to \(hr\) to return the
      beverage, if the manager has not already sent that demand, andthe manager inserts the request \((r, p)\) into its priority queue of
      pending requests.If a manager gets a beverage and it has no pending requests then
    the manager holds the beverage and sets the handling requestor and
    priority to empty.If a manager gets a beverage and it has pending requests then
    let \((r, p)\) be the request at the head of the priority queue
    (i.e. the request with the highest priority). It sends the
    beverage to requestor \(r\),
    removes \((r, p)\) from the queue of pending requests,
    and sets
     \(hr, hp  = r, p\).ExampleThe example shows a scenario. The system has two agents, Maya and
  Liu both of whom are tranquil in the initial state. There are three beverages: Tea,
  coffee and milk. Initially, these beverages are with their managers.The next diagram, stage 1, shows the state after Maya gets thirsty for tea and milk.
  So she sends requests to the managers of tea and milk. The priority
  of this request is 2. (We will discuss how priorities are obtained
  later.)Fig.3: Stage 1The next diagram, stage 2, shows the situation after the managers of tea and
  milk get requests from Maya, and respond by sending the beverages to
  Maya because there are no pending requests for these beverages. The
  beverages, tea and milk, are in the channel to Maya.Fig.4: Stage 2The stage 3 diagram shows a state after Liu becomes thirsty for
  coffee and milk. So she sends requests for coffee and milk to the
  managers of these beverages. The priority of the request is 5.Maya has received the milk beverage but tea is still in the
  channel. So, Maya remains thirsty.Fig.5: Stage 3The stage 4 diagram shows a state in which the coffee manager
  receives Liu's request and sends
  coffee to Liu because there are no pending requests for coffee. When
  the milk manager gets Liu's request, the manager puts the request on
  the priority queue of pending requests. At this point the queue has
  only Liu's request.
  The
  milk manager demands milk back from Maya because her request has
  priority 2 whereas Liu's request has priority 5. Maya is still
  thirsty because the tea hasn't arrived yet.Fig.6: Stage 4Next, Maya receives the demand for milk and responds by sending the
  milk back to the milk manager. Maya also resends its original
request for milk, with priority 2, to the milk manager.
Then Maya receives tea. Maya
  continues to be thirsty because she has only one of the two
  beverages that she needs to drink. Liu has coffee, but she remains
  thirsty because milk is in the channel from Maya to the milk manager.Fig.7: Stage 5In stage 6, the milk manager has received Maya's request for milk with
  priority 2 and received milk. So, the milk manager sends the milk to
  respond to the highest priority request in the priority queue; this
  request is from Liu. The milk manager puts Maya's request into the
  queue of pending requests.Fig.8: Stage 6In stage 7, Liu has received both milk and coffee, and so she is
drinking. Maya is still thirsty, holding tea, while Maya's request for
milk is in the queue of pending requests for milk.Fig.9: Stage 7How Priorities ChangeIf priorities don't change then the agents with high priorities
may continue to go rapidly through the tranquil, thirsty, drinking
cycle while other agents remain thirsty for ever.One way to assign priorities is as follows.
Associated with each request is a timestamp which is 
the time read from the requestor's local clock at the instant at which
the request is made. A request's timestamp does not change after the
request is created.A request's priority is a pair (timestamp,
requestor's id), with priorities compared lexicographically, and lower
values having higher priority. So, requests made earlier have higher
priority than requests made later. The requestor's id is used to break
ties.What are the requirement's of agents' local
clocks that ensure that all thirsty philosophers drink eventually?A Minimum Requirement on ClocksAn agent's clock must tick forwardbetween successive requests
by the agent. If the agent's clock remained stuck at the same
value for ever then the agent using that clock may get the highest
priority for ever, and go through tranquil, thirsty, drinking cycles
infinitely which makes other agents remain tranquil for ever.Assume that each clock reading is an integer: for example, the number of
picoseconds since January 1, 1900. (NTP units are \(2^{-32}\) of a
second.) The clock ticks forward
between successive requests by the same philosopher because the
philosopher remains in tranquil state for at least \(\gamma\) units of
time where \(\gamma\) is a positive constant. Treat each agent's clock
tick as an event.Local clocks of agents may drift apart, but let's assume that the
magnitude of the difference between clock readings of different agents
is bounded.Even if the
philosopher makes a new request when its clock ticks forward by one
unit, eventually the timestamp of a request from that philosopher will
exceed \(T\) for any value of \(T\). This ensures that while a
philosopher is thirsty, another philosopher cannot overtake it for
ever.Proof of CorrectnessThe proof of safety --- multiple philosophers don't hold the same
beverage at the same time --- is straightforward. It follows
because a token is at exactly one agent or in exactly one channel.Let's prove that a thirsty philosopher \(v\) with a request with timestamp
\(T\) drinks eventually. Let \(t[i]\) be the reading of philosopher
\(i\)'s clock.Each agent's clock reading eventually exceeds \(T\)We observe that each agent's clock ticks forward:\(
\forall i, \tau: (t[i] = \tau) \leadsto (t[i] \geq \tau + 1)
\)Therefore, each \(i\)'s clock eventually reads a
value greater than \(T\), using transitivity of \(\leadsto\).\(
\forall i, \tau: (t[i] = \tau) \leadsto (t[i] > T)
\)From the previous formula, and taking disjunction over all values of
\(\tau\):\(
\forall i:  \quad true \leadsto (t[i] > T)
\)Because clock readings never go back in time, \(t[i] > T\) is stable,
and therefore:\(
\forall i:  \quad true \leadsto always(t[i] > T)
\)Let \(Q\) be the predicate that all clocks read values that
exceed \(T\):\(Q = \forall i: t[i] > T\).From the above formula and because \(X \leadsto always(Y)\) and \(X
\leadsto always(Y')\) allows us to deduce \(X \leadsto always(Y \vee Y')\)\(
 true \leadsto always(Q)
\)The number of pending requests with timestamps
less than \(T\) decreasesFrom the above formula:\(
v.thirsty \; \leadsto \; (v.drinking \wedge Q) \vee (v.thirsty \wedge Q)
\)Next we will prove that if \(v\) is thirsty and all clocks read values
that exceed \(T\), then \(v\) will drink eventually:\(
 (v.thirsty \wedge Q) \leadsto v.drinking
\)Let \(req\) be the number of pending requests with timestamps less than
\(T\). We will prove\(
(v.thirsty \wedge Q \wedge (req = k) \; \leadsto \;
v.drinking \vee (v.thirsty \wedge Q \wedge (req < k))
\)This proof is straightforward: The request with the lowest timestamp
gets all the resources it needs.\(
(v.thirsty \wedge Q \wedge (req = 0) \; \leadsto \;
v.drinking 
\)The result follows using the rules
for variant functions.ReviewIn the algorithm, each manager of a resource maintains apriorityqueue of requests for the resource. Is the algorithm
  correct if managers maintain first-come-first-served queues rather
  than priority queues?Show that the proposed variant function is correct: show that it
  does not increase while an agent remains thirsty, and show that it
  decreases eventually.K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology



Figure Link: https://kmchandy.github.io/DistributedCollaboration/DrinkingPhilosophers/Slide1.jpg

Caption: Fig.1: Agent Lifecycle


For example, an agent needs exclusive access to a set of files for
    it to execute in its next critical section, and it waits until it
    is given this access. While it is in the critical section it
    continues to have exclusive access to these files. When it is
    executing outside the critical section it does not need access to
    these files. Each time an agent starts to wait it may wait for a
different set of files.Fig.1: Agent LifecycleThe ideas in this module can be used to manage agents that require
    read or write access to files. Multiple agents can have read
    access to a file concurrently. While an agent has write access to
    a file no other agent can have access to the file. A homework
    problem deals with read/write access.Drinking PhilosophersThe drinking philosophers name is another example of an attempt at
CS humor. A philosopher is in one of three states:tranquil,thirstyordrinking. A philosopher may remain tranquil
for ever, or it may become thirsty for one or more beverages. It
remains thirsty until it gets all the beverages for which it
waits. Only when a thirsty philosopher gets all the beverages for
which it waits does it start drinking. It continues to drinking all these
beverages until it becomes tranquil again. Philosophers drink for only
finite time.A philosopher that enters the tranquil state remains in that state for
at least a constant \(\gamma\) amount of time. So, a philosopher can't
go from drinking to thirsty instantaneously or in an arbitrarily small
amount of time.Fig.1: Drinking PhilosophersA beverage can be held by at most one philosopher. Imagine there's
only one bottle of each beverage in the system, and philosophers send
bottles to each other. One philosopher can drink vodka and cola
while another philosopher drinks gin and tonic. However, one agent cannot drink
vodka and cola while another drinks vodka and orange juice.An AlgorithmThere are many algorithms for this problem; here we discuss one.
Each beverage is an indivisible unique token. A token is
exchanged between agents and amanagerof the token. We assume
that each token has its own manager --- this assumption is merely for
convenience of exposition.Messagesrequest: An agent sends a request for a beverage to a
  manager. A request is a pair \(agent\_id, request\_priority\), the
  id of the requestor and the priority of the request.beverage: A manager sends a beverage to a requesting agent, and agents
  send beverages back to managers. A beverage is uniquely identified
  by its name.demand: A manager sends a demand to an agent for the
  beverage that the manager manages and that the agent holds.Agent ActionsWhen an agent becomes thirsty it sends requests to managers for
    all the beverages that it needs to drink.If a thirsty agent gets a demand to return a beverage to a manager
    then the agent returns the beverage and sends another request for
  the beverage. The priority of this new request is the same as the
  priority of this agent's last request.If a thirsty agent gets all the beverages that it needs to drink
    then it starts drinking.When an agent finishes drinking it returns all the beverages that
    it holds to the managers of the beverages.Manager ActionsA manager has local variable \((hr, hp)\),  which is the id of the agent to which the
  manager has most recently sent 
  the beverage, and the priority of the request made by that
  agent. \(hr\) is an acronym forhandlingrequestor,
  and \(hp\) is an acronym forhandlingpriority.
  If the manager holds the beverage then this variable is empty (\(Null\)).A manager also maintains a priority queue of pending requests ordered by
  priority.The actions of a manager are as follows.If a manager gets a request \((r, p)\) while it holds a beverage then it
    sends the beverage to the requestor \(r\), and sets
    \(hr, hp  = r, p\).If a manager gets a request \((r, p)\) while it does not hold the
    beverage and \(hp > p\) then the manager inserts the request \((r,
    p)\) into the priority queue of pending requests.If a manager gets a request \((r, p)\) while it does not hold the
    beverage and \(hp < p\) then:the manager sends a demand to \(hr\) to return the
      beverage, if the manager has not already sent that demand, andthe manager inserts the request \((r, p)\) into its priority queue of
      pending requests.If a manager gets a beverage and it has no pending requests then
    the manager holds the beverage and sets the handling requestor and
    priority to empty.If a manager gets a beverage and it has pending requests then
    let \((r, p)\) be the request at the head of the priority queue
    (i.e. the request with the highest priority). It sends the
    beverage to requestor \(r\),
    removes \((r, p)\) from the queue of pending requests,
    and sets
     \(hr, hp  = r, p\).ExampleThe example shows a scenario. The system has two agents, Maya and
  Liu both of whom are tranquil in the initial state. There are three beverages: Tea,
  coffee and milk. Initially, these beverages are with their managers.The next diagram, stage 1, shows the state after Maya gets thirsty for tea and milk.
  So she sends requests to the managers of tea and milk. The priority
  of this request is 2. (We will discuss how priorities are obtained
  later.)Fig.3: Stage 1The next diagram, stage 2, shows the situation after the managers of tea and
  milk get requests from Maya, and respond by sending the beverages to
  Maya because there are no pending requests for these beverages. The
  beverages, tea and milk, are in the channel to Maya.Fig.4: Stage 2The stage 3 diagram shows a state after Liu becomes thirsty for
  coffee and milk. So she sends requests for coffee and milk to the
  managers of these beverages. The priority of the request is 5.Maya has received the milk beverage but tea is still in the
  channel. So, Maya remains thirsty.Fig.5: Stage 3The stage 4 diagram shows a state in which the coffee manager
  receives Liu's request and sends
  coffee to Liu because there are no pending requests for coffee. When
  the milk manager gets Liu's request, the manager puts the request on
  the priority queue of pending requests. At this point the queue has
  only Liu's request.
  The
  milk manager demands milk back from Maya because her request has
  priority 2 whereas Liu's request has priority 5. Maya is still
  thirsty because the tea hasn't arrived yet.Fig.6: Stage 4Next, Maya receives the demand for milk and responds by sending the
  milk back to the milk manager. Maya also resends its original
request for milk, with priority 2, to the milk manager.
Then Maya receives tea. Maya
  continues to be thirsty because she has only one of the two
  beverages that she needs to drink. Liu has coffee, but she remains
  thirsty because milk is in the channel from Maya to the milk manager.Fig.7: Stage 5In stage 6, the milk manager has received Maya's request for milk with
  priority 2 and received milk. So, the milk manager sends the milk to
  respond to the highest priority request in the priority queue; this
  request is from Liu. The milk manager puts Maya's request into the
  queue of pending requests.Fig.8: Stage 6In stage 7, Liu has received both milk and coffee, and so she is
drinking. Maya is still thirsty, holding tea, while Maya's request for
milk is in the queue of pending requests for milk.Fig.9: Stage 7How Priorities ChangeIf priorities don't change then the agents with high priorities
may continue to go rapidly through the tranquil, thirsty, drinking
cycle while other agents remain thirsty for ever.One way to assign priorities is as follows.
Associated with each request is a timestamp which is 
the time read from the requestor's local clock at the instant at which
the request is made. A request's timestamp does not change after the
request is created.A request's priority is a pair (timestamp,
requestor's id), with priorities compared lexicographically, and lower
values having higher priority. So, requests made earlier have higher
priority than requests made later. The requestor's id is used to break
ties.What are the requirement's of agents' local
clocks that ensure that all thirsty philosophers drink eventually?A Minimum Requirement on ClocksAn agent's clock must tick forwardbetween successive requests
by the agent. If the agent's clock remained stuck at the same
value for ever then the agent using that clock may get the highest
priority for ever, and go through tranquil, thirsty, drinking cycles
infinitely which makes other agents remain tranquil for ever.Assume that each clock reading is an integer: for example, the number of
picoseconds since January 1, 1900. (NTP units are \(2^{-32}\) of a
second.) The clock ticks forward
between successive requests by the same philosopher because the
philosopher remains in tranquil state for at least \(\gamma\) units of
time where \(\gamma\) is a positive constant. Treat each agent's clock
tick as an event.Local clocks of agents may drift apart, but let's assume that the
magnitude of the difference between clock readings of different agents
is bounded.Even if the
philosopher makes a new request when its clock ticks forward by one
unit, eventually the timestamp of a request from that philosopher will
exceed \(T\) for any value of \(T\). This ensures that while a
philosopher is thirsty, another philosopher cannot overtake it for
ever.Proof of CorrectnessThe proof of safety --- multiple philosophers don't hold the same
beverage at the same time --- is straightforward. It follows
because a token is at exactly one agent or in exactly one channel.Let's prove that a thirsty philosopher \(v\) with a request with timestamp
\(T\) drinks eventually. Let \(t[i]\) be the reading of philosopher
\(i\)'s clock.Each agent's clock reading eventually exceeds \(T\)We observe that each agent's clock ticks forward:\(
\forall i, \tau: (t[i] = \tau) \leadsto (t[i] \geq \tau + 1)
\)Therefore, each \(i\)'s clock eventually reads a
value greater than \(T\), using transitivity of \(\leadsto\).\(
\forall i, \tau: (t[i] = \tau) \leadsto (t[i] > T)
\)From the previous formula, and taking disjunction over all values of
\(\tau\):\(
\forall i:  \quad true \leadsto (t[i] > T)
\)Because clock readings never go back in time, \(t[i] > T\) is stable,
and therefore:\(
\forall i:  \quad true \leadsto always(t[i] > T)
\)Let \(Q\) be the predicate that all clocks read values that
exceed \(T\):\(Q = \forall i: t[i] > T\).From the above formula and because \(X \leadsto always(Y)\) and \(X
\leadsto always(Y')\) allows us to deduce \(X \leadsto always(Y \vee Y')\)\(
 true \leadsto always(Q)
\)The number of pending requests with timestamps
less than \(T\) decreasesFrom the above formula:\(
v.thirsty \; \leadsto \; (v.drinking \wedge Q) \vee (v.thirsty \wedge Q)
\)Next we will prove that if \(v\) is thirsty and all clocks read values
that exceed \(T\), then \(v\) will drink eventually:\(
 (v.thirsty \wedge Q) \leadsto v.drinking
\)Let \(req\) be the number of pending requests with timestamps less than
\(T\). We will prove\(
(v.thirsty \wedge Q \wedge (req = k) \; \leadsto \;
v.drinking \vee (v.thirsty \wedge Q \wedge (req < k))
\)This proof is straightforward: The request with the lowest timestamp
gets all the resources it needs.\(
(v.thirsty \wedge Q \wedge (req = 0) \; \leadsto \;
v.drinking 
\)The result follows using the rules
for variant functions.ReviewIn the algorithm, each manager of a resource maintains apriorityqueue of requests for the resource. Is the algorithm
  correct if managers maintain first-come-first-served queues rather
  than priority queues?Show that the proposed variant function is correct: show that it
  does not increase while an agent remains thirsty, and show that it
  decreases eventually.K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology



Figure Link: https://kmchandy.github.io/DistributedCollaboration/DrinkingPhilosophers/Slide1.jpg

Caption: Fig.1: Agent Lifecycle


The ideas in this module can be used to manage agents that require
    read or write access to files. Multiple agents can have read
    access to a file concurrently. While an agent has write access to
    a file no other agent can have access to the file. A homework
    problem deals with read/write access.Drinking PhilosophersThe drinking philosophers name is another example of an attempt at
CS humor. A philosopher is in one of three states:tranquil,thirstyordrinking. A philosopher may remain tranquil
for ever, or it may become thirsty for one or more beverages. It
remains thirsty until it gets all the beverages for which it
waits. Only when a thirsty philosopher gets all the beverages for
which it waits does it start drinking. It continues to drinking all these
beverages until it becomes tranquil again. Philosophers drink for only
finite time.A philosopher that enters the tranquil state remains in that state for
at least a constant \(\gamma\) amount of time. So, a philosopher can't
go from drinking to thirsty instantaneously or in an arbitrarily small
amount of time.Fig.1: Drinking PhilosophersA beverage can be held by at most one philosopher. Imagine there's
only one bottle of each beverage in the system, and philosophers send
bottles to each other. One philosopher can drink vodka and cola
while another philosopher drinks gin and tonic. However, one agent cannot drink
vodka and cola while another drinks vodka and orange juice.An AlgorithmThere are many algorithms for this problem; here we discuss one.
Each beverage is an indivisible unique token. A token is
exchanged between agents and amanagerof the token. We assume
that each token has its own manager --- this assumption is merely for
convenience of exposition.Messagesrequest: An agent sends a request for a beverage to a
  manager. A request is a pair \(agent\_id, request\_priority\), the
  id of the requestor and the priority of the request.beverage: A manager sends a beverage to a requesting agent, and agents
  send beverages back to managers. A beverage is uniquely identified
  by its name.demand: A manager sends a demand to an agent for the
  beverage that the manager manages and that the agent holds.Agent ActionsWhen an agent becomes thirsty it sends requests to managers for
    all the beverages that it needs to drink.If a thirsty agent gets a demand to return a beverage to a manager
    then the agent returns the beverage and sends another request for
  the beverage. The priority of this new request is the same as the
  priority of this agent's last request.If a thirsty agent gets all the beverages that it needs to drink
    then it starts drinking.When an agent finishes drinking it returns all the beverages that
    it holds to the managers of the beverages.Manager ActionsA manager has local variable \((hr, hp)\),  which is the id of the agent to which the
  manager has most recently sent 
  the beverage, and the priority of the request made by that
  agent. \(hr\) is an acronym forhandlingrequestor,
  and \(hp\) is an acronym forhandlingpriority.
  If the manager holds the beverage then this variable is empty (\(Null\)).A manager also maintains a priority queue of pending requests ordered by
  priority.The actions of a manager are as follows.If a manager gets a request \((r, p)\) while it holds a beverage then it
    sends the beverage to the requestor \(r\), and sets
    \(hr, hp  = r, p\).If a manager gets a request \((r, p)\) while it does not hold the
    beverage and \(hp > p\) then the manager inserts the request \((r,
    p)\) into the priority queue of pending requests.If a manager gets a request \((r, p)\) while it does not hold the
    beverage and \(hp < p\) then:the manager sends a demand to \(hr\) to return the
      beverage, if the manager has not already sent that demand, andthe manager inserts the request \((r, p)\) into its priority queue of
      pending requests.If a manager gets a beverage and it has no pending requests then
    the manager holds the beverage and sets the handling requestor and
    priority to empty.If a manager gets a beverage and it has pending requests then
    let \((r, p)\) be the request at the head of the priority queue
    (i.e. the request with the highest priority). It sends the
    beverage to requestor \(r\),
    removes \((r, p)\) from the queue of pending requests,
    and sets
     \(hr, hp  = r, p\).ExampleThe example shows a scenario. The system has two agents, Maya and
  Liu both of whom are tranquil in the initial state. There are three beverages: Tea,
  coffee and milk. Initially, these beverages are with their managers.The next diagram, stage 1, shows the state after Maya gets thirsty for tea and milk.
  So she sends requests to the managers of tea and milk. The priority
  of this request is 2. (We will discuss how priorities are obtained
  later.)Fig.3: Stage 1The next diagram, stage 2, shows the situation after the managers of tea and
  milk get requests from Maya, and respond by sending the beverages to
  Maya because there are no pending requests for these beverages. The
  beverages, tea and milk, are in the channel to Maya.Fig.4: Stage 2The stage 3 diagram shows a state after Liu becomes thirsty for
  coffee and milk. So she sends requests for coffee and milk to the
  managers of these beverages. The priority of the request is 5.Maya has received the milk beverage but tea is still in the
  channel. So, Maya remains thirsty.Fig.5: Stage 3The stage 4 diagram shows a state in which the coffee manager
  receives Liu's request and sends
  coffee to Liu because there are no pending requests for coffee. When
  the milk manager gets Liu's request, the manager puts the request on
  the priority queue of pending requests. At this point the queue has
  only Liu's request.
  The
  milk manager demands milk back from Maya because her request has
  priority 2 whereas Liu's request has priority 5. Maya is still
  thirsty because the tea hasn't arrived yet.Fig.6: Stage 4Next, Maya receives the demand for milk and responds by sending the
  milk back to the milk manager. Maya also resends its original
request for milk, with priority 2, to the milk manager.
Then Maya receives tea. Maya
  continues to be thirsty because she has only one of the two
  beverages that she needs to drink. Liu has coffee, but she remains
  thirsty because milk is in the channel from Maya to the milk manager.Fig.7: Stage 5In stage 6, the milk manager has received Maya's request for milk with
  priority 2 and received milk. So, the milk manager sends the milk to
  respond to the highest priority request in the priority queue; this
  request is from Liu. The milk manager puts Maya's request into the
  queue of pending requests.Fig.8: Stage 6In stage 7, Liu has received both milk and coffee, and so she is
drinking. Maya is still thirsty, holding tea, while Maya's request for
milk is in the queue of pending requests for milk.Fig.9: Stage 7How Priorities ChangeIf priorities don't change then the agents with high priorities
may continue to go rapidly through the tranquil, thirsty, drinking
cycle while other agents remain thirsty for ever.One way to assign priorities is as follows.
Associated with each request is a timestamp which is 
the time read from the requestor's local clock at the instant at which
the request is made. A request's timestamp does not change after the
request is created.A request's priority is a pair (timestamp,
requestor's id), with priorities compared lexicographically, and lower
values having higher priority. So, requests made earlier have higher
priority than requests made later. The requestor's id is used to break
ties.What are the requirement's of agents' local
clocks that ensure that all thirsty philosophers drink eventually?A Minimum Requirement on ClocksAn agent's clock must tick forwardbetween successive requests
by the agent. If the agent's clock remained stuck at the same
value for ever then the agent using that clock may get the highest
priority for ever, and go through tranquil, thirsty, drinking cycles
infinitely which makes other agents remain tranquil for ever.Assume that each clock reading is an integer: for example, the number of
picoseconds since January 1, 1900. (NTP units are \(2^{-32}\) of a
second.) The clock ticks forward
between successive requests by the same philosopher because the
philosopher remains in tranquil state for at least \(\gamma\) units of
time where \(\gamma\) is a positive constant. Treat each agent's clock
tick as an event.Local clocks of agents may drift apart, but let's assume that the
magnitude of the difference between clock readings of different agents
is bounded.Even if the
philosopher makes a new request when its clock ticks forward by one
unit, eventually the timestamp of a request from that philosopher will
exceed \(T\) for any value of \(T\). This ensures that while a
philosopher is thirsty, another philosopher cannot overtake it for
ever.Proof of CorrectnessThe proof of safety --- multiple philosophers don't hold the same
beverage at the same time --- is straightforward. It follows
because a token is at exactly one agent or in exactly one channel.Let's prove that a thirsty philosopher \(v\) with a request with timestamp
\(T\) drinks eventually. Let \(t[i]\) be the reading of philosopher
\(i\)'s clock.Each agent's clock reading eventually exceeds \(T\)We observe that each agent's clock ticks forward:\(
\forall i, \tau: (t[i] = \tau) \leadsto (t[i] \geq \tau + 1)
\)Therefore, each \(i\)'s clock eventually reads a
value greater than \(T\), using transitivity of \(\leadsto\).\(
\forall i, \tau: (t[i] = \tau) \leadsto (t[i] > T)
\)From the previous formula, and taking disjunction over all values of
\(\tau\):\(
\forall i:  \quad true \leadsto (t[i] > T)
\)Because clock readings never go back in time, \(t[i] > T\) is stable,
and therefore:\(
\forall i:  \quad true \leadsto always(t[i] > T)
\)Let \(Q\) be the predicate that all clocks read values that
exceed \(T\):\(Q = \forall i: t[i] > T\).From the above formula and because \(X \leadsto always(Y)\) and \(X
\leadsto always(Y')\) allows us to deduce \(X \leadsto always(Y \vee Y')\)\(
 true \leadsto always(Q)
\)The number of pending requests with timestamps
less than \(T\) decreasesFrom the above formula:\(
v.thirsty \; \leadsto \; (v.drinking \wedge Q) \vee (v.thirsty \wedge Q)
\)Next we will prove that if \(v\) is thirsty and all clocks read values
that exceed \(T\), then \(v\) will drink eventually:\(
 (v.thirsty \wedge Q) \leadsto v.drinking
\)Let \(req\) be the number of pending requests with timestamps less than
\(T\). We will prove\(
(v.thirsty \wedge Q \wedge (req = k) \; \leadsto \;
v.drinking \vee (v.thirsty \wedge Q \wedge (req < k))
\)This proof is straightforward: The request with the lowest timestamp
gets all the resources it needs.\(
(v.thirsty \wedge Q \wedge (req = 0) \; \leadsto \;
v.drinking 
\)The result follows using the rules
for variant functions.ReviewIn the algorithm, each manager of a resource maintains apriorityqueue of requests for the resource. Is the algorithm
  correct if managers maintain first-come-first-served queues rather
  than priority queues?Show that the proposed variant function is correct: show that it
  does not increase while an agent remains thirsty, and show that it
  decreases eventually.K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology



Figure Link: https://kmchandy.github.io/DistributedCollaboration/DrinkingPhilosophers/Slide2.jpg

Caption: Fig.1: Drinking Philosophers


Drinking Philosophers



Figure Link: https://kmchandy.github.io/DistributedCollaboration/DrinkingPhilosophers/Slide2.jpg

Caption: Fig.1: Drinking Philosophers


A philosopher that enters the tranquil state remains in that state for
at least a constant \(\gamma\) amount of time. So, a philosopher can't
go from drinking to thirsty instantaneously or in an arbitrarily small
amount of time.Fig.1: Drinking PhilosophersA beverage can be held by at most one philosopher. Imagine there's
only one bottle of each beverage in the system, and philosophers send
bottles to each other. One philosopher can drink vodka and cola
while another philosopher drinks gin and tonic. However, one agent cannot drink
vodka and cola while another drinks vodka and orange juice.An AlgorithmThere are many algorithms for this problem; here we discuss one.
Each beverage is an indivisible unique token. A token is
exchanged between agents and amanagerof the token. We assume
that each token has its own manager --- this assumption is merely for
convenience of exposition.Messagesrequest: An agent sends a request for a beverage to a
  manager. A request is a pair \(agent\_id, request\_priority\), the
  id of the requestor and the priority of the request.beverage: A manager sends a beverage to a requesting agent, and agents
  send beverages back to managers. A beverage is uniquely identified
  by its name.demand: A manager sends a demand to an agent for the
  beverage that the manager manages and that the agent holds.Agent ActionsWhen an agent becomes thirsty it sends requests to managers for
    all the beverages that it needs to drink.If a thirsty agent gets a demand to return a beverage to a manager
    then the agent returns the beverage and sends another request for
  the beverage. The priority of this new request is the same as the
  priority of this agent's last request.If a thirsty agent gets all the beverages that it needs to drink
    then it starts drinking.When an agent finishes drinking it returns all the beverages that
    it holds to the managers of the beverages.Manager ActionsA manager has local variable \((hr, hp)\),  which is the id of the agent to which the
  manager has most recently sent 
  the beverage, and the priority of the request made by that
  agent. \(hr\) is an acronym forhandlingrequestor,
  and \(hp\) is an acronym forhandlingpriority.
  If the manager holds the beverage then this variable is empty (\(Null\)).A manager also maintains a priority queue of pending requests ordered by
  priority.The actions of a manager are as follows.If a manager gets a request \((r, p)\) while it holds a beverage then it
    sends the beverage to the requestor \(r\), and sets
    \(hr, hp  = r, p\).If a manager gets a request \((r, p)\) while it does not hold the
    beverage and \(hp > p\) then the manager inserts the request \((r,
    p)\) into the priority queue of pending requests.If a manager gets a request \((r, p)\) while it does not hold the
    beverage and \(hp < p\) then:the manager sends a demand to \(hr\) to return the
      beverage, if the manager has not already sent that demand, andthe manager inserts the request \((r, p)\) into its priority queue of
      pending requests.If a manager gets a beverage and it has no pending requests then
    the manager holds the beverage and sets the handling requestor and
    priority to empty.If a manager gets a beverage and it has pending requests then
    let \((r, p)\) be the request at the head of the priority queue
    (i.e. the request with the highest priority). It sends the
    beverage to requestor \(r\),
    removes \((r, p)\) from the queue of pending requests,
    and sets
     \(hr, hp  = r, p\).ExampleThe example shows a scenario. The system has two agents, Maya and
  Liu both of whom are tranquil in the initial state. There are three beverages: Tea,
  coffee and milk. Initially, these beverages are with their managers.The next diagram, stage 1, shows the state after Maya gets thirsty for tea and milk.
  So she sends requests to the managers of tea and milk. The priority
  of this request is 2. (We will discuss how priorities are obtained
  later.)Fig.3: Stage 1The next diagram, stage 2, shows the situation after the managers of tea and
  milk get requests from Maya, and respond by sending the beverages to
  Maya because there are no pending requests for these beverages. The
  beverages, tea and milk, are in the channel to Maya.Fig.4: Stage 2The stage 3 diagram shows a state after Liu becomes thirsty for
  coffee and milk. So she sends requests for coffee and milk to the
  managers of these beverages. The priority of the request is 5.Maya has received the milk beverage but tea is still in the
  channel. So, Maya remains thirsty.Fig.5: Stage 3The stage 4 diagram shows a state in which the coffee manager
  receives Liu's request and sends
  coffee to Liu because there are no pending requests for coffee. When
  the milk manager gets Liu's request, the manager puts the request on
  the priority queue of pending requests. At this point the queue has
  only Liu's request.
  The
  milk manager demands milk back from Maya because her request has
  priority 2 whereas Liu's request has priority 5. Maya is still
  thirsty because the tea hasn't arrived yet.Fig.6: Stage 4Next, Maya receives the demand for milk and responds by sending the
  milk back to the milk manager. Maya also resends its original
request for milk, with priority 2, to the milk manager.
Then Maya receives tea. Maya
  continues to be thirsty because she has only one of the two
  beverages that she needs to drink. Liu has coffee, but she remains
  thirsty because milk is in the channel from Maya to the milk manager.Fig.7: Stage 5In stage 6, the milk manager has received Maya's request for milk with
  priority 2 and received milk. So, the milk manager sends the milk to
  respond to the highest priority request in the priority queue; this
  request is from Liu. The milk manager puts Maya's request into the
  queue of pending requests.Fig.8: Stage 6In stage 7, Liu has received both milk and coffee, and so she is
drinking. Maya is still thirsty, holding tea, while Maya's request for
milk is in the queue of pending requests for milk.Fig.9: Stage 7How Priorities ChangeIf priorities don't change then the agents with high priorities
may continue to go rapidly through the tranquil, thirsty, drinking
cycle while other agents remain thirsty for ever.One way to assign priorities is as follows.
Associated with each request is a timestamp which is 
the time read from the requestor's local clock at the instant at which
the request is made. A request's timestamp does not change after the
request is created.A request's priority is a pair (timestamp,
requestor's id), with priorities compared lexicographically, and lower
values having higher priority. So, requests made earlier have higher
priority than requests made later. The requestor's id is used to break
ties.What are the requirement's of agents' local
clocks that ensure that all thirsty philosophers drink eventually?A Minimum Requirement on ClocksAn agent's clock must tick forwardbetween successive requests
by the agent. If the agent's clock remained stuck at the same
value for ever then the agent using that clock may get the highest
priority for ever, and go through tranquil, thirsty, drinking cycles
infinitely which makes other agents remain tranquil for ever.Assume that each clock reading is an integer: for example, the number of
picoseconds since January 1, 1900. (NTP units are \(2^{-32}\) of a
second.) The clock ticks forward
between successive requests by the same philosopher because the
philosopher remains in tranquil state for at least \(\gamma\) units of
time where \(\gamma\) is a positive constant. Treat each agent's clock
tick as an event.Local clocks of agents may drift apart, but let's assume that the
magnitude of the difference between clock readings of different agents
is bounded.Even if the
philosopher makes a new request when its clock ticks forward by one
unit, eventually the timestamp of a request from that philosopher will
exceed \(T\) for any value of \(T\). This ensures that while a
philosopher is thirsty, another philosopher cannot overtake it for
ever.Proof of CorrectnessThe proof of safety --- multiple philosophers don't hold the same
beverage at the same time --- is straightforward. It follows
because a token is at exactly one agent or in exactly one channel.Let's prove that a thirsty philosopher \(v\) with a request with timestamp
\(T\) drinks eventually. Let \(t[i]\) be the reading of philosopher
\(i\)'s clock.Each agent's clock reading eventually exceeds \(T\)We observe that each agent's clock ticks forward:\(
\forall i, \tau: (t[i] = \tau) \leadsto (t[i] \geq \tau + 1)
\)Therefore, each \(i\)'s clock eventually reads a
value greater than \(T\), using transitivity of \(\leadsto\).\(
\forall i, \tau: (t[i] = \tau) \leadsto (t[i] > T)
\)From the previous formula, and taking disjunction over all values of
\(\tau\):\(
\forall i:  \quad true \leadsto (t[i] > T)
\)Because clock readings never go back in time, \(t[i] > T\) is stable,
and therefore:\(
\forall i:  \quad true \leadsto always(t[i] > T)
\)Let \(Q\) be the predicate that all clocks read values that
exceed \(T\):\(Q = \forall i: t[i] > T\).From the above formula and because \(X \leadsto always(Y)\) and \(X
\leadsto always(Y')\) allows us to deduce \(X \leadsto always(Y \vee Y')\)\(
 true \leadsto always(Q)
\)The number of pending requests with timestamps
less than \(T\) decreasesFrom the above formula:\(
v.thirsty \; \leadsto \; (v.drinking \wedge Q) \vee (v.thirsty \wedge Q)
\)Next we will prove that if \(v\) is thirsty and all clocks read values
that exceed \(T\), then \(v\) will drink eventually:\(
 (v.thirsty \wedge Q) \leadsto v.drinking
\)Let \(req\) be the number of pending requests with timestamps less than
\(T\). We will prove\(
(v.thirsty \wedge Q \wedge (req = k) \; \leadsto \;
v.drinking \vee (v.thirsty \wedge Q \wedge (req < k))
\)This proof is straightforward: The request with the lowest timestamp
gets all the resources it needs.\(
(v.thirsty \wedge Q \wedge (req = 0) \; \leadsto \;
v.drinking 
\)The result follows using the rules
for variant functions.ReviewIn the algorithm, each manager of a resource maintains apriorityqueue of requests for the resource. Is the algorithm
  correct if managers maintain first-come-first-served queues rather
  than priority queues?Show that the proposed variant function is correct: show that it
  does not increase while an agent remains thirsty, and show that it
  decreases eventually.K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology



Figure Link: https://kmchandy.github.io/DistributedCollaboration/DrinkingPhilosophers/Slide2.jpg

Caption: Fig.1: Drinking Philosophers


A beverage can be held by at most one philosopher. Imagine there's
only one bottle of each beverage in the system, and philosophers send
bottles to each other. One philosopher can drink vodka and cola
while another philosopher drinks gin and tonic. However, one agent cannot drink
vodka and cola while another drinks vodka and orange juice.An AlgorithmThere are many algorithms for this problem; here we discuss one.
Each beverage is an indivisible unique token. A token is
exchanged between agents and amanagerof the token. We assume
that each token has its own manager --- this assumption is merely for
convenience of exposition.Messagesrequest: An agent sends a request for a beverage to a
  manager. A request is a pair \(agent\_id, request\_priority\), the
  id of the requestor and the priority of the request.beverage: A manager sends a beverage to a requesting agent, and agents
  send beverages back to managers. A beverage is uniquely identified
  by its name.demand: A manager sends a demand to an agent for the
  beverage that the manager manages and that the agent holds.Agent ActionsWhen an agent becomes thirsty it sends requests to managers for
    all the beverages that it needs to drink.If a thirsty agent gets a demand to return a beverage to a manager
    then the agent returns the beverage and sends another request for
  the beverage. The priority of this new request is the same as the
  priority of this agent's last request.If a thirsty agent gets all the beverages that it needs to drink
    then it starts drinking.When an agent finishes drinking it returns all the beverages that
    it holds to the managers of the beverages.Manager ActionsA manager has local variable \((hr, hp)\),  which is the id of the agent to which the
  manager has most recently sent 
  the beverage, and the priority of the request made by that
  agent. \(hr\) is an acronym forhandlingrequestor,
  and \(hp\) is an acronym forhandlingpriority.
  If the manager holds the beverage then this variable is empty (\(Null\)).A manager also maintains a priority queue of pending requests ordered by
  priority.The actions of a manager are as follows.If a manager gets a request \((r, p)\) while it holds a beverage then it
    sends the beverage to the requestor \(r\), and sets
    \(hr, hp  = r, p\).If a manager gets a request \((r, p)\) while it does not hold the
    beverage and \(hp > p\) then the manager inserts the request \((r,
    p)\) into the priority queue of pending requests.If a manager gets a request \((r, p)\) while it does not hold the
    beverage and \(hp < p\) then:the manager sends a demand to \(hr\) to return the
      beverage, if the manager has not already sent that demand, andthe manager inserts the request \((r, p)\) into its priority queue of
      pending requests.If a manager gets a beverage and it has no pending requests then
    the manager holds the beverage and sets the handling requestor and
    priority to empty.If a manager gets a beverage and it has pending requests then
    let \((r, p)\) be the request at the head of the priority queue
    (i.e. the request with the highest priority). It sends the
    beverage to requestor \(r\),
    removes \((r, p)\) from the queue of pending requests,
    and sets
     \(hr, hp  = r, p\).ExampleThe example shows a scenario. The system has two agents, Maya and
  Liu both of whom are tranquil in the initial state. There are three beverages: Tea,
  coffee and milk. Initially, these beverages are with their managers.The next diagram, stage 1, shows the state after Maya gets thirsty for tea and milk.
  So she sends requests to the managers of tea and milk. The priority
  of this request is 2. (We will discuss how priorities are obtained
  later.)Fig.3: Stage 1The next diagram, stage 2, shows the situation after the managers of tea and
  milk get requests from Maya, and respond by sending the beverages to
  Maya because there are no pending requests for these beverages. The
  beverages, tea and milk, are in the channel to Maya.Fig.4: Stage 2The stage 3 diagram shows a state after Liu becomes thirsty for
  coffee and milk. So she sends requests for coffee and milk to the
  managers of these beverages. The priority of the request is 5.Maya has received the milk beverage but tea is still in the
  channel. So, Maya remains thirsty.Fig.5: Stage 3The stage 4 diagram shows a state in which the coffee manager
  receives Liu's request and sends
  coffee to Liu because there are no pending requests for coffee. When
  the milk manager gets Liu's request, the manager puts the request on
  the priority queue of pending requests. At this point the queue has
  only Liu's request.
  The
  milk manager demands milk back from Maya because her request has
  priority 2 whereas Liu's request has priority 5. Maya is still
  thirsty because the tea hasn't arrived yet.Fig.6: Stage 4Next, Maya receives the demand for milk and responds by sending the
  milk back to the milk manager. Maya also resends its original
request for milk, with priority 2, to the milk manager.
Then Maya receives tea. Maya
  continues to be thirsty because she has only one of the two
  beverages that she needs to drink. Liu has coffee, but she remains
  thirsty because milk is in the channel from Maya to the milk manager.Fig.7: Stage 5In stage 6, the milk manager has received Maya's request for milk with
  priority 2 and received milk. So, the milk manager sends the milk to
  respond to the highest priority request in the priority queue; this
  request is from Liu. The milk manager puts Maya's request into the
  queue of pending requests.Fig.8: Stage 6In stage 7, Liu has received both milk and coffee, and so she is
drinking. Maya is still thirsty, holding tea, while Maya's request for
milk is in the queue of pending requests for milk.Fig.9: Stage 7How Priorities ChangeIf priorities don't change then the agents with high priorities
may continue to go rapidly through the tranquil, thirsty, drinking
cycle while other agents remain thirsty for ever.One way to assign priorities is as follows.
Associated with each request is a timestamp which is 
the time read from the requestor's local clock at the instant at which
the request is made. A request's timestamp does not change after the
request is created.A request's priority is a pair (timestamp,
requestor's id), with priorities compared lexicographically, and lower
values having higher priority. So, requests made earlier have higher
priority than requests made later. The requestor's id is used to break
ties.What are the requirement's of agents' local
clocks that ensure that all thirsty philosophers drink eventually?A Minimum Requirement on ClocksAn agent's clock must tick forwardbetween successive requests
by the agent. If the agent's clock remained stuck at the same
value for ever then the agent using that clock may get the highest
priority for ever, and go through tranquil, thirsty, drinking cycles
infinitely which makes other agents remain tranquil for ever.Assume that each clock reading is an integer: for example, the number of
picoseconds since January 1, 1900. (NTP units are \(2^{-32}\) of a
second.) The clock ticks forward
between successive requests by the same philosopher because the
philosopher remains in tranquil state for at least \(\gamma\) units of
time where \(\gamma\) is a positive constant. Treat each agent's clock
tick as an event.Local clocks of agents may drift apart, but let's assume that the
magnitude of the difference between clock readings of different agents
is bounded.Even if the
philosopher makes a new request when its clock ticks forward by one
unit, eventually the timestamp of a request from that philosopher will
exceed \(T\) for any value of \(T\). This ensures that while a
philosopher is thirsty, another philosopher cannot overtake it for
ever.Proof of CorrectnessThe proof of safety --- multiple philosophers don't hold the same
beverage at the same time --- is straightforward. It follows
because a token is at exactly one agent or in exactly one channel.Let's prove that a thirsty philosopher \(v\) with a request with timestamp
\(T\) drinks eventually. Let \(t[i]\) be the reading of philosopher
\(i\)'s clock.Each agent's clock reading eventually exceeds \(T\)We observe that each agent's clock ticks forward:\(
\forall i, \tau: (t[i] = \tau) \leadsto (t[i] \geq \tau + 1)
\)Therefore, each \(i\)'s clock eventually reads a
value greater than \(T\), using transitivity of \(\leadsto\).\(
\forall i, \tau: (t[i] = \tau) \leadsto (t[i] > T)
\)From the previous formula, and taking disjunction over all values of
\(\tau\):\(
\forall i:  \quad true \leadsto (t[i] > T)
\)Because clock readings never go back in time, \(t[i] > T\) is stable,
and therefore:\(
\forall i:  \quad true \leadsto always(t[i] > T)
\)Let \(Q\) be the predicate that all clocks read values that
exceed \(T\):\(Q = \forall i: t[i] > T\).From the above formula and because \(X \leadsto always(Y)\) and \(X
\leadsto always(Y')\) allows us to deduce \(X \leadsto always(Y \vee Y')\)\(
 true \leadsto always(Q)
\)The number of pending requests with timestamps
less than \(T\) decreasesFrom the above formula:\(
v.thirsty \; \leadsto \; (v.drinking \wedge Q) \vee (v.thirsty \wedge Q)
\)Next we will prove that if \(v\) is thirsty and all clocks read values
that exceed \(T\), then \(v\) will drink eventually:\(
 (v.thirsty \wedge Q) \leadsto v.drinking
\)Let \(req\) be the number of pending requests with timestamps less than
\(T\). We will prove\(
(v.thirsty \wedge Q \wedge (req = k) \; \leadsto \;
v.drinking \vee (v.thirsty \wedge Q \wedge (req < k))
\)This proof is straightforward: The request with the lowest timestamp
gets all the resources it needs.\(
(v.thirsty \wedge Q \wedge (req = 0) \; \leadsto \;
v.drinking 
\)The result follows using the rules
for variant functions.ReviewIn the algorithm, each manager of a resource maintains apriorityqueue of requests for the resource. Is the algorithm
  correct if managers maintain first-come-first-served queues rather
  than priority queues?Show that the proposed variant function is correct: show that it
  does not increase while an agent remains thirsty, and show that it
  decreases eventually.K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology



Figure Link: https://kmchandy.github.io/DistributedCollaboration/DrinkingPhilosophers/Slide3.jpg

Caption: Fig.3: Stage 1


Messages



Figure Link: https://kmchandy.github.io/DistributedCollaboration/DrinkingPhilosophers/Slide3.jpg

Caption: Fig.3: Stage 1


Agent Actions



Figure Link: https://kmchandy.github.io/DistributedCollaboration/DrinkingPhilosophers/Slide3.jpg

Caption: Fig.3: Stage 1


Manager Actions



Figure Link: https://kmchandy.github.io/DistributedCollaboration/DrinkingPhilosophers/Slide3.jpg

Caption: Fig.3: Stage 1


A manager also maintains a priority queue of pending requests ordered by
  priority.The actions of a manager are as follows.If a manager gets a request \((r, p)\) while it holds a beverage then it
    sends the beverage to the requestor \(r\), and sets
    \(hr, hp  = r, p\).If a manager gets a request \((r, p)\) while it does not hold the
    beverage and \(hp > p\) then the manager inserts the request \((r,
    p)\) into the priority queue of pending requests.If a manager gets a request \((r, p)\) while it does not hold the
    beverage and \(hp < p\) then:the manager sends a demand to \(hr\) to return the
      beverage, if the manager has not already sent that demand, andthe manager inserts the request \((r, p)\) into its priority queue of
      pending requests.If a manager gets a beverage and it has no pending requests then
    the manager holds the beverage and sets the handling requestor and
    priority to empty.If a manager gets a beverage and it has pending requests then
    let \((r, p)\) be the request at the head of the priority queue
    (i.e. the request with the highest priority). It sends the
    beverage to requestor \(r\),
    removes \((r, p)\) from the queue of pending requests,
    and sets
     \(hr, hp  = r, p\).ExampleThe example shows a scenario. The system has two agents, Maya and
  Liu both of whom are tranquil in the initial state. There are three beverages: Tea,
  coffee and milk. Initially, these beverages are with their managers.The next diagram, stage 1, shows the state after Maya gets thirsty for tea and milk.
  So she sends requests to the managers of tea and milk. The priority
  of this request is 2. (We will discuss how priorities are obtained
  later.)Fig.3: Stage 1The next diagram, stage 2, shows the situation after the managers of tea and
  milk get requests from Maya, and respond by sending the beverages to
  Maya because there are no pending requests for these beverages. The
  beverages, tea and milk, are in the channel to Maya.Fig.4: Stage 2The stage 3 diagram shows a state after Liu becomes thirsty for
  coffee and milk. So she sends requests for coffee and milk to the
  managers of these beverages. The priority of the request is 5.Maya has received the milk beverage but tea is still in the
  channel. So, Maya remains thirsty.Fig.5: Stage 3The stage 4 diagram shows a state in which the coffee manager
  receives Liu's request and sends
  coffee to Liu because there are no pending requests for coffee. When
  the milk manager gets Liu's request, the manager puts the request on
  the priority queue of pending requests. At this point the queue has
  only Liu's request.
  The
  milk manager demands milk back from Maya because her request has
  priority 2 whereas Liu's request has priority 5. Maya is still
  thirsty because the tea hasn't arrived yet.Fig.6: Stage 4Next, Maya receives the demand for milk and responds by sending the
  milk back to the milk manager. Maya also resends its original
request for milk, with priority 2, to the milk manager.
Then Maya receives tea. Maya
  continues to be thirsty because she has only one of the two
  beverages that she needs to drink. Liu has coffee, but she remains
  thirsty because milk is in the channel from Maya to the milk manager.Fig.7: Stage 5In stage 6, the milk manager has received Maya's request for milk with
  priority 2 and received milk. So, the milk manager sends the milk to
  respond to the highest priority request in the priority queue; this
  request is from Liu. The milk manager puts Maya's request into the
  queue of pending requests.Fig.8: Stage 6In stage 7, Liu has received both milk and coffee, and so she is
drinking. Maya is still thirsty, holding tea, while Maya's request for
milk is in the queue of pending requests for milk.Fig.9: Stage 7How Priorities ChangeIf priorities don't change then the agents with high priorities
may continue to go rapidly through the tranquil, thirsty, drinking
cycle while other agents remain thirsty for ever.One way to assign priorities is as follows.
Associated with each request is a timestamp which is 
the time read from the requestor's local clock at the instant at which
the request is made. A request's timestamp does not change after the
request is created.A request's priority is a pair (timestamp,
requestor's id), with priorities compared lexicographically, and lower
values having higher priority. So, requests made earlier have higher
priority than requests made later. The requestor's id is used to break
ties.What are the requirement's of agents' local
clocks that ensure that all thirsty philosophers drink eventually?A Minimum Requirement on ClocksAn agent's clock must tick forwardbetween successive requests
by the agent. If the agent's clock remained stuck at the same
value for ever then the agent using that clock may get the highest
priority for ever, and go through tranquil, thirsty, drinking cycles
infinitely which makes other agents remain tranquil for ever.Assume that each clock reading is an integer: for example, the number of
picoseconds since January 1, 1900. (NTP units are \(2^{-32}\) of a
second.) The clock ticks forward
between successive requests by the same philosopher because the
philosopher remains in tranquil state for at least \(\gamma\) units of
time where \(\gamma\) is a positive constant. Treat each agent's clock
tick as an event.Local clocks of agents may drift apart, but let's assume that the
magnitude of the difference between clock readings of different agents
is bounded.Even if the
philosopher makes a new request when its clock ticks forward by one
unit, eventually the timestamp of a request from that philosopher will
exceed \(T\) for any value of \(T\). This ensures that while a
philosopher is thirsty, another philosopher cannot overtake it for
ever.Proof of CorrectnessThe proof of safety --- multiple philosophers don't hold the same
beverage at the same time --- is straightforward. It follows
because a token is at exactly one agent or in exactly one channel.Let's prove that a thirsty philosopher \(v\) with a request with timestamp
\(T\) drinks eventually. Let \(t[i]\) be the reading of philosopher
\(i\)'s clock.Each agent's clock reading eventually exceeds \(T\)We observe that each agent's clock ticks forward:\(
\forall i, \tau: (t[i] = \tau) \leadsto (t[i] \geq \tau + 1)
\)Therefore, each \(i\)'s clock eventually reads a
value greater than \(T\), using transitivity of \(\leadsto\).\(
\forall i, \tau: (t[i] = \tau) \leadsto (t[i] > T)
\)From the previous formula, and taking disjunction over all values of
\(\tau\):\(
\forall i:  \quad true \leadsto (t[i] > T)
\)Because clock readings never go back in time, \(t[i] > T\) is stable,
and therefore:\(
\forall i:  \quad true \leadsto always(t[i] > T)
\)Let \(Q\) be the predicate that all clocks read values that
exceed \(T\):\(Q = \forall i: t[i] > T\).From the above formula and because \(X \leadsto always(Y)\) and \(X
\leadsto always(Y')\) allows us to deduce \(X \leadsto always(Y \vee Y')\)\(
 true \leadsto always(Q)
\)The number of pending requests with timestamps
less than \(T\) decreasesFrom the above formula:\(
v.thirsty \; \leadsto \; (v.drinking \wedge Q) \vee (v.thirsty \wedge Q)
\)Next we will prove that if \(v\) is thirsty and all clocks read values
that exceed \(T\), then \(v\) will drink eventually:\(
 (v.thirsty \wedge Q) \leadsto v.drinking
\)Let \(req\) be the number of pending requests with timestamps less than
\(T\). We will prove\(
(v.thirsty \wedge Q \wedge (req = k) \; \leadsto \;
v.drinking \vee (v.thirsty \wedge Q \wedge (req < k))
\)This proof is straightforward: The request with the lowest timestamp
gets all the resources it needs.\(
(v.thirsty \wedge Q \wedge (req = 0) \; \leadsto \;
v.drinking 
\)The result follows using the rules
for variant functions.ReviewIn the algorithm, each manager of a resource maintains apriorityqueue of requests for the resource. Is the algorithm
  correct if managers maintain first-come-first-served queues rather
  than priority queues?Show that the proposed variant function is correct: show that it
  does not increase while an agent remains thirsty, and show that it
  decreases eventually.K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology



Figure Link: https://kmchandy.github.io/DistributedCollaboration/DrinkingPhilosophers/Slide3.jpg

Caption: Fig.3: Stage 1


The actions of a manager are as follows.If a manager gets a request \((r, p)\) while it holds a beverage then it
    sends the beverage to the requestor \(r\), and sets
    \(hr, hp  = r, p\).If a manager gets a request \((r, p)\) while it does not hold the
    beverage and \(hp > p\) then the manager inserts the request \((r,
    p)\) into the priority queue of pending requests.If a manager gets a request \((r, p)\) while it does not hold the
    beverage and \(hp < p\) then:the manager sends a demand to \(hr\) to return the
      beverage, if the manager has not already sent that demand, andthe manager inserts the request \((r, p)\) into its priority queue of
      pending requests.If a manager gets a beverage and it has no pending requests then
    the manager holds the beverage and sets the handling requestor and
    priority to empty.If a manager gets a beverage and it has pending requests then
    let \((r, p)\) be the request at the head of the priority queue
    (i.e. the request with the highest priority). It sends the
    beverage to requestor \(r\),
    removes \((r, p)\) from the queue of pending requests,
    and sets
     \(hr, hp  = r, p\).ExampleThe example shows a scenario. The system has two agents, Maya and
  Liu both of whom are tranquil in the initial state. There are three beverages: Tea,
  coffee and milk. Initially, these beverages are with their managers.The next diagram, stage 1, shows the state after Maya gets thirsty for tea and milk.
  So she sends requests to the managers of tea and milk. The priority
  of this request is 2. (We will discuss how priorities are obtained
  later.)Fig.3: Stage 1The next diagram, stage 2, shows the situation after the managers of tea and
  milk get requests from Maya, and respond by sending the beverages to
  Maya because there are no pending requests for these beverages. The
  beverages, tea and milk, are in the channel to Maya.Fig.4: Stage 2The stage 3 diagram shows a state after Liu becomes thirsty for
  coffee and milk. So she sends requests for coffee and milk to the
  managers of these beverages. The priority of the request is 5.Maya has received the milk beverage but tea is still in the
  channel. So, Maya remains thirsty.Fig.5: Stage 3The stage 4 diagram shows a state in which the coffee manager
  receives Liu's request and sends
  coffee to Liu because there are no pending requests for coffee. When
  the milk manager gets Liu's request, the manager puts the request on
  the priority queue of pending requests. At this point the queue has
  only Liu's request.
  The
  milk manager demands milk back from Maya because her request has
  priority 2 whereas Liu's request has priority 5. Maya is still
  thirsty because the tea hasn't arrived yet.Fig.6: Stage 4Next, Maya receives the demand for milk and responds by sending the
  milk back to the milk manager. Maya also resends its original
request for milk, with priority 2, to the milk manager.
Then Maya receives tea. Maya
  continues to be thirsty because she has only one of the two
  beverages that she needs to drink. Liu has coffee, but she remains
  thirsty because milk is in the channel from Maya to the milk manager.Fig.7: Stage 5In stage 6, the milk manager has received Maya's request for milk with
  priority 2 and received milk. So, the milk manager sends the milk to
  respond to the highest priority request in the priority queue; this
  request is from Liu. The milk manager puts Maya's request into the
  queue of pending requests.Fig.8: Stage 6In stage 7, Liu has received both milk and coffee, and so she is
drinking. Maya is still thirsty, holding tea, while Maya's request for
milk is in the queue of pending requests for milk.Fig.9: Stage 7How Priorities ChangeIf priorities don't change then the agents with high priorities
may continue to go rapidly through the tranquil, thirsty, drinking
cycle while other agents remain thirsty for ever.One way to assign priorities is as follows.
Associated with each request is a timestamp which is 
the time read from the requestor's local clock at the instant at which
the request is made. A request's timestamp does not change after the
request is created.A request's priority is a pair (timestamp,
requestor's id), with priorities compared lexicographically, and lower
values having higher priority. So, requests made earlier have higher
priority than requests made later. The requestor's id is used to break
ties.What are the requirement's of agents' local
clocks that ensure that all thirsty philosophers drink eventually?A Minimum Requirement on ClocksAn agent's clock must tick forwardbetween successive requests
by the agent. If the agent's clock remained stuck at the same
value for ever then the agent using that clock may get the highest
priority for ever, and go through tranquil, thirsty, drinking cycles
infinitely which makes other agents remain tranquil for ever.Assume that each clock reading is an integer: for example, the number of
picoseconds since January 1, 1900. (NTP units are \(2^{-32}\) of a
second.) The clock ticks forward
between successive requests by the same philosopher because the
philosopher remains in tranquil state for at least \(\gamma\) units of
time where \(\gamma\) is a positive constant. Treat each agent's clock
tick as an event.Local clocks of agents may drift apart, but let's assume that the
magnitude of the difference between clock readings of different agents
is bounded.Even if the
philosopher makes a new request when its clock ticks forward by one
unit, eventually the timestamp of a request from that philosopher will
exceed \(T\) for any value of \(T\). This ensures that while a
philosopher is thirsty, another philosopher cannot overtake it for
ever.Proof of CorrectnessThe proof of safety --- multiple philosophers don't hold the same
beverage at the same time --- is straightforward. It follows
because a token is at exactly one agent or in exactly one channel.Let's prove that a thirsty philosopher \(v\) with a request with timestamp
\(T\) drinks eventually. Let \(t[i]\) be the reading of philosopher
\(i\)'s clock.Each agent's clock reading eventually exceeds \(T\)We observe that each agent's clock ticks forward:\(
\forall i, \tau: (t[i] = \tau) \leadsto (t[i] \geq \tau + 1)
\)Therefore, each \(i\)'s clock eventually reads a
value greater than \(T\), using transitivity of \(\leadsto\).\(
\forall i, \tau: (t[i] = \tau) \leadsto (t[i] > T)
\)From the previous formula, and taking disjunction over all values of
\(\tau\):\(
\forall i:  \quad true \leadsto (t[i] > T)
\)Because clock readings never go back in time, \(t[i] > T\) is stable,
and therefore:\(
\forall i:  \quad true \leadsto always(t[i] > T)
\)Let \(Q\) be the predicate that all clocks read values that
exceed \(T\):\(Q = \forall i: t[i] > T\).From the above formula and because \(X \leadsto always(Y)\) and \(X
\leadsto always(Y')\) allows us to deduce \(X \leadsto always(Y \vee Y')\)\(
 true \leadsto always(Q)
\)The number of pending requests with timestamps
less than \(T\) decreasesFrom the above formula:\(
v.thirsty \; \leadsto \; (v.drinking \wedge Q) \vee (v.thirsty \wedge Q)
\)Next we will prove that if \(v\) is thirsty and all clocks read values
that exceed \(T\), then \(v\) will drink eventually:\(
 (v.thirsty \wedge Q) \leadsto v.drinking
\)Let \(req\) be the number of pending requests with timestamps less than
\(T\). We will prove\(
(v.thirsty \wedge Q \wedge (req = k) \; \leadsto \;
v.drinking \vee (v.thirsty \wedge Q \wedge (req < k))
\)This proof is straightforward: The request with the lowest timestamp
gets all the resources it needs.\(
(v.thirsty \wedge Q \wedge (req = 0) \; \leadsto \;
v.drinking 
\)The result follows using the rules
for variant functions.ReviewIn the algorithm, each manager of a resource maintains apriorityqueue of requests for the resource. Is the algorithm
  correct if managers maintain first-come-first-served queues rather
  than priority queues?Show that the proposed variant function is correct: show that it
  does not increase while an agent remains thirsty, and show that it
  decreases eventually.K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology



Figure Link: https://kmchandy.github.io/DistributedCollaboration/DrinkingPhilosophers/Slide3.jpg

Caption: Fig.3: Stage 1


The next diagram, stage 1, shows the state after Maya gets thirsty for tea and milk.
  So she sends requests to the managers of tea and milk. The priority
  of this request is 2. (We will discuss how priorities are obtained
  later.)Fig.3: Stage 1The next diagram, stage 2, shows the situation after the managers of tea and
  milk get requests from Maya, and respond by sending the beverages to
  Maya because there are no pending requests for these beverages. The
  beverages, tea and milk, are in the channel to Maya.Fig.4: Stage 2The stage 3 diagram shows a state after Liu becomes thirsty for
  coffee and milk. So she sends requests for coffee and milk to the
  managers of these beverages. The priority of the request is 5.Maya has received the milk beverage but tea is still in the
  channel. So, Maya remains thirsty.Fig.5: Stage 3The stage 4 diagram shows a state in which the coffee manager
  receives Liu's request and sends
  coffee to Liu because there are no pending requests for coffee. When
  the milk manager gets Liu's request, the manager puts the request on
  the priority queue of pending requests. At this point the queue has
  only Liu's request.
  The
  milk manager demands milk back from Maya because her request has
  priority 2 whereas Liu's request has priority 5. Maya is still
  thirsty because the tea hasn't arrived yet.Fig.6: Stage 4Next, Maya receives the demand for milk and responds by sending the
  milk back to the milk manager. Maya also resends its original
request for milk, with priority 2, to the milk manager.
Then Maya receives tea. Maya
  continues to be thirsty because she has only one of the two
  beverages that she needs to drink. Liu has coffee, but she remains
  thirsty because milk is in the channel from Maya to the milk manager.Fig.7: Stage 5In stage 6, the milk manager has received Maya's request for milk with
  priority 2 and received milk. So, the milk manager sends the milk to
  respond to the highest priority request in the priority queue; this
  request is from Liu. The milk manager puts Maya's request into the
  queue of pending requests.Fig.8: Stage 6In stage 7, Liu has received both milk and coffee, and so she is
drinking. Maya is still thirsty, holding tea, while Maya's request for
milk is in the queue of pending requests for milk.Fig.9: Stage 7How Priorities ChangeIf priorities don't change then the agents with high priorities
may continue to go rapidly through the tranquil, thirsty, drinking
cycle while other agents remain thirsty for ever.One way to assign priorities is as follows.
Associated with each request is a timestamp which is 
the time read from the requestor's local clock at the instant at which
the request is made. A request's timestamp does not change after the
request is created.A request's priority is a pair (timestamp,
requestor's id), with priorities compared lexicographically, and lower
values having higher priority. So, requests made earlier have higher
priority than requests made later. The requestor's id is used to break
ties.What are the requirement's of agents' local
clocks that ensure that all thirsty philosophers drink eventually?A Minimum Requirement on ClocksAn agent's clock must tick forwardbetween successive requests
by the agent. If the agent's clock remained stuck at the same
value for ever then the agent using that clock may get the highest
priority for ever, and go through tranquil, thirsty, drinking cycles
infinitely which makes other agents remain tranquil for ever.Assume that each clock reading is an integer: for example, the number of
picoseconds since January 1, 1900. (NTP units are \(2^{-32}\) of a
second.) The clock ticks forward
between successive requests by the same philosopher because the
philosopher remains in tranquil state for at least \(\gamma\) units of
time where \(\gamma\) is a positive constant. Treat each agent's clock
tick as an event.Local clocks of agents may drift apart, but let's assume that the
magnitude of the difference between clock readings of different agents
is bounded.Even if the
philosopher makes a new request when its clock ticks forward by one
unit, eventually the timestamp of a request from that philosopher will
exceed \(T\) for any value of \(T\). This ensures that while a
philosopher is thirsty, another philosopher cannot overtake it for
ever.Proof of CorrectnessThe proof of safety --- multiple philosophers don't hold the same
beverage at the same time --- is straightforward. It follows
because a token is at exactly one agent or in exactly one channel.Let's prove that a thirsty philosopher \(v\) with a request with timestamp
\(T\) drinks eventually. Let \(t[i]\) be the reading of philosopher
\(i\)'s clock.Each agent's clock reading eventually exceeds \(T\)We observe that each agent's clock ticks forward:\(
\forall i, \tau: (t[i] = \tau) \leadsto (t[i] \geq \tau + 1)
\)Therefore, each \(i\)'s clock eventually reads a
value greater than \(T\), using transitivity of \(\leadsto\).\(
\forall i, \tau: (t[i] = \tau) \leadsto (t[i] > T)
\)From the previous formula, and taking disjunction over all values of
\(\tau\):\(
\forall i:  \quad true \leadsto (t[i] > T)
\)Because clock readings never go back in time, \(t[i] > T\) is stable,
and therefore:\(
\forall i:  \quad true \leadsto always(t[i] > T)
\)Let \(Q\) be the predicate that all clocks read values that
exceed \(T\):\(Q = \forall i: t[i] > T\).From the above formula and because \(X \leadsto always(Y)\) and \(X
\leadsto always(Y')\) allows us to deduce \(X \leadsto always(Y \vee Y')\)\(
 true \leadsto always(Q)
\)The number of pending requests with timestamps
less than \(T\) decreasesFrom the above formula:\(
v.thirsty \; \leadsto \; (v.drinking \wedge Q) \vee (v.thirsty \wedge Q)
\)Next we will prove that if \(v\) is thirsty and all clocks read values
that exceed \(T\), then \(v\) will drink eventually:\(
 (v.thirsty \wedge Q) \leadsto v.drinking
\)Let \(req\) be the number of pending requests with timestamps less than
\(T\). We will prove\(
(v.thirsty \wedge Q \wedge (req = k) \; \leadsto \;
v.drinking \vee (v.thirsty \wedge Q \wedge (req < k))
\)This proof is straightforward: The request with the lowest timestamp
gets all the resources it needs.\(
(v.thirsty \wedge Q \wedge (req = 0) \; \leadsto \;
v.drinking 
\)The result follows using the rules
for variant functions.ReviewIn the algorithm, each manager of a resource maintains apriorityqueue of requests for the resource. Is the algorithm
  correct if managers maintain first-come-first-served queues rather
  than priority queues?Show that the proposed variant function is correct: show that it
  does not increase while an agent remains thirsty, and show that it
  decreases eventually.K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology



Figure Link: https://kmchandy.github.io/DistributedCollaboration/DrinkingPhilosophers/Slide3.jpg

Caption: Fig.3: Stage 1


Maya has received the milk beverage but tea is still in the
  channel. So, Maya remains thirsty.Fig.5: Stage 3The stage 4 diagram shows a state in which the coffee manager
  receives Liu's request and sends
  coffee to Liu because there are no pending requests for coffee. When
  the milk manager gets Liu's request, the manager puts the request on
  the priority queue of pending requests. At this point the queue has
  only Liu's request.
  The
  milk manager demands milk back from Maya because her request has
  priority 2 whereas Liu's request has priority 5. Maya is still
  thirsty because the tea hasn't arrived yet.Fig.6: Stage 4Next, Maya receives the demand for milk and responds by sending the
  milk back to the milk manager. Maya also resends its original
request for milk, with priority 2, to the milk manager.
Then Maya receives tea. Maya
  continues to be thirsty because she has only one of the two
  beverages that she needs to drink. Liu has coffee, but she remains
  thirsty because milk is in the channel from Maya to the milk manager.Fig.7: Stage 5In stage 6, the milk manager has received Maya's request for milk with
  priority 2 and received milk. So, the milk manager sends the milk to
  respond to the highest priority request in the priority queue; this
  request is from Liu. The milk manager puts Maya's request into the
  queue of pending requests.Fig.8: Stage 6In stage 7, Liu has received both milk and coffee, and so she is
drinking. Maya is still thirsty, holding tea, while Maya's request for
milk is in the queue of pending requests for milk.Fig.9: Stage 7How Priorities ChangeIf priorities don't change then the agents with high priorities
may continue to go rapidly through the tranquil, thirsty, drinking
cycle while other agents remain thirsty for ever.One way to assign priorities is as follows.
Associated with each request is a timestamp which is 
the time read from the requestor's local clock at the instant at which
the request is made. A request's timestamp does not change after the
request is created.A request's priority is a pair (timestamp,
requestor's id), with priorities compared lexicographically, and lower
values having higher priority. So, requests made earlier have higher
priority than requests made later. The requestor's id is used to break
ties.What are the requirement's of agents' local
clocks that ensure that all thirsty philosophers drink eventually?A Minimum Requirement on ClocksAn agent's clock must tick forwardbetween successive requests
by the agent. If the agent's clock remained stuck at the same
value for ever then the agent using that clock may get the highest
priority for ever, and go through tranquil, thirsty, drinking cycles
infinitely which makes other agents remain tranquil for ever.Assume that each clock reading is an integer: for example, the number of
picoseconds since January 1, 1900. (NTP units are \(2^{-32}\) of a
second.) The clock ticks forward
between successive requests by the same philosopher because the
philosopher remains in tranquil state for at least \(\gamma\) units of
time where \(\gamma\) is a positive constant. Treat each agent's clock
tick as an event.Local clocks of agents may drift apart, but let's assume that the
magnitude of the difference between clock readings of different agents
is bounded.Even if the
philosopher makes a new request when its clock ticks forward by one
unit, eventually the timestamp of a request from that philosopher will
exceed \(T\) for any value of \(T\). This ensures that while a
philosopher is thirsty, another philosopher cannot overtake it for
ever.Proof of CorrectnessThe proof of safety --- multiple philosophers don't hold the same
beverage at the same time --- is straightforward. It follows
because a token is at exactly one agent or in exactly one channel.Let's prove that a thirsty philosopher \(v\) with a request with timestamp
\(T\) drinks eventually. Let \(t[i]\) be the reading of philosopher
\(i\)'s clock.Each agent's clock reading eventually exceeds \(T\)We observe that each agent's clock ticks forward:\(
\forall i, \tau: (t[i] = \tau) \leadsto (t[i] \geq \tau + 1)
\)Therefore, each \(i\)'s clock eventually reads a
value greater than \(T\), using transitivity of \(\leadsto\).\(
\forall i, \tau: (t[i] = \tau) \leadsto (t[i] > T)
\)From the previous formula, and taking disjunction over all values of
\(\tau\):\(
\forall i:  \quad true \leadsto (t[i] > T)
\)Because clock readings never go back in time, \(t[i] > T\) is stable,
and therefore:\(
\forall i:  \quad true \leadsto always(t[i] > T)
\)Let \(Q\) be the predicate that all clocks read values that
exceed \(T\):\(Q = \forall i: t[i] > T\).From the above formula and because \(X \leadsto always(Y)\) and \(X
\leadsto always(Y')\) allows us to deduce \(X \leadsto always(Y \vee Y')\)\(
 true \leadsto always(Q)
\)The number of pending requests with timestamps
less than \(T\) decreasesFrom the above formula:\(
v.thirsty \; \leadsto \; (v.drinking \wedge Q) \vee (v.thirsty \wedge Q)
\)Next we will prove that if \(v\) is thirsty and all clocks read values
that exceed \(T\), then \(v\) will drink eventually:\(
 (v.thirsty \wedge Q) \leadsto v.drinking
\)Let \(req\) be the number of pending requests with timestamps less than
\(T\). We will prove\(
(v.thirsty \wedge Q \wedge (req = k) \; \leadsto \;
v.drinking \vee (v.thirsty \wedge Q \wedge (req < k))
\)This proof is straightforward: The request with the lowest timestamp
gets all the resources it needs.\(
(v.thirsty \wedge Q \wedge (req = 0) \; \leadsto \;
v.drinking 
\)The result follows using the rules
for variant functions.ReviewIn the algorithm, each manager of a resource maintains apriorityqueue of requests for the resource. Is the algorithm
  correct if managers maintain first-come-first-served queues rather
  than priority queues?Show that the proposed variant function is correct: show that it
  does not increase while an agent remains thirsty, and show that it
  decreases eventually.K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology



Figure Link: https://kmchandy.github.io/DistributedCollaboration/DrinkingPhilosophers/Slide5.jpg

Caption: Fig.5: Stage 3


One way to assign priorities is as follows.
Associated with each request is a timestamp which is 
the time read from the requestor's local clock at the instant at which
the request is made. A request's timestamp does not change after the
request is created.A request's priority is a pair (timestamp,
requestor's id), with priorities compared lexicographically, and lower
values having higher priority. So, requests made earlier have higher
priority than requests made later. The requestor's id is used to break
ties.What are the requirement's of agents' local
clocks that ensure that all thirsty philosophers drink eventually?A Minimum Requirement on ClocksAn agent's clock must tick forwardbetween successive requests
by the agent. If the agent's clock remained stuck at the same
value for ever then the agent using that clock may get the highest
priority for ever, and go through tranquil, thirsty, drinking cycles
infinitely which makes other agents remain tranquil for ever.Assume that each clock reading is an integer: for example, the number of
picoseconds since January 1, 1900. (NTP units are \(2^{-32}\) of a
second.) The clock ticks forward
between successive requests by the same philosopher because the
philosopher remains in tranquil state for at least \(\gamma\) units of
time where \(\gamma\) is a positive constant. Treat each agent's clock
tick as an event.Local clocks of agents may drift apart, but let's assume that the
magnitude of the difference between clock readings of different agents
is bounded.Even if the
philosopher makes a new request when its clock ticks forward by one
unit, eventually the timestamp of a request from that philosopher will
exceed \(T\) for any value of \(T\). This ensures that while a
philosopher is thirsty, another philosopher cannot overtake it for
ever.Proof of CorrectnessThe proof of safety --- multiple philosophers don't hold the same
beverage at the same time --- is straightforward. It follows
because a token is at exactly one agent or in exactly one channel.Let's prove that a thirsty philosopher \(v\) with a request with timestamp
\(T\) drinks eventually. Let \(t[i]\) be the reading of philosopher
\(i\)'s clock.Each agent's clock reading eventually exceeds \(T\)We observe that each agent's clock ticks forward:\(
\forall i, \tau: (t[i] = \tau) \leadsto (t[i] \geq \tau + 1)
\)Therefore, each \(i\)'s clock eventually reads a
value greater than \(T\), using transitivity of \(\leadsto\).\(
\forall i, \tau: (t[i] = \tau) \leadsto (t[i] > T)
\)From the previous formula, and taking disjunction over all values of
\(\tau\):\(
\forall i:  \quad true \leadsto (t[i] > T)
\)Because clock readings never go back in time, \(t[i] > T\) is stable,
and therefore:\(
\forall i:  \quad true \leadsto always(t[i] > T)
\)Let \(Q\) be the predicate that all clocks read values that
exceed \(T\):\(Q = \forall i: t[i] > T\).From the above formula and because \(X \leadsto always(Y)\) and \(X
\leadsto always(Y')\) allows us to deduce \(X \leadsto always(Y \vee Y')\)\(
 true \leadsto always(Q)
\)The number of pending requests with timestamps
less than \(T\) decreasesFrom the above formula:\(
v.thirsty \; \leadsto \; (v.drinking \wedge Q) \vee (v.thirsty \wedge Q)
\)Next we will prove that if \(v\) is thirsty and all clocks read values
that exceed \(T\), then \(v\) will drink eventually:\(
 (v.thirsty \wedge Q) \leadsto v.drinking
\)Let \(req\) be the number of pending requests with timestamps less than
\(T\). We will prove\(
(v.thirsty \wedge Q \wedge (req = k) \; \leadsto \;
v.drinking \vee (v.thirsty \wedge Q \wedge (req < k))
\)This proof is straightforward: The request with the lowest timestamp
gets all the resources it needs.\(
(v.thirsty \wedge Q \wedge (req = 0) \; \leadsto \;
v.drinking 
\)The result follows using the rules
for variant functions.ReviewIn the algorithm, each manager of a resource maintains apriorityqueue of requests for the resource. Is the algorithm
  correct if managers maintain first-come-first-served queues rather
  than priority queues?Show that the proposed variant function is correct: show that it
  does not increase while an agent remains thirsty, and show that it
  decreases eventually.K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

A request's priority is a pair (timestamp,
requestor's id), with priorities compared lexicographically, and lower
values having higher priority. So, requests made earlier have higher
priority than requests made later. The requestor's id is used to break
ties.What are the requirement's of agents' local
clocks that ensure that all thirsty philosophers drink eventually?A Minimum Requirement on ClocksAn agent's clock must tick forwardbetween successive requests
by the agent. If the agent's clock remained stuck at the same
value for ever then the agent using that clock may get the highest
priority for ever, and go through tranquil, thirsty, drinking cycles
infinitely which makes other agents remain tranquil for ever.Assume that each clock reading is an integer: for example, the number of
picoseconds since January 1, 1900. (NTP units are \(2^{-32}\) of a
second.) The clock ticks forward
between successive requests by the same philosopher because the
philosopher remains in tranquil state for at least \(\gamma\) units of
time where \(\gamma\) is a positive constant. Treat each agent's clock
tick as an event.Local clocks of agents may drift apart, but let's assume that the
magnitude of the difference between clock readings of different agents
is bounded.Even if the
philosopher makes a new request when its clock ticks forward by one
unit, eventually the timestamp of a request from that philosopher will
exceed \(T\) for any value of \(T\). This ensures that while a
philosopher is thirsty, another philosopher cannot overtake it for
ever.Proof of CorrectnessThe proof of safety --- multiple philosophers don't hold the same
beverage at the same time --- is straightforward. It follows
because a token is at exactly one agent or in exactly one channel.Let's prove that a thirsty philosopher \(v\) with a request with timestamp
\(T\) drinks eventually. Let \(t[i]\) be the reading of philosopher
\(i\)'s clock.Each agent's clock reading eventually exceeds \(T\)We observe that each agent's clock ticks forward:\(
\forall i, \tau: (t[i] = \tau) \leadsto (t[i] \geq \tau + 1)
\)Therefore, each \(i\)'s clock eventually reads a
value greater than \(T\), using transitivity of \(\leadsto\).\(
\forall i, \tau: (t[i] = \tau) \leadsto (t[i] > T)
\)From the previous formula, and taking disjunction over all values of
\(\tau\):\(
\forall i:  \quad true \leadsto (t[i] > T)
\)Because clock readings never go back in time, \(t[i] > T\) is stable,
and therefore:\(
\forall i:  \quad true \leadsto always(t[i] > T)
\)Let \(Q\) be the predicate that all clocks read values that
exceed \(T\):\(Q = \forall i: t[i] > T\).From the above formula and because \(X \leadsto always(Y)\) and \(X
\leadsto always(Y')\) allows us to deduce \(X \leadsto always(Y \vee Y')\)\(
 true \leadsto always(Q)
\)The number of pending requests with timestamps
less than \(T\) decreasesFrom the above formula:\(
v.thirsty \; \leadsto \; (v.drinking \wedge Q) \vee (v.thirsty \wedge Q)
\)Next we will prove that if \(v\) is thirsty and all clocks read values
that exceed \(T\), then \(v\) will drink eventually:\(
 (v.thirsty \wedge Q) \leadsto v.drinking
\)Let \(req\) be the number of pending requests with timestamps less than
\(T\). We will prove\(
(v.thirsty \wedge Q \wedge (req = k) \; \leadsto \;
v.drinking \vee (v.thirsty \wedge Q \wedge (req < k))
\)This proof is straightforward: The request with the lowest timestamp
gets all the resources it needs.\(
(v.thirsty \wedge Q \wedge (req = 0) \; \leadsto \;
v.drinking 
\)The result follows using the rules
for variant functions.ReviewIn the algorithm, each manager of a resource maintains apriorityqueue of requests for the resource. Is the algorithm
  correct if managers maintain first-come-first-served queues rather
  than priority queues?Show that the proposed variant function is correct: show that it
  does not increase while an agent remains thirsty, and show that it
  decreases eventually.K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

What are the requirement's of agents' local
clocks that ensure that all thirsty philosophers drink eventually?A Minimum Requirement on ClocksAn agent's clock must tick forwardbetween successive requests
by the agent. If the agent's clock remained stuck at the same
value for ever then the agent using that clock may get the highest
priority for ever, and go through tranquil, thirsty, drinking cycles
infinitely which makes other agents remain tranquil for ever.Assume that each clock reading is an integer: for example, the number of
picoseconds since January 1, 1900. (NTP units are \(2^{-32}\) of a
second.) The clock ticks forward
between successive requests by the same philosopher because the
philosopher remains in tranquil state for at least \(\gamma\) units of
time where \(\gamma\) is a positive constant. Treat each agent's clock
tick as an event.Local clocks of agents may drift apart, but let's assume that the
magnitude of the difference between clock readings of different agents
is bounded.Even if the
philosopher makes a new request when its clock ticks forward by one
unit, eventually the timestamp of a request from that philosopher will
exceed \(T\) for any value of \(T\). This ensures that while a
philosopher is thirsty, another philosopher cannot overtake it for
ever.Proof of CorrectnessThe proof of safety --- multiple philosophers don't hold the same
beverage at the same time --- is straightforward. It follows
because a token is at exactly one agent or in exactly one channel.Let's prove that a thirsty philosopher \(v\) with a request with timestamp
\(T\) drinks eventually. Let \(t[i]\) be the reading of philosopher
\(i\)'s clock.Each agent's clock reading eventually exceeds \(T\)We observe that each agent's clock ticks forward:\(
\forall i, \tau: (t[i] = \tau) \leadsto (t[i] \geq \tau + 1)
\)Therefore, each \(i\)'s clock eventually reads a
value greater than \(T\), using transitivity of \(\leadsto\).\(
\forall i, \tau: (t[i] = \tau) \leadsto (t[i] > T)
\)From the previous formula, and taking disjunction over all values of
\(\tau\):\(
\forall i:  \quad true \leadsto (t[i] > T)
\)Because clock readings never go back in time, \(t[i] > T\) is stable,
and therefore:\(
\forall i:  \quad true \leadsto always(t[i] > T)
\)Let \(Q\) be the predicate that all clocks read values that
exceed \(T\):\(Q = \forall i: t[i] > T\).From the above formula and because \(X \leadsto always(Y)\) and \(X
\leadsto always(Y')\) allows us to deduce \(X \leadsto always(Y \vee Y')\)\(
 true \leadsto always(Q)
\)The number of pending requests with timestamps
less than \(T\) decreasesFrom the above formula:\(
v.thirsty \; \leadsto \; (v.drinking \wedge Q) \vee (v.thirsty \wedge Q)
\)Next we will prove that if \(v\) is thirsty and all clocks read values
that exceed \(T\), then \(v\) will drink eventually:\(
 (v.thirsty \wedge Q) \leadsto v.drinking
\)Let \(req\) be the number of pending requests with timestamps less than
\(T\). We will prove\(
(v.thirsty \wedge Q \wedge (req = k) \; \leadsto \;
v.drinking \vee (v.thirsty \wedge Q \wedge (req < k))
\)This proof is straightforward: The request with the lowest timestamp
gets all the resources it needs.\(
(v.thirsty \wedge Q \wedge (req = 0) \; \leadsto \;
v.drinking 
\)The result follows using the rules
for variant functions.ReviewIn the algorithm, each manager of a resource maintains apriorityqueue of requests for the resource. Is the algorithm
  correct if managers maintain first-come-first-served queues rather
  than priority queues?Show that the proposed variant function is correct: show that it
  does not increase while an agent remains thirsty, and show that it
  decreases eventually.K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

A Minimum Requirement on Clocks

Assume that each clock reading is an integer: for example, the number of
picoseconds since January 1, 1900. (NTP units are \(2^{-32}\) of a
second.) The clock ticks forward
between successive requests by the same philosopher because the
philosopher remains in tranquil state for at least \(\gamma\) units of
time where \(\gamma\) is a positive constant. Treat each agent's clock
tick as an event.Local clocks of agents may drift apart, but let's assume that the
magnitude of the difference between clock readings of different agents
is bounded.Even if the
philosopher makes a new request when its clock ticks forward by one
unit, eventually the timestamp of a request from that philosopher will
exceed \(T\) for any value of \(T\). This ensures that while a
philosopher is thirsty, another philosopher cannot overtake it for
ever.Proof of CorrectnessThe proof of safety --- multiple philosophers don't hold the same
beverage at the same time --- is straightforward. It follows
because a token is at exactly one agent or in exactly one channel.Let's prove that a thirsty philosopher \(v\) with a request with timestamp
\(T\) drinks eventually. Let \(t[i]\) be the reading of philosopher
\(i\)'s clock.Each agent's clock reading eventually exceeds \(T\)We observe that each agent's clock ticks forward:\(
\forall i, \tau: (t[i] = \tau) \leadsto (t[i] \geq \tau + 1)
\)Therefore, each \(i\)'s clock eventually reads a
value greater than \(T\), using transitivity of \(\leadsto\).\(
\forall i, \tau: (t[i] = \tau) \leadsto (t[i] > T)
\)From the previous formula, and taking disjunction over all values of
\(\tau\):\(
\forall i:  \quad true \leadsto (t[i] > T)
\)Because clock readings never go back in time, \(t[i] > T\) is stable,
and therefore:\(
\forall i:  \quad true \leadsto always(t[i] > T)
\)Let \(Q\) be the predicate that all clocks read values that
exceed \(T\):\(Q = \forall i: t[i] > T\).From the above formula and because \(X \leadsto always(Y)\) and \(X
\leadsto always(Y')\) allows us to deduce \(X \leadsto always(Y \vee Y')\)\(
 true \leadsto always(Q)
\)The number of pending requests with timestamps
less than \(T\) decreasesFrom the above formula:\(
v.thirsty \; \leadsto \; (v.drinking \wedge Q) \vee (v.thirsty \wedge Q)
\)Next we will prove that if \(v\) is thirsty and all clocks read values
that exceed \(T\), then \(v\) will drink eventually:\(
 (v.thirsty \wedge Q) \leadsto v.drinking
\)Let \(req\) be the number of pending requests with timestamps less than
\(T\). We will prove\(
(v.thirsty \wedge Q \wedge (req = k) \; \leadsto \;
v.drinking \vee (v.thirsty \wedge Q \wedge (req < k))
\)This proof is straightforward: The request with the lowest timestamp
gets all the resources it needs.\(
(v.thirsty \wedge Q \wedge (req = 0) \; \leadsto \;
v.drinking 
\)The result follows using the rules
for variant functions.ReviewIn the algorithm, each manager of a resource maintains apriorityqueue of requests for the resource. Is the algorithm
  correct if managers maintain first-come-first-served queues rather
  than priority queues?Show that the proposed variant function is correct: show that it
  does not increase while an agent remains thirsty, and show that it
  decreases eventually.K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

Local clocks of agents may drift apart, but let's assume that the
magnitude of the difference between clock readings of different agents
is bounded.Even if the
philosopher makes a new request when its clock ticks forward by one
unit, eventually the timestamp of a request from that philosopher will
exceed \(T\) for any value of \(T\). This ensures that while a
philosopher is thirsty, another philosopher cannot overtake it for
ever.Proof of CorrectnessThe proof of safety --- multiple philosophers don't hold the same
beverage at the same time --- is straightforward. It follows
because a token is at exactly one agent or in exactly one channel.Let's prove that a thirsty philosopher \(v\) with a request with timestamp
\(T\) drinks eventually. Let \(t[i]\) be the reading of philosopher
\(i\)'s clock.Each agent's clock reading eventually exceeds \(T\)We observe that each agent's clock ticks forward:\(
\forall i, \tau: (t[i] = \tau) \leadsto (t[i] \geq \tau + 1)
\)Therefore, each \(i\)'s clock eventually reads a
value greater than \(T\), using transitivity of \(\leadsto\).\(
\forall i, \tau: (t[i] = \tau) \leadsto (t[i] > T)
\)From the previous formula, and taking disjunction over all values of
\(\tau\):\(
\forall i:  \quad true \leadsto (t[i] > T)
\)Because clock readings never go back in time, \(t[i] > T\) is stable,
and therefore:\(
\forall i:  \quad true \leadsto always(t[i] > T)
\)Let \(Q\) be the predicate that all clocks read values that
exceed \(T\):\(Q = \forall i: t[i] > T\).From the above formula and because \(X \leadsto always(Y)\) and \(X
\leadsto always(Y')\) allows us to deduce \(X \leadsto always(Y \vee Y')\)\(
 true \leadsto always(Q)
\)The number of pending requests with timestamps
less than \(T\) decreasesFrom the above formula:\(
v.thirsty \; \leadsto \; (v.drinking \wedge Q) \vee (v.thirsty \wedge Q)
\)Next we will prove that if \(v\) is thirsty and all clocks read values
that exceed \(T\), then \(v\) will drink eventually:\(
 (v.thirsty \wedge Q) \leadsto v.drinking
\)Let \(req\) be the number of pending requests with timestamps less than
\(T\). We will prove\(
(v.thirsty \wedge Q \wedge (req = k) \; \leadsto \;
v.drinking \vee (v.thirsty \wedge Q \wedge (req < k))
\)This proof is straightforward: The request with the lowest timestamp
gets all the resources it needs.\(
(v.thirsty \wedge Q \wedge (req = 0) \; \leadsto \;
v.drinking 
\)The result follows using the rules
for variant functions.ReviewIn the algorithm, each manager of a resource maintains apriorityqueue of requests for the resource. Is the algorithm
  correct if managers maintain first-come-first-served queues rather
  than priority queues?Show that the proposed variant function is correct: show that it
  does not increase while an agent remains thirsty, and show that it
  decreases eventually.K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

Even if the
philosopher makes a new request when its clock ticks forward by one
unit, eventually the timestamp of a request from that philosopher will
exceed \(T\) for any value of \(T\). This ensures that while a
philosopher is thirsty, another philosopher cannot overtake it for
ever.Proof of CorrectnessThe proof of safety --- multiple philosophers don't hold the same
beverage at the same time --- is straightforward. It follows
because a token is at exactly one agent or in exactly one channel.Let's prove that a thirsty philosopher \(v\) with a request with timestamp
\(T\) drinks eventually. Let \(t[i]\) be the reading of philosopher
\(i\)'s clock.Each agent's clock reading eventually exceeds \(T\)We observe that each agent's clock ticks forward:\(
\forall i, \tau: (t[i] = \tau) \leadsto (t[i] \geq \tau + 1)
\)Therefore, each \(i\)'s clock eventually reads a
value greater than \(T\), using transitivity of \(\leadsto\).\(
\forall i, \tau: (t[i] = \tau) \leadsto (t[i] > T)
\)From the previous formula, and taking disjunction over all values of
\(\tau\):\(
\forall i:  \quad true \leadsto (t[i] > T)
\)Because clock readings never go back in time, \(t[i] > T\) is stable,
and therefore:\(
\forall i:  \quad true \leadsto always(t[i] > T)
\)Let \(Q\) be the predicate that all clocks read values that
exceed \(T\):\(Q = \forall i: t[i] > T\).From the above formula and because \(X \leadsto always(Y)\) and \(X
\leadsto always(Y')\) allows us to deduce \(X \leadsto always(Y \vee Y')\)\(
 true \leadsto always(Q)
\)The number of pending requests with timestamps
less than \(T\) decreasesFrom the above formula:\(
v.thirsty \; \leadsto \; (v.drinking \wedge Q) \vee (v.thirsty \wedge Q)
\)Next we will prove that if \(v\) is thirsty and all clocks read values
that exceed \(T\), then \(v\) will drink eventually:\(
 (v.thirsty \wedge Q) \leadsto v.drinking
\)Let \(req\) be the number of pending requests with timestamps less than
\(T\). We will prove\(
(v.thirsty \wedge Q \wedge (req = k) \; \leadsto \;
v.drinking \vee (v.thirsty \wedge Q \wedge (req < k))
\)This proof is straightforward: The request with the lowest timestamp
gets all the resources it needs.\(
(v.thirsty \wedge Q \wedge (req = 0) \; \leadsto \;
v.drinking 
\)The result follows using the rules
for variant functions.ReviewIn the algorithm, each manager of a resource maintains apriorityqueue of requests for the resource. Is the algorithm
  correct if managers maintain first-come-first-served queues rather
  than priority queues?Show that the proposed variant function is correct: show that it
  does not increase while an agent remains thirsty, and show that it
  decreases eventually.K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

Let's prove that a thirsty philosopher \(v\) with a request with timestamp
\(T\) drinks eventually. Let \(t[i]\) be the reading of philosopher
\(i\)'s clock.Each agent's clock reading eventually exceeds \(T\)We observe that each agent's clock ticks forward:\(
\forall i, \tau: (t[i] = \tau) \leadsto (t[i] \geq \tau + 1)
\)Therefore, each \(i\)'s clock eventually reads a
value greater than \(T\), using transitivity of \(\leadsto\).\(
\forall i, \tau: (t[i] = \tau) \leadsto (t[i] > T)
\)From the previous formula, and taking disjunction over all values of
\(\tau\):\(
\forall i:  \quad true \leadsto (t[i] > T)
\)Because clock readings never go back in time, \(t[i] > T\) is stable,
and therefore:\(
\forall i:  \quad true \leadsto always(t[i] > T)
\)Let \(Q\) be the predicate that all clocks read values that
exceed \(T\):\(Q = \forall i: t[i] > T\).From the above formula and because \(X \leadsto always(Y)\) and \(X
\leadsto always(Y')\) allows us to deduce \(X \leadsto always(Y \vee Y')\)\(
 true \leadsto always(Q)
\)The number of pending requests with timestamps
less than \(T\) decreasesFrom the above formula:\(
v.thirsty \; \leadsto \; (v.drinking \wedge Q) \vee (v.thirsty \wedge Q)
\)Next we will prove that if \(v\) is thirsty and all clocks read values
that exceed \(T\), then \(v\) will drink eventually:\(
 (v.thirsty \wedge Q) \leadsto v.drinking
\)Let \(req\) be the number of pending requests with timestamps less than
\(T\). We will prove\(
(v.thirsty \wedge Q \wedge (req = k) \; \leadsto \;
v.drinking \vee (v.thirsty \wedge Q \wedge (req < k))
\)This proof is straightforward: The request with the lowest timestamp
gets all the resources it needs.\(
(v.thirsty \wedge Q \wedge (req = 0) \; \leadsto \;
v.drinking 
\)The result follows using the rules
for variant functions.ReviewIn the algorithm, each manager of a resource maintains apriorityqueue of requests for the resource. Is the algorithm
  correct if managers maintain first-come-first-served queues rather
  than priority queues?Show that the proposed variant function is correct: show that it
  does not increase while an agent remains thirsty, and show that it
  decreases eventually.K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

Each agent's clock reading eventually exceeds \(T\)

\(
\forall i, \tau: (t[i] = \tau) \leadsto (t[i] \geq \tau + 1)
\)Therefore, each \(i\)'s clock eventually reads a
value greater than \(T\), using transitivity of \(\leadsto\).\(
\forall i, \tau: (t[i] = \tau) \leadsto (t[i] > T)
\)From the previous formula, and taking disjunction over all values of
\(\tau\):\(
\forall i:  \quad true \leadsto (t[i] > T)
\)Because clock readings never go back in time, \(t[i] > T\) is stable,
and therefore:\(
\forall i:  \quad true \leadsto always(t[i] > T)
\)Let \(Q\) be the predicate that all clocks read values that
exceed \(T\):\(Q = \forall i: t[i] > T\).From the above formula and because \(X \leadsto always(Y)\) and \(X
\leadsto always(Y')\) allows us to deduce \(X \leadsto always(Y \vee Y')\)\(
 true \leadsto always(Q)
\)The number of pending requests with timestamps
less than \(T\) decreasesFrom the above formula:\(
v.thirsty \; \leadsto \; (v.drinking \wedge Q) \vee (v.thirsty \wedge Q)
\)Next we will prove that if \(v\) is thirsty and all clocks read values
that exceed \(T\), then \(v\) will drink eventually:\(
 (v.thirsty \wedge Q) \leadsto v.drinking
\)Let \(req\) be the number of pending requests with timestamps less than
\(T\). We will prove\(
(v.thirsty \wedge Q \wedge (req = k) \; \leadsto \;
v.drinking \vee (v.thirsty \wedge Q \wedge (req < k))
\)This proof is straightforward: The request with the lowest timestamp
gets all the resources it needs.\(
(v.thirsty \wedge Q \wedge (req = 0) \; \leadsto \;
v.drinking 
\)The result follows using the rules
for variant functions.ReviewIn the algorithm, each manager of a resource maintains apriorityqueue of requests for the resource. Is the algorithm
  correct if managers maintain first-come-first-served queues rather
  than priority queues?Show that the proposed variant function is correct: show that it
  does not increase while an agent remains thirsty, and show that it
  decreases eventually.K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

Therefore, each \(i\)'s clock eventually reads a
value greater than \(T\), using transitivity of \(\leadsto\).\(
\forall i, \tau: (t[i] = \tau) \leadsto (t[i] > T)
\)From the previous formula, and taking disjunction over all values of
\(\tau\):\(
\forall i:  \quad true \leadsto (t[i] > T)
\)Because clock readings never go back in time, \(t[i] > T\) is stable,
and therefore:\(
\forall i:  \quad true \leadsto always(t[i] > T)
\)Let \(Q\) be the predicate that all clocks read values that
exceed \(T\):\(Q = \forall i: t[i] > T\).From the above formula and because \(X \leadsto always(Y)\) and \(X
\leadsto always(Y')\) allows us to deduce \(X \leadsto always(Y \vee Y')\)\(
 true \leadsto always(Q)
\)The number of pending requests with timestamps
less than \(T\) decreasesFrom the above formula:\(
v.thirsty \; \leadsto \; (v.drinking \wedge Q) \vee (v.thirsty \wedge Q)
\)Next we will prove that if \(v\) is thirsty and all clocks read values
that exceed \(T\), then \(v\) will drink eventually:\(
 (v.thirsty \wedge Q) \leadsto v.drinking
\)Let \(req\) be the number of pending requests with timestamps less than
\(T\). We will prove\(
(v.thirsty \wedge Q \wedge (req = k) \; \leadsto \;
v.drinking \vee (v.thirsty \wedge Q \wedge (req < k))
\)This proof is straightforward: The request with the lowest timestamp
gets all the resources it needs.\(
(v.thirsty \wedge Q \wedge (req = 0) \; \leadsto \;
v.drinking 
\)The result follows using the rules
for variant functions.ReviewIn the algorithm, each manager of a resource maintains apriorityqueue of requests for the resource. Is the algorithm
  correct if managers maintain first-come-first-served queues rather
  than priority queues?Show that the proposed variant function is correct: show that it
  does not increase while an agent remains thirsty, and show that it
  decreases eventually.K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

\(
\forall i, \tau: (t[i] = \tau) \leadsto (t[i] > T)
\)From the previous formula, and taking disjunction over all values of
\(\tau\):\(
\forall i:  \quad true \leadsto (t[i] > T)
\)Because clock readings never go back in time, \(t[i] > T\) is stable,
and therefore:\(
\forall i:  \quad true \leadsto always(t[i] > T)
\)Let \(Q\) be the predicate that all clocks read values that
exceed \(T\):\(Q = \forall i: t[i] > T\).From the above formula and because \(X \leadsto always(Y)\) and \(X
\leadsto always(Y')\) allows us to deduce \(X \leadsto always(Y \vee Y')\)\(
 true \leadsto always(Q)
\)The number of pending requests with timestamps
less than \(T\) decreasesFrom the above formula:\(
v.thirsty \; \leadsto \; (v.drinking \wedge Q) \vee (v.thirsty \wedge Q)
\)Next we will prove that if \(v\) is thirsty and all clocks read values
that exceed \(T\), then \(v\) will drink eventually:\(
 (v.thirsty \wedge Q) \leadsto v.drinking
\)Let \(req\) be the number of pending requests with timestamps less than
\(T\). We will prove\(
(v.thirsty \wedge Q \wedge (req = k) \; \leadsto \;
v.drinking \vee (v.thirsty \wedge Q \wedge (req < k))
\)This proof is straightforward: The request with the lowest timestamp
gets all the resources it needs.\(
(v.thirsty \wedge Q \wedge (req = 0) \; \leadsto \;
v.drinking 
\)The result follows using the rules
for variant functions.ReviewIn the algorithm, each manager of a resource maintains apriorityqueue of requests for the resource. Is the algorithm
  correct if managers maintain first-come-first-served queues rather
  than priority queues?Show that the proposed variant function is correct: show that it
  does not increase while an agent remains thirsty, and show that it
  decreases eventually.K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

From the previous formula, and taking disjunction over all values of
\(\tau\):\(
\forall i:  \quad true \leadsto (t[i] > T)
\)Because clock readings never go back in time, \(t[i] > T\) is stable,
and therefore:\(
\forall i:  \quad true \leadsto always(t[i] > T)
\)Let \(Q\) be the predicate that all clocks read values that
exceed \(T\):\(Q = \forall i: t[i] > T\).From the above formula and because \(X \leadsto always(Y)\) and \(X
\leadsto always(Y')\) allows us to deduce \(X \leadsto always(Y \vee Y')\)\(
 true \leadsto always(Q)
\)The number of pending requests with timestamps
less than \(T\) decreasesFrom the above formula:\(
v.thirsty \; \leadsto \; (v.drinking \wedge Q) \vee (v.thirsty \wedge Q)
\)Next we will prove that if \(v\) is thirsty and all clocks read values
that exceed \(T\), then \(v\) will drink eventually:\(
 (v.thirsty \wedge Q) \leadsto v.drinking
\)Let \(req\) be the number of pending requests with timestamps less than
\(T\). We will prove\(
(v.thirsty \wedge Q \wedge (req = k) \; \leadsto \;
v.drinking \vee (v.thirsty \wedge Q \wedge (req < k))
\)This proof is straightforward: The request with the lowest timestamp
gets all the resources it needs.\(
(v.thirsty \wedge Q \wedge (req = 0) \; \leadsto \;
v.drinking 
\)The result follows using the rules
for variant functions.ReviewIn the algorithm, each manager of a resource maintains apriorityqueue of requests for the resource. Is the algorithm
  correct if managers maintain first-come-first-served queues rather
  than priority queues?Show that the proposed variant function is correct: show that it
  does not increase while an agent remains thirsty, and show that it
  decreases eventually.K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

\(
\forall i:  \quad true \leadsto (t[i] > T)
\)Because clock readings never go back in time, \(t[i] > T\) is stable,
and therefore:\(
\forall i:  \quad true \leadsto always(t[i] > T)
\)Let \(Q\) be the predicate that all clocks read values that
exceed \(T\):\(Q = \forall i: t[i] > T\).From the above formula and because \(X \leadsto always(Y)\) and \(X
\leadsto always(Y')\) allows us to deduce \(X \leadsto always(Y \vee Y')\)\(
 true \leadsto always(Q)
\)The number of pending requests with timestamps
less than \(T\) decreasesFrom the above formula:\(
v.thirsty \; \leadsto \; (v.drinking \wedge Q) \vee (v.thirsty \wedge Q)
\)Next we will prove that if \(v\) is thirsty and all clocks read values
that exceed \(T\), then \(v\) will drink eventually:\(
 (v.thirsty \wedge Q) \leadsto v.drinking
\)Let \(req\) be the number of pending requests with timestamps less than
\(T\). We will prove\(
(v.thirsty \wedge Q \wedge (req = k) \; \leadsto \;
v.drinking \vee (v.thirsty \wedge Q \wedge (req < k))
\)This proof is straightforward: The request with the lowest timestamp
gets all the resources it needs.\(
(v.thirsty \wedge Q \wedge (req = 0) \; \leadsto \;
v.drinking 
\)The result follows using the rules
for variant functions.ReviewIn the algorithm, each manager of a resource maintains apriorityqueue of requests for the resource. Is the algorithm
  correct if managers maintain first-come-first-served queues rather
  than priority queues?Show that the proposed variant function is correct: show that it
  does not increase while an agent remains thirsty, and show that it
  decreases eventually.K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

Because clock readings never go back in time, \(t[i] > T\) is stable,
and therefore:\(
\forall i:  \quad true \leadsto always(t[i] > T)
\)Let \(Q\) be the predicate that all clocks read values that
exceed \(T\):\(Q = \forall i: t[i] > T\).From the above formula and because \(X \leadsto always(Y)\) and \(X
\leadsto always(Y')\) allows us to deduce \(X \leadsto always(Y \vee Y')\)\(
 true \leadsto always(Q)
\)The number of pending requests with timestamps
less than \(T\) decreasesFrom the above formula:\(
v.thirsty \; \leadsto \; (v.drinking \wedge Q) \vee (v.thirsty \wedge Q)
\)Next we will prove that if \(v\) is thirsty and all clocks read values
that exceed \(T\), then \(v\) will drink eventually:\(
 (v.thirsty \wedge Q) \leadsto v.drinking
\)Let \(req\) be the number of pending requests with timestamps less than
\(T\). We will prove\(
(v.thirsty \wedge Q \wedge (req = k) \; \leadsto \;
v.drinking \vee (v.thirsty \wedge Q \wedge (req < k))
\)This proof is straightforward: The request with the lowest timestamp
gets all the resources it needs.\(
(v.thirsty \wedge Q \wedge (req = 0) \; \leadsto \;
v.drinking 
\)The result follows using the rules
for variant functions.ReviewIn the algorithm, each manager of a resource maintains apriorityqueue of requests for the resource. Is the algorithm
  correct if managers maintain first-come-first-served queues rather
  than priority queues?Show that the proposed variant function is correct: show that it
  does not increase while an agent remains thirsty, and show that it
  decreases eventually.K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

\(
\forall i:  \quad true \leadsto always(t[i] > T)
\)Let \(Q\) be the predicate that all clocks read values that
exceed \(T\):\(Q = \forall i: t[i] > T\).From the above formula and because \(X \leadsto always(Y)\) and \(X
\leadsto always(Y')\) allows us to deduce \(X \leadsto always(Y \vee Y')\)\(
 true \leadsto always(Q)
\)The number of pending requests with timestamps
less than \(T\) decreasesFrom the above formula:\(
v.thirsty \; \leadsto \; (v.drinking \wedge Q) \vee (v.thirsty \wedge Q)
\)Next we will prove that if \(v\) is thirsty and all clocks read values
that exceed \(T\), then \(v\) will drink eventually:\(
 (v.thirsty \wedge Q) \leadsto v.drinking
\)Let \(req\) be the number of pending requests with timestamps less than
\(T\). We will prove\(
(v.thirsty \wedge Q \wedge (req = k) \; \leadsto \;
v.drinking \vee (v.thirsty \wedge Q \wedge (req < k))
\)This proof is straightforward: The request with the lowest timestamp
gets all the resources it needs.\(
(v.thirsty \wedge Q \wedge (req = 0) \; \leadsto \;
v.drinking 
\)The result follows using the rules
for variant functions.ReviewIn the algorithm, each manager of a resource maintains apriorityqueue of requests for the resource. Is the algorithm
  correct if managers maintain first-come-first-served queues rather
  than priority queues?Show that the proposed variant function is correct: show that it
  does not increase while an agent remains thirsty, and show that it
  decreases eventually.K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

Let \(Q\) be the predicate that all clocks read values that
exceed \(T\):\(Q = \forall i: t[i] > T\).From the above formula and because \(X \leadsto always(Y)\) and \(X
\leadsto always(Y')\) allows us to deduce \(X \leadsto always(Y \vee Y')\)\(
 true \leadsto always(Q)
\)The number of pending requests with timestamps
less than \(T\) decreasesFrom the above formula:\(
v.thirsty \; \leadsto \; (v.drinking \wedge Q) \vee (v.thirsty \wedge Q)
\)Next we will prove that if \(v\) is thirsty and all clocks read values
that exceed \(T\), then \(v\) will drink eventually:\(
 (v.thirsty \wedge Q) \leadsto v.drinking
\)Let \(req\) be the number of pending requests with timestamps less than
\(T\). We will prove\(
(v.thirsty \wedge Q \wedge (req = k) \; \leadsto \;
v.drinking \vee (v.thirsty \wedge Q \wedge (req < k))
\)This proof is straightforward: The request with the lowest timestamp
gets all the resources it needs.\(
(v.thirsty \wedge Q \wedge (req = 0) \; \leadsto \;
v.drinking 
\)The result follows using the rules
for variant functions.ReviewIn the algorithm, each manager of a resource maintains apriorityqueue of requests for the resource. Is the algorithm
  correct if managers maintain first-come-first-served queues rather
  than priority queues?Show that the proposed variant function is correct: show that it
  does not increase while an agent remains thirsty, and show that it
  decreases eventually.K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

\(Q = \forall i: t[i] > T\).From the above formula and because \(X \leadsto always(Y)\) and \(X
\leadsto always(Y')\) allows us to deduce \(X \leadsto always(Y \vee Y')\)\(
 true \leadsto always(Q)
\)The number of pending requests with timestamps
less than \(T\) decreasesFrom the above formula:\(
v.thirsty \; \leadsto \; (v.drinking \wedge Q) \vee (v.thirsty \wedge Q)
\)Next we will prove that if \(v\) is thirsty and all clocks read values
that exceed \(T\), then \(v\) will drink eventually:\(
 (v.thirsty \wedge Q) \leadsto v.drinking
\)Let \(req\) be the number of pending requests with timestamps less than
\(T\). We will prove\(
(v.thirsty \wedge Q \wedge (req = k) \; \leadsto \;
v.drinking \vee (v.thirsty \wedge Q \wedge (req < k))
\)This proof is straightforward: The request with the lowest timestamp
gets all the resources it needs.\(
(v.thirsty \wedge Q \wedge (req = 0) \; \leadsto \;
v.drinking 
\)The result follows using the rules
for variant functions.ReviewIn the algorithm, each manager of a resource maintains apriorityqueue of requests for the resource. Is the algorithm
  correct if managers maintain first-come-first-served queues rather
  than priority queues?Show that the proposed variant function is correct: show that it
  does not increase while an agent remains thirsty, and show that it
  decreases eventually.K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

From the above formula and because \(X \leadsto always(Y)\) and \(X
\leadsto always(Y')\) allows us to deduce \(X \leadsto always(Y \vee Y')\)\(
 true \leadsto always(Q)
\)The number of pending requests with timestamps
less than \(T\) decreasesFrom the above formula:\(
v.thirsty \; \leadsto \; (v.drinking \wedge Q) \vee (v.thirsty \wedge Q)
\)Next we will prove that if \(v\) is thirsty and all clocks read values
that exceed \(T\), then \(v\) will drink eventually:\(
 (v.thirsty \wedge Q) \leadsto v.drinking
\)Let \(req\) be the number of pending requests with timestamps less than
\(T\). We will prove\(
(v.thirsty \wedge Q \wedge (req = k) \; \leadsto \;
v.drinking \vee (v.thirsty \wedge Q \wedge (req < k))
\)This proof is straightforward: The request with the lowest timestamp
gets all the resources it needs.\(
(v.thirsty \wedge Q \wedge (req = 0) \; \leadsto \;
v.drinking 
\)The result follows using the rules
for variant functions.ReviewIn the algorithm, each manager of a resource maintains apriorityqueue of requests for the resource. Is the algorithm
  correct if managers maintain first-come-first-served queues rather
  than priority queues?Show that the proposed variant function is correct: show that it
  does not increase while an agent remains thirsty, and show that it
  decreases eventually.K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

\(
 true \leadsto always(Q)
\)The number of pending requests with timestamps
less than \(T\) decreasesFrom the above formula:\(
v.thirsty \; \leadsto \; (v.drinking \wedge Q) \vee (v.thirsty \wedge Q)
\)Next we will prove that if \(v\) is thirsty and all clocks read values
that exceed \(T\), then \(v\) will drink eventually:\(
 (v.thirsty \wedge Q) \leadsto v.drinking
\)Let \(req\) be the number of pending requests with timestamps less than
\(T\). We will prove\(
(v.thirsty \wedge Q \wedge (req = k) \; \leadsto \;
v.drinking \vee (v.thirsty \wedge Q \wedge (req < k))
\)This proof is straightforward: The request with the lowest timestamp
gets all the resources it needs.\(
(v.thirsty \wedge Q \wedge (req = 0) \; \leadsto \;
v.drinking 
\)The result follows using the rules
for variant functions.ReviewIn the algorithm, each manager of a resource maintains apriorityqueue of requests for the resource. Is the algorithm
  correct if managers maintain first-come-first-served queues rather
  than priority queues?Show that the proposed variant function is correct: show that it
  does not increase while an agent remains thirsty, and show that it
  decreases eventually.K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

The number of pending requests with timestamps
less than \(T\) decreases

From the above formula:\(
v.thirsty \; \leadsto \; (v.drinking \wedge Q) \vee (v.thirsty \wedge Q)
\)Next we will prove that if \(v\) is thirsty and all clocks read values
that exceed \(T\), then \(v\) will drink eventually:\(
 (v.thirsty \wedge Q) \leadsto v.drinking
\)Let \(req\) be the number of pending requests with timestamps less than
\(T\). We will prove\(
(v.thirsty \wedge Q \wedge (req = k) \; \leadsto \;
v.drinking \vee (v.thirsty \wedge Q \wedge (req < k))
\)This proof is straightforward: The request with the lowest timestamp
gets all the resources it needs.\(
(v.thirsty \wedge Q \wedge (req = 0) \; \leadsto \;
v.drinking 
\)The result follows using the rules
for variant functions.ReviewIn the algorithm, each manager of a resource maintains apriorityqueue of requests for the resource. Is the algorithm
  correct if managers maintain first-come-first-served queues rather
  than priority queues?Show that the proposed variant function is correct: show that it
  does not increase while an agent remains thirsty, and show that it
  decreases eventually.K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

\(
v.thirsty \; \leadsto \; (v.drinking \wedge Q) \vee (v.thirsty \wedge Q)
\)Next we will prove that if \(v\) is thirsty and all clocks read values
that exceed \(T\), then \(v\) will drink eventually:\(
 (v.thirsty \wedge Q) \leadsto v.drinking
\)Let \(req\) be the number of pending requests with timestamps less than
\(T\). We will prove\(
(v.thirsty \wedge Q \wedge (req = k) \; \leadsto \;
v.drinking \vee (v.thirsty \wedge Q \wedge (req < k))
\)This proof is straightforward: The request with the lowest timestamp
gets all the resources it needs.\(
(v.thirsty \wedge Q \wedge (req = 0) \; \leadsto \;
v.drinking 
\)The result follows using the rules
for variant functions.ReviewIn the algorithm, each manager of a resource maintains apriorityqueue of requests for the resource. Is the algorithm
  correct if managers maintain first-come-first-served queues rather
  than priority queues?Show that the proposed variant function is correct: show that it
  does not increase while an agent remains thirsty, and show that it
  decreases eventually.K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

Next we will prove that if \(v\) is thirsty and all clocks read values
that exceed \(T\), then \(v\) will drink eventually:\(
 (v.thirsty \wedge Q) \leadsto v.drinking
\)Let \(req\) be the number of pending requests with timestamps less than
\(T\). We will prove\(
(v.thirsty \wedge Q \wedge (req = k) \; \leadsto \;
v.drinking \vee (v.thirsty \wedge Q \wedge (req < k))
\)This proof is straightforward: The request with the lowest timestamp
gets all the resources it needs.\(
(v.thirsty \wedge Q \wedge (req = 0) \; \leadsto \;
v.drinking 
\)The result follows using the rules
for variant functions.ReviewIn the algorithm, each manager of a resource maintains apriorityqueue of requests for the resource. Is the algorithm
  correct if managers maintain first-come-first-served queues rather
  than priority queues?Show that the proposed variant function is correct: show that it
  does not increase while an agent remains thirsty, and show that it
  decreases eventually.K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

\(
 (v.thirsty \wedge Q) \leadsto v.drinking
\)Let \(req\) be the number of pending requests with timestamps less than
\(T\). We will prove\(
(v.thirsty \wedge Q \wedge (req = k) \; \leadsto \;
v.drinking \vee (v.thirsty \wedge Q \wedge (req < k))
\)This proof is straightforward: The request with the lowest timestamp
gets all the resources it needs.\(
(v.thirsty \wedge Q \wedge (req = 0) \; \leadsto \;
v.drinking 
\)The result follows using the rules
for variant functions.ReviewIn the algorithm, each manager of a resource maintains apriorityqueue of requests for the resource. Is the algorithm
  correct if managers maintain first-come-first-served queues rather
  than priority queues?Show that the proposed variant function is correct: show that it
  does not increase while an agent remains thirsty, and show that it
  decreases eventually.K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

Let \(req\) be the number of pending requests with timestamps less than
\(T\). We will prove\(
(v.thirsty \wedge Q \wedge (req = k) \; \leadsto \;
v.drinking \vee (v.thirsty \wedge Q \wedge (req < k))
\)This proof is straightforward: The request with the lowest timestamp
gets all the resources it needs.\(
(v.thirsty \wedge Q \wedge (req = 0) \; \leadsto \;
v.drinking 
\)The result follows using the rules
for variant functions.ReviewIn the algorithm, each manager of a resource maintains apriorityqueue of requests for the resource. Is the algorithm
  correct if managers maintain first-come-first-served queues rather
  than priority queues?Show that the proposed variant function is correct: show that it
  does not increase while an agent remains thirsty, and show that it
  decreases eventually.K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

\(
(v.thirsty \wedge Q \wedge (req = k) \; \leadsto \;
v.drinking \vee (v.thirsty \wedge Q \wedge (req < k))
\)This proof is straightforward: The request with the lowest timestamp
gets all the resources it needs.\(
(v.thirsty \wedge Q \wedge (req = 0) \; \leadsto \;
v.drinking 
\)The result follows using the rules
for variant functions.ReviewIn the algorithm, each manager of a resource maintains apriorityqueue of requests for the resource. Is the algorithm
  correct if managers maintain first-come-first-served queues rather
  than priority queues?Show that the proposed variant function is correct: show that it
  does not increase while an agent remains thirsty, and show that it
  decreases eventually.K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

This proof is straightforward: The request with the lowest timestamp
gets all the resources it needs.\(
(v.thirsty \wedge Q \wedge (req = 0) \; \leadsto \;
v.drinking 
\)The result follows using the rules
for variant functions.ReviewIn the algorithm, each manager of a resource maintains apriorityqueue of requests for the resource. Is the algorithm
  correct if managers maintain first-come-first-served queues rather
  than priority queues?Show that the proposed variant function is correct: show that it
  does not increase while an agent remains thirsty, and show that it
  decreases eventually.K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

\(
(v.thirsty \wedge Q \wedge (req = 0) \; \leadsto \;
v.drinking 
\)The result follows using the rules
for variant functions.ReviewIn the algorithm, each manager of a resource maintains apriorityqueue of requests for the resource. Is the algorithm
  correct if managers maintain first-come-first-served queues rather
  than priority queues?Show that the proposed variant function is correct: show that it
  does not increase while an agent remains thirsty, and show that it
  decreases eventually.K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

The result follows using the rules
for variant functions.ReviewIn the algorithm, each manager of a resource maintains apriorityqueue of requests for the resource. Is the algorithm
  correct if managers maintain first-come-first-served queues rather
  than priority queues?Show that the proposed variant function is correct: show that it
  does not increase while an agent remains thirsty, and show that it
  decreases eventually.K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology

K. Mani Chandy,
      Emeritus Simon Ramo Professor,
      California Institute of Technology
--- END Content from https://kmchandy.github.io/DistributedCollaboration/DrinkingPhilosophers/DrinkingPhilosophers.html ---


